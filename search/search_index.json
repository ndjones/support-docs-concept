{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"NeSI Support Documentation","text":"<p>Technical Documentation for our HPC service.</p>"},{"location":"#quickstart","title":"Quickstart","text":"<p>Creating a NeSI Account</p><p>How to get started with NeSI</p> <p>Getting Started</p><p>Walk throughs</p> <p>SSH Config</p><p>Connect now.</p>"},{"location":"#services","title":"Services","text":"<p>NeSI Account Portal</p><p>Managing your account, project, authorized users etc</p> <p>Connecting to the Clusters</p><p>Setting up your connection to the cluster.</p> <p>Transferring Files</p><p>Globus, scp, jupyter.</p> <p>Submitting Jobs</p><p>How to use our job scheduler, Slurm</p>"},{"location":"#help","title":"Help","text":"<p>Introductory Workshops</p><p>We offer introductory workshops for new HPC users on most weeks of the year.</p> <p>Help desk</p><p>The NeSI help desk can be contacted via email support@nesi.org.nz or from this site.</p> <p>Consultancy service</p><p>Scientific and HPC-focussed computational support across a range of domains.</p>"},{"location":"Glossary/","title":"Glossary","text":""},{"location":"Glossary/#cpus","title":"CPUs:","text":"<pre><code>Slectronic circuitry executes instructions of a computer program.\n</code></pre>"},{"location":"Glossary/#cpus_1","title":"CPU's:","text":"<pre><code>Slectronic circuitry executes instructions of a computer program.\n</code></pre>"},{"location":"Glossary/#cpu","title":"CPU:","text":"<pre><code>Slectronic circuitry executes instructions of a computer program.\n</code></pre>"},{"location":"Glossary/#cuda","title":"CUDA:","text":"<pre><code>CUDA (formerly Compute Unified Device Architecture) is a parallel  computing platform and programming model created by NVIDIA and implemented by the  graphics processing units (GPUs) that they produce. CUDA gives developers access  to the virtual instruction set and memory of the parallel computational elements in CUDA GPUs.\n</code></pre>"},{"location":"Glossary/#hdf","title":"HDF:","text":"<pre><code>HDF (also known as HDF4) is a library and multi-object file format for  storing and managing data between machines.\n</code></pre>"},{"location":"Glossary/#hpcs","title":"HPCs:","text":"<pre><code>Like a regular computer, but larger. Primarily used for heating data centers.\n</code></pre>"},{"location":"Glossary/#hpc","title":"HPC:","text":"<pre><code>Like a regular computer, but larger. Primarily used for heating data centers.\n</code></pre>"},{"location":"Glossary/#mpi","title":"MPI:","text":"<pre><code>A standardised message-passing standard designed to function on parallel computing architectures.\n</code></pre>"},{"location":"Glossary/#nesis","title":"NeSI's:","text":"<pre><code>New Zealand national high performance computing platform.\n</code></pre>"},{"location":"Glossary/#nesi","title":"NeSI:","text":"<pre><code>New Zealand national high performance computing platform.\n</code></pre>"},{"location":"Glossary/#niwas","title":"NIWA's:","text":"<pre><code>Crown Research Institute, conducts research across a broad range of disciplines in the environmental sciences.\n</code></pre>"},{"location":"Glossary/#niwa","title":"NIWA:","text":"<pre><code>Crown Research Institute, conducts research across a broad range of disciplines in the environmental sciences.\n</code></pre>"},{"location":"Glossary/#scp","title":"SCP:","text":"<pre><code>Means of securely transferring files between over an SSH connection.\n</code></pre>"},{"location":"Glossary/#ssh","title":"SSH:","text":"<pre><code>A network communication protocol that enables two computers to communicate\n</code></pre>"},{"location":"Glossary/#vpn","title":"VPN:","text":"<pre><code>Method of extending access to a private network.\n</code></pre>"},{"location":"Glossary/#supercomputers","title":"supercomputers:","text":"<pre><code>Like a regular computer, but larger. Primarily used for heating data centers.\n</code></pre>"},{"location":"Glossary/#supercomputer","title":"supercomputer:","text":"<pre><code>Like a regular computer, but larger. Primarily used for heating data centers.\n</code></pre>"},{"location":"Glossary/#supercomputings","title":"supercomputings:","text":"<pre><code>Like a regular computer, but larger. Primarily used for heating data centers.\n</code></pre>"},{"location":"Glossary/#supercomputing","title":"supercomputing:","text":"<pre><code>Like a regular computer, but larger. Primarily used for heating data centers.\n</code></pre>"},{"location":"Glossary/#abaquss","title":"ABAQUS's:","text":"<pre><code>Finite Element Analysis software for modeling, visualization and best-in-class implicit and explicit dynamics FEA.\n</code></pre>"},{"location":"Glossary/#abaqus","title":"ABAQUS:","text":"<pre><code>Finite Element Analysis software for modeling, visualization and best-in-class implicit and explicit dynamics FEA.\n</code></pre>"},{"location":"Glossary/#abricates","title":"ABRicate's:","text":"<pre><code>Mass screening of contigs for antimicrobial and virulence genes\n</code></pre>"},{"location":"Glossary/#abricate","title":"ABRicate:","text":"<pre><code>Mass screening of contigs for antimicrobial and virulence genes\n</code></pre>"},{"location":"Glossary/#abysss","title":"ABySS's:","text":"<pre><code>Assembly By Short Sequences - a de novo, parallel, paired-end sequence assembler\n</code></pre>"},{"location":"Glossary/#abyss","title":"ABySS:","text":"<pre><code>Assembly By Short Sequences - a de novo, parallel, paired-end sequence assembler\n</code></pre>"},{"location":"Glossary/#actcs","title":"ACTC's:","text":"<pre><code>ACTC converts independent triangles into triangle strips or fans.\n</code></pre>"},{"location":"Glossary/#actc","title":"ACTC:","text":"<pre><code>ACTC converts independent triangles into triangle strips or fans.\n</code></pre>"},{"location":"Glossary/#agats","title":"AGAT's:","text":"<pre><code>Suite of tools to handle gene annotations in any GTF/GFF format.\n</code></pre>"},{"location":"Glossary/#agat","title":"AGAT:","text":"<pre><code>Suite of tools to handle gene annotations in any GTF/GFF format.\n</code></pre>"},{"location":"Glossary/#ages","title":"AGE's:","text":"<pre><code>Alignment of sequences with structural variants.\n</code></pre>"},{"location":"Glossary/#age","title":"AGE:","text":"<pre><code>Alignment of sequences with structural variants.\n</code></pre>"},{"location":"Glossary/#amoss","title":"AMOS's:","text":"<pre><code>Collection of tools for genome assembly\n</code></pre>"},{"location":"Glossary/#amos","title":"AMOS:","text":"<pre><code>Collection of tools for genome assembly\n</code></pre>"},{"location":"Glossary/#amrfinderpluss","title":"AMRFinderPlus's:","text":"<pre><code>NCBI Antimicrobial Resistance Gene Finder Plus\n</code></pre>"},{"location":"Glossary/#amrfinderplus","title":"AMRFinderPlus:","text":"<pre><code>NCBI Antimicrobial Resistance Gene Finder Plus\n</code></pre>"},{"location":"Glossary/#anicalculators","title":"ANIcalculator's:","text":"<pre><code>Calculate the bidirectional average nucleotide identity (gANI) and\n</code></pre>"},{"location":"Glossary/#anicalculator","title":"ANIcalculator:","text":"<pre><code>Calculate the bidirectional average nucleotide identity (gANI) and\n</code></pre>"},{"location":"Glossary/#annovars","title":"ANNOVAR's:","text":"<pre><code>Efficient software tool to utilize update-to-date information to functionally\n</code></pre>"},{"location":"Glossary/#annovar","title":"ANNOVAR:","text":"<pre><code>Efficient software tool to utilize update-to-date information to functionally\n</code></pre>"},{"location":"Glossary/#antlrs","title":"ANTLR's:","text":"<pre><code>ANTLR, ANother Tool for Language Recognition, (formerly PCCTS)  is a language tool that provides a framework for constructing recognizers,  compilers, and translators from grammatical descriptions containing  Java, C#, C++, or Python actions.\n</code></pre>"},{"location":"Glossary/#antlr","title":"ANTLR:","text":"<pre><code>ANTLR, ANother Tool for Language Recognition, (formerly PCCTS)  is a language tool that provides a framework for constructing recognizers,  compilers, and translators from grammatical descriptions containing  Java, C#, C++, or Python actions.\n</code></pre>"},{"location":"Glossary/#antss","title":"ANTS's:","text":"<pre><code>Ants is a versatile, easy to use Python library for developing ancillary applications.                   This package is restricted to the UM user group.\n</code></pre>"},{"location":"Glossary/#ants","title":"ANTS:","text":"<pre><code>Ants is a versatile, easy to use Python library for developing ancillary applications.                   This package is restricted to the UM user group.\n</code></pre>"},{"location":"Glossary/#antss_1","title":"ANTs's:","text":"<pre><code>ANTs extracts information from complex datasets that include imaging. ANTs is useful for managing,\n</code></pre>"},{"location":"Glossary/#ants_1","title":"ANTs:","text":"<pre><code>ANTs extracts information from complex datasets that include imaging. ANTs is useful for managing,\n</code></pre>"},{"location":"Glossary/#aoccs","title":"AOCC's:","text":"<pre><code>AMD Optimized C/C++ &amp; Fortran compilers (AOCC) based on LLVM 13.0\n</code></pre>"},{"location":"Glossary/#aocc","title":"AOCC:","text":"<pre><code>AMD Optimized C/C++ &amp; Fortran compilers (AOCC) based on LLVM 13.0\n</code></pre>"},{"location":"Glossary/#aocl-bliss","title":"AOCL-BLIS's:","text":"<pre><code>Optimized version of BLIS for AMD EPYC family of processors..\n</code></pre>"},{"location":"Glossary/#aocl-blis","title":"AOCL-BLIS:","text":"<pre><code>Optimized version of BLIS for AMD EPYC family of processors..\n</code></pre>"},{"location":"Glossary/#aocl-fftws","title":"AOCL-FFTW's:","text":"<pre><code>Optimized version of FFTW for AMD EPYC family of processors.\n</code></pre>"},{"location":"Glossary/#aocl-fftw","title":"AOCL-FFTW:","text":"<pre><code>Optimized version of FFTW for AMD EPYC family of processors.\n</code></pre>"},{"location":"Glossary/#aocl-scalapacks","title":"AOCL-ScaLAPACK's:","text":"<pre><code>Optimized version of ScaLAPACK for AMD EPYC family of processors.\n</code></pre>"},{"location":"Glossary/#aocl-scalapack","title":"AOCL-ScaLAPACK:","text":"<pre><code>Optimized version of ScaLAPACK for AMD EPYC family of processors.\n</code></pre>"},{"location":"Glossary/#aprs","title":"APR's:","text":"<pre><code>Apache Portable Runtime (APR) libraries.\n</code></pre>"},{"location":"Glossary/#apr","title":"APR:","text":"<pre><code>Apache Portable Runtime (APR) libraries.\n</code></pre>"},{"location":"Glossary/#apr-utils","title":"APR-util's:","text":"<pre><code>Apache Portable Runtime (APR) util libraries.\n</code></pre>"},{"location":"Glossary/#apr-util","title":"APR-util:","text":"<pre><code>Apache Portable Runtime (APR) util libraries.\n</code></pre>"},{"location":"Glossary/#arcsis","title":"ARCSI's:","text":"<pre><code>The Atmospheric and Radiometric Correction of Satellite Imagery (ARCSI) software provides a command line tool for the atmospheric correction of Earth Observation imagery. The aim of ARCSI is to provide as automatic as possible method of retrieving the atmospheric correction parameters and using them to parameterise 6S.Universal Command Line Environment for AWS.\n</code></pre>"},{"location":"Glossary/#arcsi","title":"ARCSI:","text":"<pre><code>The Atmospheric and Radiometric Correction of Satellite Imagery (ARCSI) software provides a command line tool for the atmospheric correction of Earth Observation imagery. The aim of ARCSI is to provide as automatic as possible method of retrieving the atmospheric correction parameters and using them to parameterise 6S.Universal Command Line Environment for AWS.\n</code></pre>"},{"location":"Glossary/#aribas","title":"ARIBA's:","text":"<pre><code>Antimicrobial Resistance Identification By Assembly\n</code></pre>"},{"location":"Glossary/#ariba","title":"ARIBA:","text":"<pre><code>Antimicrobial Resistance Identification By Assembly\n</code></pre>"},{"location":"Glossary/#atks","title":"ATK's:","text":"<pre><code>ATK provides the set of accessibility interfaces that are implemented by other toolkits and applications.\n</code></pre>"},{"location":"Glossary/#atk","title":"ATK:","text":"<pre><code>ATK provides the set of accessibility interfaces that are implemented by other toolkits and applications.\n</code></pre>"},{"location":"Glossary/#augustuss","title":"AUGUSTUS's:","text":"<pre><code>AUGUSTUS is a program that predicts genes in eukaryotic genomic sequences\n</code></pre>"},{"location":"Glossary/#augustus","title":"AUGUSTUS:","text":"<pre><code>AUGUSTUS is a program that predicts genes in eukaryotic genomic sequences\n</code></pre>"},{"location":"Glossary/#adapterremovals","title":"AdapterRemoval's:","text":"<pre><code>Ssearches for and removes remnant adapter sequences\n</code></pre>"},{"location":"Glossary/#adapterremoval","title":"AdapterRemoval:","text":"<pre><code>Ssearches for and removes remnant adapter sequences\n</code></pre>"},{"location":"Glossary/#advisors","title":"Advisor's:","text":"<pre><code>Vectorization Optimization and Thread Prototyping\n</code></pre>"},{"location":"Glossary/#advisor","title":"Advisor:","text":"<pre><code>Vectorization Optimization and Thread Prototyping\n</code></pre>"},{"location":"Glossary/#alphafolds","title":"AlphaFold's:","text":"<pre><code>AlphaFold can predict protein structures with atomic accuracy even where no similar structure is known\n</code></pre>"},{"location":"Glossary/#alphafold","title":"AlphaFold:","text":"<pre><code>AlphaFold can predict protein structures with atomic accuracy even where no similar structure is known\n</code></pre>"},{"location":"Glossary/#alphafold2dbs","title":"AlphaFold2DB's:","text":"<pre><code>AlphaFold2 databases\n</code></pre>"},{"location":"Glossary/#alphafold2db","title":"AlphaFold2DB:","text":"<pre><code>AlphaFold2 databases\n</code></pre>"},{"location":"Glossary/#alwaysintelmkls","title":"AlwaysIntelMKL's:","text":"<pre><code>Overrides the MKL internal utility function mkl_serv_intel_cpu_true\n</code></pre>"},{"location":"Glossary/#alwaysintelmkl","title":"AlwaysIntelMKL:","text":"<pre><code>Overrides the MKL internal utility function mkl_serv_intel_cpu_true\n</code></pre>"},{"location":"Glossary/#ambers","title":"Amber's:","text":"<pre><code>Amber (originally Assisted Model Building with Energy\n</code></pre>"},{"location":"Glossary/#amber","title":"Amber:","text":"<pre><code>Amber (originally Assisted Model Building with Energy\n</code></pre>"},{"location":"Glossary/#anaconda2s","title":"Anaconda2's:","text":"<pre><code>Built to complement the rich, open source Python community, the Anaconda platform provides an enterprise-ready data analytics platform  that empowers companies to adopt a modern open data science analytics architecture.  IMPORTANT: This version of Anaconda Python comes with Intel MKL support to speed up            certain types of mathematical computations, such as linear algebra or FFT.            The module sets             MKL_NUM_THREADS=1             to run MKL on a single thread by default, avoiding accidental oversubscription            of cores. The number of threads can be increased for large problems, please            refer to the Intel MKL documentation for guidance.\n</code></pre>"},{"location":"Glossary/#anaconda2","title":"Anaconda2:","text":"<pre><code>Built to complement the rich, open source Python community, the Anaconda platform provides an enterprise-ready data analytics platform  that empowers companies to adopt a modern open data science analytics architecture.  IMPORTANT: This version of Anaconda Python comes with Intel MKL support to speed up            certain types of mathematical computations, such as linear algebra or FFT.            The module sets             MKL_NUM_THREADS=1             to run MKL on a single thread by default, avoiding accidental oversubscription            of cores. The number of threads can be increased for large problems, please            refer to the Intel MKL documentation for guidance.\n</code></pre>"},{"location":"Glossary/#anaconda3s","title":"Anaconda3's:","text":"<pre><code>Built to complement the rich, open source Python community, the Anaconda platform provides an enterprise-ready data analytics platform  that empowers companies to adopt a modern open data science analytics architecture.\n</code></pre>"},{"location":"Glossary/#anaconda3","title":"Anaconda3:","text":"<pre><code>Built to complement the rich, open source Python community, the Anaconda platform provides an enterprise-ready data analytics platform  that empowers companies to adopt a modern open data science analytics architecture.\n</code></pre>"},{"location":"Glossary/#apptainers","title":"Apptainer's:","text":"<pre><code>Apptainer is a portable application stack packaging and runtime utility.\n</code></pre>"},{"location":"Glossary/#apptainer","title":"Apptainer:","text":"<pre><code>Apptainer is a portable application stack packaging and runtime utility.\n</code></pre>"},{"location":"Glossary/#armadillos","title":"Armadillo's:","text":"<pre><code>C++ linear algebra library (matrix maths) aiming towards a good balance between speed and ease of use.\n</code></pre>"},{"location":"Glossary/#armadillo","title":"Armadillo:","text":"<pre><code>C++ linear algebra library (matrix maths) aiming towards a good balance between speed and ease of use.\n</code></pre>"},{"location":"Glossary/#arrows","title":"Arrow's:","text":"<pre><code>Apache Arrow, a cross-language development platform for in-memory data.\n</code></pre>"},{"location":"Glossary/#arrow","title":"Arrow:","text":"<pre><code>Apache Arrow, a cross-language development platform for in-memory data.\n</code></pre>"},{"location":"Glossary/#aspera-clis","title":"Aspera-CLI's:","text":"<pre><code>IBM Aspera Command-Line Interface (the Aspera CLI) is\n</code></pre>"},{"location":"Glossary/#aspera-cli","title":"Aspera-CLI:","text":"<pre><code>IBM Aspera Command-Line Interface (the Aspera CLI) is\n</code></pre>"},{"location":"Glossary/#augustuss_1","title":"Augustus's:","text":"<pre><code>AUGUSTUS is a program that predicts genes in eukaryotic genomic sequences.\n</code></pre>"},{"location":"Glossary/#augustus_1","title":"Augustus:","text":"<pre><code>AUGUSTUS is a program that predicts genes in eukaryotic genomic sequences.\n</code></pre>"},{"location":"Glossary/#autodock-gpus","title":"AutoDock-GPU's:","text":"<pre><code>OpenCL and Cuda accelerated version of AutoDock. It leverages its embarrasingly\n</code></pre>"},{"location":"Glossary/#autodock-gpu","title":"AutoDock-GPU:","text":"<pre><code>OpenCL and Cuda accelerated version of AutoDock. It leverages its embarrasingly\n</code></pre>"},{"location":"Glossary/#autodock_vinas","title":"AutoDock_Vina's:","text":"<pre><code>AutoDock Vina is an open-source program for doing molecular docking.\n</code></pre>"},{"location":"Glossary/#autodock_vina","title":"AutoDock_Vina:","text":"<pre><code>AutoDock Vina is an open-source program for doing molecular docking.\n</code></pre>"},{"location":"Glossary/#bbmaps","title":"BBMap's:","text":"<pre><code>BBMap short read aligner, and other bioinformatic tools.\n</code></pre>"},{"location":"Glossary/#bbmap","title":"BBMap:","text":"<pre><code>BBMap short read aligner, and other bioinformatic tools.\n</code></pre>"},{"location":"Glossary/#bcftoolss","title":"BCFtools's:","text":"<pre><code>Manipulate variant calls in the Variant Call Format (VCF) and its binary counterpart BCF.\n</code></pre>"},{"location":"Glossary/#bcftools","title":"BCFtools:","text":"<pre><code>Manipulate variant calls in the Variant Call Format (VCF) and its binary counterpart BCF.\n</code></pre>"},{"location":"Glossary/#bcl-converts","title":"BCL-Convert's:","text":"<pre><code>Converts per cycle binary data output by Illumina sequencers containing basecall\n</code></pre>"},{"location":"Glossary/#bcl-convert","title":"BCL-Convert:","text":"<pre><code>Converts per cycle binary data output by Illumina sequencers containing basecall\n</code></pre>"},{"location":"Glossary/#beasts","title":"BEAST's:","text":"<pre><code>Bayesian MCMC phylogenetic analysis of molecular sequences for reconstructing\n</code></pre>"},{"location":"Glossary/#beast","title":"BEAST:","text":"<pre><code>Bayesian MCMC phylogenetic analysis of molecular sequences for reconstructing\n</code></pre>"},{"location":"Glossary/#bedopss","title":"BEDOPS's:","text":"<pre><code>BEDOPS is an open-source command-line toolkit that performs highly\n</code></pre>"},{"location":"Glossary/#bedops","title":"BEDOPS:","text":"<pre><code>BEDOPS is an open-source command-line toolkit that performs highly\n</code></pre>"},{"location":"Glossary/#bedtoolss","title":"BEDTools's:","text":"<pre><code>The BEDTools utilities allow one to address common genomics tasks such as finding feature overlaps\n</code></pre>"},{"location":"Glossary/#bedtools","title":"BEDTools:","text":"<pre><code>The BEDTools utilities allow one to address common genomics tasks such as finding feature overlaps\n</code></pre>"},{"location":"Glossary/#beefs","title":"BEEF's:","text":"<pre><code>BEEF is a library implementing the Bayesian Error Estimation Functional, a description of which can be found here:  http://dx.doi.org/10.1103/PhysRevB.85.235149\n</code></pre>"},{"location":"Glossary/#beef","title":"BEEF:","text":"<pre><code>BEEF is a library implementing the Bayesian Error Estimation Functional, a description of which can be found here:  http://dx.doi.org/10.1103/PhysRevB.85.235149\n</code></pre>"},{"location":"Glossary/#blasrs","title":"BLASR's:","text":"<pre><code>BLASR (Basic Local Alignment with Successive Refinement) rapidly maps\n</code></pre>"},{"location":"Glossary/#blasr","title":"BLASR:","text":"<pre><code>BLASR (Basic Local Alignment with Successive Refinement) rapidly maps\n</code></pre>"},{"location":"Glossary/#blasts","title":"BLAST's:","text":"<pre><code>Basic Local Alignment Search Tool, or BLAST, is an algorithm\n</code></pre>"},{"location":"Glossary/#blast","title":"BLAST:","text":"<pre><code>Basic Local Alignment Search Tool, or BLAST, is an algorithm\n</code></pre>"},{"location":"Glossary/#blastdbs","title":"BLASTDB's:","text":"<pre><code>BLAST databases downloaded from NCBI.\n</code></pre>"},{"location":"Glossary/#blastdb","title":"BLASTDB:","text":"<pre><code>BLAST databases downloaded from NCBI.\n</code></pre>"},{"location":"Glossary/#blats","title":"BLAT's:","text":"<pre><code>BLAT on DNA is designed to quickly find sequences of 95% and greater similarity of length 25 bases or more.\n</code></pre>"},{"location":"Glossary/#blat","title":"BLAT:","text":"<pre><code>BLAT on DNA is designed to quickly find sequences of 95% and greater similarity of length 25 bases or more.\n</code></pre>"},{"location":"Glossary/#bliss","title":"BLIS's:","text":"<pre><code>BLIS is a portable software framework for instantiating high-performance\n</code></pre>"},{"location":"Glossary/#blis","title":"BLIS:","text":"<pre><code>BLIS is a portable software framework for instantiating high-performance\n</code></pre>"},{"location":"Glossary/#bolt-lmms","title":"BOLT-LMM's:","text":"<pre><code>The BOLT-LMM algorithm for mixed model association testing,\n</code></pre>"},{"location":"Glossary/#bolt-lmm","title":"BOLT-LMM:","text":"<pre><code>The BOLT-LMM algorithm for mixed model association testing,\n</code></pre>"},{"location":"Glossary/#brakers","title":"BRAKER's:","text":"<pre><code>Pipeline for fully automated prediction of protein coding genes with GeneMark-ES/ET\n</code></pre>"},{"location":"Glossary/#braker","title":"BRAKER:","text":"<pre><code>Pipeline for fully automated prediction of protein coding genes with GeneMark-ES/ET\n</code></pre>"},{"location":"Glossary/#buscos","title":"BUSCO's:","text":"<pre><code>Assessing genome assembly and annotation completeness with Benchmarking Universal Single-Copy Orthologs\n</code></pre>"},{"location":"Glossary/#busco","title":"BUSCO:","text":"<pre><code>Assessing genome assembly and annotation completeness with Benchmarking Universal Single-Copy Orthologs\n</code></pre>"},{"location":"Glossary/#bwas","title":"BWA's:","text":"<pre><code>Burrows-Wheeler Aligner (BWA) is an efficient program that aligns\n</code></pre>"},{"location":"Glossary/#bwa","title":"BWA:","text":"<pre><code>Burrows-Wheeler Aligner (BWA) is an efficient program that aligns\n</code></pre>"},{"location":"Glossary/#bamtoolss","title":"BamTools's:","text":"<pre><code>BamTools provides both a programmer's API and an end-user's toolkit for handling BAM files.\n</code></pre>"},{"location":"Glossary/#bamtools","title":"BamTools:","text":"<pre><code>BamTools provides both a programmer's API and an end-user's toolkit for handling BAM files.\n</code></pre>"},{"location":"Glossary/#bandages","title":"Bandage's:","text":"<pre><code>Bandage is a program for visualising de novo assembly graphs\n</code></pre>"},{"location":"Glossary/#bandage","title":"Bandage:","text":"<pre><code>Bandage is a program for visualising de novo assembly graphs\n</code></pre>"},{"location":"Glossary/#basilisks","title":"Basilisk's:","text":"<pre><code>Basilisk is a Free Software program for the solution of partial differential equations on adaptive Cartesian meshes.\n</code></pre>"},{"location":"Glossary/#basilisk","title":"Basilisk:","text":"<pre><code>Basilisk is a Free Software program for the solution of partial differential equations on adaptive Cartesian meshes.\n</code></pre>"},{"location":"Glossary/#baypasss","title":"BayPass's:","text":"<pre><code>Genome-Wide Scan for Adaptive Differentiation and Association Analysis with population-specific covariables\n</code></pre>"},{"location":"Glossary/#baypass","title":"BayPass:","text":"<pre><code>Genome-Wide Scan for Adaptive Differentiation and Association Analysis with population-specific covariables\n</code></pre>"},{"location":"Glossary/#bayescans","title":"BayeScan's:","text":"<pre><code>Identify candidate loci under natural selection from genetic data,\n</code></pre>"},{"location":"Glossary/#bayescan","title":"BayeScan:","text":"<pre><code>Identify candidate loci under natural selection from genetic data,\n</code></pre>"},{"location":"Glossary/#bayesasss","title":"BayesAss's:","text":"<pre><code>Program for inference of recent immigration rates between populations using unlinked multilocus genotypes\n</code></pre>"},{"location":"Glossary/#bayesass","title":"BayesAss:","text":"<pre><code>Program for inference of recent immigration rates between populations using unlinked multilocus genotypes\n</code></pre>"},{"location":"Glossary/#bazels","title":"Bazel's:","text":"<pre><code>Bazel is a build tool that builds code quickly and reliably.\n</code></pre>"},{"location":"Glossary/#bazel","title":"Bazel:","text":"<pre><code>Bazel is a build tool that builds code quickly and reliably.\n</code></pre>"},{"location":"Glossary/#beagles","title":"Beagle's:","text":"<pre><code>Package for phasing genotypes and for imputing ungenotyped markers.\n</code></pre>"},{"location":"Glossary/#beagle","title":"Beagle:","text":"<pre><code>Package for phasing genotypes and for imputing ungenotyped markers.\n</code></pre>"},{"location":"Glossary/#berkeleygws","title":"BerkeleyGW's:","text":"<pre><code>The BerkeleyGW Package is a set of computer codes that calculates the quasiparticle\n</code></pre>"},{"location":"Glossary/#berkeleygw","title":"BerkeleyGW:","text":"<pre><code>The BerkeleyGW Package is a set of computer codes that calculates the quasiparticle\n</code></pre>"},{"location":"Glossary/#bifrosts","title":"Bifrost's:","text":"<pre><code>Highly parallel construction, indexing and querying of colored and compacted de Bruijn graphs.\n</code></pre>"},{"location":"Glossary/#bifrost","title":"Bifrost:","text":"<pre><code>Highly parallel construction, indexing and querying of colored and compacted de Bruijn graphs.\n</code></pre>"},{"location":"Glossary/#bio-db-bigfiles","title":"Bio-DB-BigFile's:","text":"<pre><code>Read BigWig and BigBed genome feature databases\n</code></pre>"},{"location":"Glossary/#bio-db-bigfile","title":"Bio-DB-BigFile:","text":"<pre><code>Read BigWig and BigBed genome feature databases\n</code></pre>"},{"location":"Glossary/#bio-db-htss","title":"Bio-DB-HTS's:","text":"<pre><code>Read files using HTSlib including BAM/CRAM, Tabix and BCF database files\n</code></pre>"},{"location":"Glossary/#bio-db-hts","title":"Bio-DB-HTS:","text":"<pre><code>Read files using HTSlib including BAM/CRAM, Tabix and BCF database files\n</code></pre>"},{"location":"Glossary/#bioconductors","title":"BioConductor's:","text":"<pre><code>Tools for the analysis of high-throughput genomic data in R.\n</code></pre>"},{"location":"Glossary/#bioconductor","title":"BioConductor:","text":"<pre><code>Tools for the analysis of high-throughput genomic data in R.\n</code></pre>"},{"location":"Glossary/#bismarks","title":"Bismark's:","text":"<pre><code>A tool to map bisulfite converted sequence reads and\n</code></pre>"},{"location":"Glossary/#bismark","title":"Bismark:","text":"<pre><code>A tool to map bisulfite converted sequence reads and\n</code></pre>"},{"location":"Glossary/#bisons","title":"Bison's:","text":"<pre><code>Bison is a general-purpose parser generator that converts an annotated context-free grammar into a deterministic LR or generalized LR (GLR) parser employing LALR(1) parser tables.\n</code></pre>"},{"location":"Glossary/#bison","title":"Bison:","text":"<pre><code>Bison is a general-purpose parser generator that converts an annotated context-free grammar into a deterministic LR or generalized LR (GLR) parser employing LALR(1) parser tables.\n</code></pre>"},{"location":"Glossary/#blenderpys","title":"BlenderPy's:","text":"<pre><code>Blender provides a pipeline for 3D modeling, rigging, animation, simulation, rendering,\n</code></pre>"},{"location":"Glossary/#blenderpy","title":"BlenderPy:","text":"<pre><code>Blender provides a pipeline for 3D modeling, rigging, animation, simulation, rendering,\n</code></pre>"},{"location":"Glossary/#boosts","title":"Boost's:","text":"<pre><code>Boost provides free peer-reviewed portable C++ source libraries.\n</code></pre>"},{"location":"Glossary/#boost","title":"Boost:","text":"<pre><code>Boost provides free peer-reviewed portable C++ source libraries.\n</code></pre>"},{"location":"Glossary/#bowties","title":"Bowtie's:","text":"<pre><code>Ultrafast, memory-efficient short read aligner.\n</code></pre>"},{"location":"Glossary/#bowtie","title":"Bowtie:","text":"<pre><code>Ultrafast, memory-efficient short read aligner.\n</code></pre>"},{"location":"Glossary/#bowtie2s","title":"Bowtie2's:","text":"<pre><code>Ultrafast and memory-efficient tool for aligning\n</code></pre>"},{"location":"Glossary/#bowtie2","title":"Bowtie2:","text":"<pre><code>Ultrafast and memory-efficient tool for aligning\n</code></pre>"},{"location":"Glossary/#bpipes","title":"Bpipe's:","text":"<pre><code>A platform for running big bioinformatics jobs that consist of a series of processing stages\n</code></pre>"},{"location":"Glossary/#bpipe","title":"Bpipe:","text":"<pre><code>A platform for running big bioinformatics jobs that consist of a series of processing stages\n</code></pre>"},{"location":"Glossary/#brackens","title":"Bracken's:","text":"<pre><code>Hghly accurate statistical method that computes the abundance of\n</code></pre>"},{"location":"Glossary/#bracken","title":"Bracken:","text":"<pre><code>Hghly accurate statistical method that computes the abundance of\n</code></pre>"},{"location":"Glossary/#breakdancers","title":"BreakDancer's:","text":"<pre><code>Genome-wide detection of structural variants from next generation paired-end sequencing reads.\n</code></pre>"},{"location":"Glossary/#breakdancer","title":"BreakDancer:","text":"<pre><code>Genome-wide detection of structural variants from next generation paired-end sequencing reads.\n</code></pre>"},{"location":"Glossary/#breakseq2s","title":"BreakSeq2's:","text":"<pre><code>Nucleotide-resolution analysis of structural variants\n</code></pre>"},{"location":"Glossary/#breakseq2","title":"BreakSeq2:","text":"<pre><code>Nucleotide-resolution analysis of structural variants\n</code></pre>"},{"location":"Glossary/#ccls","title":"CCL's:","text":"<pre><code>Clozure CL (often called CCL for short) is a free Common Lisp implementation\n</code></pre>"},{"location":"Glossary/#ccl","title":"CCL:","text":"<pre><code>Clozure CL (often called CCL for short) is a free Common Lisp implementation\n</code></pre>"},{"location":"Glossary/#cd-hits","title":"CD-HIT's:","text":"<pre><code>CD-HIT is a very widely used program for clustering and\n</code></pre>"},{"location":"Glossary/#cd-hit","title":"CD-HIT:","text":"<pre><code>CD-HIT is a very widely used program for clustering and\n</code></pre>"},{"location":"Glossary/#cdos","title":"CDO's:","text":"<pre><code>CDO is a collection of command line Operators to manipulate and analyse Climate and NWP model Data.\n</code></pre>"},{"location":"Glossary/#cdo","title":"CDO:","text":"<pre><code>CDO is a collection of command line Operators to manipulate and analyse Climate and NWP model Data.\n</code></pre>"},{"location":"Glossary/#cfitsios","title":"CFITSIO's:","text":"<pre><code>CFITSIO is a library of C and Fortran subroutines for reading and writing data files in\n</code></pre>"},{"location":"Glossary/#cfitsio","title":"CFITSIO:","text":"<pre><code>CFITSIO is a library of C and Fortran subroutines for reading and writing data files in\n</code></pre>"},{"location":"Glossary/#cgals","title":"CGAL's:","text":"<pre><code>The goal of the CGAL Open Source Project is to provide easy access to efficient\n</code></pre>"},{"location":"Glossary/#cgal","title":"CGAL:","text":"<pre><code>The goal of the CGAL Open Source Project is to provide easy access to efficient\n</code></pre>"},{"location":"Glossary/#cmakes","title":"CMake's:","text":"<pre><code>CMake, the cross-platform, open-source build system.  CMake is a family of tools designed to build, test and package software.\n</code></pre>"},{"location":"Glossary/#cmake","title":"CMake:","text":"<pre><code>CMake, the cross-platform, open-source build system.  CMake is a family of tools designed to build, test and package software.\n</code></pre>"},{"location":"Glossary/#cnvnators","title":"CNVnator's:","text":"<pre><code>Copy Number Variation discovery and genotyping from depth of read mapping.\n</code></pre>"},{"location":"Glossary/#cnvnator","title":"CNVnator:","text":"<pre><code>Copy Number Variation discovery and genotyping from depth of read mapping.\n</code></pre>"},{"location":"Glossary/#cnvpytors","title":"CNVpytor's:","text":"<pre><code>Python package and command line tool for CNV/CNA analysis from depth-of-coverage by mapped read\n</code></pre>"},{"location":"Glossary/#cnvpytor","title":"CNVpytor:","text":"<pre><code>Python package and command line tool for CNV/CNA analysis from depth-of-coverage by mapped read\n</code></pre>"},{"location":"Glossary/#comsols","title":"COMSOL's:","text":"<pre><code>COMSOL is a multiphysics solver that provides a unified workflow for electrical, mechanical, fluid, and chemical applications.\n</code></pre>"},{"location":"Glossary/#comsol","title":"COMSOL:","text":"<pre><code>COMSOL is a multiphysics solver that provides a unified workflow for electrical, mechanical, fluid, and chemical applications.\n</code></pre>"},{"location":"Glossary/#concocts","title":"CONCOCT's:","text":"<pre><code>Program for unsupervised binning of metagenomic contigs by using nucleotide composition,\n</code></pre>"},{"location":"Glossary/#concoct","title":"CONCOCT:","text":"<pre><code>Program for unsupervised binning of metagenomic contigs by using nucleotide composition,\n</code></pre>"},{"location":"Glossary/#cp2ks","title":"CP2K's:","text":"<pre><code>CP2K is a freely available (GPL) program, written in Fortran 95, to perform atomistic and molecular\n</code></pre>"},{"location":"Glossary/#cp2k","title":"CP2K:","text":"<pre><code>CP2K is a freely available (GPL) program, written in Fortran 95, to perform atomistic and molecular\n</code></pre>"},{"location":"Glossary/#craminos","title":"CRAMINO's:","text":"<pre><code>A tool for quick quality assessment of cram and bam files, intended for long read sequencing\n</code></pre>"},{"location":"Glossary/#cramino","title":"CRAMINO:","text":"<pre><code>A tool for quick quality assessment of cram and bam files, intended for long read sequencing\n</code></pre>"},{"location":"Glossary/#ctpls","title":"CTPL's:","text":"<pre><code>C++ Thread Pool Library\n</code></pre>"},{"location":"Glossary/#ctpl","title":"CTPL:","text":"<pre><code>C++ Thread Pool Library\n</code></pre>"},{"location":"Glossary/#cudas","title":"CUDA's:","text":"<pre><code>CUDA (formerly Compute Unified Device Architecture) is a parallel  computing platform and programming model created by NVIDIA and implemented by the  graphics processing units (GPUs) that they produce. CUDA gives developers access  to the virtual instruction set and memory of the parallel computational elements in CUDA GPUs.\n</code></pre>"},{"location":"Glossary/#cunits","title":"CUnit's:","text":"<pre><code>Automated testing framework for C.\n</code></pre>"},{"location":"Glossary/#cunit","title":"CUnit:","text":"<pre><code>Automated testing framework for C.\n</code></pre>"},{"location":"Glossary/#canus","title":"Canu's:","text":"<pre><code>Sequence assembler designed for high-noise single-molecule sequencing.\n</code></pre>"},{"location":"Glossary/#canu","title":"Canu:","text":"<pre><code>Sequence assembler designed for high-noise single-molecule sequencing.\n</code></pre>"},{"location":"Glossary/#capnprotos","title":"CapnProto's:","text":"<pre><code>Fast data interchange format and capability-based RPC system.\n</code></pre>"},{"location":"Glossary/#capnproto","title":"CapnProto:","text":"<pre><code>Fast data interchange format and capability-based RPC system.\n</code></pre>"},{"location":"Glossary/#catch2s","title":"Catch2's:","text":"<pre><code>A modern, C++-native, header-only,  test framework for unit-tests, TDD and BDD\n</code></pre>"},{"location":"Glossary/#catch2","title":"Catch2:","text":"<pre><code>A modern, C++-native, header-only,  test framework for unit-tests, TDD and BDD\n</code></pre>"},{"location":"Glossary/#cellrangers","title":"CellRanger's:","text":"<pre><code>Cell Ranger is a set of analysis pipelines that process Chromium\n</code></pre>"},{"location":"Glossary/#cellranger","title":"CellRanger:","text":"<pre><code>Cell Ranger is a set of analysis pipelines that process Chromium\n</code></pre>"},{"location":"Glossary/#centrifuges","title":"Centrifuge's:","text":"<pre><code>Classifier for metagenomic sequences\n</code></pre>"},{"location":"Glossary/#centrifuge","title":"Centrifuge:","text":"<pre><code>Classifier for metagenomic sequences\n</code></pre>"},{"location":"Glossary/#cereals","title":"Cereal's:","text":"<pre><code>C++11 serialization library\n</code></pre>"},{"location":"Glossary/#cereal","title":"Cereal:","text":"<pre><code>C++11 serialization library\n</code></pre>"},{"location":"Glossary/#charms","title":"Charm++'s:","text":"<pre><code>An asynchronous message-driven C++ communication library targeted towards tightly coupled, high-performance parallel machines.\n</code></pre>"},{"location":"Glossary/#charm","title":"Charm++:","text":"<pre><code>An asynchronous message-driven C++ communication library targeted towards tightly coupled, high-performance parallel machines.\n</code></pre>"},{"location":"Glossary/#checkms","title":"CheckM's:","text":"<pre><code>CheckM provides a set of tools for assessing the quality of genomes recovered from isolates,\n</code></pre>"},{"location":"Glossary/#checkm","title":"CheckM:","text":"<pre><code>CheckM provides a set of tools for assessing the quality of genomes recovered from isolates,\n</code></pre>"},{"location":"Glossary/#checkm2s","title":"CheckM2's:","text":"<pre><code>Rapid assessment of genome bin quality using machine learning\n</code></pre>"},{"location":"Glossary/#checkm2","title":"CheckM2:","text":"<pre><code>Rapid assessment of genome bin quality using machine learning\n</code></pre>"},{"location":"Glossary/#checkvs","title":"CheckV's:","text":"<pre><code>Assess the quality of metagenome-assembled viral genomes.\n</code></pre>"},{"location":"Glossary/#checkv","title":"CheckV:","text":"<pre><code>Assess the quality of metagenome-assembled viral genomes.\n</code></pre>"},{"location":"Glossary/#circlators","title":"Circlator's:","text":"<pre><code>A tool to circularize genome assemblies\n</code></pre>"},{"location":"Glossary/#circlator","title":"Circlator:","text":"<pre><code>A tool to circularize genome assemblies\n</code></pre>"},{"location":"Glossary/#circoss","title":"Circos's:","text":"<pre><code>Package for visualizing data in a circular layout - this makes Circos ideal for exploring\n</code></pre>"},{"location":"Glossary/#circos","title":"Circos:","text":"<pre><code>Package for visualizing data in a circular layout - this makes Circos ideal for exploring\n</code></pre>"},{"location":"Glossary/#clair3s","title":"Clair3's:","text":"<pre><code>Syumphonizing pileup and full-alignment for high-performance long-read variant calling.\n</code></pre>"},{"location":"Glossary/#clair3","title":"Clair3:","text":"<pre><code>Syumphonizing pileup and full-alignment for high-performance long-read variant calling.\n</code></pre>"},{"location":"Glossary/#clustal-omegas","title":"Clustal-Omega's:","text":"<pre><code>Clustal Omega is a multiple sequence alignment\n</code></pre>"},{"location":"Glossary/#clustal-omega","title":"Clustal-Omega:","text":"<pre><code>Clustal Omega is a multiple sequence alignment\n</code></pre>"},{"location":"Glossary/#clustalw2s","title":"ClustalW2's:","text":"<pre><code>ClustalW2 is a general purpose multiple sequence alignment program for DNA or proteins.\n</code></pre>"},{"location":"Glossary/#clustalw2","title":"ClustalW2:","text":"<pre><code>ClustalW2 is a general purpose multiple sequence alignment program for DNA or proteins.\n</code></pre>"},{"location":"Glossary/#corsets","title":"Corset's:","text":"<pre><code>Clusters contigs and counts reads from de novo assembled transcriptomes.\n</code></pre>"},{"location":"Glossary/#corset","title":"Corset:","text":"<pre><code>Clusters contigs and counts reads from de novo assembled transcriptomes.\n</code></pre>"},{"location":"Glossary/#coverms","title":"CoverM's:","text":"<pre><code>DNA read coverage and relative abundance calculator focused on metagenomics applications\n</code></pre>"},{"location":"Glossary/#coverm","title":"CoverM:","text":"<pre><code>DNA read coverage and relative abundance calculator focused on metagenomics applications\n</code></pre>"},{"location":"Glossary/#cppunits","title":"CppUnit's:","text":"<pre><code>C++ port of the JUnit framework for unit testing.\n</code></pre>"},{"location":"Glossary/#cppunit","title":"CppUnit:","text":"<pre><code>C++ port of the JUnit framework for unit testing.\n</code></pre>"},{"location":"Glossary/#craycces","title":"CrayCCE's:","text":"<pre><code>Toolchain using Cray compiler wrapper, using PrgEnv-cray (PE release: February 2023).\n</code></pre>"},{"location":"Glossary/#craycce","title":"CrayCCE:","text":"<pre><code>Toolchain using Cray compiler wrapper, using PrgEnv-cray (PE release: February 2023).\n</code></pre>"},{"location":"Glossary/#craygnus","title":"CrayGNU's:","text":"<pre><code>Toolchain using Cray compiler wrapper, using PrgEnv-gnu module (PE release: 23.02).\n</code></pre>"},{"location":"Glossary/#craygnu","title":"CrayGNU:","text":"<pre><code>Toolchain using Cray compiler wrapper, using PrgEnv-gnu module (PE release: 23.02).\n</code></pre>"},{"location":"Glossary/#crayintels","title":"CrayIntel's:","text":"<pre><code>Toolchain using Cray compiler wrapper, using PrgEnv-intel (PE release: February 2023 with Intel 19 compiler).\n</code></pre>"},{"location":"Glossary/#crayintel","title":"CrayIntel:","text":"<pre><code>Toolchain using Cray compiler wrapper, using PrgEnv-intel (PE release: February 2023 with Intel 19 compiler).\n</code></pre>"},{"location":"Glossary/#cubeguis","title":"CubeGUI's:","text":"<pre><code>Graphical report explorer report explorer for Scalasca and Score-P\n</code></pre>"},{"location":"Glossary/#cubegui","title":"CubeGUI:","text":"<pre><code>Graphical report explorer report explorer for Scalasca and Score-P\n</code></pre>"},{"location":"Glossary/#cubelibs","title":"CubeLib's:","text":"<pre><code>Cube general purpose C++ library component and command-line tools.\n</code></pre>"},{"location":"Glossary/#cubelib","title":"CubeLib:","text":"<pre><code>Cube general purpose C++ library component and command-line tools.\n</code></pre>"},{"location":"Glossary/#cubewriters","title":"CubeWriter's:","text":"<pre><code>Cube high-performance C writer library component.\n</code></pre>"},{"location":"Glossary/#cubewriter","title":"CubeWriter:","text":"<pre><code>Cube high-performance C writer library component.\n</code></pre>"},{"location":"Glossary/#cufflinkss","title":"Cufflinks's:","text":"<pre><code>Transcript assembly, differential expression, and differential regulation for RNA-Seq\n</code></pre>"},{"location":"Glossary/#cufflinks","title":"Cufflinks:","text":"<pre><code>Transcript assembly, differential expression, and differential regulation for RNA-Seq\n</code></pre>"},{"location":"Glossary/#cytoscapes","title":"Cytoscape's:","text":"<pre><code>Cytoscape is an open source software platform for visualizing molecular interaction networks and\n</code></pre>"},{"location":"Glossary/#cytoscape","title":"Cytoscape:","text":"<pre><code>Cytoscape is an open source software platform for visualizing molecular interaction networks and\n</code></pre>"},{"location":"Glossary/#d-geniess","title":"D-Genies's:","text":"<pre><code>D-Genies also allows to display dot plots from other aligners by uploading their PAF or MAF alignment file.\n</code></pre>"},{"location":"Glossary/#d-genies","title":"D-Genies:","text":"<pre><code>D-Genies also allows to display dot plots from other aligners by uploading their PAF or MAF alignment file.\n</code></pre>"},{"location":"Glossary/#das_tools","title":"DAS_Tool's:","text":"<pre><code>DAS Tool is an automated method that integrates the results of a flexible number of binning\n</code></pre>"},{"location":"Glossary/#das_tool","title":"DAS_Tool:","text":"<pre><code>DAS Tool is an automated method that integrates the results of a flexible number of binning\n</code></pre>"},{"location":"Glossary/#dbs","title":"DB's:","text":"<pre><code>Berkeley DB enables the development of custom data management solutions,\n</code></pre>"},{"location":"Glossary/#db","title":"DB:","text":"<pre><code>Berkeley DB enables the development of custom data management solutions,\n</code></pre>"},{"location":"Glossary/#dbg2olcs","title":"DBG2OLC's:","text":"<pre><code>DBG2OLC:Efficient Assembly of Large Genomes Using Long Erroneous Reads of the Third Generation\n</code></pre>"},{"location":"Glossary/#dbg2olc","title":"DBG2OLC:","text":"<pre><code>DBG2OLC:Efficient Assembly of Large Genomes Using Long Erroneous Reads of the Third Generation\n</code></pre>"},{"location":"Glossary/#diamonds","title":"DIAMOND's:","text":"<pre><code>Sequence aligner for protein and translated DNA searches\n</code></pre>"},{"location":"Glossary/#diamond","title":"DIAMOND:","text":"<pre><code>Sequence aligner for protein and translated DNA searches\n</code></pre>"},{"location":"Glossary/#discovardenovos","title":"DISCOVARdenovo's:","text":"<pre><code>Assembler suitable for large genomes based on Illumina reads of length 250 or longer.\n</code></pre>"},{"location":"Glossary/#discovardenovo","title":"DISCOVARdenovo:","text":"<pre><code>Assembler suitable for large genomes based on Illumina reads of length 250 or longer.\n</code></pre>"},{"location":"Glossary/#drams","title":"DRAM's:","text":"<pre><code>Tool for annotating metagenomic assembled genomes and VirSorter identified viral contigs..\n</code></pre>"},{"location":"Glossary/#dram","title":"DRAM:","text":"<pre><code>Tool for annotating metagenomic assembled genomes and VirSorter identified viral contigs..\n</code></pre>"},{"location":"Glossary/#dalilites","title":"DaliLite's:","text":"<pre><code>Tool set for simulating/evaluating SVs, merging and comparing SVs within and among samples,\n</code></pre>"},{"location":"Glossary/#dalilite","title":"DaliLite:","text":"<pre><code>Tool set for simulating/evaluating SVs, merging and comparing SVs within and among samples,\n</code></pre>"},{"location":"Glossary/#deconseqs","title":"DeconSeq's:","text":"<pre><code>A tool that can be used to automatically detect and efficiently remove sequence contaminations\n</code></pre>"},{"location":"Glossary/#deconseq","title":"DeconSeq:","text":"<pre><code>A tool that can be used to automatically detect and efficiently remove sequence contaminations\n</code></pre>"},{"location":"Glossary/#deeplabcuts","title":"DeepLabCut's:","text":"<pre><code>Efficient method for 3D markerless pose estimation based on transfer learning with deep neural networks.\n</code></pre>"},{"location":"Glossary/#deeplabcut","title":"DeepLabCut:","text":"<pre><code>Efficient method for 3D markerless pose estimation based on transfer learning with deep neural networks.\n</code></pre>"},{"location":"Glossary/#defaultmoduless","title":"DefaultModules's:","text":"<pre><code>Defines the set of modules loaded by default\n</code></pre>"},{"location":"Glossary/#defaultmodules","title":"DefaultModules:","text":"<pre><code>Defines the set of modules loaded by default\n</code></pre>"},{"location":"Glossary/#delft3ds","title":"Delft3D's:","text":"<pre><code>Integrated simulation of sediment transport and morphology, waves, water quality and ecology.\n</code></pre>"},{"location":"Glossary/#delft3d","title":"Delft3D:","text":"<pre><code>Integrated simulation of sediment transport and morphology, waves, water quality and ecology.\n</code></pre>"},{"location":"Glossary/#delft3d_fms","title":"Delft3D_FM's:","text":"<pre><code>3D modeling suite to investigate hydrodynamics, sediment transport and morphology and water quality for fluvial, estuarine and coastal environments\n</code></pre>"},{"location":"Glossary/#delft3d_fm","title":"Delft3D_FM:","text":"<pre><code>3D modeling suite to investigate hydrodynamics, sediment transport and morphology and water quality for fluvial, estuarine and coastal environments\n</code></pre>"},{"location":"Glossary/#dellys","title":"Delly's:","text":"<pre><code>Structural variant discovery by integrated paired-end and split-read analysis\n</code></pre>"},{"location":"Glossary/#delly","title":"Delly:","text":"<pre><code>Structural variant discovery by integrated paired-end and split-read analysis\n</code></pre>"},{"location":"Glossary/#dorados","title":"Dorado's:","text":"<pre><code>High-performance, easy-to-use, open source basecaller for Oxford Nanopore reads.\n</code></pre>"},{"location":"Glossary/#dorado","title":"Dorado:","text":"<pre><code>High-performance, easy-to-use, open source basecaller for Oxford Nanopore reads.\n</code></pre>"},{"location":"Glossary/#dsuites","title":"Dsuite's:","text":"<pre><code>Fast calculation of the ABBA-BABA statistics across many populations/species\n</code></pre>"},{"location":"Glossary/#dsuite","title":"Dsuite:","text":"<pre><code>Fast calculation of the ABBA-BABA statistics across many populations/species\n</code></pre>"},{"location":"Glossary/#edtas","title":"EDTA's:","text":"<pre><code>Automated whole-genome de-novo TE annotation and benchmarking the annotation performance of TE libraries.\n</code></pre>"},{"location":"Glossary/#edta","title":"EDTA:","text":"<pre><code>Automated whole-genome de-novo TE annotation and benchmarking the annotation performance of TE libraries.\n</code></pre>"},{"location":"Glossary/#eigensofts","title":"EIGENSOFT's:","text":"<pre><code>The EIGENSOFT package combines functionality from our population genetics methods (Patterson et al.\n</code></pre>"},{"location":"Glossary/#eigensoft","title":"EIGENSOFT:","text":"<pre><code>The EIGENSOFT package combines functionality from our population genetics methods (Patterson et al.\n</code></pre>"},{"location":"Glossary/#elpas","title":"ELPA's:","text":"<pre><code>Eigenvalue SoLvers for Petaflop-Applications .\n</code></pre>"},{"location":"Glossary/#elpa","title":"ELPA:","text":"<pre><code>Eigenvalue SoLvers for Petaflop-Applications .\n</code></pre>"},{"location":"Glossary/#emans","title":"EMAN's:","text":"<pre><code>EMAN is a powerful image processing library as well as a complete software suite\n</code></pre>"},{"location":"Glossary/#eman","title":"EMAN:","text":"<pre><code>EMAN is a powerful image processing library as well as a complete software suite\n</code></pre>"},{"location":"Glossary/#eman2s","title":"EMAN2's:","text":"<pre><code>Greyscale scientific image processing suite with a primary focus on processing data from transmission electron microscopes\n</code></pre>"},{"location":"Glossary/#eman2","title":"EMAN2:","text":"<pre><code>Greyscale scientific image processing suite with a primary focus on processing data from transmission electron microscopes\n</code></pre>"},{"location":"Glossary/#embosss","title":"EMBOSS's:","text":"<pre><code>EMBOSS is 'The European Molecular Biology Open Software Suite'.\n</code></pre>"},{"location":"Glossary/#emboss","title":"EMBOSS:","text":"<pre><code>EMBOSS is 'The European Molecular Biology Open Software Suite'.\n</code></pre>"},{"location":"Glossary/#emirges","title":"EMIRGE's:","text":"<pre><code>Reconstructs full length ribosomal genes from short read\n</code></pre>"},{"location":"Glossary/#emirge","title":"EMIRGE:","text":"<pre><code>Reconstructs full length ribosomal genes from short read\n</code></pre>"},{"location":"Glossary/#enmtmls","title":"ENMTML's:","text":"<pre><code>R package for integrated construction of Ecological Niche Models.\n</code></pre>"},{"location":"Glossary/#enmtml","title":"ENMTML:","text":"<pre><code>R package for integrated construction of Ecological Niche Models.\n</code></pre>"},{"location":"Glossary/#esmfs","title":"ESMF's:","text":"<pre><code>The Earth System Modeling Framework (ESMF) is software for building and coupling weather,  climate, and related models.\n</code></pre>"},{"location":"Glossary/#esmf","title":"ESMF:","text":"<pre><code>The Earth System Modeling Framework (ESMF) is software for building and coupling weather,  climate, and related models.\n</code></pre>"},{"location":"Glossary/#etes","title":"ETE's:","text":"<pre><code>A Python framework for the analysis and visualization of phylogenetic trees\n</code></pre>"},{"location":"Glossary/#ete","title":"ETE:","text":"<pre><code>A Python framework for the analysis and visualization of phylogenetic trees\n</code></pre>"},{"location":"Glossary/#easybuilds","title":"EasyBuild's:","text":"<pre><code>EasyBuild is a software build and installation framework\n</code></pre>"},{"location":"Glossary/#easybuild","title":"EasyBuild:","text":"<pre><code>EasyBuild is a software build and installation framework\n</code></pre>"},{"location":"Glossary/#eigens","title":"Eigen's:","text":"<pre><code>Eigen is a C++ template library for linear algebra:\n</code></pre>"},{"location":"Glossary/#eigen","title":"Eigen:","text":"<pre><code>Eigen is a C++ template library for linear algebra:\n</code></pre>"},{"location":"Glossary/#elmers","title":"Elmer's:","text":"<pre><code>Elmer is an open source multiphysical simulation software mainly developed by\n</code></pre>"},{"location":"Glossary/#elmer","title":"Elmer:","text":"<pre><code>Elmer is an open source multiphysical simulation software mainly developed by\n</code></pre>"},{"location":"Glossary/#embrees","title":"Embree's:","text":"<pre><code>Embree is a collection of high-performance ray tracing kernels, developed at Intel. The target users of Embree are graphics application engineers who want to improve the performance of their photo-realistic rendering application by leveraging Embree's performance-optimized ray tracing kernels.\n</code></pre>"},{"location":"Glossary/#embree","title":"Embree:","text":"<pre><code>Embree is a collection of high-performance ray tracing kernels, developed at Intel. The target users of Embree are graphics application engineers who want to improve the performance of their photo-realistic rendering application by leveraging Embree's performance-optimized ray tracing kernels.\n</code></pre>"},{"location":"Glossary/#energypluss","title":"EnergyPlus's:","text":"<pre><code>Energy simulation program used to model energy consumption and water use in buildings.\n</code></pre>"},{"location":"Glossary/#energyplus","title":"EnergyPlus:","text":"<pre><code>Energy simulation program used to model energy consumption and water use in buildings.\n</code></pre>"},{"location":"Glossary/#erlangotps","title":"ErlangOTP's:","text":"<pre><code>Erlang is a programming language used to build massively scalable\n</code></pre>"},{"location":"Glossary/#erlangotp","title":"ErlangOTP:","text":"<pre><code>Erlang is a programming language used to build massively scalable\n</code></pre>"},{"location":"Glossary/#eukrep-eukccs","title":"EukRep-EukCC's:","text":"<pre><code>Completeness and contamination estimator for metagenomic assembled microbial eukaryotic genomes.\n</code></pre>"},{"location":"Glossary/#eukrep-eukcc","title":"EukRep-EukCC:","text":"<pre><code>Completeness and contamination estimator for metagenomic assembled microbial eukaryotic genomes.\n</code></pre>"},{"location":"Glossary/#exabayess","title":"ExaBayes's:","text":"<pre><code>Bayesian tree inference, particularly suitable for large-scale analyses.\n</code></pre>"},{"location":"Glossary/#exabayes","title":"ExaBayes:","text":"<pre><code>Bayesian tree inference, particularly suitable for large-scale analyses.\n</code></pre>"},{"location":"Glossary/#examls","title":"ExaML's:","text":"<pre><code>Exascale Maximum Likelihood for phylogenetic inference using MPI.\n</code></pre>"},{"location":"Glossary/#examl","title":"ExaML:","text":"<pre><code>Exascale Maximum Likelihood for phylogenetic inference using MPI.\n</code></pre>"},{"location":"Glossary/#extraes","title":"Extrae's:","text":"<pre><code>Extrae is capable of instrumenting applications based on MPI, OpenMP, pthreads, CUDA1, OpenCL1, and StarSs1 using different instrumentation approaches\n</code></pre>"},{"location":"Glossary/#extrae","title":"Extrae:","text":"<pre><code>Extrae is capable of instrumenting applications based on MPI, OpenMP, pthreads, CUDA1, OpenCL1, and StarSs1 using different instrumentation approaches\n</code></pre>"},{"location":"Glossary/#falcons","title":"FALCON's:","text":"<pre><code>Falcon: a set of tools for fast aligning long reads for consensus and assembly\n</code></pre>"},{"location":"Glossary/#falcon","title":"FALCON:","text":"<pre><code>Falcon: a set of tools for fast aligning long reads for consensus and assembly\n</code></pre>"},{"location":"Glossary/#fastx-toolkits","title":"FASTX-Toolkit's:","text":"<pre><code>Tools for Short-Reads FASTA/FASTQ files preprocessing.\n</code></pre>"},{"location":"Glossary/#fastx-toolkit","title":"FASTX-Toolkit:","text":"<pre><code>Tools for Short-Reads FASTA/FASTQ files preprocessing.\n</code></pre>"},{"location":"Glossary/#fcms","title":"FCM's:","text":"<pre><code>FCM Build - A powerful build system for modern Fortran software applications. FCM Version Control - Wrappers to the Subversion version control system, usage conventions and processes for scientific software development.\n</code></pre>"},{"location":"Glossary/#fcm","title":"FCM:","text":"<pre><code>FCM Build - A powerful build system for modern Fortran software applications. FCM Version Control - Wrappers to the Subversion version control system, usage conventions and processes for scientific software development.\n</code></pre>"},{"location":"Glossary/#fdss","title":"FDS's:","text":"<pre><code>Fire Dynamics Simulator (FDS) is a large-eddy simulation (LES) code for low-speed flows,  with an emphasis on smoke and heat transport from fires.\n</code></pre>"},{"location":"Glossary/#fds","title":"FDS:","text":"<pre><code>Fire Dynamics Simulator (FDS) is a large-eddy simulation (LES) code for low-speed flows,  with an emphasis on smoke and heat transport from fires.\n</code></pre>"},{"location":"Glossary/#fftws","title":"FFTW's:","text":"<pre><code>FFTW is a C subroutine library for computing the discrete Fourier transform (DFT)  in one or more dimensions, of arbitrary input size, and of both real and complex data.\n</code></pre>"},{"location":"Glossary/#fftw","title":"FFTW:","text":"<pre><code>FFTW is a C subroutine library for computing the discrete Fourier transform (DFT)  in one or more dimensions, of arbitrary input size, and of both real and complex data.\n</code></pre>"},{"location":"Glossary/#ffmpegs","title":"FFmpeg's:","text":"<pre><code>A complete, cross-platform solution to record, convert and stream audio and video.\n</code></pre>"},{"location":"Glossary/#ffmpeg","title":"FFmpeg:","text":"<pre><code>A complete, cross-platform solution to record, convert and stream audio and video.\n</code></pre>"},{"location":"Glossary/#fltks","title":"FLTK's:","text":"<pre><code>FLTK is a cross-platform C++ GUI toolkit for UNIX/Linux (X11), Microsoft Windows,\n</code></pre>"},{"location":"Glossary/#fltk","title":"FLTK:","text":"<pre><code>FLTK is a cross-platform C++ GUI toolkit for UNIX/Linux (X11), Microsoft Windows,\n</code></pre>"},{"location":"Glossary/#ftgls","title":"FTGL's:","text":"<pre><code>FTGL is a free cross-platform Open Source C++ library that uses Freetype2 to simplify rendering fonts in OpenGL applications. FTGL supports bitmaps, pixmaps, texture maps, outlines, polygon mesh, and extruded polygon rendering modes.\n</code></pre>"},{"location":"Glossary/#ftgl","title":"FTGL:","text":"<pre><code>FTGL is a free cross-platform Open Source C++ library that uses Freetype2 to simplify rendering fonts in OpenGL applications. FTGL supports bitmaps, pixmaps, texture maps, outlines, polygon mesh, and extruded polygon rendering modes.\n</code></pre>"},{"location":"Glossary/#fastanis","title":"FastANI's:","text":"<pre><code>Tool for fast alignment-free computation of\n</code></pre>"},{"location":"Glossary/#fastani","title":"FastANI:","text":"<pre><code>Tool for fast alignment-free computation of\n</code></pre>"},{"location":"Glossary/#fastmes","title":"FastME's:","text":"<pre><code>FastME: a comprehensive, accurate and fast distance-based phylogeny inference program.\n</code></pre>"},{"location":"Glossary/#fastme","title":"FastME:","text":"<pre><code>FastME: a comprehensive, accurate and fast distance-based phylogeny inference program.\n</code></pre>"},{"location":"Glossary/#fastqcs","title":"FastQC's:","text":"<pre><code>A set of tools (in Java) for working with next generation sequencing data in the BAM format.\n</code></pre>"},{"location":"Glossary/#fastqc","title":"FastQC:","text":"<pre><code>A set of tools (in Java) for working with next generation sequencing data in the BAM format.\n</code></pre>"},{"location":"Glossary/#fastq_screens","title":"FastQ_Screen's:","text":"<pre><code>FastQ Screen allows you to screen a library of sequences in FastQ\n</code></pre>"},{"location":"Glossary/#fastq_screen","title":"FastQ_Screen:","text":"<pre><code>FastQ Screen allows you to screen a library of sequences in FastQ\n</code></pre>"},{"location":"Glossary/#fasttrees","title":"FastTree's:","text":"<pre><code>FastTree infers approximately-maximum-likelihood phylogenetic trees from alignments of nucleotide\n</code></pre>"},{"location":"Glossary/#fasttree","title":"FastTree:","text":"<pre><code>FastTree infers approximately-maximum-likelihood phylogenetic trees from alignments of nucleotide\n</code></pre>"},{"location":"Glossary/#file-renames","title":"File-Rename's:","text":"<pre><code>A Perl version of the rename utility, with support for regular expressions.\n</code></pre>"},{"location":"Glossary/#file-rename","title":"File-Rename:","text":"<pre><code>A Perl version of the rename utility, with support for regular expressions.\n</code></pre>"},{"location":"Glossary/#filtlongs","title":"Filtlong's:","text":"<pre><code>Tool for filtering long reads by quality.\n</code></pre>"},{"location":"Glossary/#filtlong","title":"Filtlong:","text":"<pre><code>Tool for filtering long reads by quality.\n</code></pre>"},{"location":"Glossary/#fimtypers","title":"FimTyper's:","text":"<pre><code>Identifies the FimH type in total or partial sequenced isolates of E. coli..\n</code></pre>"},{"location":"Glossary/#fimtyper","title":"FimTyper:","text":"<pre><code>Identifies the FimH type in total or partial sequenced isolates of E. coli..\n</code></pre>"},{"location":"Glossary/#flexiblass","title":"FlexiBLAS's:","text":"<pre><code>FlexiBLAS is a wrapper library that enables the exchange of the BLAS and LAPACK implementation\n</code></pre>"},{"location":"Glossary/#flexiblas","title":"FlexiBLAS:","text":"<pre><code>FlexiBLAS is a wrapper library that enables the exchange of the BLAS and LAPACK implementation\n</code></pre>"},{"location":"Glossary/#flyes","title":"Flye's:","text":"<pre><code>Flye is a de novo assembler for long and noisy reads, such as those produced by PacBio\n</code></pre>"},{"location":"Glossary/#flye","title":"Flye:","text":"<pre><code>Flye is a de novo assembler for long and noisy reads, such as those produced by PacBio\n</code></pre>"},{"location":"Glossary/#foxs","title":"FoX's:","text":"<pre><code>FoX is an XML library written in Fortran 95.\n</code></pre>"},{"location":"Glossary/#fox","title":"FoX:","text":"<pre><code>FoX is an XML library written in Fortran 95.\n</code></pre>"},{"location":"Glossary/#fraggenescans","title":"FragGeneScan's:","text":"<pre><code>FragGeneScan is an application for finding (fragmented) genes in short reads.\n</code></pre>"},{"location":"Glossary/#fraggenescan","title":"FragGeneScan:","text":"<pre><code>FragGeneScan is an application for finding (fragmented) genes in short reads.\n</code></pre>"},{"location":"Glossary/#freebayess","title":"FreeBayes's:","text":"<pre><code>Genetic variant detector designed to find polymorphisms smaller than the length of a short-read sequencing alignment.\n</code></pre>"},{"location":"Glossary/#freebayes","title":"FreeBayes:","text":"<pre><code>Genetic variant detector designed to find polymorphisms smaller than the length of a short-read sequencing alignment.\n</code></pre>"},{"location":"Glossary/#freesurfers","title":"FreeSurfer's:","text":"<pre><code>FreeSurfer is a set of tools for analysis and visualization of structural and functional brain imaging data.\n</code></pre>"},{"location":"Glossary/#freesurfer","title":"FreeSurfer:","text":"<pre><code>FreeSurfer is a set of tools for analysis and visualization of structural and functional brain imaging data.\n</code></pre>"},{"location":"Glossary/#fribidis","title":"FriBidi's:","text":"<pre><code>Free Implementation of the Unicode Bidirectional Algorithm.\n</code></pre>"},{"location":"Glossary/#fribidi","title":"FriBidi:","text":"<pre><code>Free Implementation of the Unicode Bidirectional Algorithm.\n</code></pre>"},{"location":"Glossary/#gatks","title":"GATK's:","text":"<pre><code>The Genome Analysis Toolkit or GATK is a software package developed at the Broad Institute\n</code></pre>"},{"location":"Glossary/#gatk","title":"GATK:","text":"<pre><code>The Genome Analysis Toolkit or GATK is a software package developed at the Broad Institute\n</code></pre>"},{"location":"Glossary/#gccs","title":"GCC's:","text":"<pre><code>The GNU Compiler Collection includes front ends for C, C++, Objective-C, Fortran, Java, and Ada,\n</code></pre>"},{"location":"Glossary/#gcc","title":"GCC:","text":"<pre><code>The GNU Compiler Collection includes front ends for C, C++, Objective-C, Fortran, Java, and Ada,\n</code></pre>"},{"location":"Glossary/#gcccores","title":"GCCcore's:","text":"<pre><code>The GNU Compiler Collection includes front ends for C, C++, Objective-C, Fortran, Java, and Ada,\n</code></pre>"},{"location":"Glossary/#gcccore","title":"GCCcore:","text":"<pre><code>The GNU Compiler Collection includes front ends for C, C++, Objective-C, Fortran, Java, and Ada,\n</code></pre>"},{"location":"Glossary/#gds","title":"GD's:","text":"<pre><code>Interface to Gd Graphics Library\n</code></pre>"},{"location":"Glossary/#gd","title":"GD:","text":"<pre><code>Interface to Gd Graphics Library\n</code></pre>"},{"location":"Glossary/#gdals","title":"GDAL's:","text":"<pre><code>GDAL is a translator library for raster geospatial data formats that is released under an X/MIT style  Open Source license by the Open Source Geospatial Foundation. As a library, it presents a single abstract data model  to the calling application for all supported formats. It also comes with a variety of useful command-line utilities for  data translation and processing.  NOTE: The GDAL IO cache by default uses 5% of total memory. This seems not necessary. This module sets GDAL_CACHEMAX=256 (256MB),   which should have no performance impact. Feel free to change if necessary, using 'export GDAL_CACHEMAX=xxx' (in your job script)   after loading the GDAL module.\n</code></pre>"},{"location":"Glossary/#gdal","title":"GDAL:","text":"<pre><code>GDAL is a translator library for raster geospatial data formats that is released under an X/MIT style  Open Source license by the Open Source Geospatial Foundation. As a library, it presents a single abstract data model  to the calling application for all supported formats. It also comes with a variety of useful command-line utilities for  data translation and processing.  NOTE: The GDAL IO cache by default uses 5% of total memory. This seems not necessary. This module sets GDAL_CACHEMAX=256 (256MB),   which should have no performance impact. Feel free to change if necessary, using 'export GDAL_CACHEMAX=xxx' (in your job script)   after loading the GDAL module.\n</code></pre>"},{"location":"Glossary/#gemmas","title":"GEMMA's:","text":"<pre><code>Genome-wide Efficient Mixed Model Association\n</code></pre>"},{"location":"Glossary/#gemma","title":"GEMMA:","text":"<pre><code>Genome-wide Efficient Mixed Model Association\n</code></pre>"},{"location":"Glossary/#geoss","title":"GEOS's:","text":"<pre><code>GEOS (Geometry Engine - Open Source) is a C++ port of the  Java Topology Suite (JTS)\n</code></pre>"},{"location":"Glossary/#geos","title":"GEOS:","text":"<pre><code>GEOS (Geometry Engine - Open Source) is a C++ port of the  Java Topology Suite (JTS)\n</code></pre>"},{"location":"Glossary/#glms","title":"GLM's:","text":"<pre><code>OpenGL Mathematics (GLM) is a header only C++ mathematics library for graphics software based on\n</code></pre>"},{"location":"Glossary/#glm","title":"GLM:","text":"<pre><code>OpenGL Mathematics (GLM) is a header only C++ mathematics library for graphics software based on\n</code></pre>"},{"location":"Glossary/#glpks","title":"GLPK's:","text":"<pre><code>GNU Linear Programming Kit is intended for solving large-scale linear programming (LP), mixed integer programming (MIP), and other related problems.\n</code></pre>"},{"location":"Glossary/#glpk","title":"GLPK:","text":"<pre><code>GNU Linear Programming Kit is intended for solving large-scale linear programming (LP), mixed integer programming (MIP), and other related problems.\n</code></pre>"},{"location":"Glossary/#glibs","title":"GLib's:","text":"<pre><code>GLib is one of the base libraries of the GTK+ project\n</code></pre>"},{"location":"Glossary/#glib","title":"GLib:","text":"<pre><code>GLib is one of the base libraries of the GTK+ project\n</code></pre>"},{"location":"Glossary/#gmap-gsnaps","title":"GMAP-GSNAP's:","text":"<pre><code>GMAP: A Genomic Mapping and Alignment Program for mRNA and EST Sequences\n</code></pre>"},{"location":"Glossary/#gmap-gsnap","title":"GMAP-GSNAP:","text":"<pre><code>GMAP: A Genomic Mapping and Alignment Program for mRNA and EST Sequences\n</code></pre>"},{"location":"Glossary/#gmps","title":"GMP's:","text":"<pre><code>GMP is a free library for arbitrary precision arithmetic,\n</code></pre>"},{"location":"Glossary/#gmp","title":"GMP:","text":"<pre><code>GMP is a free library for arbitrary precision arithmetic,\n</code></pre>"},{"location":"Glossary/#gmts","title":"GMT's:","text":"<pre><code>GMT is an open source collection of about 80 command-line tools for manipulating  geographic and Cartesian data sets (including filtering, trend fitting, gridding, projecting,  etc.) and producing PostScript illustrations ranging from simple x-y plots via contour maps  to artificially illuminated surfaces and 3D perspective views; the GMT supplements add another  40 more specialized and discipline-specific tools.\n</code></pre>"},{"location":"Glossary/#gmt","title":"GMT:","text":"<pre><code>GMT is an open source collection of about 80 command-line tools for manipulating  geographic and Cartesian data sets (including filtering, trend fitting, gridding, projecting,  etc.) and producing PostScript illustrations ranging from simple x-y plots via contour maps  to artificially illuminated surfaces and 3D perspective views; the GMT supplements add another  40 more specialized and discipline-specific tools.\n</code></pre>"},{"location":"Glossary/#golds","title":"GOLD's:","text":"<pre><code>A genetic algorithm for docking flexible ligands into protein binding sites\n</code></pre>"},{"location":"Glossary/#gold","title":"GOLD:","text":"<pre><code>A genetic algorithm for docking flexible ligands into protein binding sites\n</code></pre>"},{"location":"Glossary/#gobject-introspections","title":"GObject-Introspection's:","text":"<pre><code>GObject introspection is a middleware layer between C libraries\n</code></pre>"},{"location":"Glossary/#gobject-introspection","title":"GObject-Introspection:","text":"<pre><code>GObject introspection is a middleware layer between C libraries\n</code></pre>"},{"location":"Glossary/#gradss","title":"GRADS's:","text":"<pre><code>The Grid Analysis and Display System (GrADS) is an interactive desktop tool that is used for easy access, manipulation, and visualization of earth science data.\n</code></pre>"},{"location":"Glossary/#grads","title":"GRADS:","text":"<pre><code>The Grid Analysis and Display System (GrADS) is an interactive desktop tool that is used for easy access, manipulation, and visualization of earth science data.\n</code></pre>"},{"location":"Glossary/#grasss","title":"GRASS's:","text":"<pre><code>The Geographic Resources Analysis Support System - used for geospatial data management and analysis, image processing, graphics and maps production, spatial modeling, and visualization\n</code></pre>"},{"location":"Glossary/#grass","title":"GRASS:","text":"<pre><code>The Geographic Resources Analysis Support System - used for geospatial data management and analysis, image processing, graphics and maps production, spatial modeling, and visualization\n</code></pre>"},{"location":"Glossary/#gridsss","title":"GRIDSS's:","text":"<pre><code>GRIDSS is a module software suite containing tools useful for the detection of genomic rearrangements.\n</code></pre>"},{"location":"Glossary/#gridss","title":"GRIDSS:","text":"<pre><code>GRIDSS is a module software suite containing tools useful for the detection of genomic rearrangements.\n</code></pre>"},{"location":"Glossary/#gromacss","title":"GROMACS's:","text":"<pre><code>GROMACS is a versatile package to perform molecular dynamics,  i.e. simulate the Newtonian equations of motion for systems with hundreds to millions of particles.\n</code></pre>"},{"location":"Glossary/#gromacs","title":"GROMACS:","text":"<pre><code>GROMACS is a versatile package to perform molecular dynamics,  i.e. simulate the Newtonian equations of motion for systems with hundreds to millions of particles.\n</code></pre>"},{"location":"Glossary/#gsls","title":"GSL's:","text":"<pre><code>The GNU Scientific Library (GSL) is a numerical library for C and C++  programmers.  The library provides a wide range of mathematical routines  such as random number generators, special functions and least-squares fitting.\n</code></pre>"},{"location":"Glossary/#gsl","title":"GSL:","text":"<pre><code>The GNU Scientific Library (GSL) is a numerical library for C and C++  programmers.  The library provides a wide range of mathematical routines  such as random number generators, special functions and least-squares fitting.\n</code></pre>"},{"location":"Glossary/#gst-plugins-bases","title":"GST-plugins-base's:","text":"<pre><code>GStreamer plug-ins and elements.\n</code></pre>"},{"location":"Glossary/#gst-plugins-base","title":"GST-plugins-base:","text":"<pre><code>GStreamer plug-ins and elements.\n</code></pre>"},{"location":"Glossary/#gstreamers","title":"GStreamer's:","text":"<pre><code>GStreamer is a library for constructing graphs of media-handling  components. The applications it supports range from simple  Ogg/Vorbis playback, audio/video streaming to complex audio  (mixing) and video (non-linear editing) processing.\n</code></pre>"},{"location":"Glossary/#gstreamer","title":"GStreamer:","text":"<pre><code>GStreamer is a library for constructing graphs of media-handling  components. The applications it supports range from simple  Ogg/Vorbis playback, audio/video streaming to complex audio  (mixing) and video (non-linear editing) processing.\n</code></pre>"},{"location":"Glossary/#gtdb-tks","title":"GTDB-Tk's:","text":"<pre><code>A toolkit for assigning objective taxonomic classifications to bacterial and archaeal genomes.\n</code></pre>"},{"location":"Glossary/#gtdb-tk","title":"GTDB-Tk:","text":"<pre><code>A toolkit for assigning objective taxonomic classifications to bacterial and archaeal genomes.\n</code></pre>"},{"location":"Glossary/#gtks","title":"GTK+'s:","text":"<pre><code>GTK+ is the primary library used to construct user interfaces in GNOME.\n</code></pre>"},{"location":"Glossary/#gtk","title":"GTK+:","text":"<pre><code>GTK+ is the primary library used to construct user interfaces in GNOME.\n</code></pre>"},{"location":"Glossary/#gtss","title":"GTS's:","text":"<pre><code>GTS stands for the GNU Triangulated Surface Library.  It is an Open Source Free Software Library intended to provide a set of useful  functions to deal with 3D surfaces meshed with interconnected triangles.\n</code></pre>"},{"location":"Glossary/#gts","title":"GTS:","text":"<pre><code>GTS stands for the GNU Triangulated Surface Library.  It is an Open Source Free Software Library intended to provide a set of useful  functions to deal with 3D surfaces meshed with interconnected triangles.\n</code></pre>"},{"location":"Glossary/#genemark-ess","title":"GeneMark-ES's:","text":"<pre><code>Eukaryotic gene prediction suite with automatic training\n</code></pre>"},{"location":"Glossary/#genemark-es","title":"GeneMark-ES:","text":"<pre><code>Eukaryotic gene prediction suite with automatic training\n</code></pre>"},{"location":"Glossary/#genomethreaders","title":"GenomeThreader's:","text":"<pre><code>GenomeThreader is a software tool to compute gene structure predictions.\n</code></pre>"},{"location":"Glossary/#genomethreader","title":"GenomeThreader:","text":"<pre><code>GenomeThreader is a software tool to compute gene structure predictions.\n</code></pre>"},{"location":"Glossary/#gerriss","title":"Gerris's:","text":"<pre><code>Gerris is a Free Software program for the solution of the partial differential equations describing fluid flow. This module also includes GfsView, a visualisation application for Gerris output.\n</code></pre>"},{"location":"Glossary/#gerris","title":"Gerris:","text":"<pre><code>Gerris is a Free Software program for the solution of the partial differential equations describing fluid flow. This module also includes GfsView, a visualisation application for Gerris output.\n</code></pre>"},{"location":"Glossary/#getorganelles","title":"GetOrganelle's:","text":"<pre><code>Toolkit to assemble organelle genome from genomic skimming data.\n</code></pre>"},{"location":"Glossary/#getorganelle","title":"GetOrganelle:","text":"<pre><code>Toolkit to assemble organelle genome from genomic skimming data.\n</code></pre>"},{"location":"Glossary/#glimmerhmms","title":"GlimmerHMM's:","text":"<pre><code>Gene finder based on a Generalized Hidden Markov Model.\n</code></pre>"},{"location":"Glossary/#glimmerhmm","title":"GlimmerHMM:","text":"<pre><code>Gene finder based on a Generalized Hidden Markov Model.\n</code></pre>"},{"location":"Glossary/#gos","title":"Go's:","text":"<pre><code>An open source programming language\n</code></pre>"},{"location":"Glossary/#go","title":"Go:","text":"<pre><code>An open source programming language\n</code></pre>"},{"location":"Glossary/#graphvizs","title":"Graphviz's:","text":"<pre><code>Graphviz is open source graph visualization software. Graph visualization\n</code></pre>"},{"location":"Glossary/#graphviz","title":"Graphviz:","text":"<pre><code>Graphviz is open source graph visualization software. Graph visualization\n</code></pre>"},{"location":"Glossary/#gubbinss","title":"Gubbins's:","text":"<pre><code>Genealogies Unbiased By recomBinations In Nucleotide Sequences\n</code></pre>"},{"location":"Glossary/#gubbins","title":"Gubbins:","text":"<pre><code>Genealogies Unbiased By recomBinations In Nucleotide Sequences\n</code></pre>"},{"location":"Glossary/#guiles","title":"Guile's:","text":"<pre><code>Guile is the GNU Ubiquitous Intelligent Language for Extensions,\n</code></pre>"},{"location":"Glossary/#guile","title":"Guile:","text":"<pre><code>Guile is the GNU Ubiquitous Intelligent Language for Extensions,\n</code></pre>"},{"location":"Glossary/#hdfs","title":"HDF's:","text":"<pre><code>HDF (also known as HDF4) is a library and multi-object file format for  storing and managing data between machines.\n</code></pre>"},{"location":"Glossary/#hdf-eoss","title":"HDF-EOS's:","text":"<pre><code>HDF-EOS (Hierarchical Data Format - Earth Observing System) is a self-describing file format for transfer of various types of data between different machines based upon HDF. HDF-EOS is a standard format to store data collected from EOS satellites: Terra, Aqua and Aura.\n</code></pre>"},{"location":"Glossary/#hdf-eos","title":"HDF-EOS:","text":"<pre><code>HDF-EOS (Hierarchical Data Format - Earth Observing System) is a self-describing file format for transfer of various types of data between different machines based upon HDF. HDF-EOS is a standard format to store data collected from EOS satellites: Terra, Aqua and Aura.\n</code></pre>"},{"location":"Glossary/#hdf-eos5s","title":"HDF-EOS5's:","text":"<pre><code>HDF-EOS (Hierarchical Data Format - Earth Observing System) is a self-describing file format for transfer of various types of data between different machines based upon HDF. HDF-EOS is a standard format to store data collected from EOS satellites: Terra, Aqua and Aura.\n</code></pre>"},{"location":"Glossary/#hdf-eos5","title":"HDF-EOS5:","text":"<pre><code>HDF-EOS (Hierarchical Data Format - Earth Observing System) is a self-describing file format for transfer of various types of data between different machines based upon HDF. HDF-EOS is a standard format to store data collected from EOS satellites: Terra, Aqua and Aura.\n</code></pre>"},{"location":"Glossary/#hdf5s","title":"HDF5's:","text":"<pre><code>HDF5 is a unique technology suite that makes possible the management of\n</code></pre>"},{"location":"Glossary/#hdf5","title":"HDF5:","text":"<pre><code>HDF5 is a unique technology suite that makes possible the management of\n</code></pre>"},{"location":"Glossary/#hisat2s","title":"HISAT2's:","text":"<pre><code>HISAT2 is a fast and sensitive alignment program for mapping next-generation sequencing reads\n</code></pre>"},{"location":"Glossary/#hisat2","title":"HISAT2:","text":"<pre><code>HISAT2 is a fast and sensitive alignment program for mapping next-generation sequencing reads\n</code></pre>"},{"location":"Glossary/#hmmers","title":"HMMER's:","text":"<pre><code>HMMER is used for searching sequence databases for homologs of protein sequences,\n</code></pre>"},{"location":"Glossary/#hmmer","title":"HMMER:","text":"<pre><code>HMMER is used for searching sequence databases for homologs of protein sequences,\n</code></pre>"},{"location":"Glossary/#hmmer2s","title":"HMMER2's:","text":"<pre><code>HMMER is used for searching sequence databases for homologs of protein sequences,\n</code></pre>"},{"location":"Glossary/#hmmer2","title":"HMMER2:","text":"<pre><code>HMMER is used for searching sequence databases for homologs of protein sequences,\n</code></pre>"},{"location":"Glossary/#hopss","title":"HOPS's:","text":"<pre><code>Pipeline which focuses on screening MALT data for the presence of a user-specified list of target species.\n</code></pre>"},{"location":"Glossary/#hops","title":"HOPS:","text":"<pre><code>Pipeline which focuses on screening MALT data for the presence of a user-specified list of target species.\n</code></pre>"},{"location":"Glossary/#htseqs","title":"HTSeq's:","text":"<pre><code>HTSeq is a Python library to facilitate processing and analysis\n</code></pre>"},{"location":"Glossary/#htseq","title":"HTSeq:","text":"<pre><code>HTSeq is a Python library to facilitate processing and analysis\n</code></pre>"},{"location":"Glossary/#htslibs","title":"HTSlib's:","text":"<pre><code>A C library for reading/writing high-throughput sequencing data.\n</code></pre>"},{"location":"Glossary/#htslib","title":"HTSlib:","text":"<pre><code>A C library for reading/writing high-throughput sequencing data.\n</code></pre>"},{"location":"Glossary/#harfbuzzs","title":"HarfBuzz's:","text":"<pre><code>HarfBuzz is an OpenType text shaping engine.\n</code></pre>"},{"location":"Glossary/#harfbuzz","title":"HarfBuzz:","text":"<pre><code>HarfBuzz is an OpenType text shaping engine.\n</code></pre>"},{"location":"Glossary/#hpcgridrunners","title":"HpcGridRunner's:","text":"<pre><code>HPC GridRunner is a simple command-line interface to high throughput computing using a variety of different grid computing platforms, including LSF, SGE, SLURM, and PBS.\n</code></pre>"},{"location":"Glossary/#hpcgridrunner","title":"HpcGridRunner:","text":"<pre><code>HPC GridRunner is a simple command-line interface to high throughput computing using a variety of different grid computing platforms, including LSF, SGE, SLURM, and PBS.\n</code></pre>"},{"location":"Glossary/#humanns","title":"Humann's:","text":"<pre><code>Pipeline for efficiently and accurately determining the coverage and abundance of microbial pathways in a community from metagenomic data.\n</code></pre>"},{"location":"Glossary/#humann","title":"Humann:","text":"<pre><code>Pipeline for efficiently and accurately determining the coverage and abundance of microbial pathways in a community from metagenomic data.\n</code></pre>"},{"location":"Glossary/#hybpipers","title":"HybPiper's:","text":"<pre><code>Extracting Coding Sequence and Introns for Phylogenetics from High-Throughput Sequencing Reads Using Target Enrichment.\n</code></pre>"},{"location":"Glossary/#hybpiper","title":"HybPiper:","text":"<pre><code>Extracting Coding Sequence and Introns for Phylogenetics from High-Throughput Sequencing Reads Using Target Enrichment.\n</code></pre>"},{"location":"Glossary/#hypres","title":"Hypre's:","text":"<pre><code>Hypre is a library for solving large, sparse linear systems of equations on massively\n</code></pre>"},{"location":"Glossary/#hypre","title":"Hypre:","text":"<pre><code>Hypre is a library for solving large, sparse linear systems of equations on massively\n</code></pre>"},{"location":"Glossary/#icus","title":"ICU's:","text":"<pre><code>ICU is a mature, widely used set of C/C++ and Java libraries providing Unicode and Globalization  support for software applications.\n</code></pre>"},{"location":"Glossary/#icu","title":"ICU:","text":"<pre><code>ICU is a mature, widely used set of C/C++ and Java libraries providing Unicode and Globalization  support for software applications.\n</code></pre>"},{"location":"Glossary/#idbas","title":"IDBA's:","text":"<pre><code>Iterative de Bruijn graph assembler for second-generation sequencing reads.\n</code></pre>"},{"location":"Glossary/#idba","title":"IDBA:","text":"<pre><code>Iterative de Bruijn graph assembler for second-generation sequencing reads.\n</code></pre>"},{"location":"Glossary/#idba-uds","title":"IDBA-UD's:","text":"<pre><code>IDBA-UD is a iterative De Bruijn Graph De Novo Assembler for Short Reads\n</code></pre>"},{"location":"Glossary/#idba-ud","title":"IDBA-UD:","text":"<pre><code>IDBA-UD is a iterative De Bruijn Graph De Novo Assembler for Short Reads\n</code></pre>"},{"location":"Glossary/#idls","title":"IDL's:","text":"<pre><code>IDL is the trusted scientific programming language used across disciplines to extract meaningful visualizations from complex numerical data.\n</code></pre>"},{"location":"Glossary/#idl","title":"IDL:","text":"<pre><code>IDL is the trusted scientific programming language used across disciplines to extract meaningful visualizations from complex numerical data.\n</code></pre>"},{"location":"Glossary/#igvs","title":"IGV's:","text":"<pre><code>The Integrative Genomics Viewer (IGV) is a high-performance visualization\n</code></pre>"},{"location":"Glossary/#igv","title":"IGV:","text":"<pre><code>The Integrative Genomics Viewer (IGV) is a high-performance visualization\n</code></pre>"},{"location":"Glossary/#imputes","title":"IMPUTE's:","text":"<pre><code>Genotype imputation and haplotype phasing.\n</code></pre>"},{"location":"Glossary/#impute","title":"IMPUTE:","text":"<pre><code>Genotype imputation and haplotype phasing.\n</code></pre>"},{"location":"Glossary/#iq-trees","title":"IQ-TREE's:","text":"<pre><code>Efficient phylogenomic software by maximum likelihood\n</code></pre>"},{"location":"Glossary/#iq-tree","title":"IQ-TREE:","text":"<pre><code>Efficient phylogenomic software by maximum likelihood\n</code></pre>"},{"location":"Glossary/#irkernels","title":"IRkernel's:","text":"<pre><code>R packages for providing R kernel for Jupyter.\n</code></pre>"},{"location":"Glossary/#irkernel","title":"IRkernel:","text":"<pre><code>R packages for providing R kernel for Jupyter.\n</code></pre>"},{"location":"Glossary/#isa-ls","title":"ISA-L's:","text":"<pre><code>Intelligent Storage Acceleration Library\n</code></pre>"},{"location":"Glossary/#isa-l","title":"ISA-L:","text":"<pre><code>Intelligent Storage Acceleration Library\n</code></pre>"},{"location":"Glossary/#imagemagicks","title":"ImageMagick's:","text":"<pre><code>ImageMagick is a software suite to create, edit, compose, or convert bitmap images\n</code></pre>"},{"location":"Glossary/#imagemagick","title":"ImageMagick:","text":"<pre><code>ImageMagick is a software suite to create, edit, compose, or convert bitmap images\n</code></pre>"},{"location":"Glossary/#infernals","title":"Infernal's:","text":"<pre><code>Infernal ('INFERence of RNA ALignment') is for searching DNA sequence databases\n</code></pre>"},{"location":"Glossary/#infernal","title":"Infernal:","text":"<pre><code>Infernal ('INFERence of RNA ALignment') is for searching DNA sequence databases\n</code></pre>"},{"location":"Glossary/#inspectors","title":"Inspector's:","text":"<pre><code>Intel Inspector XE is an easy to use memory error checker and thread checker for serial\n</code></pre>"},{"location":"Glossary/#inspector","title":"Inspector:","text":"<pre><code>Intel Inspector XE is an easy to use memory error checker and thread checker for serial\n</code></pre>"},{"location":"Glossary/#interproscans","title":"InterProScan's:","text":"<pre><code>Sequence analysis application (nucleotide and protein sequences) that combines\n</code></pre>"},{"location":"Glossary/#interproscan","title":"InterProScan:","text":"<pre><code>Sequence analysis application (nucleotide and protein sequences) that combines\n</code></pre>"},{"location":"Glossary/#jagss","title":"JAGS's:","text":"<pre><code>Just Another Gibbs Sampler - a program for the statistical analysis of Bayesian hierarchical models by Markov Chain Monte Carlo.\n</code></pre>"},{"location":"Glossary/#jags","title":"JAGS:","text":"<pre><code>Just Another Gibbs Sampler - a program for the statistical analysis of Bayesian hierarchical models by Markov Chain Monte Carlo.\n</code></pre>"},{"location":"Glossary/#junits","title":"JUnit's:","text":"<pre><code>A programmer-oriented testing framework for Java.\n</code></pre>"},{"location":"Glossary/#junit","title":"JUnit:","text":"<pre><code>A programmer-oriented testing framework for Java.\n</code></pre>"},{"location":"Glossary/#jaspers","title":"JasPer's:","text":"<pre><code>The JasPer Project is an open-source initiative to provide a free  software-based reference implementation of the codec specified in the JPEG-2000 Part-1 standard.\n</code></pre>"},{"location":"Glossary/#jasper","title":"JasPer:","text":"<pre><code>The JasPer Project is an open-source initiative to provide a free  software-based reference implementation of the codec specified in the JPEG-2000 Part-1 standard.\n</code></pre>"},{"location":"Glossary/#javas","title":"Java's:","text":"<pre><code>Java Platform, Standard Edition (Java SE) lets you develop and deploy  Java applications on desktops and servers.\n</code></pre>"},{"location":"Glossary/#java","title":"Java:","text":"<pre><code>Java Platform, Standard Edition (Java SE) lets you develop and deploy  Java applications on desktops and servers.\n</code></pre>"},{"location":"Glossary/#jellyfishs","title":"Jellyfish's:","text":"<pre><code>Jellyfish is a tool for fast, memory-efficient counting of k-mers in DNA.\n</code></pre>"},{"location":"Glossary/#jellyfish","title":"Jellyfish:","text":"<pre><code>Jellyfish is a tool for fast, memory-efficient counting of k-mers in DNA.\n</code></pre>"},{"location":"Glossary/#jsoncpps","title":"JsonCpp's:","text":"<pre><code>JsonCpp is a C++ library that allows manipulating JSON values,\n</code></pre>"},{"location":"Glossary/#jsoncpp","title":"JsonCpp:","text":"<pre><code>JsonCpp is a C++ library that allows manipulating JSON values,\n</code></pre>"},{"location":"Glossary/#julias","title":"Julia's:","text":"<pre><code>A high-level, high-performance dynamic language for technical computing.  This version was compiled from source with USE_INTEL_JITEVENTS=1 to enable profiling with VTune.\n</code></pre>"},{"location":"Glossary/#julia","title":"Julia:","text":"<pre><code>A high-level, high-performance dynamic language for technical computing.  This version was compiled from source with USE_INTEL_JITEVENTS=1 to enable profiling with VTune.\n</code></pre>"},{"location":"Glossary/#jupyterlabs","title":"JupyterLab's:","text":"<pre><code>An extensible environment for interactive and reproducible computing, based on the Jupyter Notebook and Architecture.\n</code></pre>"},{"location":"Glossary/#jupyterlab","title":"JupyterLab:","text":"<pre><code>An extensible environment for interactive and reproducible computing, based on the Jupyter Notebook and Architecture.\n</code></pre>"},{"location":"Glossary/#kats","title":"KAT's:","text":"<pre><code>The K-mer Analysis Toolkit (KAT) contains a number of tools that analyse and compare K-mer spectra.\n</code></pre>"},{"location":"Glossary/#kat","title":"KAT:","text":"<pre><code>The K-mer Analysis Toolkit (KAT) contains a number of tools that analyse and compare K-mer spectra.\n</code></pre>"},{"location":"Glossary/#kealibs","title":"KEALib's:","text":"<pre><code>KEALib provides an implementation of the GDAL data model. The format supports raster attribute tables, image pyramids, meta-data and in-built statistics while also handling very large files and compression throughout. Based on the HDF5 standard, it also provides a base from which other formats can be derived and is a good choice for long term data archiving. An independent software library (libkea) provides complete access to the KEA image format and a GDAL driver allowing KEA images to be used from any GDAL supported software.\n</code></pre>"},{"location":"Glossary/#kealib","title":"KEALib:","text":"<pre><code>KEALib provides an implementation of the GDAL data model. The format supports raster attribute tables, image pyramids, meta-data and in-built statistics while also handling very large files and compression throughout. Based on the HDF5 standard, it also provides a base from which other formats can be derived and is a good choice for long term data archiving. An independent software library (libkea) provides complete access to the KEA image format and a GDAL driver allowing KEA images to be used from any GDAL supported software.\n</code></pre>"},{"location":"Glossary/#kmcs","title":"KMC's:","text":"<pre><code>Disk-based programm for counting k-mers from (possibly gzipped) FASTQ/FASTA files.\n</code></pre>"},{"location":"Glossary/#kmc","title":"KMC:","text":"<pre><code>Disk-based programm for counting k-mers from (possibly gzipped) FASTQ/FASTA files.\n</code></pre>"},{"location":"Glossary/#kaijus","title":"Kaiju's:","text":"<pre><code>Kaiju is a program for sensitive taxonomic classification of high-throughput\n</code></pre>"},{"location":"Glossary/#kaiju","title":"Kaiju:","text":"<pre><code>Kaiju is a program for sensitive taxonomic classification of high-throughput\n</code></pre>"},{"location":"Glossary/#kent_toolss","title":"Kent_tools's:","text":"<pre><code>Collection of tools used by the UCSC genome browser.\n</code></pre>"},{"location":"Glossary/#kent_tools","title":"Kent_tools:","text":"<pre><code>Collection of tools used by the UCSC genome browser.\n</code></pre>"},{"location":"Glossary/#korfsnaps","title":"KorfSNAP's:","text":"<pre><code>Semi-HMM-based Nucleic Acid Parser\n</code></pre>"},{"location":"Glossary/#korfsnap","title":"KorfSNAP:","text":"<pre><code>Semi-HMM-based Nucleic Acid Parser\n</code></pre>"},{"location":"Glossary/#kraken2s","title":"Kraken2's:","text":"<pre><code>Taxonomic sequence classifier.\n</code></pre>"},{"location":"Glossary/#kraken2","title":"Kraken2:","text":"<pre><code>Taxonomic sequence classifier.\n</code></pre>"},{"location":"Glossary/#kronatoolss","title":"KronaTools's:","text":"<pre><code>Krona Tools is a set of scripts to create Krona charts from\n</code></pre>"},{"location":"Glossary/#kronatools","title":"KronaTools:","text":"<pre><code>Krona Tools is a set of scripts to create Krona charts from\n</code></pre>"},{"location":"Glossary/#kyotocabinets","title":"KyotoCabinet's:","text":"<pre><code>Library of routines for managing a database.\n</code></pre>"},{"location":"Glossary/#kyotocabinet","title":"KyotoCabinet:","text":"<pre><code>Library of routines for managing a database.\n</code></pre>"},{"location":"Glossary/#lames","title":"LAME's:","text":"<pre><code>LAME is a high quality MPEG Audio Layer III (MP3) encoder licensed under the LGPL.\n</code></pre>"},{"location":"Glossary/#lame","title":"LAME:","text":"<pre><code>LAME is a high quality MPEG Audio Layer III (MP3) encoder licensed under the LGPL.\n</code></pre>"},{"location":"Glossary/#lammpss","title":"LAMMPS's:","text":"<pre><code>LAMMPS is a classical molecular dynamics code, and an acronym for Large-scale Atomic/Molecular Massively Parallel Simulator. LAMMPS has potentials for solid-state materials (metals, semiconductors) and soft matter (biomolecules, polymers) and coarse-grained or mesoscopic systems. It can be used to model atoms or, more generically, as a parallel particle simulator at the atomic, meso, or continuum scale. LAMMPS runs on single processors or in parallel using message-passing techniques and a spatial-decomposition of the simulation domain. The code is designed to be easy to modify or extend with new functionality.\n</code></pre>"},{"location":"Glossary/#lammps","title":"LAMMPS:","text":"<pre><code>LAMMPS is a classical molecular dynamics code, and an acronym for Large-scale Atomic/Molecular Massively Parallel Simulator. LAMMPS has potentials for solid-state materials (metals, semiconductors) and soft matter (biomolecules, polymers) and coarse-grained or mesoscopic systems. It can be used to model atoms or, more generically, as a parallel particle simulator at the atomic, meso, or continuum scale. LAMMPS runs on single processors or in parallel using message-passing techniques and a spatial-decomposition of the simulation domain. The code is designed to be easy to modify or extend with new functionality.\n</code></pre>"},{"location":"Glossary/#lasts","title":"LAST's:","text":"<pre><code>LAST finds similar regions between sequences.\n</code></pre>"},{"location":"Glossary/#last","title":"LAST:","text":"<pre><code>LAST finds similar regions between sequences.\n</code></pre>"},{"location":"Glossary/#lastzs","title":"LASTZ's:","text":"<pre><code>LASTZ is a program for aligning DNA sequences, a pairwise aligner. Originally designed to\n</code></pre>"},{"location":"Glossary/#lastz","title":"LASTZ:","text":"<pre><code>LASTZ is a program for aligning DNA sequences, a pairwise aligner. Originally designed to\n</code></pre>"},{"location":"Glossary/#ldcs","title":"LDC's:","text":"<pre><code>D programming language compiler\n</code></pre>"},{"location":"Glossary/#ldc","title":"LDC:","text":"<pre><code>D programming language compiler\n</code></pre>"},{"location":"Glossary/#lefses","title":"LEfSe's:","text":"<pre><code>Determines the features most likely to explain differences between classes by coupling standard tests for statistical significance\n</code></pre>"},{"location":"Glossary/#lefse","title":"LEfSe:","text":"<pre><code>Determines the features most likely to explain differences between classes by coupling standard tests for statistical significance\n</code></pre>"},{"location":"Glossary/#linkss","title":"LINKS's:","text":"<pre><code>Alignment-free scaffolding of genome assembly drafts with long reads\n</code></pre>"},{"location":"Glossary/#links","title":"LINKS:","text":"<pre><code>Alignment-free scaffolding of genome assembly drafts with long reads\n</code></pre>"},{"location":"Glossary/#llvms","title":"LLVM's:","text":"<pre><code>The LLVM Core libraries provide a modern source- and target-independent  optimizer, along with code generation support for many popular CPUs  (as well as some less common ones!) These libraries are built around a well  specified code representation known as the LLVM intermediate representation  (\"LLVM IR\"). The LLVM Core libraries are well documented, and it is  particularly easy to invent your own language (or port an existing compiler)  to use LLVM as an optimizer and code generator.   This build includes the clang C/C++ compiler frontend.\n</code></pre>"},{"location":"Glossary/#llvm","title":"LLVM:","text":"<pre><code>The LLVM Core libraries provide a modern source- and target-independent  optimizer, along with code generation support for many popular CPUs  (as well as some less common ones!) These libraries are built around a well  specified code representation known as the LLVM intermediate representation  (\"LLVM IR\"). The LLVM Core libraries are well documented, and it is  particularly easy to invent your own language (or port an existing compiler)  to use LLVM as an optimizer and code generator.   This build includes the clang C/C++ compiler frontend.\n</code></pre>"},{"location":"Glossary/#lmdbs","title":"LMDB's:","text":"<pre><code>LMDB is a fast, memory-efficient database. With memory-mapped files, it has the read performance\n</code></pre>"},{"location":"Glossary/#lmdb","title":"LMDB:","text":"<pre><code>LMDB is a fast, memory-efficient database. With memory-mapped files, it has the read performance\n</code></pre>"},{"location":"Glossary/#lsd2s","title":"LSD2's:","text":"<pre><code>Least-squares methods to estimate rates and dates from phylogenies\n</code></pre>"},{"location":"Glossary/#lsd2","title":"LSD2:","text":"<pre><code>Least-squares methods to estimate rates and dates from phylogenies\n</code></pre>"},{"location":"Glossary/#ltr_retrievers","title":"LTR_retriever's:","text":"<pre><code>Highly accurate and sensitive program for identification of LTR retrotransposons; The LTR Assembly Index (LAI) is also included in this package.\n</code></pre>"},{"location":"Glossary/#ltr_retriever","title":"LTR_retriever:","text":"<pre><code>Highly accurate and sensitive program for identification of LTR retrotransposons; The LTR Assembly Index (LAI) is also included in this package.\n</code></pre>"},{"location":"Glossary/#lumpys","title":"LUMPY's:","text":"<pre><code>A probabilistic framework for structural variant discovery.\n</code></pre>"},{"location":"Glossary/#lumpy","title":"LUMPY:","text":"<pre><code>A probabilistic framework for structural variant discovery.\n</code></pre>"},{"location":"Glossary/#lzos","title":"LZO's:","text":"<pre><code>Portable lossless data compression library\n</code></pre>"},{"location":"Glossary/#lzo","title":"LZO:","text":"<pre><code>Portable lossless data compression library\n</code></pre>"},{"location":"Glossary/#libtiffs","title":"LibTIFF's:","text":"<pre><code>tiff: Library and tools for reading and writing TIFF data files\n</code></pre>"},{"location":"Glossary/#libtiff","title":"LibTIFF:","text":"<pre><code>tiff: Library and tools for reading and writing TIFF data files\n</code></pre>"},{"location":"Glossary/#libavs","title":"Libav's:","text":"<pre><code>Libraries for dealing with multimedia formats of all sorts.  Forked from FFmpeg\n</code></pre>"},{"location":"Glossary/#libav","title":"Libav:","text":"<pre><code>Libraries for dealing with multimedia formats of all sorts.  Forked from FFmpeg\n</code></pre>"},{"location":"Glossary/#libints","title":"Libint's:","text":"<pre><code>Libint library is used to evaluate the traditional (electron repulsion) and certain novel two-body  matrix elements (integrals) over Cartesian Gaussian functions used in modern atomic and molecular theory.\n</code></pre>"},{"location":"Glossary/#libint","title":"Libint:","text":"<pre><code>Libint library is used to evaluate the traditional (electron repulsion) and certain novel two-body  matrix elements (integrals) over Cartesian Gaussian functions used in modern atomic and molecular theory.\n</code></pre>"},{"location":"Glossary/#littlecmss","title":"LittleCMS's:","text":"<pre><code>Color management engine.\n</code></pre>"},{"location":"Glossary/#littlecms","title":"LittleCMS:","text":"<pre><code>Color management engine.\n</code></pre>"},{"location":"Glossary/#lokis","title":"Loki's:","text":"<pre><code>Loki is a C++ library of designs, containing flexible implementations of common design patterns and\n</code></pre>"},{"location":"Glossary/#loki","title":"Loki:","text":"<pre><code>Loki is a C++ library of designs, containing flexible implementations of common design patterns and\n</code></pre>"},{"location":"Glossary/#longstitchs","title":"LongStitch's:","text":"<pre><code>A genome assembly correction and scaffolding pipeline using long reads\n</code></pre>"},{"location":"Glossary/#longstitch","title":"LongStitch:","text":"<pre><code>A genome assembly correction and scaffolding pipeline using long reads\n</code></pre>"},{"location":"Glossary/#m4s","title":"M4's:","text":"<pre><code>GNU M4 is an implementation of the traditional Unix macro processor. It is mostly SVR4 compatible\n</code></pre>"},{"location":"Glossary/#m4","title":"M4:","text":"<pre><code>GNU M4 is an implementation of the traditional Unix macro processor. It is mostly SVR4 compatible\n</code></pre>"},{"location":"Glossary/#maffts","title":"MAFFT's:","text":"<pre><code>Multiple sequence alignment program offering a range of methods.\n</code></pre>"},{"location":"Glossary/#mafft","title":"MAFFT:","text":"<pre><code>Multiple sequence alignment program offering a range of methods.\n</code></pre>"},{"location":"Glossary/#magmas","title":"MAGMA's:","text":"<pre><code>Tool for gene analysis and generalized gene-set analysis of GWAS data.\n</code></pre>"},{"location":"Glossary/#magma","title":"MAGMA:","text":"<pre><code>Tool for gene analysis and generalized gene-set analysis of GWAS data.\n</code></pre>"},{"location":"Glossary/#makers","title":"MAKER's:","text":"<pre><code>Genome annotation pipeline\n</code></pre>"},{"location":"Glossary/#maker","title":"MAKER:","text":"<pre><code>Genome annotation pipeline\n</code></pre>"},{"location":"Glossary/#matios","title":"MATIO's:","text":"<pre><code>matio is an C library for reading and writing Matlab MAT files.\n</code></pre>"},{"location":"Glossary/#matio","title":"MATIO:","text":"<pre><code>matio is an C library for reading and writing Matlab MAT files.\n</code></pre>"},{"location":"Glossary/#matlabs","title":"MATLAB's:","text":"<pre><code>A high-level language and interactive environment for numerical computing.\n</code></pre>"},{"location":"Glossary/#matlab","title":"MATLAB:","text":"<pre><code>A high-level language and interactive environment for numerical computing.\n</code></pre>"},{"location":"Glossary/#mcls","title":"MCL's:","text":"<pre><code>The MCL algorithm is short for the Markov Cluster Algorithm, a fast\n</code></pre>"},{"location":"Glossary/#mcl","title":"MCL:","text":"<pre><code>The MCL algorithm is short for the Markov Cluster Algorithm, a fast\n</code></pre>"},{"location":"Glossary/#mcrs","title":"MCR's:","text":"<pre><code>The Matlab Compiler Runtime is required for running compiled MATLAB executables without MATLAB itself.\n</code></pre>"},{"location":"Glossary/#mcr","title":"MCR:","text":"<pre><code>The Matlab Compiler Runtime is required for running compiled MATLAB executables without MATLAB itself.\n</code></pre>"},{"location":"Glossary/#megahits","title":"MEGAHIT's:","text":"<pre><code>An ultra-fast single-node solution for large and complex\n</code></pre>"},{"location":"Glossary/#megahit","title":"MEGAHIT:","text":"<pre><code>An ultra-fast single-node solution for large and complex\n</code></pre>"},{"location":"Glossary/#metabolics","title":"METABOLIC's:","text":"<pre><code>Metabolic And Biogeochemistry anaLyses In microbes\n</code></pre>"},{"location":"Glossary/#metabolic","title":"METABOLIC:","text":"<pre><code>Metabolic And Biogeochemistry anaLyses In microbes\n</code></pre>"},{"location":"Glossary/#metiss","title":"METIS's:","text":"<pre><code>METIS is a set of serial programs for partitioning graphs, partitioning finite element meshes,\n</code></pre>"},{"location":"Glossary/#metis","title":"METIS:","text":"<pre><code>METIS is a set of serial programs for partitioning graphs, partitioning finite element meshes,\n</code></pre>"},{"location":"Glossary/#mmseqs2s","title":"MMseqs2's:","text":"<pre><code>MMseqs2: ultra fast and sensitive search and clustering suite\n</code></pre>"},{"location":"Glossary/#mmseqs2","title":"MMseqs2:","text":"<pre><code>MMseqs2: ultra fast and sensitive search and clustering suite\n</code></pre>"},{"location":"Glossary/#mob-suites","title":"MOB-suite's:","text":"<pre><code>Software tools for clustering, reconstruction and typing of plasmids from draft assemblies.\n</code></pre>"},{"location":"Glossary/#mob-suite","title":"MOB-suite:","text":"<pre><code>Software tools for clustering, reconstruction and typing of plasmids from draft assemblies.\n</code></pre>"},{"location":"Glossary/#modflows","title":"MODFLOW's:","text":"<pre><code>MODFLOW is the U.S. Geological Survey modular finite-difference flow model, which is a computer code that solves the groundwater flow equation. The program is used by hydrogeologists to simulate the flow of groundwater through aquifers.\n</code></pre>"},{"location":"Glossary/#modflow","title":"MODFLOW:","text":"<pre><code>MODFLOW is the U.S. Geological Survey modular finite-difference flow model, which is a computer code that solves the groundwater flow equation. The program is used by hydrogeologists to simulate the flow of groundwater through aquifers.\n</code></pre>"},{"location":"Glossary/#mpfrs","title":"MPFR's:","text":"<pre><code>The MPFR library is a C library for multiple-precision\n</code></pre>"},{"location":"Glossary/#mpfr","title":"MPFR:","text":"<pre><code>The MPFR library is a C library for multiple-precision\n</code></pre>"},{"location":"Glossary/#msmcs","title":"MSMC's:","text":"<pre><code>Multiple Sequentially Markovian Coalescent, infers population size and gene flow from multiple genome sequences\n</code></pre>"},{"location":"Glossary/#msmc","title":"MSMC:","text":"<pre><code>Multiple Sequentially Markovian Coalescent, infers population size and gene flow from multiple genome sequences\n</code></pre>"},{"location":"Glossary/#mumpss","title":"MUMPS's:","text":"<pre><code>A parallel sparse direct solver\n</code></pre>"},{"location":"Glossary/#mumps","title":"MUMPS:","text":"<pre><code>A parallel sparse direct solver\n</code></pre>"},{"location":"Glossary/#mummers","title":"MUMmer's:","text":"<pre><code>MUMmer is a system for rapidly aligning entire genomes,\n</code></pre>"},{"location":"Glossary/#mummer","title":"MUMmer:","text":"<pre><code>MUMmer is a system for rapidly aligning entire genomes,\n</code></pre>"},{"location":"Glossary/#muscles","title":"MUSCLE's:","text":"<pre><code>MUSCLE is a program for creating multiple alignments of amino acid or nucleotide\n</code></pre>"},{"location":"Glossary/#muscle","title":"MUSCLE:","text":"<pre><code>MUSCLE is a program for creating multiple alignments of amino acid or nucleotide\n</code></pre>"},{"location":"Glossary/#musts","title":"MUST's:","text":"<pre><code>MUST detects usage errors of the Message Passing Interface (MPI) and reports them to the user.\n</code></pre>"},{"location":"Glossary/#must","title":"MUST:","text":"<pre><code>MUST detects usage errors of the Message Passing Interface (MPI) and reports them to the user.\n</code></pre>"},{"location":"Glossary/#masurcas","title":"MaSuRCA's:","text":"<pre><code>MaSuRCA is whole genome assembly software. It combines the efficiency of the de Bruijn graph\n</code></pre>"},{"location":"Glossary/#masurca","title":"MaSuRCA:","text":"<pre><code>MaSuRCA is whole genome assembly software. It combines the efficiency of the de Bruijn graph\n</code></pre>"},{"location":"Glossary/#magmas_1","title":"Magma's:","text":"<pre><code>Magma is a large, well-supported software package designed for computations in algebra, number theory, algebraic geometry and algebraic combinatorics. It provides a mathematically rigorous environment     for defining and working with structures such as groups, rings, fields, modules, algebras, schemes, curves, graphs, designs, codes and many others. Magma also supports a number of databases designed     to aid computational research in those areas of mathematics which are algebraic in nature.\n</code></pre>"},{"location":"Glossary/#magma_1","title":"Magma:","text":"<pre><code>Magma is a large, well-supported software package designed for computations in algebra, number theory, algebraic geometry and algebraic combinatorics. It provides a mathematically rigorous environment     for defining and working with structures such as groups, rings, fields, modules, algebras, schemes, curves, graphs, designs, codes and many others. Magma also supports a number of databases designed     to aid computational research in those areas of mathematics which are algebraic in nature.\n</code></pre>"},{"location":"Glossary/#markerminers","title":"MarkerMiner's:","text":"<pre><code>Workflow for effective discovery of SCN loci in flowering plants angiosperms\n</code></pre>"},{"location":"Glossary/#markerminer","title":"MarkerMiner:","text":"<pre><code>Workflow for effective discovery of SCN loci in flowering plants angiosperms\n</code></pre>"},{"location":"Glossary/#mashs","title":"Mash's:","text":"<pre><code>Fast genome and metagenome distance estimation using MinHash\n</code></pre>"},{"location":"Glossary/#mash","title":"Mash:","text":"<pre><code>Fast genome and metagenome distance estimation using MinHash\n</code></pre>"},{"location":"Glossary/#mavens","title":"Maven's:","text":"<pre><code>Binary maven install, Apache Maven is a software project management and comprehension tool. Based on\n</code></pre>"},{"location":"Glossary/#maven","title":"Maven:","text":"<pre><code>Binary maven install, Apache Maven is a software project management and comprehension tool. Based on\n</code></pre>"},{"location":"Glossary/#maxbins","title":"MaxBin's:","text":"<pre><code>MaxBin is software for binning assembled metagenomic sequences based on\n</code></pre>"},{"location":"Glossary/#maxbin","title":"MaxBin:","text":"<pre><code>MaxBin is software for binning assembled metagenomic sequences based on\n</code></pre>"},{"location":"Glossary/#meraculouss","title":"Meraculous's:","text":"<pre><code>Eukaryotic genome assembler for Illumina sequence data.\n</code></pre>"},{"location":"Glossary/#meraculous","title":"Meraculous:","text":"<pre><code>Eukaryotic genome assembler for Illumina sequence data.\n</code></pre>"},{"location":"Glossary/#mesas","title":"Mesa's:","text":"<pre><code>Mesa is an open-source implementation of the OpenGL specification -  a system for rendering interactive 3D graphics.   Note that this build enables CPU-based rendering with OpenSWR and LLVM.  The module is intended to be used with visualisation software, such as  ParaView, on nodes where no GPU hardware is available.   Both on-screen and off-screen rendering are supported.  IMPORTANT: The OpenSWR software rasteriser can use multiple threads for            best performance. The number of threads is controlled by the            environment variable KNOB_MAX_WORKER_THREADS. The module sets             KNOB_MAX_WORKER_THREADS=1             by default to avoid accidental oversubscription of nodes.\n</code></pre>"},{"location":"Glossary/#mesa","title":"Mesa:","text":"<pre><code>Mesa is an open-source implementation of the OpenGL specification -  a system for rendering interactive 3D graphics.   Note that this build enables CPU-based rendering with OpenSWR and LLVM.  The module is intended to be used with visualisation software, such as  ParaView, on nodes where no GPU hardware is available.   Both on-screen and off-screen rendering are supported.  IMPORTANT: The OpenSWR software rasteriser can use multiple threads for            best performance. The number of threads is controlled by the            environment variable KNOB_MAX_WORKER_THREADS. The module sets             KNOB_MAX_WORKER_THREADS=1             by default to avoid accidental oversubscription of nodes.\n</code></pre>"},{"location":"Glossary/#mesons","title":"Meson's:","text":"<pre><code>Meson is a cross-platform build system designed to be both as fast and as user friendly as possible.\n</code></pre>"},{"location":"Glossary/#meson","title":"Meson:","text":"<pre><code>Meson is a cross-platform build system designed to be both as fast and as user friendly as possible.\n</code></pre>"},{"location":"Glossary/#metabats","title":"MetaBAT's:","text":"<pre><code>An efficient tool for accurately reconstructing single genomes from complex microbial communities\n</code></pre>"},{"location":"Glossary/#metabat","title":"MetaBAT:","text":"<pre><code>An efficient tool for accurately reconstructing single genomes from complex microbial communities\n</code></pre>"},{"location":"Glossary/#metaeuks","title":"MetaEuk's:","text":"<pre><code>MetaEuk is a modular toolkit designed for large-scale gene discovery and annotation in eukaryotic\n</code></pre>"},{"location":"Glossary/#metaeuk","title":"MetaEuk:","text":"<pre><code>MetaEuk is a modular toolkit designed for large-scale gene discovery and annotation in eukaryotic\n</code></pre>"},{"location":"Glossary/#metageneannotators","title":"MetaGeneAnnotator's:","text":"<pre><code>MetaGeneAnnotator is a gene-finding program for prokaryote and phage.\n</code></pre>"},{"location":"Glossary/#metageneannotator","title":"MetaGeneAnnotator:","text":"<pre><code>MetaGeneAnnotator is a gene-finding program for prokaryote and phage.\n</code></pre>"},{"location":"Glossary/#metaphlans","title":"MetaPhlAn's:","text":"<pre><code>MetaPhlAn is a computational tool for profiling the composition of microbial\n</code></pre>"},{"location":"Glossary/#metaphlan","title":"MetaPhlAn:","text":"<pre><code>MetaPhlAn is a computational tool for profiling the composition of microbial\n</code></pre>"},{"location":"Glossary/#metaphlan2s","title":"MetaPhlAn2's:","text":"<pre><code>MetaPhlAn is a computational tool for profiling the composition of microbial\n</code></pre>"},{"location":"Glossary/#metaphlan2","title":"MetaPhlAn2:","text":"<pre><code>MetaPhlAn is a computational tool for profiling the composition of microbial\n</code></pre>"},{"location":"Glossary/#metasvs","title":"MetaSV's:","text":"<pre><code>Structural-variant caller\n</code></pre>"},{"location":"Glossary/#metasv","title":"MetaSV:","text":"<pre><code>Structural-variant caller\n</code></pre>"},{"location":"Glossary/#metavelvets","title":"MetaVelvet's:","text":"<pre><code>A short read assember for metagenomics\n</code></pre>"},{"location":"Glossary/#metavelvet","title":"MetaVelvet:","text":"<pre><code>A short read assember for metagenomics\n</code></pre>"},{"location":"Glossary/#metaxa2s","title":"Metaxa2's:","text":"<pre><code>Taxonomic classification of rRNA.\n</code></pre>"},{"location":"Glossary/#metaxa2","title":"Metaxa2:","text":"<pre><code>Taxonomic classification of rRNA.\n</code></pre>"},{"location":"Glossary/#miniconda3s","title":"Miniconda3's:","text":"<pre><code>A platform for Python-based data analytics\n</code></pre>"},{"location":"Glossary/#miniconda3","title":"Miniconda3:","text":"<pre><code>A platform for Python-based data analytics\n</code></pre>"},{"location":"Glossary/#minimac3s","title":"Minimac3's:","text":"<pre><code>Low memory and more computationally efficient implementation of the genotype imputation algorithms.\n</code></pre>"},{"location":"Glossary/#minimac3","title":"Minimac3:","text":"<pre><code>Low memory and more computationally efficient implementation of the genotype imputation algorithms.\n</code></pre>"},{"location":"Glossary/#mitozs","title":"MitoZ's:","text":"<pre><code>Toolkit which aims to automatically filter pair-end raw data,\n</code></pre>"},{"location":"Glossary/#mitoz","title":"MitoZ:","text":"<pre><code>Toolkit which aims to automatically filter pair-end raw data,\n</code></pre>"},{"location":"Glossary/#molcass","title":"Molcas's:","text":"<pre><code>Molcas is an ab initio quantum chemistry software package\n</code></pre>"},{"location":"Glossary/#molcas","title":"Molcas:","text":"<pre><code>Molcas is an ab initio quantum chemistry software package\n</code></pre>"},{"location":"Glossary/#molpros","title":"Molpro's:","text":"<pre><code>Molpro is a complete system of ab initio programs for molecular electronic structure calculations.\n</code></pre>"},{"location":"Glossary/#molpro","title":"Molpro:","text":"<pre><code>Molpro is a complete system of ab initio programs for molecular electronic structure calculations.\n</code></pre>"},{"location":"Glossary/#monos","title":"Mono's:","text":"<pre><code>An open source, cross-platform, implementation of C# and the CLR that is\n</code></pre>"},{"location":"Glossary/#mono","title":"Mono:","text":"<pre><code>An open source, cross-platform, implementation of C# and the CLR that is\n</code></pre>"},{"location":"Glossary/#monocle3s","title":"Monocle3's:","text":"<pre><code>An analysis toolkit for single-cell RNA-seq.\n</code></pre>"},{"location":"Glossary/#monocle3","title":"Monocle3:","text":"<pre><code>An analysis toolkit for single-cell RNA-seq.\n</code></pre>"},{"location":"Glossary/#mothurs","title":"Mothur's:","text":"<pre><code>Mothur is a single piece of open-source, expandable software\n</code></pre>"},{"location":"Glossary/#mothur","title":"Mothur:","text":"<pre><code>Mothur is a single piece of open-source, expandable software\n</code></pre>"},{"location":"Glossary/#motioncorrs","title":"MotionCorr's:","text":"<pre><code>Motion Correction for Dose-Fractionation Stack, by Dr. Xueming Li of the Cheng Laboratory at UCSF. The actual executable is named dosefgpu_driftcorr\n</code></pre>"},{"location":"Glossary/#motioncorr","title":"MotionCorr:","text":"<pre><code>Motion Correction for Dose-Fractionation Stack, by Dr. Xueming Li of the Cheng Laboratory at UCSF. The actual executable is named dosefgpu_driftcorr\n</code></pre>"},{"location":"Glossary/#mrbayess","title":"MrBayes's:","text":"<pre><code>MrBayes is a program for the Bayesian estimation of phylogeny.\n</code></pre>"},{"location":"Glossary/#mrbayes","title":"MrBayes:","text":"<pre><code>MrBayes is a program for the Bayesian estimation of phylogeny.\n</code></pre>"},{"location":"Glossary/#mules","title":"Mule's:","text":"<pre><code>Mule is an API written in Python which allows you to access and manipulate files produced by the UM (Unified Model, of the Met Office (UK)).\n</code></pre>"},{"location":"Glossary/#mule","title":"Mule:","text":"<pre><code>Mule is an API written in Python which allows you to access and manipulate files produced by the UM (Unified Model, of the Met Office (UK)).\n</code></pre>"},{"location":"Glossary/#multiqcs","title":"MultiQC's:","text":"<pre><code>Aggregate results from bioinformatics analyses across many samples into a single\n</code></pre>"},{"location":"Glossary/#multiqc","title":"MultiQC:","text":"<pre><code>Aggregate results from bioinformatics analyses across many samples into a single\n</code></pre>"},{"location":"Glossary/#namds","title":"NAMD's:","text":"<pre><code>NAMD is a parallel molecular dynamics code designed for high-performance simulation of\n</code></pre>"},{"location":"Glossary/#namd","title":"NAMD:","text":"<pre><code>NAMD is a parallel molecular dynamics code designed for high-performance simulation of\n</code></pre>"},{"location":"Glossary/#nasms","title":"NASM's:","text":"<pre><code>NASM: General-purpose x86 assembler\n</code></pre>"},{"location":"Glossary/#nasm","title":"NASM:","text":"<pre><code>NASM: General-purpose x86 assembler\n</code></pre>"},{"location":"Glossary/#ncargs","title":"NCARG's:","text":"<pre><code>NCAR Graphics is a Fortran and C based software package for\n</code></pre>"},{"location":"Glossary/#ncarg","title":"NCARG:","text":"<pre><code>NCAR Graphics is a Fortran and C based software package for\n</code></pre>"},{"location":"Glossary/#nccls","title":"NCCL's:","text":"<pre><code>The NVIDIA Collective Communications Library (NCCL) implements multi-GPU and multi-node collective\n</code></pre>"},{"location":"Glossary/#nccl","title":"NCCL:","text":"<pre><code>The NVIDIA Collective Communications Library (NCCL) implements multi-GPU and multi-node collective\n</code></pre>"},{"location":"Glossary/#ncls","title":"NCL's:","text":"<pre><code>NCL is an interpreted language designed specifically for scientific data analysis and   visualization.\n</code></pre>"},{"location":"Glossary/#ncl","title":"NCL:","text":"<pre><code>NCL is an interpreted language designed specifically for scientific data analysis and   visualization.\n</code></pre>"},{"location":"Glossary/#ncos","title":"NCO's:","text":"<pre><code>manipulates and analyzes data stored in netCDF-accessible formats, including DAP, HDF4, and HDF5\n</code></pre>"},{"location":"Glossary/#nco","title":"NCO:","text":"<pre><code>manipulates and analyzes data stored in netCDF-accessible formats, including DAP, HDF4, and HDF5\n</code></pre>"},{"location":"Glossary/#ncviews","title":"NCVIEW's:","text":"<pre><code>Ncview is a visual browser for netCDF format files. Typically you would use ncview to get a quick and easy, push-button look at your netCDF files. You can view simple movies of the data, view along various dimensions, take a look at the actual data values, change color maps, invert the data, etc.\n</code></pre>"},{"location":"Glossary/#ncview","title":"NCVIEW:","text":"<pre><code>Ncview is a visual browser for netCDF format files. Typically you would use ncview to get a quick and easy, push-button look at your netCDF files. You can view simple movies of the data, view along various dimensions, take a look at the actual data values, change color maps, invert the data, etc.\n</code></pre>"},{"location":"Glossary/#ngss","title":"NGS's:","text":"<pre><code>NGS is a new, domain-specific API for accessing reads, alignments and pileups produced from\n</code></pre>"},{"location":"Glossary/#ngs","title":"NGS:","text":"<pre><code>NGS is a new, domain-specific API for accessing reads, alignments and pileups produced from\n</code></pre>"},{"location":"Glossary/#nlopts","title":"NLopt's:","text":"<pre><code>NLopt is a free/open-source library for nonlinear optimization, providing a common interface for a number of different free optimization routines available online as well as original implementations of various other algorithms\n</code></pre>"},{"location":"Glossary/#nlopt","title":"NLopt:","text":"<pre><code>NLopt is a free/open-source library for nonlinear optimization, providing a common interface for a number of different free optimization routines available online as well as original implementations of various other algorithms\n</code></pre>"},{"location":"Glossary/#nsprs","title":"NSPR's:","text":"<pre><code>Netscape Portable Runtime (NSPR) provides a platform-neutral API for system level\n</code></pre>"},{"location":"Glossary/#nspr","title":"NSPR:","text":"<pre><code>Netscape Portable Runtime (NSPR) provides a platform-neutral API for system level\n</code></pre>"},{"location":"Glossary/#nsss","title":"NSS's:","text":"<pre><code>Network Security Services (NSS) is a set of libraries designed to support cross-platform development\n</code></pre>"},{"location":"Glossary/#nss","title":"NSS:","text":"<pre><code>Network Security Services (NSS) is a set of libraries designed to support cross-platform development\n</code></pre>"},{"location":"Glossary/#nvhpcs","title":"NVHPC's:","text":"<pre><code>C, C++ and Fortran compilers included with the NVIDIA HPC SDK (previously: PGI)\n</code></pre>"},{"location":"Glossary/#nvhpc","title":"NVHPC:","text":"<pre><code>C, C++ and Fortran compilers included with the NVIDIA HPC SDK (previously: PGI)\n</code></pre>"},{"location":"Glossary/#nwchems","title":"NWChem's:","text":"<pre><code>NWChem aims to provide its users with computational chemistry tools that are scalable both in\n</code></pre>"},{"location":"Glossary/#nwchem","title":"NWChem:","text":"<pre><code>NWChem aims to provide its users with computational chemistry tools that are scalable both in\n</code></pre>"},{"location":"Glossary/#nanocomps","title":"NanoComp's:","text":"<pre><code>Comparing runs of Oxford Nanopore sequencing data and alignments\n</code></pre>"},{"location":"Glossary/#nanocomp","title":"NanoComp:","text":"<pre><code>Comparing runs of Oxford Nanopore sequencing data and alignments\n</code></pre>"},{"location":"Glossary/#nanolyses","title":"NanoLyse's:","text":"<pre><code>Removing reads mapping to the lambda genome.\n</code></pre>"},{"location":"Glossary/#nanolyse","title":"NanoLyse:","text":"<pre><code>Removing reads mapping to the lambda genome.\n</code></pre>"},{"location":"Glossary/#nanoplots","title":"NanoPlot's:","text":"<pre><code>Plotting suite for Oxford Nanopore sequencing data and alignments.\n</code></pre>"},{"location":"Glossary/#nanoplot","title":"NanoPlot:","text":"<pre><code>Plotting suite for Oxford Nanopore sequencing data and alignments.\n</code></pre>"},{"location":"Glossary/#nanostats","title":"NanoStat's:","text":"<pre><code>Tool for phasing genomic variants using DNA sequencing reads, also called read-based phasing or haplotype assembly.\n</code></pre>"},{"location":"Glossary/#nanostat","title":"NanoStat:","text":"<pre><code>Tool for phasing genomic variants using DNA sequencing reads, also called read-based phasing or haplotype assembly.\n</code></pre>"},{"location":"Glossary/#newton-xs","title":"Newton-X's:","text":"<pre><code>NX is a general-purpose program package for simulating the dynamics of electronically excited molecules and molecular assemblies.\n</code></pre>"},{"location":"Glossary/#newton-x","title":"Newton-X:","text":"<pre><code>NX is a general-purpose program package for simulating the dynamics of electronically excited molecules and molecular assemblies.\n</code></pre>"},{"location":"Glossary/#nextgenmaps","title":"NextGenMap's:","text":"<pre><code>NextGenMap is a flexible highly sensitive short read mapping tool that\n</code></pre>"},{"location":"Glossary/#nextgenmap","title":"NextGenMap:","text":"<pre><code>NextGenMap is a flexible highly sensitive short read mapping tool that\n</code></pre>"},{"location":"Glossary/#nextflows","title":"Nextflow's:","text":"<pre><code>Nextflow is a reactive workflow framework and a programming DSL\n</code></pre>"},{"location":"Glossary/#nextflow","title":"Nextflow:","text":"<pre><code>Nextflow is a reactive workflow framework and a programming DSL\n</code></pre>"},{"location":"Glossary/#ninjas","title":"Ninja's:","text":"<pre><code>Ninja is a small build system with a focus on speed.\n</code></pre>"},{"location":"Glossary/#ninja","title":"Ninja:","text":"<pre><code>Ninja is a small build system with a focus on speed.\n</code></pre>"},{"location":"Glossary/#nsight-computes","title":"Nsight-Compute's:","text":"<pre><code>NVIDIA\u00ae Nsight\u2122 Compute is an interactive kernel profiler for CUDA applications. It provides detailed\n</code></pre>"},{"location":"Glossary/#nsight-compute","title":"Nsight-Compute:","text":"<pre><code>NVIDIA\u00ae Nsight\u2122 Compute is an interactive kernel profiler for CUDA applications. It provides detailed\n</code></pre>"},{"location":"Glossary/#nsight-systemss","title":"Nsight-Systems's:","text":"<pre><code>NVIDIA\u00ae Nsight\u2122 Systems is a system-wide performance analysis tool designed to visualize an\n</code></pre>"},{"location":"Glossary/#nsight-systems","title":"Nsight-Systems:","text":"<pre><code>NVIDIA\u00ae Nsight\u2122 Systems is a system-wide performance analysis tool designed to visualize an\n</code></pre>"},{"location":"Glossary/#oasis3-mcts","title":"OASIS3-MCT's:","text":"<pre><code>The OASIS coupler is a software allowing synchronized exchanges of coupling information between numerical codes representing different components of the climate system.\n</code></pre>"},{"location":"Glossary/#oasis3-mct","title":"OASIS3-MCT:","text":"<pre><code>The OASIS coupler is a software allowing synchronized exchanges of coupling information between numerical codes representing different components of the climate system.\n</code></pre>"},{"location":"Glossary/#obitoolss","title":"OBITools's:","text":"<pre><code>Manipulate various data and sequence files.\n</code></pre>"},{"location":"Glossary/#obitools","title":"OBITools:","text":"<pre><code>Manipulate various data and sequence files.\n</code></pre>"},{"location":"Glossary/#ocis","title":"OCI's:","text":"<pre><code>Oracle Call Interface (OCI) is the comprehensive, high performance, native C language interface to Oracle Database for custom or packaged applications.  NOTE: This package is only available on Maui Ancil nodes that provide database access.\n</code></pre>"},{"location":"Glossary/#oci","title":"OCI:","text":"<pre><code>Oracle Call Interface (OCI) is the comprehensive, high performance, native C language interface to Oracle Database for custom or packaged applications.  NOTE: This package is only available on Maui Ancil nodes that provide database access.\n</code></pre>"},{"location":"Glossary/#omas","title":"OMA's:","text":"<pre><code>Orthologous MAtrix project is a method and database for the inference\n</code></pre>"},{"location":"Glossary/#oma","title":"OMA:","text":"<pre><code>Orthologous MAtrix project is a method and database for the inference\n</code></pre>"},{"location":"Glossary/#opari2s","title":"OPARI2's:","text":"<pre><code>source-to-source instrumentation tool for OpenMP and hybrid codes.\n</code></pre>"},{"location":"Glossary/#opari2","title":"OPARI2:","text":"<pre><code>source-to-source instrumentation tool for OpenMP and hybrid codes.\n</code></pre>"},{"location":"Glossary/#orcas","title":"ORCA's:","text":"<pre><code>ORCA is a flexible, efficient and easy-to-use general purpose tool for quantum chemistry\n</code></pre>"},{"location":"Glossary/#orca","title":"ORCA:","text":"<pre><code>ORCA is a flexible, efficient and easy-to-use general purpose tool for quantum chemistry\n</code></pre>"},{"location":"Glossary/#osprays","title":"OSPRay's:","text":"<pre><code>OSPRay features interactive CPU rendering capabilities geared towards Scientific Visualization applications. Advanced shading effects such as Ambient Occlusion, shadows, and transparency can be rendered interactively, enabling new insights into data exploration.\n</code></pre>"},{"location":"Glossary/#ospray","title":"OSPRay:","text":"<pre><code>OSPRay features interactive CPU rendering capabilities geared towards Scientific Visualization applications. Advanced shading effects such as Ambient Occlusion, shadows, and transparency can be rendered interactively, enabling new insights into data exploration.\n</code></pre>"},{"location":"Glossary/#osu-micro-benchmarkss","title":"OSU-Micro-Benchmarks's:","text":"<pre><code>OSU Micro-Benchmarks for MPI\n</code></pre>"},{"location":"Glossary/#osu-micro-benchmarks","title":"OSU-Micro-Benchmarks:","text":"<pre><code>OSU Micro-Benchmarks for MPI\n</code></pre>"},{"location":"Glossary/#octaves","title":"Octave's:","text":"<pre><code>GNU Octave is a high-level interpreted language, primarily intended for numerical computations.\n</code></pre>"},{"location":"Glossary/#octave","title":"Octave:","text":"<pre><code>GNU Octave is a high-level interpreted language, primarily intended for numerical computations.\n</code></pre>"},{"location":"Glossary/#octopuss","title":"Octopus's:","text":"<pre><code>Octopus is a scientific program aimed at the ab initio virtual experimentation\n</code></pre>"},{"location":"Glossary/#octopus","title":"Octopus:","text":"<pre><code>Octopus is a scientific program aimed at the ab initio virtual experimentation\n</code></pre>"},{"location":"Glossary/#openblass","title":"OpenBLAS's:","text":"<pre><code>OpenBLAS is an optimized BLAS library based on GotoBLAS2 1.13 BSD version.\n</code></pre>"},{"location":"Glossary/#openblas","title":"OpenBLAS:","text":"<pre><code>OpenBLAS is an optimized BLAS library based on GotoBLAS2 1.13 BSD version.\n</code></pre>"},{"location":"Glossary/#openbabels","title":"OpenBabel's:","text":"<pre><code>Open Babel is a chemical toolbox designed to speak the many\n</code></pre>"},{"location":"Glossary/#openbabel","title":"OpenBabel:","text":"<pre><code>Open Babel is a chemical toolbox designed to speak the many\n</code></pre>"},{"location":"Glossary/#opencmisss","title":"OpenCMISS's:","text":"<pre><code>OpenCMISS is a set of libraries and applications which provide modelling and\n</code></pre>"},{"location":"Glossary/#opencmiss","title":"OpenCMISS:","text":"<pre><code>OpenCMISS is a set of libraries and applications which provide modelling and\n</code></pre>"},{"location":"Glossary/#opencvs","title":"OpenCV's:","text":"<pre><code>OpenCV (Open Source Computer Vision Library) is an open source computer vision\n</code></pre>"},{"location":"Glossary/#opencv","title":"OpenCV:","text":"<pre><code>OpenCV (Open Source Computer Vision Library) is an open source computer vision\n</code></pre>"},{"location":"Glossary/#openfasts","title":"OpenFAST's:","text":"<pre><code>Wind turbine multiphysics simulation tool\n</code></pre>"},{"location":"Glossary/#openfast","title":"OpenFAST:","text":"<pre><code>Wind turbine multiphysics simulation tool\n</code></pre>"},{"location":"Glossary/#openfoams","title":"OpenFOAM's:","text":"<pre><code>OpenFOAM is a free, open source CFD software package.\n</code></pre>"},{"location":"Glossary/#openfoam","title":"OpenFOAM:","text":"<pre><code>OpenFOAM is a free, open source CFD software package.\n</code></pre>"},{"location":"Glossary/#openjpegs","title":"OpenJPEG's:","text":"<pre><code>An open-source JPEG 2000 codec written in C\n</code></pre>"},{"location":"Glossary/#openjpeg","title":"OpenJPEG:","text":"<pre><code>An open-source JPEG 2000 codec written in C\n</code></pre>"},{"location":"Glossary/#openmpis","title":"OpenMPI's:","text":"<pre><code>The Open MPI Project is an open source MPI-3 implementation. This version is built with CUDA support enabled.\n</code></pre>"},{"location":"Glossary/#openmpi","title":"OpenMPI:","text":"<pre><code>The Open MPI Project is an open source MPI-3 implementation. This version is built with CUDA support enabled.\n</code></pre>"},{"location":"Glossary/#openssls","title":"OpenSSL's:","text":"<pre><code>The OpenSSL Project is a collaborative effort to develop a robust, commercial-grade, full-featured,\n</code></pre>"},{"location":"Glossary/#openssl","title":"OpenSSL:","text":"<pre><code>The OpenSSL Project is a collaborative effort to develop a robust, commercial-grade, full-featured,\n</code></pre>"},{"location":"Glossary/#openseess","title":"OpenSees's:","text":"<pre><code>OpenSees is a software framework for developing applications to simulate the performance of structural and geotechnical systems subjected to earthquakes.\n</code></pre>"},{"location":"Glossary/#opensees","title":"OpenSees:","text":"<pre><code>OpenSees is a software framework for developing applications to simulate the performance of structural and geotechnical systems subjected to earthquakes.\n</code></pre>"},{"location":"Glossary/#openseespys","title":"OpenSeesPy's:","text":"<pre><code>OpenSees is a software framework for developing applications to simulate the performance of structural and geotechnical systems subjected to earthquakes.\n</code></pre>"},{"location":"Glossary/#openseespy","title":"OpenSeesPy:","text":"<pre><code>OpenSees is a software framework for developing applications to simulate the performance of structural and geotechnical systems subjected to earthquakes.\n</code></pre>"},{"location":"Glossary/#openslides","title":"OpenSlide's:","text":"<pre><code>OpenSlide is a C library that provides a simple interface to\n</code></pre>"},{"location":"Glossary/#openslide","title":"OpenSlide:","text":"<pre><code>OpenSlide is a C library that provides a simple interface to\n</code></pre>"},{"location":"Glossary/#orfms","title":"OrfM's:","text":"<pre><code>A simple and not slow open reading frame (ORF) caller.\n</code></pre>"},{"location":"Glossary/#orfm","title":"OrfM:","text":"<pre><code>A simple and not slow open reading frame (ORF) caller.\n</code></pre>"},{"location":"Glossary/#orthofillers","title":"OrthoFiller's:","text":"<pre><code>Identifies missing annotations for evolutionarily conserved genes.\n</code></pre>"},{"location":"Glossary/#orthofiller","title":"OrthoFiller:","text":"<pre><code>Identifies missing annotations for evolutionarily conserved genes.\n</code></pre>"},{"location":"Glossary/#orthofinders","title":"OrthoFinder's:","text":"<pre><code>OrthoFinder is a fast, accurate and comprehensive platform for comparative genomics\n</code></pre>"},{"location":"Glossary/#orthofinder","title":"OrthoFinder:","text":"<pre><code>OrthoFinder is a fast, accurate and comprehensive platform for comparative genomics\n</code></pre>"},{"location":"Glossary/#orthomcls","title":"OrthoMCL's:","text":"<pre><code>Genome-scale algorithm for grouping orthologous protein sequences.\n</code></pre>"},{"location":"Glossary/#orthomcl","title":"OrthoMCL:","text":"<pre><code>Genome-scale algorithm for grouping orthologous protein sequences.\n</code></pre>"},{"location":"Glossary/#paleomixs","title":"PALEOMIX's:","text":"<pre><code>pipelines and tools designed to aid the rapid processing of High-Throughput Sequencing (HTS) data.\n</code></pre>"},{"location":"Glossary/#paleomix","title":"PALEOMIX:","text":"<pre><code>pipelines and tools designed to aid the rapid processing of High-Throughput Sequencing (HTS) data.\n</code></pre>"},{"location":"Glossary/#pamls","title":"PAML's:","text":"<pre><code>PAML is a package of programs for phylogenetic\n</code></pre>"},{"location":"Glossary/#paml","title":"PAML:","text":"<pre><code>PAML is a package of programs for phylogenetic\n</code></pre>"},{"location":"Glossary/#papis","title":"PAPI's:","text":"<pre><code>PAPI provides the tool designer and application engineer with a consistent interface and\n</code></pre>"},{"location":"Glossary/#papi","title":"PAPI:","text":"<pre><code>PAPI provides the tool designer and application engineer with a consistent interface and\n</code></pre>"},{"location":"Glossary/#pbjellys","title":"PBJelly's:","text":"<pre><code>PBJelly is a highly automated pipeline that aligns long sequencing reads (such as PacBio RS reads or\n</code></pre>"},{"location":"Glossary/#pbjelly","title":"PBJelly:","text":"<pre><code>PBJelly is a highly automated pipeline that aligns long sequencing reads (such as PacBio RS reads or\n</code></pre>"},{"location":"Glossary/#pcres","title":"PCRE's:","text":"<pre><code>The PCRE library is a set of functions that implement regular expression pattern matching using\n</code></pre>"},{"location":"Glossary/#pcre","title":"PCRE:","text":"<pre><code>The PCRE library is a set of functions that implement regular expression pattern matching using\n</code></pre>"},{"location":"Glossary/#pcre2s","title":"PCRE2's:","text":"<pre><code>The PCRE library is a set of functions that implement regular expression pattern matching using the same syntax  and semantics as Perl 5.\n</code></pre>"},{"location":"Glossary/#pcre2","title":"PCRE2:","text":"<pre><code>The PCRE library is a set of functions that implement regular expression pattern matching using the same syntax  and semantics as Perl 5.\n</code></pre>"},{"location":"Glossary/#pdts","title":"PDT's:","text":"<pre><code>Program Database Toolkit (PDT) is a framework for analyzing source code written in several programming languages and for making rich program\n</code></pre>"},{"location":"Glossary/#pdt","title":"PDT:","text":"<pre><code>Program Database Toolkit (PDT) is a framework for analyzing source code written in several programming languages and for making rich program\n</code></pre>"},{"location":"Glossary/#pears","title":"PEAR's:","text":"<pre><code>Memory-efficient,fully parallelized and highly accurate pair-end read merger.\n</code></pre>"},{"location":"Glossary/#pear","title":"PEAR:","text":"<pre><code>Memory-efficient,fully parallelized and highly accurate pair-end read merger.\n</code></pre>"},{"location":"Glossary/#pests","title":"PEST++'s:","text":"<pre><code>PEST++ is a software suite aimed at supporting\n</code></pre>"},{"location":"Glossary/#pest","title":"PEST++:","text":"<pre><code>PEST++ is a software suite aimed at supporting\n</code></pre>"},{"location":"Glossary/#petscs","title":"PETSc's:","text":"<pre><code>PETSc, pronounced PET-see (the S is silent), is a suite of data structures and routines for the\n</code></pre>"},{"location":"Glossary/#petsc","title":"PETSc:","text":"<pre><code>PETSc, pronounced PET-see (the S is silent), is a suite of data structures and routines for the\n</code></pre>"},{"location":"Glossary/#pffts","title":"PFFT's:","text":"<pre><code>PFFT is a software library for computing massively parallel, fast Fourier\n</code></pre>"},{"location":"Glossary/#pfft","title":"PFFT:","text":"<pre><code>PFFT is a software library for computing massively parallel, fast Fourier\n</code></pre>"},{"location":"Glossary/#pgis","title":"PGI's:","text":"<pre><code>C, C++ and Fortran compilers from The Portland Group - PGI\n</code></pre>"},{"location":"Glossary/#pgi","title":"PGI:","text":"<pre><code>C, C++ and Fortran compilers from The Portland Group - PGI\n</code></pre>"},{"location":"Glossary/#phasiuss","title":"PHASIUS's:","text":"<pre><code>A tool to visualize phase block structure from (many) BAM or CRAM files together with BED annotation\n</code></pre>"},{"location":"Glossary/#phasius","title":"PHASIUS:","text":"<pre><code>A tool to visualize phase block structure from (many) BAM or CRAM files together with BED annotation\n</code></pre>"},{"location":"Glossary/#plinks","title":"PLINK's:","text":"<pre><code>PLINK is a free, open-source whole genome association analysis toolset,\n</code></pre>"},{"location":"Glossary/#plink","title":"PLINK:","text":"<pre><code>PLINK is a free, open-source whole genome association analysis toolset,\n</code></pre>"},{"location":"Glossary/#plumeds","title":"PLUMED's:","text":"<pre><code>PLUMED is an open source library for free energy calculations in molecular systems which\n</code></pre>"},{"location":"Glossary/#plumed","title":"PLUMED:","text":"<pre><code>PLUMED is an open source library for free energy calculations in molecular systems which\n</code></pre>"},{"location":"Glossary/#pranks","title":"PRANK's:","text":"<pre><code>Probabilistic multiple alignment program for DNA, codon and amino-acid sequences. .\n</code></pre>"},{"location":"Glossary/#prank","title":"PRANK:","text":"<pre><code>Probabilistic multiple alignment program for DNA, codon and amino-acid sequences. .\n</code></pre>"},{"location":"Glossary/#projs","title":"PROJ's:","text":"<pre><code>Program proj is a standard Unix filter function which converts  geographic longitude and latitude coordinates into cartesian coordinates\n</code></pre>"},{"location":"Glossary/#proj","title":"PROJ:","text":"<pre><code>Program proj is a standard Unix filter function which converts  geographic longitude and latitude coordinates into cartesian coordinates\n</code></pre>"},{"location":"Glossary/#pangos","title":"Pango's:","text":"<pre><code>Pango is a library for laying out and rendering of text, with an emphasis on internationalization.\n</code></pre>"},{"location":"Glossary/#pango","title":"Pango:","text":"<pre><code>Pango is a library for laying out and rendering of text, with an emphasis on internationalization.\n</code></pre>"},{"location":"Glossary/#parmetiss","title":"ParMETIS's:","text":"<pre><code>ParMETIS is an MPI-based parallel library that implements a variety of algorithms for partitioning unstructured graphs,\n</code></pre>"},{"location":"Glossary/#parmetis","title":"ParMETIS:","text":"<pre><code>ParMETIS is an MPI-based parallel library that implements a variety of algorithms for partitioning unstructured graphs,\n</code></pre>"},{"location":"Glossary/#paraviews","title":"ParaView's:","text":"<pre><code>ParaView is a scientific parallel visualizer.  This version supports CPU-only rendering without X context using the OSMesa library, it does not support GPU rendering, and it does not provide a GUI.  Use the GALLIUM_DRIVER environment variable to choose a software renderer, it is recommended to use  GALLIUM_DRIVER=swr  for best performance.  Ray tracing using the OSPRay library is also supported.\n</code></pre>"},{"location":"Glossary/#paraview","title":"ParaView:","text":"<pre><code>ParaView is a scientific parallel visualizer.  This version supports CPU-only rendering without X context using the OSMesa library, it does not support GPU rendering, and it does not provide a GUI.  Use the GALLIUM_DRIVER environment variable to choose a software renderer, it is recommended to use  GALLIUM_DRIVER=swr  for best performance.  Ray tracing using the OSPRay library is also supported.\n</code></pre>"},{"location":"Glossary/#parallels","title":"Parallel's:","text":"<pre><code>parallel: Build and execute shell commands in parallel\n</code></pre>"},{"location":"Glossary/#parallel","title":"Parallel:","text":"<pre><code>parallel: Build and execute shell commands in parallel\n</code></pre>"},{"location":"Glossary/#parallelios","title":"ParallelIO's:","text":"<pre><code>A high-level Parallel I/O Library for structured grid applications\n</code></pre>"},{"location":"Glossary/#parallelio","title":"ParallelIO:","text":"<pre><code>A high-level Parallel I/O Library for structured grid applications\n</code></pre>"},{"location":"Glossary/#paravers","title":"Paraver's:","text":"<pre><code>Performance visualization and analysis tool based on traces.\n</code></pre>"},{"location":"Glossary/#paraver","title":"Paraver:","text":"<pre><code>Performance visualization and analysis tool based on traces.\n</code></pre>"},{"location":"Glossary/#peregrines","title":"Peregrine's:","text":"<pre><code>Genome assembler for long reads (length &gt; 10kb, accuracy &gt; 99%).\n</code></pre>"},{"location":"Glossary/#peregrine","title":"Peregrine:","text":"<pre><code>Genome assembler for long reads (length &gt; 10kb, accuracy &gt; 99%).\n</code></pre>"},{"location":"Glossary/#perls","title":"Perl's:","text":"<pre><code>Larry Wall's Practical Extraction and Report Language\n</code></pre>"},{"location":"Glossary/#perl","title":"Perl:","text":"<pre><code>Larry Wall's Practical Extraction and Report Language\n</code></pre>"},{"location":"Glossary/#phymls","title":"PhyML's:","text":"<pre><code>Phylogenetic estimation using Maximum Likelihood\n</code></pre>"},{"location":"Glossary/#phyml","title":"PhyML:","text":"<pre><code>Phylogenetic estimation using Maximum Likelihood\n</code></pre>"},{"location":"Glossary/#phylophlans","title":"PhyloPhlAn's:","text":"<pre><code>Integrated pipeline for large-scale phylogenetic profiling of genomes and metagenomes.\n</code></pre>"},{"location":"Glossary/#phylophlan","title":"PhyloPhlAn:","text":"<pre><code>Integrated pipeline for large-scale phylogenetic profiling of genomes and metagenomes.\n</code></pre>"},{"location":"Glossary/#pilons","title":"Pilon's:","text":"<pre><code>Pilon is an automated genome assembly improvement and variant detection tool\n</code></pre>"},{"location":"Glossary/#pilon","title":"Pilon:","text":"<pre><code>Pilon is an automated genome assembly improvement and variant detection tool\n</code></pre>"},{"location":"Glossary/#pnetcdfs","title":"PnetCDF's:","text":"<pre><code>Parallel netCDF: A Parallel I/O Library for NetCDF File Access\n</code></pre>"},{"location":"Glossary/#pnetcdf","title":"PnetCDF:","text":"<pre><code>Parallel netCDF: A Parallel I/O Library for NetCDF File Access\n</code></pre>"},{"location":"Glossary/#porechops","title":"Porechop's:","text":"<pre><code>Porechop is a tool for finding and removing adapters from Oxford Nanopore reads.\n</code></pre>"},{"location":"Glossary/#porechop","title":"Porechop:","text":"<pre><code>Porechop is a tool for finding and removing adapters from Oxford Nanopore reads.\n</code></pre>"},{"location":"Glossary/#postgresqls","title":"PostgreSQL's:","text":"<pre><code>Client-side programs and libraries for accessing PostgreSQL databases.\n</code></pre>"},{"location":"Glossary/#postgresql","title":"PostgreSQL:","text":"<pre><code>Client-side programs and libraries for accessing PostgreSQL databases.\n</code></pre>"},{"location":"Glossary/#prodigals","title":"Prodigal's:","text":"<pre><code>Prodigal (Prokaryotic Dynamic Programming Genefinding Algorithm)\n</code></pre>"},{"location":"Glossary/#prodigal","title":"Prodigal:","text":"<pre><code>Prodigal (Prokaryotic Dynamic Programming Genefinding Algorithm)\n</code></pre>"},{"location":"Glossary/#prothints","title":"ProtHint's:","text":"<pre><code>Pipeline for predicting and scoring hints (in the form of introns, start and\n</code></pre>"},{"location":"Glossary/#prothint","title":"ProtHint:","text":"<pre><code>Pipeline for predicting and scoring hints (in the form of introns, start and\n</code></pre>"},{"location":"Glossary/#proteinorthos","title":"Proteinortho's:","text":"<pre><code>Proteinortho is a tool to detect orthologous genes within different species.\n</code></pre>"},{"location":"Glossary/#proteinortho","title":"Proteinortho:","text":"<pre><code>Proteinortho is a tool to detect orthologous genes within different species.\n</code></pre>"},{"location":"Glossary/#pyopengls","title":"PyOpenGL's:","text":"<pre><code>PyOpenGL is the most common cross platform Python binding to OpenGL and related APIs.\n</code></pre>"},{"location":"Glossary/#pyopengl","title":"PyOpenGL:","text":"<pre><code>PyOpenGL is the most common cross platform Python binding to OpenGL and related APIs.\n</code></pre>"},{"location":"Glossary/#pyqts","title":"PyQt's:","text":"<pre><code>PyQt5 is a set of Python bindings for v5 of the Qt application framework from The Qt Company.\n</code></pre>"},{"location":"Glossary/#pyqt","title":"PyQt:","text":"<pre><code>PyQt5 is a set of Python bindings for v5 of the Qt application framework from The Qt Company.\n</code></pre>"},{"location":"Glossary/#pytorchs","title":"PyTorch's:","text":"<pre><code>Tensors and Dynamic neural networks in Python with strong GPU acceleration.\n</code></pre>"},{"location":"Glossary/#pytorch","title":"PyTorch:","text":"<pre><code>Tensors and Dynamic neural networks in Python with strong GPU acceleration.\n</code></pre>"},{"location":"Glossary/#pythons","title":"Python's:","text":"<pre><code>Python is a programming language that lets you work more quickly and integrate your systems more effectively.\n</code></pre>"},{"location":"Glossary/#python","title":"Python:","text":"<pre><code>Python is a programming language that lets you work more quickly and integrate your systems more effectively.\n</code></pre>"},{"location":"Glossary/#python-gpus","title":"Python-GPU's:","text":"<pre><code>The python packages which depend on CUDA: pycuda, pygpu, and scikit-cuda.\n</code></pre>"},{"location":"Glossary/#python-gpu","title":"Python-GPU:","text":"<pre><code>The python packages which depend on CUDA: pycuda, pygpu, and scikit-cuda.\n</code></pre>"},{"location":"Glossary/#python-geos","title":"Python-Geo's:","text":"<pre><code>Python packages for geospatial data I/O, mostly based on the OSGEO libraries GDAL and OGR\n</code></pre>"},{"location":"Glossary/#python-geo","title":"Python-Geo:","text":"<pre><code>Python packages for geospatial data I/O, mostly based on the OSGEO libraries GDAL and OGR\n</code></pre>"},{"location":"Glossary/#qiime2s","title":"QIIME2's:","text":"<pre><code>An open-source bioinformatics pipeline for microbiome analysis\n</code></pre>"},{"location":"Glossary/#qiime2","title":"QIIME2:","text":"<pre><code>An open-source bioinformatics pipeline for microbiome analysis\n</code></pre>"},{"location":"Glossary/#quasts","title":"QUAST's:","text":"<pre><code>Evaluates genome assemblies\n</code></pre>"},{"location":"Glossary/#quast","title":"QUAST:","text":"<pre><code>Evaluates genome assemblies\n</code></pre>"},{"location":"Glossary/#qt5s","title":"Qt5's:","text":"<pre><code>Qt is a comprehensive cross-platform C++ application framework.\n</code></pre>"},{"location":"Glossary/#qt5","title":"Qt5:","text":"<pre><code>Qt is a comprehensive cross-platform C++ application framework.\n</code></pre>"},{"location":"Glossary/#quantumespressos","title":"QuantumESPRESSO's:","text":"<pre><code>Quantum ESPRESSO  is an integrated suite of computer codes\n</code></pre>"},{"location":"Glossary/#quantumespresso","title":"QuantumESPRESSO:","text":"<pre><code>Quantum ESPRESSO  is an integrated suite of computer codes\n</code></pre>"},{"location":"Glossary/#quicktrees","title":"QuickTree's:","text":"<pre><code>Efficient implementation of the Neighbor-Joining algorithm, capable of reconstructing phylogenies from huge alignments .\n</code></pre>"},{"location":"Glossary/#quicktree","title":"QuickTree:","text":"<pre><code>Efficient implementation of the Neighbor-Joining algorithm, capable of reconstructing phylogenies from huge alignments .\n</code></pre>"},{"location":"Glossary/#rs","title":"R's:","text":"<pre><code>R is a free software environment for statistical computing and graphics.\n</code></pre>"},{"location":"Glossary/#r","title":"R:","text":"<pre><code>R is a free software environment for statistical computing and graphics.\n</code></pre>"},{"location":"Glossary/#r-geos","title":"R-Geo's:","text":"<pre><code>R packages for Geometric and Geospatial data which depend\n</code></pre>"},{"location":"Glossary/#r-geo","title":"R-Geo:","text":"<pre><code>R packages for Geometric and Geospatial data which depend\n</code></pre>"},{"location":"Glossary/#r-bundle-bioconductors","title":"R-bundle-Bioconductor's:","text":"<pre><code>Bioconductor provides tools for the analysis and comprehension\n</code></pre>"},{"location":"Glossary/#r-bundle-bioconductor","title":"R-bundle-Bioconductor:","text":"<pre><code>Bioconductor provides tools for the analysis and comprehension\n</code></pre>"},{"location":"Glossary/#rangs-gshhss","title":"RANGS-GSHHS's:","text":"<pre><code>A binary file set RANGS (Regionally Accessible Nested Global Shorelines) based on GSHHS (Global Self-consistent Hierarchical High-resolution Shorelines) data.  Note: RANGS-GSHHS is used with NCL.\n</code></pre>"},{"location":"Glossary/#rangs-gshhs","title":"RANGS-GSHHS:","text":"<pre><code>A binary file set RANGS (Regionally Accessible Nested Global Shorelines) based on GSHHS (Global Self-consistent Hierarchical High-resolution Shorelines) data.  Note: RANGS-GSHHS is used with NCL.\n</code></pre>"},{"location":"Glossary/#raxmls","title":"RAxML's:","text":"<pre><code>RAxML search algorithm for maximum likelihood based inference of phylogenetic trees.\n</code></pre>"},{"location":"Glossary/#raxml","title":"RAxML:","text":"<pre><code>RAxML search algorithm for maximum likelihood based inference of phylogenetic trees.\n</code></pre>"},{"location":"Glossary/#raxml-ngs","title":"RAxML-NG's:","text":"<pre><code>RAxML-NG is a phylogenetic tree inference tool which uses maximum-likelihood (ML)\n</code></pre>"},{"location":"Glossary/#raxml-ng","title":"RAxML-NG:","text":"<pre><code>RAxML-NG is a phylogenetic tree inference tool which uses maximum-likelihood (ML)\n</code></pre>"},{"location":"Glossary/#rdp-classifiers","title":"RDP-Classifier's:","text":"<pre><code>The RDP Classifier is a naive Bayesian classifier that can rapidly and accurately provides taxonomic\n</code></pre>"},{"location":"Glossary/#rdp-classifier","title":"RDP-Classifier:","text":"<pre><code>The RDP Classifier is a naive Bayesian classifier that can rapidly and accurately provides taxonomic\n</code></pre>"},{"location":"Glossary/#recons","title":"RECON's:","text":"<pre><code>De novo identification and classification of repeat sequence families from genomic sequences\n</code></pre>"},{"location":"Glossary/#recon","title":"RECON:","text":"<pre><code>De novo identification and classification of repeat sequence families from genomic sequences\n</code></pre>"},{"location":"Glossary/#rmblasts","title":"RMBlast's:","text":"<pre><code>RMBlast supports RepeatMasker searches by adding a few necessary features to the stock NCBI blastn program. These include:\n</code></pre>"},{"location":"Glossary/#rmblast","title":"RMBlast:","text":"<pre><code>RMBlast supports RepeatMasker searches by adding a few necessary features to the stock NCBI blastn program. These include:\n</code></pre>"},{"location":"Glossary/#rnammers","title":"RNAmmer's:","text":"<pre><code>consistent and rapid annotation of ribosomal RNA genes.\n</code></pre>"},{"location":"Glossary/#rnammer","title":"RNAmmer:","text":"<pre><code>consistent and rapid annotation of ribosomal RNA genes.\n</code></pre>"},{"location":"Glossary/#rocms","title":"ROCm's:","text":"<pre><code>Platform for GPU Enabled HPC and UltraScale Computing\n</code></pre>"},{"location":"Glossary/#rocm","title":"ROCm:","text":"<pre><code>Platform for GPU Enabled HPC and UltraScale Computing\n</code></pre>"},{"location":"Glossary/#roots","title":"ROOT's:","text":"<pre><code>The ROOT system provides a set of OO frameworks with all the functionality\n</code></pre>"},{"location":"Glossary/#root","title":"ROOT:","text":"<pre><code>The ROOT system provides a set of OO frameworks with all the functionality\n</code></pre>"},{"location":"Glossary/#rsems","title":"RSEM's:","text":"<pre><code>Estimates gene and isoform expression levels from RNA-Seq data\n</code></pre>"},{"location":"Glossary/#rsem","title":"RSEM:","text":"<pre><code>Estimates gene and isoform expression levels from RNA-Seq data\n</code></pre>"},{"location":"Glossary/#rsgislibs","title":"RSGISLib's:","text":"<pre><code>The Remote Sensing and GIS software library (RSGISLib) is a\n</code></pre>"},{"location":"Glossary/#rsgislib","title":"RSGISLib:","text":"<pre><code>The Remote Sensing and GIS software library (RSGISLib) is a\n</code></pre>"},{"location":"Glossary/#racons","title":"Racon's:","text":"<pre><code>Ultrafast consensus module for raw de novo genome assembly of long uncorrected reads.\n</code></pre>"},{"location":"Glossary/#racon","title":"Racon:","text":"<pre><code>Ultrafast consensus module for raw de novo genome assembly of long uncorrected reads.\n</code></pre>"},{"location":"Glossary/#ragouts","title":"Ragout's:","text":"<pre><code>Tool for chromosome assembly using multiple references.\n</code></pre>"},{"location":"Glossary/#ragout","title":"Ragout:","text":"<pre><code>Tool for chromosome assembly using multiple references.\n</code></pre>"},{"location":"Glossary/#rapidnjs","title":"RapidNJ's:","text":"<pre><code>An algorithmic engineered implementation of canonical neighbour-joining.\n</code></pre>"},{"location":"Glossary/#rapidnj","title":"RapidNJ:","text":"<pre><code>An algorithmic engineered implementation of canonical neighbour-joining.\n</code></pre>"},{"location":"Glossary/#ravens","title":"Raven's:","text":"<pre><code>De novo genome assembler for long uncorrected reads.\n</code></pre>"},{"location":"Glossary/#raven","title":"Raven:","text":"<pre><code>De novo genome assembler for long uncorrected reads.\n</code></pre>"},{"location":"Glossary/#rcorrectors","title":"Rcorrector's:","text":"<pre><code>kmer-based error correction method for RNA-seq data.\n</code></pre>"},{"location":"Glossary/#rcorrector","title":"Rcorrector:","text":"<pre><code>kmer-based error correction method for RNA-seq data.\n</code></pre>"},{"location":"Glossary/#relions","title":"Relion's:","text":"<pre><code>RELION (for REgularised LIkelihood OptimisatioN, pronounce rely-on)\n</code></pre>"},{"location":"Glossary/#relion","title":"Relion:","text":"<pre><code>RELION (for REgularised LIkelihood OptimisatioN, pronounce rely-on)\n</code></pre>"},{"location":"Glossary/#repeatmaskers","title":"RepeatMasker's:","text":"<pre><code>Screens DNA sequences for interspersed repeats and low complexity DNA sequences.\n</code></pre>"},{"location":"Glossary/#repeatmasker","title":"RepeatMasker:","text":"<pre><code>Screens DNA sequences for interspersed repeats and low complexity DNA sequences.\n</code></pre>"},{"location":"Glossary/#repeatmodelers","title":"RepeatModeler's:","text":"<pre><code>De novo transposable element (TE) family identification and modeling package.\n</code></pre>"},{"location":"Glossary/#repeatmodeler","title":"RepeatModeler:","text":"<pre><code>De novo transposable element (TE) family identification and modeling package.\n</code></pre>"},{"location":"Glossary/#repeatscouts","title":"RepeatScout's:","text":"<pre><code>De novo identification of repeat families in large genomes\n</code></pre>"},{"location":"Glossary/#repeatscout","title":"RepeatScout:","text":"<pre><code>De novo identification of repeat families in large genomes\n</code></pre>"},{"location":"Glossary/#riskscapes","title":"Riskscape's:","text":"<pre><code>RiskScape is an open-source spatial data processing application used for multi-hazard risk analysis. RiskScape is highly customisable, letting modellers tailor the risk analysis to suit the problem domain and input data being modelled.\n</code></pre>"},{"location":"Glossary/#riskscape","title":"Riskscape:","text":"<pre><code>RiskScape is an open-source spatial data processing application used for multi-hazard risk analysis. RiskScape is highly customisable, letting modellers tailor the risk analysis to suit the problem domain and input data being modelled.\n</code></pre>"},{"location":"Glossary/#roarys","title":"Roary's:","text":"<pre><code>Rapid large-scale prokaryote pan genome analysis\n</code></pre>"},{"location":"Glossary/#roary","title":"Roary:","text":"<pre><code>Rapid large-scale prokaryote pan genome analysis\n</code></pre>"},{"location":"Glossary/#rosettas","title":"Rosetta's:","text":"<pre><code>Rosetta is the premier software suite for modeling macromolecular structures. As a flexible,\n</code></pre>"},{"location":"Glossary/#rosetta","title":"Rosetta:","text":"<pre><code>Rosetta is the premier software suite for modeling macromolecular structures. As a flexible,\n</code></pre>"},{"location":"Glossary/#rstudios","title":"Rstudio's:","text":"<pre><code>RStudio is a set of integrated tools designed to help you be more productive with R.\n</code></pre>"},{"location":"Glossary/#rstudio","title":"Rstudio:","text":"<pre><code>RStudio is a set of integrated tools designed to help you be more productive with R.\n</code></pre>"},{"location":"Glossary/#rubys","title":"Ruby's:","text":"<pre><code>Ruby is a dynamic, open source programming language with\n</code></pre>"},{"location":"Glossary/#ruby","title":"Ruby:","text":"<pre><code>Ruby is a dynamic, open source programming language with\n</code></pre>"},{"location":"Glossary/#rusts","title":"Rust's:","text":"<pre><code>Systems programming language that runs blazingly fast, prevents segfaults,\n</code></pre>"},{"location":"Glossary/#rust","title":"Rust:","text":"<pre><code>Systems programming language that runs blazingly fast, prevents segfaults,\n</code></pre>"},{"location":"Glossary/#sages","title":"SAGE's:","text":"<pre><code>Ppackage containing programs for use in the genetic analysis of\n</code></pre>"},{"location":"Glossary/#sage","title":"SAGE:","text":"<pre><code>Ppackage containing programs for use in the genetic analysis of\n</code></pre>"},{"location":"Glossary/#samtoolss","title":"SAMtools's:","text":"<pre><code>Samtools is a suite of programs for interacting with high-throughput sequencing data.\n</code></pre>"},{"location":"Glossary/#samtools","title":"SAMtools:","text":"<pre><code>Samtools is a suite of programs for interacting with high-throughput sequencing data.\n</code></pre>"},{"location":"Glossary/#sass","title":"SAS's:","text":"<pre><code>SAS is a statistical software suite developed by SAS Institute for data management, advanced analytics, multivariate analysis, business intelligence, criminal investigation, and predictive analytics. - Homepage: https://www.sas.com/en_nz/home.html/\n</code></pre>"},{"location":"Glossary/#sas","title":"SAS:","text":"<pre><code>SAS is a statistical software suite developed by SAS Institute for data management, advanced analytics, multivariate analysis, business intelligence, criminal investigation, and predictive analytics. - Homepage: https://www.sas.com/en_nz/home.html/\n</code></pre>"},{"location":"Glossary/#scotchs","title":"SCOTCH's:","text":"<pre><code>Software package and libraries for sequential and parallel graph partitioning, static mapping, and sparse matrix block ordering, and sequential mesh and hypergraph partitioning.\n</code></pre>"},{"location":"Glossary/#scotch","title":"SCOTCH:","text":"<pre><code>Software package and libraries for sequential and parallel graph partitioning, static mapping, and sparse matrix block ordering, and sequential mesh and hypergraph partitioning.\n</code></pre>"},{"location":"Glossary/#sconss","title":"SCons's:","text":"<pre><code>SCons is a software construction tool.\n</code></pre>"},{"location":"Glossary/#scons","title":"SCons:","text":"<pre><code>SCons is a software construction tool.\n</code></pre>"},{"location":"Glossary/#sdl2s","title":"SDL2's:","text":"<pre><code>Simple DirectMedia Layer, a cross-platform multimedia library\n</code></pre>"},{"location":"Glossary/#sdl2","title":"SDL2:","text":"<pre><code>Simple DirectMedia Layer, a cross-platform multimedia library\n</code></pre>"},{"location":"Glossary/#sepps","title":"SEPP's:","text":"<pre><code>SATe-enabled Phylogenetic Placement. Phylogenetic placement of short reads into reference alignments and trees.\n</code></pre>"},{"location":"Glossary/#sepp","title":"SEPP:","text":"<pre><code>SATe-enabled Phylogenetic Placement. Phylogenetic placement of short reads into reference alignments and trees.\n</code></pre>"},{"location":"Glossary/#shapeit4s","title":"SHAPEIT4's:","text":"<pre><code>Estimation of haplotypes (aka phasing)\n</code></pre>"},{"location":"Glossary/#shapeit4","title":"SHAPEIT4:","text":"<pre><code>Estimation of haplotypes (aka phasing)\n</code></pre>"},{"location":"Glossary/#sionlibs","title":"SIONlib's:","text":"<pre><code>Scalable I/O library for parallel access to task-local files.\n</code></pre>"},{"location":"Glossary/#sionlib","title":"SIONlib:","text":"<pre><code>Scalable I/O library for parallel access to task-local files.\n</code></pre>"},{"location":"Glossary/#sips","title":"SIP's:","text":"<pre><code>SIP is a tool that makes it very easy to create Python bindings for C and C++ libraries.\n</code></pre>"},{"location":"Glossary/#sip","title":"SIP:","text":"<pre><code>SIP is a tool that makes it very easy to create Python bindings for C and C++ libraries.\n</code></pre>"},{"location":"Glossary/#skesas","title":"SKESA's:","text":"<pre><code>SKESA is a de-novo sequence read assembler for cultured single isolate genomes based on DeBruijn graphs.\n</code></pre>"},{"location":"Glossary/#skesa","title":"SKESA:","text":"<pre><code>SKESA is a de-novo sequence read assembler for cultured single isolate genomes based on DeBruijn graphs.\n</code></pre>"},{"location":"Glossary/#smrt-links","title":"SMRT-Link's:","text":"<pre><code>PacBio\u2019s open-source software suite is designed for use with Single Molecule,\n</code></pre>"},{"location":"Glossary/#smrt-link","title":"SMRT-Link:","text":"<pre><code>PacBio\u2019s open-source software suite is designed for use with Single Molecule,\n</code></pre>"},{"location":"Glossary/#snvoter-nanomethphases","title":"SNVoter-NanoMethPhase's:","text":"<pre><code>SNVoter - A top up tool to enhance SNV calling from Nanopore sequencing data &amp;\n</code></pre>"},{"location":"Glossary/#snvoter-nanomethphase","title":"SNVoter-NanoMethPhase:","text":"<pre><code>SNVoter - A top up tool to enhance SNV calling from Nanopore sequencing data &amp;\n</code></pre>"},{"location":"Glossary/#soapdenovo2s","title":"SOAPdenovo2's:","text":"<pre><code>Short Oligonucleotide Analysis Package - novel short-read assembly\n</code></pre>"},{"location":"Glossary/#soapdenovo2","title":"SOAPdenovo2:","text":"<pre><code>Short Oligonucleotide Analysis Package - novel short-read assembly\n</code></pre>"},{"location":"Glossary/#socis","title":"SOCI's:","text":"<pre><code>Database access library for C++ that makes the illusion of embedding SQL queries in the\n</code></pre>"},{"location":"Glossary/#soci","title":"SOCI:","text":"<pre><code>Database access library for C++ that makes the illusion of embedding SQL queries in the\n</code></pre>"},{"location":"Glossary/#spadess","title":"SPAdes's:","text":"<pre><code>Genome assembler for single-cell and isolates data sets\n</code></pre>"},{"location":"Glossary/#spades","title":"SPAdes:","text":"<pre><code>Genome assembler for single-cell and isolates data sets\n</code></pre>"},{"location":"Glossary/#specfem3ds","title":"SPECFEM3D's:","text":"<pre><code>SPECFEM3D Cartesian simulates acoustic (fluid), elastic (solid), coupled acoustic/elastic, poroelastic or seismic wave propagation in any type of conforming mesh of hexahedra (structured or not.) It can, for instance, model seismic waves propagating in sedimentary basins or any other regional geological model following earthquakes. It can also be used for non-destructive testing or for ocean acoustics.\n</code></pre>"},{"location":"Glossary/#specfem3d","title":"SPECFEM3D:","text":"<pre><code>SPECFEM3D Cartesian simulates acoustic (fluid), elastic (solid), coupled acoustic/elastic, poroelastic or seismic wave propagation in any type of conforming mesh of hexahedra (structured or not.) It can, for instance, model seismic waves propagating in sedimentary basins or any other regional geological model following earthquakes. It can also be used for non-destructive testing or for ocean acoustics.\n</code></pre>"},{"location":"Glossary/#spiders","title":"SPIDER's:","text":"<pre><code>System for Processing Image Data from Electron microscopy and Related fields\n</code></pre>"},{"location":"Glossary/#spider","title":"SPIDER:","text":"<pre><code>System for Processing Image Data from Electron microscopy and Related fields\n</code></pre>"},{"location":"Glossary/#sqlites","title":"SQLite's:","text":"<pre><code>SQLite: SQL Database Engine in a C Library\n</code></pre>"},{"location":"Glossary/#sqlite","title":"SQLite:","text":"<pre><code>SQLite: SQL Database Engine in a C Library\n</code></pre>"},{"location":"Glossary/#sqlpluss","title":"SQLplus's:","text":"<pre><code>SQL*Plus is an interactive and batch query tool that is installed with every Oracle Database installation. It has a command-line user interface and a Windows Graphical User Interface (GUI).\n</code></pre>"},{"location":"Glossary/#sqlplus","title":"SQLplus:","text":"<pre><code>SQL*Plus is an interactive and batch query tool that is installed with every Oracle Database installation. It has a command-line user interface and a Windows Graphical User Interface (GUI).\n</code></pre>"},{"location":"Glossary/#ssaha2s","title":"SSAHA2's:","text":"<pre><code>Pairwise sequence alignment program designed for the efficient mapping of sequencing\n</code></pre>"},{"location":"Glossary/#ssaha2","title":"SSAHA2:","text":"<pre><code>Pairwise sequence alignment program designed for the efficient mapping of sequencing\n</code></pre>"},{"location":"Glossary/#stars","title":"STAR's:","text":"<pre><code>Fast universal RNA-seq aligner\n</code></pre>"},{"location":"Glossary/#star","title":"STAR:","text":"<pre><code>Fast universal RNA-seq aligner\n</code></pre>"},{"location":"Glossary/#star-fusions","title":"STAR-Fusion's:","text":"<pre><code>Processes the output generated by the STAR aligner to map junction reads and spanning reads to a reference annotation set\n</code></pre>"},{"location":"Glossary/#star-fusion","title":"STAR-Fusion:","text":"<pre><code>Processes the output generated by the STAR aligner to map junction reads and spanning reads to a reference annotation set\n</code></pre>"},{"location":"Glossary/#sundialss","title":"SUNDIALS's:","text":"<pre><code>SUNDIALS: SUite of Nonlinear and DIfferential/ALgebraic Equation Solvers\n</code></pre>"},{"location":"Glossary/#sundials","title":"SUNDIALS:","text":"<pre><code>SUNDIALS: SUite of Nonlinear and DIfferential/ALgebraic Equation Solvers\n</code></pre>"},{"location":"Glossary/#survivors","title":"SURVIVOR's:","text":"<pre><code>Tool set for simulating/evaluating SVs, merging and comparing SVs within and among samples,\n</code></pre>"},{"location":"Glossary/#survivor","title":"SURVIVOR:","text":"<pre><code>Tool set for simulating/evaluating SVs, merging and comparing SVs within and among samples,\n</code></pre>"},{"location":"Glossary/#swigs","title":"SWIG's:","text":"<pre><code>SWIG is a software development tool that connects programs written in C and C++ with  a variety of high-level programming languages.\n</code></pre>"},{"location":"Glossary/#swig","title":"SWIG:","text":"<pre><code>SWIG is a software development tool that connects programs written in C and C++ with  a variety of high-level programming languages.\n</code></pre>"},{"location":"Glossary/#salmons","title":"Salmon's:","text":"<pre><code>Salmon is a wicked-fast program to produce a highly-accurate,\n</code></pre>"},{"location":"Glossary/#salmon","title":"Salmon:","text":"<pre><code>Salmon is a wicked-fast program to produce a highly-accurate,\n</code></pre>"},{"location":"Glossary/#sambambas","title":"Sambamba's:","text":"<pre><code>Tools for working with SAM/BAM data\n</code></pre>"},{"location":"Glossary/#sambamba","title":"Sambamba:","text":"<pre><code>Tools for working with SAM/BAM data\n</code></pre>"},{"location":"Glossary/#scalapacks","title":"ScaLAPACK's:","text":"<pre><code>The ScaLAPACK (or Scalable LAPACK) library includes a subset of LAPACK routines\n</code></pre>"},{"location":"Glossary/#scalapack","title":"ScaLAPACK:","text":"<pre><code>The ScaLAPACK (or Scalable LAPACK) library includes a subset of LAPACK routines\n</code></pre>"},{"location":"Glossary/#scalascas","title":"Scalasca's:","text":"<pre><code>Tool that supports the performance optimization of\n</code></pre>"},{"location":"Glossary/#scalasca","title":"Scalasca:","text":"<pre><code>Tool that supports the performance optimization of\n</code></pre>"},{"location":"Glossary/#score-ps","title":"Score-P's:","text":"<pre><code>Measurement infrastructure is a highly scalable and easy-to-use\n</code></pre>"},{"location":"Glossary/#score-p","title":"Score-P:","text":"<pre><code>Measurement infrastructure is a highly scalable and easy-to-use\n</code></pre>"},{"location":"Glossary/#seqan3s","title":"SeqAn3's:","text":"<pre><code>C++ library of efficient algorithms and data structures for the\n</code></pre>"},{"location":"Glossary/#seqan3","title":"SeqAn3:","text":"<pre><code>C++ library of efficient algorithms and data structures for the\n</code></pre>"},{"location":"Glossary/#seqkits","title":"SeqKit's:","text":"<pre><code>Ultrafast toolkit for FASTA/Q file manipulation\n</code></pre>"},{"location":"Glossary/#seqkit","title":"SeqKit:","text":"<pre><code>Ultrafast toolkit for FASTA/Q file manipulation\n</code></pre>"},{"location":"Glossary/#seqmonks","title":"SeqMonk's:","text":"<pre><code>A tool to visualise and analyse high throughput mapped sequence data.\n</code></pre>"},{"location":"Glossary/#seqmonk","title":"SeqMonk:","text":"<pre><code>A tool to visualise and analyse high throughput mapped sequence data.\n</code></pre>"},{"location":"Glossary/#sibelias","title":"SiBELia's:","text":"<pre><code>A comparative genomics tool for analysing genomic variations that correlate with pathogens, or\n</code></pre>"},{"location":"Glossary/#sibelia","title":"SiBELia:","text":"<pre><code>A comparative genomics tool for analysing genomic variations that correlate with pathogens, or\n</code></pre>"},{"location":"Glossary/#siestas","title":"Siesta's:","text":"<pre><code>SIESTA is both a method and its computer program implementation, to perform efficient electronic\n</code></pre>"},{"location":"Glossary/#siesta","title":"Siesta:","text":"<pre><code>SIESTA is both a method and its computer program implementation, to perform efficient electronic\n</code></pre>"},{"location":"Glossary/#signalps","title":"SignalP's:","text":"<pre><code>SignalP predicts the presence  and  location of signal peptide  cleavage sites\n</code></pre>"},{"location":"Glossary/#signalp","title":"SignalP:","text":"<pre><code>SignalP predicts the presence  and  location of signal peptide  cleavage sites\n</code></pre>"},{"location":"Glossary/#sniffless","title":"Sniffles's:","text":"<pre><code>A fast structural variant caller for long-read sequencing.\n</code></pre>"},{"location":"Glossary/#sniffles","title":"Sniffles:","text":"<pre><code>A fast structural variant caller for long-read sequencing.\n</code></pre>"},{"location":"Glossary/#sortmernas","title":"SortMeRNA's:","text":"<pre><code>SortMeRNA is a biological sequence analysis tool for filtering, mapping and OTU-picking NGS reads.\n</code></pre>"},{"location":"Glossary/#sortmerna","title":"SortMeRNA:","text":"<pre><code>SortMeRNA is a biological sequence analysis tool for filtering, mapping and OTU-picking NGS reads.\n</code></pre>"},{"location":"Glossary/#sourcetrackers","title":"SourceTracker's:","text":"<pre><code>SourceTracker is a Bayesian approach to estimating the proportion of a novel community that comes\n</code></pre>"},{"location":"Glossary/#sourcetracker","title":"SourceTracker:","text":"<pre><code>SourceTracker is a Bayesian approach to estimating the proportion of a novel community that comes\n</code></pre>"},{"location":"Glossary/#spacks","title":"Spack's:","text":"<pre><code>Spack is a package manager for supercomputers, Linux, and macOS. It makes installing scientific\n</code></pre>"},{"location":"Glossary/#spack","title":"Spack:","text":"<pre><code>Spack is a package manager for supercomputers, Linux, and macOS. It makes installing scientific\n</code></pre>"},{"location":"Glossary/#sparks","title":"Spark's:","text":"<pre><code>Spark is Hadoop MapReduce done in memory\n</code></pre>"},{"location":"Glossary/#spark","title":"Spark:","text":"<pre><code>Spark is Hadoop MapReduce done in memory\n</code></pre>"},{"location":"Glossary/#squeezemetas","title":"SqueezeMeta's:","text":"<pre><code>fully automated metagenomics pipeline, from reads to bins.\n</code></pre>"},{"location":"Glossary/#squeezemeta","title":"SqueezeMeta:","text":"<pre><code>fully automated metagenomics pipeline, from reads to bins.\n</code></pre>"},{"location":"Glossary/#stackss","title":"Stacks's:","text":"<pre><code>Stacks is a software pipeline for building loci from short-read sequences, such as those generated on\n</code></pre>"},{"location":"Glossary/#stacks","title":"Stacks:","text":"<pre><code>Stacks is a software pipeline for building loci from short-read sequences, such as those generated on\n</code></pre>"},{"location":"Glossary/#stringties","title":"StringTie's:","text":"<pre><code>StringTie is a fast and highly efficient assembler of RNA-Seq alignments into potential transcripts.\n</code></pre>"},{"location":"Glossary/#stringtie","title":"StringTie:","text":"<pre><code>StringTie is a fast and highly efficient assembler of RNA-Seq alignments into potential transcripts.\n</code></pre>"},{"location":"Glossary/#structures","title":"Structure's:","text":"<pre><code>The program structure is a free software package for using multi-locus genotype data to investigate\n</code></pre>"},{"location":"Glossary/#structure","title":"Structure:","text":"<pre><code>The program structure is a free software package for using multi-locus genotype data to investigate\n</code></pre>"},{"location":"Glossary/#subreads","title":"Subread's:","text":"<pre><code>High performance read alignment, quantification and mutation discovery\n</code></pre>"},{"location":"Glossary/#subread","title":"Subread:","text":"<pre><code>High performance read alignment, quantification and mutation discovery\n</code></pre>"},{"location":"Glossary/#subversions","title":"Subversion's:","text":"<pre><code>Subversion is an open source version control system.\n</code></pre>"},{"location":"Glossary/#subversion","title":"Subversion:","text":"<pre><code>Subversion is an open source version control system.\n</code></pre>"},{"location":"Glossary/#suitesparses","title":"SuiteSparse's:","text":"<pre><code>SuiteSparse is a collection of libraries manipulate sparse matrices.\n</code></pre>"},{"location":"Glossary/#suitesparse","title":"SuiteSparse:","text":"<pre><code>SuiteSparse is a collection of libraries manipulate sparse matrices.\n</code></pre>"},{"location":"Glossary/#superlus","title":"SuperLU's:","text":"<pre><code>Solution of large, sparse, nonsymmetric systems of linear equations.\n</code></pre>"},{"location":"Glossary/#superlu","title":"SuperLU:","text":"<pre><code>Solution of large, sparse, nonsymmetric systems of linear equations.\n</code></pre>"},{"location":"Glossary/#supernovas","title":"Supernova's:","text":"<pre><code>Supernova is a software package for de novo assembly from Chromium Linked-Reads\n</code></pre>"},{"location":"Glossary/#supernova","title":"Supernova:","text":"<pre><code>Supernova is a software package for de novo assembly from Chromium Linked-Reads\n</code></pre>"},{"location":"Glossary/#tmhmms","title":"TMHMM's:","text":"<pre><code>Prediction of transmembrane helices in proteins\n</code></pre>"},{"location":"Glossary/#tmhmm","title":"TMHMM:","text":"<pre><code>Prediction of transmembrane helices in proteins\n</code></pre>"},{"location":"Glossary/#tsebras","title":"TSEBRA's:","text":"<pre><code>Transcript Selector for BRAKER\n</code></pre>"},{"location":"Glossary/#tsebra","title":"TSEBRA:","text":"<pre><code>Transcript Selector for BRAKER\n</code></pre>"},{"location":"Glossary/#turbomoles","title":"TURBOMOLE's:","text":"<pre><code>Program Package For Electronic Structure Calculations.\n</code></pre>"},{"location":"Glossary/#turbomole","title":"TURBOMOLE:","text":"<pre><code>Program Package For Electronic Structure Calculations.\n</code></pre>"},{"location":"Glossary/#twl-ninjas","title":"TWL-NINJA's:","text":"<pre><code>Nearly Infinite Neighbor Joining Application.\n</code></pre>"},{"location":"Glossary/#twl-ninja","title":"TWL-NINJA:","text":"<pre><code>Nearly Infinite Neighbor Joining Application.\n</code></pre>"},{"location":"Glossary/#tcls","title":"Tcl's:","text":"<pre><code>Tcl (Tool Command Language) is a very powerful but easy to learn dynamic programming language,\n</code></pre>"},{"location":"Glossary/#tcl","title":"Tcl:","text":"<pre><code>Tcl (Tool Command Language) is a very powerful but easy to learn dynamic programming language,\n</code></pre>"},{"location":"Glossary/#tensorflows","title":"TensorFlow's:","text":"<pre><code>An open-source software library for Machine Intelligence\n</code></pre>"},{"location":"Glossary/#tensorflow","title":"TensorFlow:","text":"<pre><code>An open-source software library for Machine Intelligence\n</code></pre>"},{"location":"Glossary/#tensorrts","title":"TensorRT's:","text":"<pre><code>NVIDIA TensorRT is a platform for high-performance deep learning inference\n</code></pre>"},{"location":"Glossary/#tensorrt","title":"TensorRT:","text":"<pre><code>NVIDIA TensorRT is a platform for high-performance deep learning inference\n</code></pre>"},{"location":"Glossary/#theanos","title":"Theano's:","text":"<pre><code>Theano is a Python library that allows you to define, optimize,\n</code></pre>"},{"location":"Glossary/#theano","title":"Theano:","text":"<pre><code>Theano is a Python library that allows you to define, optimize,\n</code></pre>"},{"location":"Glossary/#tks","title":"Tk's:","text":"<pre><code>Tk is an open source, cross-platform widget toolchain that provides a library of basic elements for\n</code></pre>"},{"location":"Glossary/#tk","title":"Tk:","text":"<pre><code>Tk is an open source, cross-platform widget toolchain that provides a library of basic elements for\n</code></pre>"},{"location":"Glossary/#tophats","title":"TopHat's:","text":"<pre><code>TopHat is a fast splice junction mapper for RNA-Seq reads.\n</code></pre>"},{"location":"Glossary/#tophat","title":"TopHat:","text":"<pre><code>TopHat is a fast splice junction mapper for RNA-Seq reads.\n</code></pre>"},{"location":"Glossary/#transdecoders","title":"TransDecoder's:","text":"<pre><code>TransDecoder identifies candidate coding regions within transcript sequences.\n</code></pre>"},{"location":"Glossary/#transdecoder","title":"TransDecoder:","text":"<pre><code>TransDecoder identifies candidate coding regions within transcript sequences.\n</code></pre>"},{"location":"Glossary/#treemixs","title":"TreeMix's:","text":"<pre><code>TreeMix is a method for inferring the patterns of population splits and mixtures in the history of a\n</code></pre>"},{"location":"Glossary/#treemix","title":"TreeMix:","text":"<pre><code>TreeMix is a method for inferring the patterns of population splits and mixtures in the history of a\n</code></pre>"},{"location":"Glossary/#trilinoss","title":"Trilinos's:","text":"<pre><code>The Trilinos Project is an effort to develop algorithms and enabling technologies\n</code></pre>"},{"location":"Glossary/#trilinos","title":"Trilinos:","text":"<pre><code>The Trilinos Project is an effort to develop algorithms and enabling technologies\n</code></pre>"},{"location":"Glossary/#trimgalores","title":"TrimGalore's:","text":"<pre><code>A wrapper of FastQC and cutadapt to automate quality and adapter trimming\n</code></pre>"},{"location":"Glossary/#trimgalore","title":"TrimGalore:","text":"<pre><code>A wrapper of FastQC and cutadapt to automate quality and adapter trimming\n</code></pre>"},{"location":"Glossary/#trimmomatics","title":"Trimmomatic's:","text":"<pre><code>Trimmomatic performs a variety of useful trimming tasks for illumina\n</code></pre>"},{"location":"Glossary/#trimmomatic","title":"Trimmomatic:","text":"<pre><code>Trimmomatic performs a variety of useful trimming tasks for illumina\n</code></pre>"},{"location":"Glossary/#trinitys","title":"Trinity's:","text":"<pre><code>Trinity represents a novel method for the efficient and robust de novo reconstruction\n</code></pre>"},{"location":"Glossary/#trinity","title":"Trinity:","text":"<pre><code>Trinity represents a novel method for the efficient and robust de novo reconstruction\n</code></pre>"},{"location":"Glossary/#trinotates","title":"Trinotate's:","text":"<pre><code>C++ library of efficient algorithms and data structures for the\n</code></pre>"},{"location":"Glossary/#trinotate","title":"Trinotate:","text":"<pre><code>C++ library of efficient algorithms and data structures for the\n</code></pre>"},{"location":"Glossary/#trycyclers","title":"Trycycler's:","text":"<pre><code>Tool for generating consensus long-read assemblies for bacterial genomes.\n</code></pre>"},{"location":"Glossary/#trycycler","title":"Trycycler:","text":"<pre><code>Tool for generating consensus long-read assemblies for bacterial genomes.\n</code></pre>"},{"location":"Glossary/#tuiviews","title":"TuiView's:","text":"<pre><code>TuiView is a lightweight raster GIS with powerful raster attribute table manipulation\n</code></pre>"},{"location":"Glossary/#tuiview","title":"TuiView:","text":"<pre><code>TuiView is a lightweight raster GIS with powerful raster attribute table manipulation\n</code></pre>"},{"location":"Glossary/#turbovncs","title":"TurboVNC's:","text":"<pre><code>TurboVNC is a derivative of VNC (Virtual Network Computing) that is tuned to provide\n</code></pre>"},{"location":"Glossary/#turbovnc","title":"TurboVNC:","text":"<pre><code>TurboVNC is a derivative of VNC (Virtual Network Computing) that is tuned to provide\n</code></pre>"},{"location":"Glossary/#ucxs","title":"UCX's:","text":"<pre><code>Unified Communication X\n</code></pre>"},{"location":"Glossary/#ucx","title":"UCX:","text":"<pre><code>Unified Communication X\n</code></pre>"},{"location":"Glossary/#udunitss","title":"UDUNITS's:","text":"<pre><code>UDUNITS supports conversion of unit specifications between formatted and binary forms,  arithmetic manipulation of units, and conversion of values between compatible scales of measurement.\n</code></pre>"},{"location":"Glossary/#udunits","title":"UDUNITS:","text":"<pre><code>UDUNITS supports conversion of unit specifications between formatted and binary forms,  arithmetic manipulation of units, and conversion of values between compatible scales of measurement.\n</code></pre>"},{"location":"Glossary/#usearchs","title":"USEARCH's:","text":"<pre><code>USEARCH is a unique sequence analysis tool which offers search and clustering algorithms that are\n</code></pre>"},{"location":"Glossary/#usearch","title":"USEARCH:","text":"<pre><code>USEARCH is a unique sequence analysis tool which offers search and clustering algorithms that are\n</code></pre>"},{"location":"Glossary/#unicyclers","title":"Unicycler's:","text":"<pre><code>Assembly pipeline for bacterial genomes. It can assemble Illumina-only read sets\n</code></pre>"},{"location":"Glossary/#unicycler","title":"Unicycler:","text":"<pre><code>Assembly pipeline for bacterial genomes. It can assemble Illumina-only read sets\n</code></pre>"},{"location":"Glossary/#vasps","title":"VASP's:","text":"<pre><code>The Vienna Ab initio Simulation Package (VASP) is a computer program for atomic scale materials modelling, e.g. electronic structure calculations and quantum-mechanical molecular dynamics, from first principles.\n</code></pre>"},{"location":"Glossary/#vasp","title":"VASP:","text":"<pre><code>The Vienna Ab initio Simulation Package (VASP) is a computer program for atomic scale materials modelling, e.g. electronic structure calculations and quantum-mechanical molecular dynamics, from first principles.\n</code></pre>"},{"location":"Glossary/#vcf-kits","title":"VCF-kit's:","text":"<pre><code>VCF-kit is a command-line based collection of utilities for performing analysis on\n</code></pre>"},{"location":"Glossary/#vcf-kit","title":"VCF-kit:","text":"<pre><code>VCF-kit is a command-line based collection of utilities for performing analysis on\n</code></pre>"},{"location":"Glossary/#vcftoolss","title":"VCFtools's:","text":"<pre><code>The aim of VCFtools is to provide\n</code></pre>"},{"location":"Glossary/#vcftools","title":"VCFtools:","text":"<pre><code>The aim of VCFtools is to provide\n</code></pre>"},{"location":"Glossary/#veps","title":"VEP's:","text":"<pre><code>Variant Effect Predictor (VEP) determines the effect of your\n</code></pre>"},{"location":"Glossary/#vep","title":"VEP:","text":"<pre><code>Variant Effect Predictor (VEP) determines the effect of your\n</code></pre>"},{"location":"Glossary/#vestas","title":"VESTA's:","text":"<pre><code>VESTA is a 3D visualization program for structural models, volumetric data such as electron/nuclear densities, and crystal morphologies. Run 'VESTA-gui' to launch.\n</code></pre>"},{"location":"Glossary/#vesta","title":"VESTA:","text":"<pre><code>VESTA is a 3D visualization program for structural models, volumetric data such as electron/nuclear densities, and crystal morphologies. Run 'VESTA-gui' to launch.\n</code></pre>"},{"location":"Glossary/#vibrants","title":"VIBRANT's:","text":"<pre><code>Virus Identification By iteRative ANnoTation\n</code></pre>"},{"location":"Glossary/#vibrant","title":"VIBRANT:","text":"<pre><code>Virus Identification By iteRative ANnoTation\n</code></pre>"},{"location":"Glossary/#vmds","title":"VMD's:","text":"<pre><code>VMD is a molecular visualization program for displaying, animating, and analyzing large biomolecular\n</code></pre>"},{"location":"Glossary/#vmd","title":"VMD:","text":"<pre><code>VMD is a molecular visualization program for displaying, animating, and analyzing large biomolecular\n</code></pre>"},{"location":"Glossary/#vsearchs","title":"VSEARCH's:","text":"<pre><code>An open source alternative to the metagenomics tool USEARCH.\n</code></pre>"},{"location":"Glossary/#vsearch","title":"VSEARCH:","text":"<pre><code>An open source alternative to the metagenomics tool USEARCH.\n</code></pre>"},{"location":"Glossary/#vtks","title":"VTK's:","text":"<pre><code>The Visualization Toolkit (VTK) is an open-source, freely available software system for  3D computer graphics, image processing and visualization. VTK consists of a C++ class library and several  interpreted interface layers including Tcl/Tk, Java, and Python. VTK supports a wide variety of visualization  algorithms including: scalar, vector, tensor, texture, and volumetric methods; and advanced modeling techniques  such as: implicit modeling, polygon reduction, mesh smoothing, cutting, contouring, and Delaunay triangulation.\n</code></pre>"},{"location":"Glossary/#vtk","title":"VTK:","text":"<pre><code>The Visualization Toolkit (VTK) is an open-source, freely available software system for  3D computer graphics, image processing and visualization. VTK consists of a C++ class library and several  interpreted interface layers including Tcl/Tk, Java, and Python. VTK supports a wide variety of visualization  algorithms including: scalar, vector, tensor, texture, and volumetric methods; and advanced modeling techniques  such as: implicit modeling, polygon reduction, mesh smoothing, cutting, contouring, and Delaunay triangulation.\n</code></pre>"},{"location":"Glossary/#vtunes","title":"VTune's:","text":"<pre><code>Intel VTune Amplifier XE is the premier performance profiler for C, C++, C#, Fortran,  Assembly and Java.\n</code></pre>"},{"location":"Glossary/#vtune","title":"VTune:","text":"<pre><code>Intel VTune Amplifier XE is the premier performance profiler for C, C++, C#, Fortran,  Assembly and Java.\n</code></pre>"},{"location":"Glossary/#valgrinds","title":"Valgrind's:","text":"<pre><code>Valgrind: Debugging and profiling tools\n</code></pre>"},{"location":"Glossary/#valgrind","title":"Valgrind:","text":"<pre><code>Valgrind: Debugging and profiling tools\n</code></pre>"},{"location":"Glossary/#varscans","title":"VarScan's:","text":"<pre><code>Variant calling and somatic mutation/CNV detection for next-generation sequencing data\n</code></pre>"},{"location":"Glossary/#varscan","title":"VarScan:","text":"<pre><code>Variant calling and somatic mutation/CNV detection for next-generation sequencing data\n</code></pre>"},{"location":"Glossary/#velvets","title":"Velvet's:","text":"<pre><code>Sequence assembler for very short reads\n</code></pre>"},{"location":"Glossary/#velvet","title":"Velvet:","text":"<pre><code>Sequence assembler for very short reads\n</code></pre>"},{"location":"Glossary/#velvetoptimisers","title":"VelvetOptimiser's:","text":"<pre><code>Perl script for optimising the three primary parameter options of the Velvet de novo sequence assembler.\n</code></pre>"},{"location":"Glossary/#velvetoptimiser","title":"VelvetOptimiser:","text":"<pre><code>Perl script for optimising the three primary parameter options of the Velvet de novo sequence assembler.\n</code></pre>"},{"location":"Glossary/#viennarnas","title":"ViennaRNA's:","text":"<pre><code>The Vienna RNA Package consists of a C code library and several\n</code></pre>"},{"location":"Glossary/#viennarna","title":"ViennaRNA:","text":"<pre><code>The Vienna RNA Package consists of a C code library and several\n</code></pre>"},{"location":"Glossary/#vims","title":"Vim's:","text":"<pre><code>Vim is an advanced text editor that seeks to provide the power   of the de-facto Unix editor 'Vi', with a more complete feature set.\n</code></pre>"},{"location":"Glossary/#vim","title":"Vim:","text":"<pre><code>Vim is an advanced text editor that seeks to provide the power   of the de-facto Unix editor 'Vi', with a more complete feature set.\n</code></pre>"},{"location":"Glossary/#virhostmatchers","title":"VirHostMatcher's:","text":"<pre><code>Tools for computing various oligonucleotide frequency (ONF) based distance/dissimialrity measures.\n</code></pre>"},{"location":"Glossary/#virhostmatcher","title":"VirHostMatcher:","text":"<pre><code>Tools for computing various oligonucleotide frequency (ONF) based distance/dissimialrity measures.\n</code></pre>"},{"location":"Glossary/#virsorters","title":"VirSorter's:","text":"<pre><code>VirSorter: mining viral signal from microbial genomic data.\n</code></pre>"},{"location":"Glossary/#virsorter","title":"VirSorter:","text":"<pre><code>VirSorter: mining viral signal from microbial genomic data.\n</code></pre>"},{"location":"Glossary/#virtualgls","title":"VirtualGL's:","text":"<pre><code>VirtualGL is an open source toolkit that gives any Linux or\n</code></pre>"},{"location":"Glossary/#virtualgl","title":"VirtualGL:","text":"<pre><code>VirtualGL is an open source toolkit that gives any Linux or\n</code></pre>"},{"location":"Glossary/#visits","title":"VisIt's:","text":"<pre><code>VisIt is an Open Source, interactive, scalable, visualization, animation and analysis tool.  This version supports interactive CPU-only rendering with the VisIt GUI using the Mesa library. It does not support GPU rendering.  Use the GALLIUM_DRIVER environment variable to choose a software renderer, it is recommended to use  GALLIUM_DRIVER=swr  for best performance.\n</code></pre>"},{"location":"Glossary/#visit","title":"VisIt:","text":"<pre><code>VisIt is an Open Source, interactive, scalable, visualization, animation and analysis tool.  This version supports interactive CPU-only rendering with the VisIt GUI using the Mesa library. It does not support GPU rendering.  Use the GALLIUM_DRIVER environment variable to choose a software renderer, it is recommended to use  GALLIUM_DRIVER=swr  for best performance.\n</code></pre>"},{"location":"Glossary/#waafles","title":"WAAFLE's:","text":"<pre><code>Workflow to Annotate Assemblies and Find LGT Events.\n</code></pre>"},{"location":"Glossary/#waafle","title":"WAAFLE:","text":"<pre><code>Workflow to Annotate Assemblies and Find LGT Events.\n</code></pre>"},{"location":"Glossary/#whatshaps","title":"WhatsHap's:","text":"<pre><code>Tool for phasing genomic variants using DNA sequencing reads, also called read-based phasing or haplotype assembly.\n</code></pre>"},{"location":"Glossary/#whatshap","title":"WhatsHap:","text":"<pre><code>Tool for phasing genomic variants using DNA sequencing reads, also called read-based phasing or haplotype assembly.\n</code></pre>"},{"location":"Glossary/#wise2s","title":"Wise2's:","text":"<pre><code>Aligning proteins or protein HMMs to DNA\n</code></pre>"},{"location":"Glossary/#wise2","title":"Wise2:","text":"<pre><code>Aligning proteins or protein HMMs to DNA\n</code></pre>"},{"location":"Glossary/#xconvs","title":"XCONV's:","text":"<pre><code>Xconv is a program designed to convert model output into a format suitable for use in various plotting packages. Xconv is designed to be simple to use with a point and click, windows based interface.\n</code></pre>"},{"location":"Glossary/#xconv","title":"XCONV:","text":"<pre><code>Xconv is a program designed to convert model output into a format suitable for use in various plotting packages. Xconv is designed to be simple to use with a point and click, windows based interface.\n</code></pre>"},{"location":"Glossary/#xgkss","title":"XGKS's:","text":"<pre><code>XGKS is a level 2C implementation of the ANSI Graphical Kernel System\n</code></pre>"},{"location":"Glossary/#xgks","title":"XGKS:","text":"<pre><code>XGKS is a level 2C implementation of the ANSI Graphical Kernel System\n</code></pre>"},{"location":"Glossary/#xhmms","title":"XHMM's:","text":"<pre><code>Calls copy number variation (CNV) from normalized read-depth data from exome capture or other targeted sequencing experiments.\n</code></pre>"},{"location":"Glossary/#xhmm","title":"XHMM:","text":"<pre><code>Calls copy number variation (CNV) from normalized read-depth data from exome capture or other targeted sequencing experiments.\n</code></pre>"},{"location":"Glossary/#xioss","title":"XIOS's:","text":"<pre><code>XIOS stands for XML-IO-Server and is a library dedicated to I/O management in climate codes. This version uses netCDF4 with the parallel HDF5 library, and it contains a patch for various problems in the mesh connectivity algorithm\n</code></pre>"},{"location":"Glossary/#xios","title":"XIOS:","text":"<pre><code>XIOS stands for XML-IO-Server and is a library dedicated to I/O management in climate codes. This version uses netCDF4 with the parallel HDF5 library, and it contains a patch for various problems in the mesh connectivity algorithm\n</code></pre>"},{"location":"Glossary/#xmds2s","title":"XMDS2's:","text":"<pre><code>Fast integrator of stochastic partial differential equations.\n</code></pre>"},{"location":"Glossary/#xmds2","title":"XMDS2:","text":"<pre><code>Fast integrator of stochastic partial differential equations.\n</code></pre>"},{"location":"Glossary/#xsds","title":"XSD's:","text":"<pre><code>CodeSynthesis XSD is an open-source, cross-platform W3C XML Schema to C++ data binding compiler.\n</code></pre>"},{"location":"Glossary/#xsd","title":"XSD:","text":"<pre><code>CodeSynthesis XSD is an open-source, cross-platform W3C XML Schema to C++ data binding compiler.\n</code></pre>"},{"location":"Glossary/#xzs","title":"XZ's:","text":"<pre><code>xz: XZ utilities\n</code></pre>"},{"location":"Glossary/#xz","title":"XZ:","text":"<pre><code>xz: XZ utilities\n</code></pre>"},{"location":"Glossary/#xerces-cs","title":"Xerces-C++'s:","text":"<pre><code>Xerces-C++ is a validating XML parser written in a portable\n</code></pre>"},{"location":"Glossary/#xerces-c","title":"Xerces-C++:","text":"<pre><code>Xerces-C++ is a validating XML parser written in a portable\n</code></pre>"},{"location":"Glossary/#yaxts","title":"YAXT's:","text":"<pre><code>Yet Another eXchange Tool - Library that performs halo exchange with MPI for domain decomposed simulations.\n</code></pre>"},{"location":"Glossary/#yaxt","title":"YAXT:","text":"<pre><code>Yet Another eXchange Tool - Library that performs halo exchange with MPI for domain decomposed simulations.\n</code></pre>"},{"location":"Glossary/#yades","title":"Yade's:","text":"<pre><code>Yade is an extensible open-source framework for discrete numerical models,\n</code></pre>"},{"location":"Glossary/#yade","title":"Yade:","text":"<pre><code>Yade is an extensible open-source framework for discrete numerical models,\n</code></pre>"},{"location":"Glossary/#yasms","title":"Yasm's:","text":"<pre><code>Yasm: Complete rewrite of the NASM assembler with BSD license\n</code></pre>"},{"location":"Glossary/#yasm","title":"Yasm:","text":"<pre><code>Yasm: Complete rewrite of the NASM assembler with BSD license\n</code></pre>"},{"location":"Glossary/#zeromqs","title":"ZeroMQ's:","text":"<pre><code>ZeroMQ looks like an embeddable networking library but acts like a concurrency framework.\n</code></pre>"},{"location":"Glossary/#zeromq","title":"ZeroMQ:","text":"<pre><code>ZeroMQ looks like an embeddable networking library but acts like a concurrency framework.\n</code></pre>"},{"location":"Glossary/#zips","title":"Zip's:","text":"<pre><code>Zip is a compression and file packaging/archive utility.\n</code></pre>"},{"location":"Glossary/#zip","title":"Zip:","text":"<pre><code>Zip is a compression and file packaging/archive utility.\n</code></pre>"},{"location":"Glossary/#zonations","title":"Zonation's:","text":"<pre><code>Spatial conservation prioritisation framework for large-scale conservation planning.\n</code></pre>"},{"location":"Glossary/#zonation","title":"Zonation:","text":"<pre><code>Spatial conservation prioritisation framework for large-scale conservation planning.\n</code></pre>"},{"location":"Glossary/#angsds","title":"angsd's:","text":"<pre><code>Program for analysing NGS data.\n</code></pre>"},{"location":"Glossary/#angsd","title":"angsd:","text":"<pre><code>Program for analysing NGS data.\n</code></pre>"},{"location":"Glossary/#ants_2","title":"ant's:","text":"<pre><code>Apache Ant is a Java library and command-line tool whose mission is to drive processes described in\n</code></pre>"},{"location":"Glossary/#ant","title":"ant:","text":"<pre><code>Apache Ant is a Java library and command-line tool whose mission is to drive processes described in\n</code></pre>"},{"location":"Glossary/#antismashs","title":"antiSMASH's:","text":"<pre><code>antiSMASH allows the rapid genome-wide identification, annotation and analysis of secondary\n</code></pre>"},{"location":"Glossary/#antismash","title":"antiSMASH:","text":"<pre><code>antiSMASH allows the rapid genome-wide identification, annotation and analysis of secondary\n</code></pre>"},{"location":"Glossary/#any2fastas","title":"any2fasta's:","text":"<pre><code>Convert various sequence formats to FASTA\n</code></pre>"},{"location":"Glossary/#any2fasta","title":"any2fasta:","text":"<pre><code>Convert various sequence formats to FASTA\n</code></pre>"},{"location":"Glossary/#argtables","title":"argtable's:","text":"<pre><code>Argtable is an ANSI C library for parsing GNU style\n</code></pre>"},{"location":"Glossary/#argtable","title":"argtable:","text":"<pre><code>Argtable is an ANSI C library for parsing GNU style\n</code></pre>"},{"location":"Glossary/#aria2s","title":"aria2's:","text":"<pre><code>aria2 is a lightweight multi-protocol &amp; multi-source command-line download utility.\n</code></pre>"},{"location":"Glossary/#aria2","title":"aria2:","text":"<pre><code>aria2 is a lightweight multi-protocol &amp; multi-source command-line download utility.\n</code></pre>"},{"location":"Glossary/#arpack-ngs","title":"arpack-ng's:","text":"<pre><code>ARPACK is a collection of Fortran77 subroutines designed to solve large scale eigenvalue problems.\n</code></pre>"},{"location":"Glossary/#arpack-ng","title":"arpack-ng:","text":"<pre><code>ARPACK is a collection of Fortran77 subroutines designed to solve large scale eigenvalue problems.\n</code></pre>"},{"location":"Glossary/#at-spi2-atks","title":"at-spi2-atk's:","text":"<pre><code>AT-SPI 2 toolkit bridge\n</code></pre>"},{"location":"Glossary/#at-spi2-atk","title":"at-spi2-atk:","text":"<pre><code>AT-SPI 2 toolkit bridge\n</code></pre>"},{"location":"Glossary/#attrs","title":"attr's:","text":"<pre><code>Commands for Manipulating Filesystem Extended Attributes\n</code></pre>"},{"location":"Glossary/#attr","title":"attr:","text":"<pre><code>Commands for Manipulating Filesystem Extended Attributes\n</code></pre>"},{"location":"Glossary/#azul-zulus","title":"azul-zulu's:","text":"<pre><code>Java Development Kit (JDK), and a compliant implementation of the Java Standard Edition (SE) specification.\n</code></pre>"},{"location":"Glossary/#azul-zulu","title":"azul-zulu:","text":"<pre><code>Java Development Kit (JDK), and a compliant implementation of the Java Standard Edition (SE) specification.\n</code></pre>"},{"location":"Glossary/#bamutils","title":"bamUtil's:","text":"<pre><code>Repository that contains several programs\n</code></pre>"},{"location":"Glossary/#bamutil","title":"bamUtil:","text":"<pre><code>Repository that contains several programs\n</code></pre>"},{"location":"Glossary/#barrnaps","title":"barrnap's:","text":"<pre><code>Barrnap predicts the location of ribosomal RNA genes in genomes.\n</code></pre>"},{"location":"Glossary/#barrnap","title":"barrnap:","text":"<pre><code>Barrnap predicts the location of ribosomal RNA genes in genomes.\n</code></pre>"},{"location":"Glossary/#bcl2fastq2s","title":"bcl2fastq2's:","text":"<pre><code>bcl2fastq Conversion Software both demultiplexes data and converts BCL files generated by\n</code></pre>"},{"location":"Glossary/#bcl2fastq2","title":"bcl2fastq2:","text":"<pre><code>bcl2fastq Conversion Software both demultiplexes data and converts BCL files generated by\n</code></pre>"},{"location":"Glossary/#binutilss","title":"binutils's:","text":"<pre><code>binutils: GNU binary utilities\n</code></pre>"},{"location":"Glossary/#binutils","title":"binutils:","text":"<pre><code>binutils: GNU binary utilities\n</code></pre>"},{"location":"Glossary/#bioawks","title":"bioawk's:","text":"<pre><code>An extension to awk, adding the support of several common biological data formats\n</code></pre>"},{"location":"Glossary/#bioawk","title":"bioawk:","text":"<pre><code>An extension to awk, adding the support of several common biological data formats\n</code></pre>"},{"location":"Glossary/#blasr_libcpps","title":"blasr_libcpp's:","text":"<pre><code>Blasr_libcpp is a library used by blasr and other executables such as samtoh5, loadPulses for\n</code></pre>"},{"location":"Glossary/#blasr_libcpp","title":"blasr_libcpp:","text":"<pre><code>Blasr_libcpp is a library used by blasr and other executables such as samtoh5, loadPulses for\n</code></pre>"},{"location":"Glossary/#breseqs","title":"breseq's:","text":"<pre><code>breseq is a computational pipeline for the analysis of short-read re-sequencing data\n</code></pre>"},{"location":"Glossary/#breseq","title":"breseq:","text":"<pre><code>breseq is a computational pipeline for the analysis of short-read re-sequencing data\n</code></pre>"},{"location":"Glossary/#bsddb3s","title":"bsddb3's:","text":"<pre><code>bsddb3 is a nearly complete Python binding of the\n</code></pre>"},{"location":"Glossary/#bsddb3","title":"bsddb3:","text":"<pre><code>bsddb3 is a nearly complete Python binding of the\n</code></pre>"},{"location":"Glossary/#bzip2s","title":"bzip2's:","text":"<pre><code>bzip2 is a freely available, patent free, high-quality data compressor. It typically\n</code></pre>"},{"location":"Glossary/#bzip2","title":"bzip2:","text":"<pre><code>bzip2 is a freely available, patent free, high-quality data compressor. It typically\n</code></pre>"},{"location":"Glossary/#c-aress","title":"c-ares's:","text":"<pre><code>c-ares is a C library for asynchronous DNS requests (including name resolves)\n</code></pre>"},{"location":"Glossary/#c-ares","title":"c-ares:","text":"<pre><code>c-ares is a C library for asynchronous DNS requests (including name resolves)\n</code></pre>"},{"location":"Glossary/#cairos","title":"cairo's:","text":"<pre><code>Cairo is a 2D graphics library with support for multiple output devices.\n</code></pre>"},{"location":"Glossary/#cairo","title":"cairo:","text":"<pre><code>Cairo is a 2D graphics library with support for multiple output devices.\n</code></pre>"},{"location":"Glossary/#cdbfastas","title":"cdbfasta's:","text":"<pre><code>Fasta file indexing and retrival tool\n</code></pre>"},{"location":"Glossary/#cdbfasta","title":"cdbfasta:","text":"<pre><code>Fasta file indexing and retrival tool\n</code></pre>"},{"location":"Glossary/#chewbbacas","title":"chewBBACA's:","text":"<pre><code>A complete suite for gene-by-gene schema creation and strain identification..\n</code></pre>"},{"location":"Glossary/#chewbbaca","title":"chewBBACA:","text":"<pre><code>A complete suite for gene-by-gene schema creation and strain identification..\n</code></pre>"},{"location":"Glossary/#choppers","title":"chopper's:","text":"<pre><code>Rust implementation of NanoFilt+NanoLyse\n</code></pre>"},{"location":"Glossary/#chopper","title":"chopper:","text":"<pre><code>Rust implementation of NanoFilt+NanoLyse\n</code></pre>"},{"location":"Glossary/#cromwells","title":"cromwell's:","text":"<pre><code>Workflow Management System geared towards scientific workflows.\n</code></pre>"},{"location":"Glossary/#cromwell","title":"cromwell:","text":"<pre><code>Workflow Management System geared towards scientific workflows.\n</code></pre>"},{"location":"Glossary/#ctagss","title":"ctags's:","text":"<pre><code>Ctags generates an index (or tag) file of language objects found in source files that allows these\n</code></pre>"},{"location":"Glossary/#ctags","title":"ctags:","text":"<pre><code>Ctags generates an index (or tag) file of language objects found in source files that allows these\n</code></pre>"},{"location":"Glossary/#ctffinds","title":"ctffind's:","text":"<pre><code>ctffind is a program for finding CTFs of electron micrographs\n</code></pre>"},{"location":"Glossary/#ctffind","title":"ctffind:","text":"<pre><code>ctffind is a program for finding CTFs of electron micrographs\n</code></pre>"},{"location":"Glossary/#cudnns","title":"cuDNN's:","text":"<pre><code>The NVIDIA CUDA Deep Neural Network library (cuDNN) is a GPU-accelerated library of primitives for     deep neural networks.\n</code></pre>"},{"location":"Glossary/#cudnn","title":"cuDNN:","text":"<pre><code>The NVIDIA CUDA Deep Neural Network library (cuDNN) is a GPU-accelerated library of primitives for     deep neural networks.\n</code></pre>"},{"location":"Glossary/#cutadapts","title":"cutadapt's:","text":"<pre><code>cutadapt removes adapter sequences\n</code></pre>"},{"location":"Glossary/#cutadapt","title":"cutadapt:","text":"<pre><code>cutadapt removes adapter sequences\n</code></pre>"},{"location":"Glossary/#cutesvs","title":"cuteSV's:","text":"<pre><code>Fast and scalable long-read-based SV detection\n</code></pre>"},{"location":"Glossary/#cutesv","title":"cuteSV:","text":"<pre><code>Fast and scalable long-read-based SV detection\n</code></pre>"},{"location":"Glossary/#cwltools","title":"cwltool's:","text":"<pre><code>Common Workflow Language tool description reference implementation\n</code></pre>"},{"location":"Glossary/#cwltool","title":"cwltool:","text":"<pre><code>Common Workflow Language tool description reference implementation\n</code></pre>"},{"location":"Glossary/#cyvcf2s","title":"cyvcf2's:","text":"<pre><code>cython + htslib == fast VCF and BCF processing\n</code></pre>"},{"location":"Glossary/#cyvcf2","title":"cyvcf2:","text":"<pre><code>cython + htslib == fast VCF and BCF processing\n</code></pre>"},{"location":"Glossary/#dammits","title":"dammit's:","text":"<pre><code>de novo transcriptome annotator..\n</code></pre>"},{"location":"Glossary/#dammit","title":"dammit:","text":"<pre><code>de novo transcriptome annotator..\n</code></pre>"},{"location":"Glossary/#deeptoolss","title":"deepTools's:","text":"<pre><code>deepTools is a suite of python tools particularly developed for the efficient analysis of\n</code></pre>"},{"location":"Glossary/#deeptools","title":"deepTools:","text":"<pre><code>deepTools is a suite of python tools particularly developed for the efficient analysis of\n</code></pre>"},{"location":"Glossary/#devtoolss","title":"devtools's:","text":"<pre><code>R functions that simplify and expedite common tasks in package development.\n</code></pre>"},{"location":"Glossary/#devtools","title":"devtools:","text":"<pre><code>R functions that simplify and expedite common tasks in package development.\n</code></pre>"},{"location":"Glossary/#double-conversions","title":"double-conversion's:","text":"<pre><code>Efficient binary-decimal and decimal-binary conversion routines for IEEE doubles.\n</code></pre>"},{"location":"Glossary/#double-conversion","title":"double-conversion:","text":"<pre><code>Efficient binary-decimal and decimal-binary conversion routines for IEEE doubles.\n</code></pre>"},{"location":"Glossary/#dreps","title":"drep's:","text":"<pre><code>Rapid and accurate comparison and de-replication of microbial genomes\n</code></pre>"},{"location":"Glossary/#drep","title":"drep:","text":"<pre><code>Rapid and accurate comparison and de-replication of microbial genomes\n</code></pre>"},{"location":"Glossary/#dtcmps","title":"dtcmp's:","text":"<pre><code>DTCMP Library provides pre-defined and user-defined\n</code></pre>"},{"location":"Glossary/#dtcmp","title":"dtcmp:","text":"<pre><code>DTCMP Library provides pre-defined and user-defined\n</code></pre>"},{"location":"Glossary/#dupholds","title":"duphold's:","text":"<pre><code>uphold your DUP and DEL calls\n</code></pre>"},{"location":"Glossary/#duphold","title":"duphold:","text":"<pre><code>uphold your DUP and DEL calls\n</code></pre>"},{"location":"Glossary/#duplex-toolss","title":"duplex-tools's:","text":"<pre><code>Range of tools to support operations on Duplex Sequencing read pairs.\n</code></pre>"},{"location":"Glossary/#duplex-tools","title":"duplex-tools:","text":"<pre><code>Range of tools to support operations on Duplex Sequencing read pairs.\n</code></pre>"},{"location":"Glossary/#ednas","title":"eDNA's:","text":"<pre><code>A suite of tools to conduct metabarcoding analyses targeting any group of organisms. Includes utilities\n</code></pre>"},{"location":"Glossary/#edna","title":"eDNA:","text":"<pre><code>A suite of tools to conduct metabarcoding analyses targeting any group of organisms. Includes utilities\n</code></pre>"},{"location":"Glossary/#eccodess","title":"ecCodes's:","text":"<pre><code>ecCodes is a package developed by ECMWF which provides an application programming interface and\n</code></pre>"},{"location":"Glossary/#eccodes","title":"ecCodes:","text":"<pre><code>ecCodes is a package developed by ECMWF which provides an application programming interface and\n</code></pre>"},{"location":"Glossary/#eccodess_1","title":"eccodes's:","text":"<pre><code>ecCodes is a package developed by ECMWF which provides an application programming interface and a set of tools for decoding and encoding messages in the following formats:  - WMO FM-92 GRIB edition 1 and edition 2 - WMO FM-94 BUFR edition 3 and edition 4  - WMO GTS abbreviated header (only decoding)  A useful set of command line tools provide quick access to the messages.\n</code></pre>"},{"location":"Glossary/#eccodes_1","title":"eccodes:","text":"<pre><code>ecCodes is a package developed by ECMWF which provides an application programming interface and a set of tools for decoding and encoding messages in the following formats:  - WMO FM-92 GRIB edition 1 and edition 2 - WMO FM-94 BUFR edition 3 and edition 4  - WMO GTS abbreviated header (only decoding)  A useful set of command line tools provide quick access to the messages.\n</code></pre>"},{"location":"Glossary/#ectypers","title":"ectyper's:","text":"<pre><code>Standalone versatile serotyping module for Escherichia coli..\n</code></pre>"},{"location":"Glossary/#ectyper","title":"ectyper:","text":"<pre><code>Standalone versatile serotyping module for Escherichia coli..\n</code></pre>"},{"location":"Glossary/#edlibs","title":"edlib's:","text":"<pre><code>Lightweight, super fast library for sequence alignment using edit (Levenshtein) distance.\n</code></pre>"},{"location":"Glossary/#edlib","title":"edlib:","text":"<pre><code>Lightweight, super fast library for sequence alignment using edit (Levenshtein) distance.\n</code></pre>"},{"location":"Glossary/#eggnog-mappers","title":"eggnog-mapper's:","text":"<pre><code>Tool for fast functional annotation of novel sequences (genes or proteins)\n</code></pre>"},{"location":"Glossary/#eggnog-mapper","title":"eggnog-mapper:","text":"<pre><code>Tool for fast functional annotation of novel sequences (genes or proteins)\n</code></pre>"},{"location":"Glossary/#ensmallens","title":"ensmallen's:","text":"<pre><code>C++ header-only library for numerical optimization\n</code></pre>"},{"location":"Glossary/#ensmallen","title":"ensmallen:","text":"<pre><code>C++ header-only library for numerical optimization\n</code></pre>"},{"location":"Glossary/#entrez-directs","title":"entrez-direct's:","text":"<pre><code>an advanced method for accessing the NCBI's set of interconnected databases\n</code></pre>"},{"location":"Glossary/#entrez-direct","title":"entrez-direct:","text":"<pre><code>an advanced method for accessing the NCBI's set of interconnected databases\n</code></pre>"},{"location":"Glossary/#exonerates","title":"exonerate's:","text":"<pre><code>Generic tool for pairwise sequence comparison\n</code></pre>"},{"location":"Glossary/#exonerate","title":"exonerate:","text":"<pre><code>Generic tool for pairwise sequence comparison\n</code></pre>"},{"location":"Glossary/#expats","title":"expat's:","text":"<pre><code>Expat is an XML parser library written in C. It is a stream-oriented parser  in which an application registers handlers for things the parser might find  in the XML document (like start tags)\n</code></pre>"},{"location":"Glossary/#expat","title":"expat:","text":"<pre><code>Expat is an XML parser library written in C. It is a stream-oriented parser  in which an application registers handlers for things the parser might find  in the XML document (like start tags)\n</code></pre>"},{"location":"Glossary/#faststructures","title":"fastStructure's:","text":"<pre><code>fastStructure is an algorithm for inferring population structure from large SNP genotype data. It is based on a variational Bayesian framework for posterior inference and is written in Python2.x.\n</code></pre>"},{"location":"Glossary/#faststructure","title":"fastStructure:","text":"<pre><code>fastStructure is an algorithm for inferring population structure from large SNP genotype data. It is based on a variational Bayesian framework for posterior inference and is written in Python2.x.\n</code></pre>"},{"location":"Glossary/#fastps","title":"fastp's:","text":"<pre><code>A tool designed to provide fast all-in-one preprocessing for FastQ files.\n</code></pre>"},{"location":"Glossary/#fastp","title":"fastp:","text":"<pre><code>A tool designed to provide fast all-in-one preprocessing for FastQ files.\n</code></pre>"},{"location":"Glossary/#fastq-toolss","title":"fastq-tools's:","text":"<pre><code>A collection of small and efficient programs for performing some common and\n</code></pre>"},{"location":"Glossary/#fastq-tools","title":"fastq-tools:","text":"<pre><code>A collection of small and efficient programs for performing some common and\n</code></pre>"},{"location":"Glossary/#fcgenes","title":"fcGENE's:","text":"<pre><code>Format converting tool for genotype Data.\n</code></pre>"},{"location":"Glossary/#fcgene","title":"fcGENE:","text":"<pre><code>Format converting tool for genotype Data.\n</code></pre>"},{"location":"Glossary/#fgbios","title":"fgbio's:","text":"<pre><code>A set of tools to analyze genomic data with a focus on Next Generation Sequencing.\n</code></pre>"},{"location":"Glossary/#fgbio","title":"fgbio:","text":"<pre><code>A set of tools to analyze genomic data with a focus on Next Generation Sequencing.\n</code></pre>"},{"location":"Glossary/#fineradstructures","title":"fineRADstructure's:","text":"<pre><code>A package for population structure inference from RAD-seq data\n</code></pre>"},{"location":"Glossary/#fineradstructure","title":"fineRADstructure:","text":"<pre><code>A package for population structure inference from RAD-seq data\n</code></pre>"},{"location":"Glossary/#flatbufferss","title":"flatbuffers's:","text":"<pre><code>FlatBuffers: Memory Efficient Serialization Library\n</code></pre>"},{"location":"Glossary/#flatbuffers","title":"flatbuffers:","text":"<pre><code>FlatBuffers: Memory Efficient Serialization Library\n</code></pre>"},{"location":"Glossary/#flexs","title":"flex's:","text":"<pre><code>Flex (Fast Lexical Analyzer) is a tool for generating scanners. A scanner,  sometimes called a tokenizer, is a program which recognizes lexical patterns  in text.\n</code></pre>"},{"location":"Glossary/#flex","title":"flex:","text":"<pre><code>Flex (Fast Lexical Analyzer) is a tool for generating scanners. A scanner,  sometimes called a tokenizer, is a program which recognizes lexical patterns  in text.\n</code></pre>"},{"location":"Glossary/#fmlrcs","title":"fmlrc's:","text":"<pre><code>Tool for performing  hybrid correction of long read sequencing\n</code></pre>"},{"location":"Glossary/#fmlrc","title":"fmlrc:","text":"<pre><code>Tool for performing  hybrid correction of long read sequencing\n</code></pre>"},{"location":"Glossary/#fmts","title":"fmt's:","text":"<pre><code>Formatting library providing a fast and safe alternative to C stdio and C++ iostreams.\n</code></pre>"},{"location":"Glossary/#fmt","title":"fmt:","text":"<pre><code>Formatting library providing a fast and safe alternative to C stdio and C++ iostreams.\n</code></pre>"},{"location":"Glossary/#forges","title":"forge's:","text":"<pre><code>Arm Forge combines Arm DDT, the leading debugger for time-saving high performance application debugging, and Arm MAP, the trusted performance profiler for invaluable optimization advice.\n</code></pre>"},{"location":"Glossary/#forge","title":"forge:","text":"<pre><code>Arm Forge combines Arm DDT, the leading debugger for time-saving high performance application debugging, and Arm MAP, the trusted performance profiler for invaluable optimization advice.\n</code></pre>"},{"location":"Glossary/#fosss","title":"foss's:","text":"<pre><code>GNU Compiler Collection (GCC) based compiler toolchain, including\n</code></pre>"},{"location":"Glossary/#foss","title":"foss:","text":"<pre><code>GNU Compiler Collection (GCC) based compiler toolchain, including\n</code></pre>"},{"location":"Glossary/#funcx-endpoints","title":"funcx-endpoint's:","text":"<pre><code>funcX is a distributed Function as a Service (FaaS) platform that enables flexible,\n</code></pre>"},{"location":"Glossary/#funcx-endpoint","title":"funcx-endpoint:","text":"<pre><code>funcX is a distributed Function as a Service (FaaS) platform that enables flexible,\n</code></pre>"},{"location":"Glossary/#fxtracts","title":"fxtract's:","text":"<pre><code>Extract sequences from a fastx (fasta or fastq) file given a subsequence.\n</code></pre>"},{"location":"Glossary/#fxtract","title":"fxtract:","text":"<pre><code>Extract sequences from a fastx (fasta or fastq) file given a subsequence.\n</code></pre>"},{"location":"Glossary/#g2clibs","title":"g2clib's:","text":"<pre><code>Library contains GRIB2 encoder/decoder ('C' version).\n</code></pre>"},{"location":"Glossary/#g2clib","title":"g2clib:","text":"<pre><code>Library contains GRIB2 encoder/decoder ('C' version).\n</code></pre>"},{"location":"Glossary/#g2libs","title":"g2lib's:","text":"<pre><code>Library contains GRIB2 encoder/decoder and search/indexing routines.\n</code></pre>"},{"location":"Glossary/#g2lib","title":"g2lib:","text":"<pre><code>Library contains GRIB2 encoder/decoder and search/indexing routines.\n</code></pre>"},{"location":"Glossary/#ga4ghs","title":"ga4gh's:","text":"<pre><code>A reference implementation of the GA4GH API\n</code></pre>"},{"location":"Glossary/#ga4gh","title":"ga4gh:","text":"<pre><code>A reference implementation of the GA4GH API\n</code></pre>"},{"location":"Glossary/#geanys","title":"geany's:","text":"<pre><code>A GTK+ based text editor with with basic features of an integrated development environment.\n</code></pre>"},{"location":"Glossary/#geany","title":"geany:","text":"<pre><code>A GTK+ based text editor with with basic features of an integrated development environment.\n</code></pre>"},{"location":"Glossary/#genometoolss","title":"genometools's:","text":"<pre><code>GenomeTools: A Comprehensive Software Library for Efficient Processing of\n</code></pre>"},{"location":"Glossary/#genometools","title":"genometools:","text":"<pre><code>GenomeTools: A Comprehensive Software Library for Efficient Processing of\n</code></pre>"},{"location":"Glossary/#gettexts","title":"gettext's:","text":"<pre><code>GNU 'gettext' is an important step for the GNU Translation Project, as it is an asset on which we may build many other steps. This package offers to programmers, translators, and even users, a well integrated set of tools and documentation\n</code></pre>"},{"location":"Glossary/#gettext","title":"gettext:","text":"<pre><code>GNU 'gettext' is an important step for the GNU Translation Project, as it is an asset on which we may build many other steps. This package offers to programmers, translators, and even users, a well integrated set of tools and documentation\n</code></pre>"},{"location":"Glossary/#gffreads","title":"gffread's:","text":"<pre><code>GFF/GTF parsing utility providing format conversions,\n</code></pre>"},{"location":"Glossary/#gffread","title":"gffread:","text":"<pre><code>GFF/GTF parsing utility providing format conversions,\n</code></pre>"},{"location":"Glossary/#giflibs","title":"giflib's:","text":"<pre><code>giflib is a library for reading and writing gif images.\n</code></pre>"},{"location":"Glossary/#giflib","title":"giflib:","text":"<pre><code>giflib is a library for reading and writing gif images.\n</code></pre>"},{"location":"Glossary/#gimkls","title":"gimkl's:","text":"<pre><code>GNU Compiler Collection (GCC) based compiler toolchain with Intel MPI and MKL\n</code></pre>"},{"location":"Glossary/#gimkl","title":"gimkl:","text":"<pre><code>GNU Compiler Collection (GCC) based compiler toolchain with Intel MPI and MKL\n</code></pre>"},{"location":"Glossary/#gimpis","title":"gimpi's:","text":"<pre><code>GNU Compiler Collection (GCC) based compiler toolchain with Intel MPI.\n</code></pre>"},{"location":"Glossary/#gimpi","title":"gimpi:","text":"<pre><code>GNU Compiler Collection (GCC) based compiler toolchain with Intel MPI.\n</code></pre>"},{"location":"Glossary/#gits","title":"git's:","text":"<pre><code>Git is a free and open source distributed version control system designed\n</code></pre>"},{"location":"Glossary/#git","title":"git:","text":"<pre><code>Git is a free and open source distributed version control system designed\n</code></pre>"},{"location":"Glossary/#globus-automate-clients","title":"globus-automate-client's:","text":"<pre><code>Client for the Globus Flows service.\n</code></pre>"},{"location":"Glossary/#globus-automate-client","title":"globus-automate-client:","text":"<pre><code>Client for the Globus Flows service.\n</code></pre>"},{"location":"Glossary/#globus-compute-endpoints","title":"globus-compute-endpoint's:","text":"<pre><code>Globus Compute is a distributed Function as a Service (FaaS) platform that enables flexible,\n</code></pre>"},{"location":"Glossary/#globus-compute-endpoint","title":"globus-compute-endpoint:","text":"<pre><code>Globus Compute is a distributed Function as a Service (FaaS) platform that enables flexible,\n</code></pre>"},{"location":"Glossary/#gmshs","title":"gmsh's:","text":"<pre><code>Gmsh is a 3D finite element grid generator with a build-in CAD engine and post-processor..\n</code></pre>"},{"location":"Glossary/#gmsh","title":"gmsh:","text":"<pre><code>Gmsh is a 3D finite element grid generator with a build-in CAD engine and post-processor..\n</code></pre>"},{"location":"Glossary/#gnuplots","title":"gnuplot's:","text":"<pre><code>Portable interactive, function plotting utility\n</code></pre>"},{"location":"Glossary/#gnuplot","title":"gnuplot:","text":"<pre><code>Portable interactive, function plotting utility\n</code></pre>"},{"location":"Glossary/#gompis","title":"gompi's:","text":"<pre><code>GNU Compiler Collection (GCC) based compiler toolchain,\n</code></pre>"},{"location":"Glossary/#gompi","title":"gompi:","text":"<pre><code>GNU Compiler Collection (GCC) based compiler toolchain,\n</code></pre>"},{"location":"Glossary/#google-sparsehashs","title":"google-sparsehash's:","text":"<pre><code>An extremely memory-efficient hash_map implementation. 2 bits/entry overhead! The SparseHash library\n</code></pre>"},{"location":"Glossary/#google-sparsehash","title":"google-sparsehash:","text":"<pre><code>An extremely memory-efficient hash_map implementation. 2 bits/entry overhead! The SparseHash library\n</code></pre>"},{"location":"Glossary/#googletests","title":"googletest's:","text":"<pre><code>Google's C++ test framework\n</code></pre>"},{"location":"Glossary/#googletest","title":"googletest:","text":"<pre><code>Google's C++ test framework\n</code></pre>"},{"location":"Glossary/#gperfs","title":"gperf's:","text":"<pre><code>Pperfect hash function generator.\n</code></pre>"},{"location":"Glossary/#gperf","title":"gperf:","text":"<pre><code>Pperfect hash function generator.\n</code></pre>"},{"location":"Glossary/#grib_apis","title":"grib_api's:","text":"<pre><code>The ECMWF GRIB API is an application program interface accessible from C, FORTRAN and Python programs developed for encoding and decoding WMO FM-92 GRIB edition 1 and edition 2 messages. A useful set of command line tools is also provided to give quick access to GRIB messages.  Note that JPEG and Python support have been disabled for this build.\n</code></pre>"},{"location":"Glossary/#grib_api","title":"grib_api:","text":"<pre><code>The ECMWF GRIB API is an application program interface accessible from C, FORTRAN and Python programs developed for encoding and decoding WMO FM-92 GRIB edition 1 and edition 2 messages. A useful set of command line tools is also provided to give quick access to GRIB messages.  Note that JPEG and Python support have been disabled for this build.\n</code></pre>"},{"location":"Glossary/#grive2s","title":"grive2's:","text":"<pre><code>Command line tool for Google Drive.\n</code></pre>"},{"location":"Glossary/#grive2","title":"grive2:","text":"<pre><code>Command line tool for Google Drive.\n</code></pre>"},{"location":"Glossary/#gsorts","title":"gsort's:","text":"<pre><code>Tool to sort genomic files according to a genomefile.\n</code></pre>"},{"location":"Glossary/#gsort","title":"gsort:","text":"<pre><code>Tool to sort genomic files according to a genomefile.\n</code></pre>"},{"location":"Glossary/#h5pps","title":"h5pp's:","text":"<pre><code>A simple C++17 wrapper for HDF5.\n</code></pre>"},{"location":"Glossary/#h5pp","title":"h5pp:","text":"<pre><code>A simple C++17 wrapper for HDF5.\n</code></pre>"},{"location":"Glossary/#help2mans","title":"help2man's:","text":"<pre><code>help2man produces simple manual pages from the '--help' and '--version' output of other commands.\n</code></pre>"},{"location":"Glossary/#help2man","title":"help2man:","text":"<pre><code>help2man produces simple manual pages from the '--help' and '--version' output of other commands.\n</code></pre>"},{"location":"Glossary/#hifiasms","title":"hifiasm's:","text":"<pre><code>Hifiasm: a haplotype-resolved assembler for accurate Hifi reads.\n</code></pre>"},{"location":"Glossary/#hifiasm","title":"hifiasm:","text":"<pre><code>Hifiasm: a haplotype-resolved assembler for accurate Hifi reads.\n</code></pre>"},{"location":"Glossary/#hunspells","title":"hunspell's:","text":"<pre><code>Spell checker and morphological analyzer library and program designed for languages\n</code></pre>"},{"location":"Glossary/#hunspell","title":"hunspell:","text":"<pre><code>Spell checker and morphological analyzer library and program designed for languages\n</code></pre>"},{"location":"Glossary/#hypothesiss","title":"hypothesis's:","text":"<pre><code>Hypothesis is an advanced testing library for Python. It lets you write tests which are parametrized\n</code></pre>"},{"location":"Glossary/#hypothesis","title":"hypothesis:","text":"<pre><code>Hypothesis is an advanced testing library for Python. It lets you write tests which are parametrized\n</code></pre>"},{"location":"Glossary/#iccs","title":"icc's:","text":"<pre><code>Intel C and C++ compilers\n</code></pre>"},{"location":"Glossary/#icc","title":"icc:","text":"<pre><code>Intel C and C++ compilers\n</code></pre>"},{"location":"Glossary/#icciforts","title":"iccifort's:","text":"<pre><code>Intel C, C++ &amp; Fortran compilers\n</code></pre>"},{"location":"Glossary/#iccifort","title":"iccifort:","text":"<pre><code>Intel C, C++ &amp; Fortran compilers\n</code></pre>"},{"location":"Glossary/#iforts","title":"ifort's:","text":"<pre><code>Intel Fortran compiler\n</code></pre>"},{"location":"Glossary/#ifort","title":"ifort:","text":"<pre><code>Intel Fortran compiler\n</code></pre>"},{"location":"Glossary/#iimpis","title":"iimpi's:","text":"<pre><code>Intel C/C++ and Fortran compilers, alongside Intel MPI.\n</code></pre>"},{"location":"Glossary/#iimpi","title":"iimpi:","text":"<pre><code>Intel C/C++ and Fortran compilers, alongside Intel MPI.\n</code></pre>"},{"location":"Glossary/#imkls","title":"imkl's:","text":"<pre><code>Intel Math Kernel Library is a library of highly optimized,\n</code></pre>"},{"location":"Glossary/#imkl","title":"imkl:","text":"<pre><code>Intel Math Kernel Library is a library of highly optimized,\n</code></pre>"},{"location":"Glossary/#imkl-fftws","title":"imkl-FFTW's:","text":"<pre><code>FFTW interfaces using Intel oneAPI Math Kernel Library\n</code></pre>"},{"location":"Glossary/#imkl-fftw","title":"imkl-FFTW:","text":"<pre><code>FFTW interfaces using Intel oneAPI Math Kernel Library\n</code></pre>"},{"location":"Glossary/#impis","title":"impi's:","text":"<pre><code>Intel MPI Library, compatible with MPICH ABI\n</code></pre>"},{"location":"Glossary/#impi","title":"impi:","text":"<pre><code>Intel MPI Library, compatible with MPICH ABI\n</code></pre>"},{"location":"Glossary/#intels","title":"intel's:","text":"<pre><code>Compiler toolchain including Intel compilers, Intel MPI and Intel Math Kernel Library (MKL).\n</code></pre>"},{"location":"Glossary/#intel","title":"intel:","text":"<pre><code>Compiler toolchain including Intel compilers, Intel MPI and Intel Math Kernel Library (MKL).\n</code></pre>"},{"location":"Glossary/#intel-compilerss","title":"intel-compilers's:","text":"<pre><code>Intel C, C++ &amp; Fortran compilers (classic and oneAPI)\n</code></pre>"},{"location":"Glossary/#intel-compilers","title":"intel-compilers:","text":"<pre><code>Intel C, C++ &amp; Fortran compilers (classic and oneAPI)\n</code></pre>"},{"location":"Glossary/#ipyrads","title":"ipyrad's:","text":"<pre><code>ipyrad is an interactive toolkit for assembly and analysis of restriction-site associated genomic\n</code></pre>"},{"location":"Glossary/#ipyrad","title":"ipyrad:","text":"<pre><code>ipyrad is an interactive toolkit for assembly and analysis of restriction-site associated genomic\n</code></pre>"},{"location":"Glossary/#ispcs","title":"ispc's:","text":"<pre><code>Intel SPMD Program Compilers; An open-source compiler for high-performance  SIMD programming on the CPU. ispc is a compiler for a variant of the C programming language,  with extensions for 'single program, multiple data' (SPMD) programming.  Under the SPMD model, the programmer writes a program that generally appears  to be a regular serial program, though the execution model is actually that  a number of program instances execute in parallel on the hardware.\n</code></pre>"},{"location":"Glossary/#ispc","title":"ispc:","text":"<pre><code>Intel SPMD Program Compilers; An open-source compiler for high-performance  SIMD programming on the CPU. ispc is a compiler for a variant of the C programming language,  with extensions for 'single program, multiple data' (SPMD) programming.  Under the SPMD model, the programmer writes a program that generally appears  to be a regular serial program, though the execution model is actually that  a number of program instances execute in parallel on the hardware.\n</code></pre>"},{"location":"Glossary/#jbigkits","title":"jbigkit's:","text":"<pre><code>JBIG-KIT is a software implementation of the JBIG1 data compression standard\n</code></pre>"},{"location":"Glossary/#jbigkit","title":"jbigkit:","text":"<pre><code>JBIG-KIT is a software implementation of the JBIG1 data compression standard\n</code></pre>"},{"location":"Glossary/#jcvis","title":"jcvi's:","text":"<pre><code>Collection of Python libraries to parse bioinformatics files, or perform computation related to assembly, annotation, and comparative genomics.\n</code></pre>"},{"location":"Glossary/#jcvi","title":"jcvi:","text":"<pre><code>Collection of Python libraries to parse bioinformatics files, or perform computation related to assembly, annotation, and comparative genomics.\n</code></pre>"},{"location":"Glossary/#jemallocs","title":"jemalloc's:","text":"<pre><code>A general purpose malloc(3) implementation that emphasizes fragmentation avoidance and\n</code></pre>"},{"location":"Glossary/#jemalloc","title":"jemalloc:","text":"<pre><code>A general purpose malloc(3) implementation that emphasizes fragmentation avoidance and\n</code></pre>"},{"location":"Glossary/#jqs","title":"jq's:","text":"<pre><code>Lightweight and flexible command-line JSON processor.\n</code></pre>"},{"location":"Glossary/#jq","title":"jq:","text":"<pre><code>Lightweight and flexible command-line JSON processor.\n</code></pre>"},{"location":"Glossary/#json-cs","title":"json-c's:","text":"<pre><code>JSON-C implements a reference counting object model that allows you to easily construct JSON objects\n</code></pre>"},{"location":"Glossary/#json-c","title":"json-c:","text":"<pre><code>JSON-C implements a reference counting object model that allows you to easily construct JSON objects\n</code></pre>"},{"location":"Glossary/#jvarkits","title":"jvarkit's:","text":"<pre><code>Java utilities for Bioinformatics\n</code></pre>"},{"location":"Glossary/#jvarkit","title":"jvarkit:","text":"<pre><code>Java utilities for Bioinformatics\n</code></pre>"},{"location":"Glossary/#kalign2s","title":"kalign2's:","text":"<pre><code>Kalign is a fast multiple sequence alignment program for biological sequences.\n</code></pre>"},{"location":"Glossary/#kalign2","title":"kalign2:","text":"<pre><code>Kalign is a fast multiple sequence alignment program for biological sequences.\n</code></pre>"},{"location":"Glossary/#kallistos","title":"kallisto's:","text":"<pre><code>kallisto is a program for quantifying abundances of transcripts from RNA-Seq data, or more generally\n</code></pre>"},{"location":"Glossary/#kallisto","title":"kallisto:","text":"<pre><code>kallisto is a program for quantifying abundances of transcripts from RNA-Seq data, or more generally\n</code></pre>"},{"location":"Glossary/#kinetos","title":"kineto's:","text":"<pre><code>A CPU+GPU Profiling library that provides access to timeline traces and hardware performance counters\n</code></pre>"},{"location":"Glossary/#kineto","title":"kineto:","text":"<pre><code>A CPU+GPU Profiling library that provides access to timeline traces and hardware performance counters\n</code></pre>"},{"location":"Glossary/#kmas","title":"kma's:","text":"<pre><code>KMA is a mapping method designed to map raw reads directly against redundant databases,\n</code></pre>"},{"location":"Glossary/#kma","title":"kma:","text":"<pre><code>KMA is a mapping method designed to map raw reads directly against redundant databases,\n</code></pre>"},{"location":"Glossary/#libflames","title":"libFLAME's:","text":"<pre><code>libFLAME is a portable library for dense matrix computations,\n</code></pre>"},{"location":"Glossary/#libflame","title":"libFLAME:","text":"<pre><code>libFLAME is a portable library for dense matrix computations,\n</code></pre>"},{"location":"Glossary/#libglus","title":"libGLU's:","text":"<pre><code>The OpenGL Utility Library (GLU) is a computer graphics library for OpenGL.\n</code></pre>"},{"location":"Glossary/#libglu","title":"libGLU:","text":"<pre><code>The OpenGL Utility Library (GLU) is a computer graphics library for OpenGL.\n</code></pre>"},{"location":"Glossary/#libkmls","title":"libKML's:","text":"<pre><code>Reference implementation of OGC KML 2.2\n</code></pre>"},{"location":"Glossary/#libkml","title":"libKML:","text":"<pre><code>Reference implementation of OGC KML 2.2\n</code></pre>"},{"location":"Glossary/#libcircles","title":"libcircle's:","text":"<pre><code>API for distributing embarrassingly parallel workloads using self-stabilization.\n</code></pre>"},{"location":"Glossary/#libcircle","title":"libcircle:","text":"<pre><code>API for distributing embarrassingly parallel workloads using self-stabilization.\n</code></pre>"},{"location":"Glossary/#libconfigs","title":"libconfig's:","text":"<pre><code>A Library for processing structured configuration files\n</code></pre>"},{"location":"Glossary/#libconfig","title":"libconfig:","text":"<pre><code>A Library for processing structured configuration files\n</code></pre>"},{"location":"Glossary/#libdeflates","title":"libdeflate's:","text":"<pre><code>Heavily optimized library for DEFLATE/zlib/gzip compression and decompression.\n</code></pre>"},{"location":"Glossary/#libdeflate","title":"libdeflate:","text":"<pre><code>Heavily optimized library for DEFLATE/zlib/gzip compression and decompression.\n</code></pre>"},{"location":"Glossary/#libdrms","title":"libdrm's:","text":"<pre><code>Direct Rendering Manager runtime library.\n</code></pre>"},{"location":"Glossary/#libdrm","title":"libdrm:","text":"<pre><code>Direct Rendering Manager runtime library.\n</code></pre>"},{"location":"Glossary/#libdwarfs","title":"libdwarf's:","text":"<pre><code>The DWARF Debugging Information Format is of interest to programmers working on compilers\n</code></pre>"},{"location":"Glossary/#libdwarf","title":"libdwarf:","text":"<pre><code>The DWARF Debugging Information Format is of interest to programmers working on compilers\n</code></pre>"},{"location":"Glossary/#libepoxys","title":"libepoxy's:","text":"<pre><code>Library for handling OpenGL function pointer management\n</code></pre>"},{"location":"Glossary/#libepoxy","title":"libepoxy:","text":"<pre><code>Library for handling OpenGL function pointer management\n</code></pre>"},{"location":"Glossary/#libevents","title":"libevent's:","text":"<pre><code>The libevent API provides a mechanism to execute a callback function when a specific  event occurs on a file descriptor or after a timeout has been reached.  Furthermore, libevent also support callbacks due to signals or regular timeouts.\n</code></pre>"},{"location":"Glossary/#libevent","title":"libevent:","text":"<pre><code>The libevent API provides a mechanism to execute a callback function when a specific  event occurs on a file descriptor or after a timeout has been reached.  Furthermore, libevent also support callbacks due to signals or regular timeouts.\n</code></pre>"},{"location":"Glossary/#libffis","title":"libffi's:","text":"<pre><code>The libffi library provides a portable, high level programming interface to various calling\n</code></pre>"},{"location":"Glossary/#libffi","title":"libffi:","text":"<pre><code>The libffi library provides a portable, high level programming interface to various calling\n</code></pre>"},{"location":"Glossary/#libgcrypts","title":"libgcrypt's:","text":"<pre><code>Libgpg-error is a small library that defines common error values for all GnuPG components.\n</code></pre>"},{"location":"Glossary/#libgcrypt","title":"libgcrypt:","text":"<pre><code>Libgpg-error is a small library that defines common error values for all GnuPG components.\n</code></pre>"},{"location":"Glossary/#libgds","title":"libgd's:","text":"<pre><code>GD is an open source code library for the dynamic creation of images by programmers.\n</code></pre>"},{"location":"Glossary/#libgd","title":"libgd:","text":"<pre><code>GD is an open source code library for the dynamic creation of images by programmers.\n</code></pre>"},{"location":"Glossary/#libgeotiffs","title":"libgeotiff's:","text":"<pre><code>Library for reading and writing coordinate system information from/to GeoTIFF files\n</code></pre>"},{"location":"Glossary/#libgeotiff","title":"libgeotiff:","text":"<pre><code>Library for reading and writing coordinate system information from/to GeoTIFF files\n</code></pre>"},{"location":"Glossary/#libglvnds","title":"libglvnd's:","text":"<pre><code>libglvnd is a vendor-neutral dispatch layer for arbitrating OpenGL API calls between multiple vendors.\n</code></pre>"},{"location":"Glossary/#libglvnd","title":"libglvnd:","text":"<pre><code>libglvnd is a vendor-neutral dispatch layer for arbitrating OpenGL API calls between multiple vendors.\n</code></pre>"},{"location":"Glossary/#libgpg-errors","title":"libgpg-error's:","text":"<pre><code>Libgpg-error is a small library that defines common error values for all GnuPG components.\n</code></pre>"},{"location":"Glossary/#libgpg-error","title":"libgpg-error:","text":"<pre><code>Libgpg-error is a small library that defines common error values for all GnuPG components.\n</code></pre>"},{"location":"Glossary/#libgpuarrays","title":"libgpuarray's:","text":"<pre><code>Arrays on GPU device memory, for Theano\n</code></pre>"},{"location":"Glossary/#libgpuarray","title":"libgpuarray:","text":"<pre><code>Arrays on GPU device memory, for Theano\n</code></pre>"},{"location":"Glossary/#libgtextutilss","title":"libgtextutils's:","text":"<pre><code>ligtextutils is a dependency of fastx-toolkit and is provided via the same upstream\n</code></pre>"},{"location":"Glossary/#libgtextutils","title":"libgtextutils:","text":"<pre><code>ligtextutils is a dependency of fastx-toolkit and is provided via the same upstream\n</code></pre>"},{"location":"Glossary/#libiconvs","title":"libiconv's:","text":"<pre><code>Libiconv converts from one character encoding to another through Unicode conversion\n</code></pre>"},{"location":"Glossary/#libiconv","title":"libiconv:","text":"<pre><code>Libiconv converts from one character encoding to another through Unicode conversion\n</code></pre>"},{"location":"Glossary/#libjpeg-turbos","title":"libjpeg-turbo's:","text":"<pre><code>libjpeg-turbo is a fork of the original IJG libjpeg which uses SIMD to accelerate baseline JPEG\n</code></pre>"},{"location":"Glossary/#libjpeg-turbo","title":"libjpeg-turbo:","text":"<pre><code>libjpeg-turbo is a fork of the original IJG libjpeg which uses SIMD to accelerate baseline JPEG\n</code></pre>"},{"location":"Glossary/#libmathevals","title":"libmatheval's:","text":"<pre><code>GNU libmatheval is a library (callable from C and Fortran) to parse\n</code></pre>"},{"location":"Glossary/#libmatheval","title":"libmatheval:","text":"<pre><code>GNU libmatheval is a library (callable from C and Fortran) to parse\n</code></pre>"},{"location":"Glossary/#libpciaccesss","title":"libpciaccess's:","text":"<pre><code>Generic PCI access library.\n</code></pre>"},{"location":"Glossary/#libpciaccess","title":"libpciaccess:","text":"<pre><code>Generic PCI access library.\n</code></pre>"},{"location":"Glossary/#libpngs","title":"libpng's:","text":"<pre><code>libpng is the official PNG reference library\n</code></pre>"},{"location":"Glossary/#libpng","title":"libpng:","text":"<pre><code>libpng is the official PNG reference library\n</code></pre>"},{"location":"Glossary/#libreadlines","title":"libreadline's:","text":"<pre><code>The GNU Readline library provides a set of functions for use by applications that\n</code></pre>"},{"location":"Glossary/#libreadline","title":"libreadline:","text":"<pre><code>The GNU Readline library provides a set of functions for use by applications that\n</code></pre>"},{"location":"Glossary/#libspatialites","title":"libspatialite's:","text":"<pre><code>SpatiaLite is an open source library intended to extend the SQLite core to support\n</code></pre>"},{"location":"Glossary/#libspatialite","title":"libspatialite:","text":"<pre><code>SpatiaLite is an open source library intended to extend the SQLite core to support\n</code></pre>"},{"location":"Glossary/#libtools","title":"libtool's:","text":"<pre><code>GNU libtool is a generic library support script. Libtool hides the complexity of using shared libraries\n</code></pre>"},{"location":"Glossary/#libtool","title":"libtool:","text":"<pre><code>GNU libtool is a generic library support script. Libtool hides the complexity of using shared libraries\n</code></pre>"},{"location":"Glossary/#libunistrings","title":"libunistring's:","text":"<pre><code>This library provides functions for manipulating Unicode strings and for manipulating C strings\n</code></pre>"},{"location":"Glossary/#libunistring","title":"libunistring:","text":"<pre><code>This library provides functions for manipulating Unicode strings and for manipulating C strings\n</code></pre>"},{"location":"Glossary/#libunwinds","title":"libunwind's:","text":"<pre><code>Define a portable and efficient C programming API to determine the call-chain of a program.\n</code></pre>"},{"location":"Glossary/#libunwind","title":"libunwind:","text":"<pre><code>Define a portable and efficient C programming API to determine the call-chain of a program.\n</code></pre>"},{"location":"Glossary/#libvpxs","title":"libvpx's:","text":"<pre><code>The WebM Project is dedicated to developing a high-quality, open video format for the web that's freely available to everyone.\n</code></pre>"},{"location":"Glossary/#libvpx","title":"libvpx:","text":"<pre><code>The WebM Project is dedicated to developing a high-quality, open video format for the web that's freely available to everyone.\n</code></pre>"},{"location":"Glossary/#libxcs","title":"libxc's:","text":"<pre><code>Libxc is a library of exchange-correlation functionals for density-functional theory.  The aim is to provide a portable, well tested and reliable set of exchange and correlation functionals.\n</code></pre>"},{"location":"Glossary/#libxc","title":"libxc:","text":"<pre><code>Libxc is a library of exchange-correlation functionals for density-functional theory.  The aim is to provide a portable, well tested and reliable set of exchange and correlation functionals.\n</code></pre>"},{"location":"Glossary/#libxslts","title":"libxslt's:","text":"<pre><code>Libxslt is the XSLT C library developed for the GNOME project\n</code></pre>"},{"location":"Glossary/#libxslt","title":"libxslt:","text":"<pre><code>Libxslt is the XSLT C library developed for the GNOME project\n</code></pre>"},{"location":"Glossary/#libxsmms","title":"libxsmm's:","text":"<pre><code>LIBXSMM is a library for small dense and small sparse matrix-matrix multiplications\n</code></pre>"},{"location":"Glossary/#libxsmm","title":"libxsmm:","text":"<pre><code>LIBXSMM is a library for small dense and small sparse matrix-matrix multiplications\n</code></pre>"},{"location":"Glossary/#libzstds","title":"libzstd's:","text":"<pre><code>Fast lossless compression algorithm.\n</code></pre>"},{"location":"Glossary/#libzstd","title":"libzstd:","text":"<pre><code>Fast lossless compression algorithm.\n</code></pre>"},{"location":"Glossary/#lighttpds","title":"lighttpd's:","text":"<pre><code>A web server.\n</code></pre>"},{"location":"Glossary/#lighttpd","title":"lighttpd:","text":"<pre><code>A web server.\n</code></pre>"},{"location":"Glossary/#likwids","title":"likwid's:","text":"<pre><code>Command line tools for Linux to support programmers in developing high\n</code></pre>"},{"location":"Glossary/#likwid","title":"likwid:","text":"<pre><code>Command line tools for Linux to support programmers in developing high\n</code></pre>"},{"location":"Glossary/#lp_solves","title":"lp_solve's:","text":"<pre><code>Mixed Integer Linear Programming (MILP) solver\n</code></pre>"},{"location":"Glossary/#lp_solve","title":"lp_solve:","text":"<pre><code>Mixed Integer Linear Programming (MILP) solver\n</code></pre>"},{"location":"Glossary/#lwgrps","title":"lwgrp's:","text":"<pre><code>The light-weight group library defines data structures and collective operations to\n</code></pre>"},{"location":"Glossary/#lwgrp","title":"lwgrp:","text":"<pre><code>The light-weight group library defines data structures and collective operations to\n</code></pre>"},{"location":"Glossary/#lz4s","title":"lz4's:","text":"<pre><code>LZ4 is lossless compression algorithm, providing compression speed at 400 MB/s per core.  It features an extremely fast decoder, with speed in multiple GB/s per core.\n</code></pre>"},{"location":"Glossary/#lz4","title":"lz4:","text":"<pre><code>LZ4 is lossless compression algorithm, providing compression speed at 400 MB/s per core.  It features an extremely fast decoder, with speed in multiple GB/s per core.\n</code></pre>"},{"location":"Glossary/#magmas_2","title":"magma's:","text":"<pre><code>The MAGMA project aims to develop a dense linear algebra library similar to\n</code></pre>"},{"location":"Glossary/#magma_2","title":"magma:","text":"<pre><code>The MAGMA project aims to develop a dense linear algebra library similar to\n</code></pre>"},{"location":"Glossary/#makedepends","title":"makedepend's:","text":"<pre><code>The makedepend package contains a C-preprocessor like utility to determine build-time dependencies.\n</code></pre>"},{"location":"Glossary/#makedepend","title":"makedepend:","text":"<pre><code>The makedepend package contains a C-preprocessor like utility to determine build-time dependencies.\n</code></pre>"},{"location":"Glossary/#mantas","title":"manta's:","text":"<pre><code>Manta calls structural variants (SVs) and indels from mapped paired-end sequencing reads.\n</code></pre>"},{"location":"Glossary/#manta","title":"manta:","text":"<pre><code>Manta calls structural variants (SVs) and indels from mapped paired-end sequencing reads.\n</code></pre>"},{"location":"Glossary/#mapdamages","title":"mapDamage's:","text":"<pre><code>tracks and quantifies DNA damage patterns among ancient\n</code></pre>"},{"location":"Glossary/#mapdamage","title":"mapDamage:","text":"<pre><code>tracks and quantifies DNA damage patterns among ancient\n</code></pre>"},{"location":"Glossary/#merantks","title":"meRanTK's:","text":"<pre><code>High performance toolkit for complete analysis of methylated RNA data.\n</code></pre>"},{"location":"Glossary/#merantk","title":"meRanTK:","text":"<pre><code>High performance toolkit for complete analysis of methylated RNA data.\n</code></pre>"},{"location":"Glossary/#medakas","title":"medaka's:","text":"<pre><code>Medaka is a tool to create a consensus sequence from nanopore sequencing data.\n</code></pre>"},{"location":"Glossary/#medaka","title":"medaka:","text":"<pre><code>Medaka is a tool to create a consensus sequence from nanopore sequencing data.\n</code></pre>"},{"location":"Glossary/#megalodons","title":"megalodon's:","text":"<pre><code>Tool to extract high accuracy modified base and sequence variant calls from raw nanopore reads\n</code></pre>"},{"location":"Glossary/#megalodon","title":"megalodon:","text":"<pre><code>Tool to extract high accuracy modified base and sequence variant calls from raw nanopore reads\n</code></pre>"},{"location":"Glossary/#metawraps","title":"metaWRAP's:","text":"<pre><code>Flexible pipeline for genome-resolved metagenomic data analysis.\n</code></pre>"},{"location":"Glossary/#metawrap","title":"metaWRAP:","text":"<pre><code>Flexible pipeline for genome-resolved metagenomic data analysis.\n</code></pre>"},{"location":"Glossary/#mirdeep2s","title":"miRDeep2's:","text":"<pre><code>Completely overhauled tool which discovers microRNA genes by analyzing sequenced RNAs\n</code></pre>"},{"location":"Glossary/#mirdeep2","title":"miRDeep2:","text":"<pre><code>Completely overhauled tool which discovers microRNA genes by analyzing sequenced RNAs\n</code></pre>"},{"location":"Glossary/#midls","title":"midl's:","text":"<pre><code>The Met Office library is a collection of routines written in IDL at the Met Office, originally written in PV-WAVE in the Hadley Centre in the early 1990s, with development continuing to the present day. Its main purpose is to support analysis and visualisation of PP data produced by the Unified Model (UM).\n</code></pre>"},{"location":"Glossary/#midl","title":"midl:","text":"<pre><code>The Met Office library is a collection of routines written in IDL at the Met Office, originally written in PV-WAVE in the Hadley Centre in the early 1990s, with development continuing to the present day. Its main purpose is to support analysis and visualisation of PP data produced by the Unified Model (UM).\n</code></pre>"},{"location":"Glossary/#mimallocs","title":"mimalloc's:","text":"<pre><code>mimalloc is a general purpose allocator with excellent performance characteristics.\n</code></pre>"},{"location":"Glossary/#mimalloc","title":"mimalloc:","text":"<pre><code>mimalloc is a general purpose allocator with excellent performance characteristics.\n</code></pre>"},{"location":"Glossary/#miniasms","title":"miniasm's:","text":"<pre><code>Fast OLC-based de novo assembler for noisy long reads.\n</code></pre>"},{"location":"Glossary/#miniasm","title":"miniasm:","text":"<pre><code>Fast OLC-based de novo assembler for noisy long reads.\n</code></pre>"},{"location":"Glossary/#minieigens","title":"minieigen's:","text":"<pre><code>A small wrapper for core parts of Eigen, c++ library for linear algebra.\n</code></pre>"},{"location":"Glossary/#minieigen","title":"minieigen:","text":"<pre><code>A small wrapper for core parts of Eigen, c++ library for linear algebra.\n</code></pre>"},{"location":"Glossary/#minimap2s","title":"minimap2's:","text":"<pre><code>Minimap2 is a fast sequence mapping and alignment\n</code></pre>"},{"location":"Glossary/#minimap2","title":"minimap2:","text":"<pre><code>Minimap2 is a fast sequence mapping and alignment\n</code></pre>"},{"location":"Glossary/#mjpegtoolss","title":"mjpegtools's:","text":"<pre><code>The mjpeg programs are a set of tools that can do recording of videos and playback, simple cut-and-paste editing and the MPEG compression of audio and video under Linux.\n</code></pre>"},{"location":"Glossary/#mjpegtools","title":"mjpegtools:","text":"<pre><code>The mjpeg programs are a set of tools that can do recording of videos and playback, simple cut-and-paste editing and the MPEG compression of audio and video under Linux.\n</code></pre>"},{"location":"Glossary/#mlpacks","title":"mlpack's:","text":"<pre><code>Fast, and flexible C++ machine learning library with bindings to other languages\n</code></pre>"},{"location":"Glossary/#mlpack","title":"mlpack:","text":"<pre><code>Fast, and flexible C++ machine learning library with bindings to other languages\n</code></pre>"},{"location":"Glossary/#mo_tidls","title":"mo_tidl's:","text":"<pre><code>The Met Office library is a collection of routines written in IDL at the Met Office, originally written in PV-WAVE in the Hadley Centre in the early 1990s, with development continuing to the present day. Its main purpose is to support analysis and visualisation of PP data produced by the Unified Model (UM).\n</code></pre>"},{"location":"Glossary/#mo_tidl","title":"mo_tidl:","text":"<pre><code>The Met Office library is a collection of routines written in IDL at the Met Office, originally written in PV-WAVE in the Hadley Centre in the early 1990s, with development continuing to the present day. Its main purpose is to support analysis and visualisation of PP data produced by the Unified Model (UM).\n</code></pre>"},{"location":"Glossary/#modbam2beds","title":"modbam2bed's:","text":"<pre><code>A program to aggregate modified base counts stored in a modified-base BAM file to a bedMethyl file.\n</code></pre>"},{"location":"Glossary/#modbam2bed","title":"modbam2bed:","text":"<pre><code>A program to aggregate modified base counts stored in a modified-base BAM file to a bedMethyl file.\n</code></pre>"},{"location":"Glossary/#mpi4pys","title":"mpi4py's:","text":"<pre><code>This package provides Python bindings for the Message Passing Interface (MPI) standard. It is implemented on top of the MPI-1/2/3 specification and exposes an API which grounds on the standard MPI-2 C++ bindings.\n</code></pre>"},{"location":"Glossary/#mpi4py","title":"mpi4py:","text":"<pre><code>This package provides Python bindings for the Message Passing Interface (MPI) standard. It is implemented on top of the MPI-1/2/3 specification and exposes an API which grounds on the standard MPI-2 C++ bindings.\n</code></pre>"},{"location":"Glossary/#mpifileutilss","title":"mpifileutils's:","text":"<pre><code>MPI-Based File Utilities For Distributed Systems\n</code></pre>"},{"location":"Glossary/#mpifileutils","title":"mpifileutils:","text":"<pre><code>MPI-Based File Utilities For Distributed Systems\n</code></pre>"},{"location":"Glossary/#muparsers","title":"muParser's:","text":"<pre><code>muParser is an extensible high performance math expression\n</code></pre>"},{"location":"Glossary/#muparser","title":"muParser:","text":"<pre><code>muParser is an extensible high performance math expression\n</code></pre>"},{"location":"Glossary/#nanos","title":"nano's:","text":"<pre><code>nano is a text editor.\n</code></pre>"},{"location":"Glossary/#nano","title":"nano:","text":"<pre><code>nano is a text editor.\n</code></pre>"},{"location":"Glossary/#nanoqcs","title":"nanoQC's:","text":"<pre><code>Create fastQC-like plots for Oxford Nanopore sequencing data.\n</code></pre>"},{"location":"Glossary/#nanoqc","title":"nanoQC:","text":"<pre><code>Create fastQC-like plots for Oxford Nanopore sequencing data.\n</code></pre>"},{"location":"Glossary/#nanofilts","title":"nanofilt's:","text":"<pre><code>Filtering and trimming of long read sequencing data.\n</code></pre>"},{"location":"Glossary/#nanofilt","title":"nanofilt:","text":"<pre><code>Filtering and trimming of long read sequencing data.\n</code></pre>"},{"location":"Glossary/#nanogets","title":"nanoget's:","text":"<pre><code>Functions to extract information from Oxford Nanopore sequencing data and alignments\n</code></pre>"},{"location":"Glossary/#nanoget","title":"nanoget:","text":"<pre><code>Functions to extract information from Oxford Nanopore sequencing data and alignments\n</code></pre>"},{"location":"Glossary/#nanomaths","title":"nanomath's:","text":"<pre><code>A few simple math function for other Oxford Nanopore processing scripts\n</code></pre>"},{"location":"Glossary/#nanomath","title":"nanomath:","text":"<pre><code>A few simple math function for other Oxford Nanopore processing scripts\n</code></pre>"},{"location":"Glossary/#nanopolishs","title":"nanopolish's:","text":"<pre><code>Software package for signal-level analysis of Oxford Nanopore sequencing data.\n</code></pre>"},{"location":"Glossary/#nanopolish","title":"nanopolish:","text":"<pre><code>Software package for signal-level analysis of Oxford Nanopore sequencing data.\n</code></pre>"},{"location":"Glossary/#ncbi-vdbs","title":"ncbi-vdb's:","text":"<pre><code>The SRA Toolkit and SDK from NCBI is a collection of tools and libraries for\n</code></pre>"},{"location":"Glossary/#ncbi-vdb","title":"ncbi-vdb:","text":"<pre><code>The SRA Toolkit and SDK from NCBI is a collection of tools and libraries for\n</code></pre>"},{"location":"Glossary/#nccmps","title":"nccmp's:","text":"<pre><code>nccmp compares two NetCDF files bitwise, semantically or with a user defined tolerance (absolute or relative percentage). Parallel comparisons are done in local memory without requiring temporary files. Highly recommended for regression testing scientific models or datasets in a test-driven development environment.\n</code></pre>"},{"location":"Glossary/#nccmp","title":"nccmp:","text":"<pre><code>nccmp compares two NetCDF files bitwise, semantically or with a user defined tolerance (absolute or relative percentage). Parallel comparisons are done in local memory without requiring temporary files. Highly recommended for regression testing scientific models or datasets in a test-driven development environment.\n</code></pre>"},{"location":"Glossary/#ncviews_1","title":"ncview's:","text":"<pre><code>Visual browser for netCDF format files.\n</code></pre>"},{"location":"Glossary/#ncview_1","title":"ncview:","text":"<pre><code>Visual browser for netCDF format files.\n</code></pre>"},{"location":"Glossary/#nes","title":"ne's:","text":"<pre><code>ne is a free (GPL'd) text editor based on the POSIX standard\n</code></pre>"},{"location":"Glossary/#ne","title":"ne:","text":"<pre><code>ne is a free (GPL'd) text editor based on the POSIX standard\n</code></pre>"},{"location":"Glossary/#nearlines","title":"nearline's:","text":"<pre><code>NeSI nearline client provides 'End User' access to NeSI's archive filesystem Nearline which is connected to a Tape library.\n</code></pre>"},{"location":"Glossary/#netcdfs","title":"netCDF's:","text":"<pre><code>NetCDF (network Common Data Form) is a set of software libraries\n</code></pre>"},{"location":"Glossary/#netcdf","title":"netCDF:","text":"<pre><code>NetCDF (network Common Data Form) is a set of software libraries\n</code></pre>"},{"location":"Glossary/#netcdf-cs","title":"netCDF-C++'s:","text":"<pre><code>NetCDF (network Common Data Form) is a set of software libraries\n</code></pre>"},{"location":"Glossary/#netcdf-c","title":"netCDF-C++:","text":"<pre><code>NetCDF (network Common Data Form) is a set of software libraries\n</code></pre>"},{"location":"Glossary/#netcdf-c4s","title":"netCDF-C++4's:","text":"<pre><code>NetCDF (network Common Data Form) is a set of software libraries   and machine-independent data formats that support the creation, access, and sharing of array-oriented   scientific data.\n</code></pre>"},{"location":"Glossary/#netcdf-c4","title":"netCDF-C++4:","text":"<pre><code>NetCDF (network Common Data Form) is a set of software libraries   and machine-independent data formats that support the creation, access, and sharing of array-oriented   scientific data.\n</code></pre>"},{"location":"Glossary/#netcdf-fortrans","title":"netCDF-Fortran's:","text":"<pre><code>NetCDF (network Common Data Form) is a set of software libraries   and machine-independent data formats that support the creation, access, and sharing of array-oriented   scientific data.\n</code></pre>"},{"location":"Glossary/#netcdf-fortran","title":"netCDF-Fortran:","text":"<pre><code>NetCDF (network Common Data Form) is a set of software libraries   and machine-independent data formats that support the creation, access, and sharing of array-oriented   scientific data.\n</code></pre>"},{"location":"Glossary/#nettles","title":"nettle's:","text":"<pre><code>Nettle is a cryptographic library that is designed to fit easily\n</code></pre>"},{"location":"Glossary/#nettle","title":"nettle:","text":"<pre><code>Nettle is a cryptographic library that is designed to fit easily\n</code></pre>"},{"location":"Glossary/#networkxs","title":"networkx's:","text":"<pre><code>NetworkX is a Python package for the creation, manipulation, and study of the structure, dynamics,\n</code></pre>"},{"location":"Glossary/#networkx","title":"networkx:","text":"<pre><code>NetworkX is a Python package for the creation, manipulation, and study of the structure, dynamics,\n</code></pre>"},{"location":"Glossary/#nodejss","title":"nodejs's:","text":"<pre><code>Node.js is a platform built on Chrome's JavaScript runtime  for easily building fast, scalable network applications. Node.js uses an  event-driven, non-blocking I/O model that makes it lightweight and efficient,  perfect for data-intensive real-time applications that run across distributed devices.\n</code></pre>"},{"location":"Glossary/#nodejs","title":"nodejs:","text":"<pre><code>Node.js is a platform built on Chrome's JavaScript runtime  for easily building fast, scalable network applications. Node.js uses an  event-driven, non-blocking I/O model that makes it lightweight and efficient,  perfect for data-intensive real-time applications that run across distributed devices.\n</code></pre>"},{"location":"Glossary/#nsegs","title":"nseg's:","text":"<pre><code>Used to mask nucleic acid sequences\n</code></pre>"},{"location":"Glossary/#nseg","title":"nseg:","text":"<pre><code>Used to mask nucleic acid sequences\n</code></pre>"},{"location":"Glossary/#nsyncs","title":"nsync's:","text":"<pre><code>nsync is a C library that exports various synchronization primitives, such as mutexes\n</code></pre>"},{"location":"Glossary/#nsync","title":"nsync:","text":"<pre><code>nsync is a C library that exports various synchronization primitives, such as mutexes\n</code></pre>"},{"location":"Glossary/#nullarbors","title":"nullarbor's:","text":"<pre><code>Reads to report pipeline for bacterial isolate NGS data.\n</code></pre>"},{"location":"Glossary/#nullarbor","title":"nullarbor:","text":"<pre><code>Reads to report pipeline for bacterial isolate NGS data.\n</code></pre>"},{"location":"Glossary/#ont-guppy-gpus","title":"ont-guppy-gpu's:","text":"<pre><code>Data processing toolkit that contains the Oxford Nanopore Technologies' basecalling algorithms,\n</code></pre>"},{"location":"Glossary/#ont-guppy-gpu","title":"ont-guppy-gpu:","text":"<pre><code>Data processing toolkit that contains the Oxford Nanopore Technologies' basecalling algorithms,\n</code></pre>"},{"location":"Glossary/#padlocs","title":"padloc's:","text":"<pre><code>Prokaryotic Antiviral Defence LOCator\n</code></pre>"},{"location":"Glossary/#padloc","title":"padloc:","text":"<pre><code>Prokaryotic Antiviral Defence LOCator\n</code></pre>"},{"location":"Glossary/#pairtoolss","title":"pairtools's:","text":"<pre><code>CLI tools to process mapped Hi-C data\n</code></pre>"},{"location":"Glossary/#pairtools","title":"pairtools:","text":"<pre><code>CLI tools to process mapped Hi-C data\n</code></pre>"},{"location":"Glossary/#panaroos","title":"panaroo's:","text":"<pre><code>A pangenome analysis pipeline.\n</code></pre>"},{"location":"Glossary/#panaroo","title":"panaroo:","text":"<pre><code>A pangenome analysis pipeline.\n</code></pre>"},{"location":"Glossary/#pandocs","title":"pandoc's:","text":"<pre><code>Almost universal document converter\n</code></pre>"},{"location":"Glossary/#pandoc","title":"pandoc:","text":"<pre><code>Almost universal document converter\n</code></pre>"},{"location":"Glossary/#parallel-fastq-dumps","title":"parallel-fastq-dump's:","text":"<pre><code>parallel fastq-dump wrapper\n</code></pre>"},{"location":"Glossary/#parallel-fastq-dump","title":"parallel-fastq-dump:","text":"<pre><code>parallel fastq-dump wrapper\n</code></pre>"},{"location":"Glossary/#parasails","title":"parasail's:","text":"<pre><code>parasail is a SIMD C (C99) library containing implementations\n</code></pre>"},{"location":"Glossary/#parasail","title":"parasail:","text":"<pre><code>parasail is a SIMD C (C99) library containing implementations\n</code></pre>"},{"location":"Glossary/#patchelfs","title":"patchelf's:","text":"<pre><code>PatchELF is a small utility to modify the dynamic linker and RPATH of ELF executables.\n</code></pre>"},{"location":"Glossary/#patchelf","title":"patchelf:","text":"<pre><code>PatchELF is a small utility to modify the dynamic linker and RPATH of ELF executables.\n</code></pre>"},{"location":"Glossary/#pauvres","title":"pauvre's:","text":"<pre><code>Tools for plotting Oxford Nanopore and other long-read data.\n</code></pre>"},{"location":"Glossary/#pauvre","title":"pauvre:","text":"<pre><code>Tools for plotting Oxford Nanopore and other long-read data.\n</code></pre>"},{"location":"Glossary/#pfunits","title":"pfunit's:","text":"<pre><code>pFUnit is a unit testing framework enabling JUnit-like testing of serial  and MPI-parallel software written in Fortran.\n</code></pre>"},{"location":"Glossary/#pfunit","title":"pfunit:","text":"<pre><code>pFUnit is a unit testing framework enabling JUnit-like testing of serial  and MPI-parallel software written in Fortran.\n</code></pre>"},{"location":"Glossary/#phyxs","title":"phyx's:","text":"<pre><code>phyx performs phylogenetics analyses on trees and sequences.\n</code></pre>"},{"location":"Glossary/#phyx","title":"phyx:","text":"<pre><code>phyx performs phylogenetics analyses on trees and sequences.\n</code></pre>"},{"location":"Glossary/#picards","title":"picard's:","text":"<pre><code>A set of tools (in Java) for working with next generation sequencing data in the BAM format.\n</code></pre>"},{"location":"Glossary/#picard","title":"picard:","text":"<pre><code>A set of tools (in Java) for working with next generation sequencing data in the BAM format.\n</code></pre>"},{"location":"Glossary/#pixmans","title":"pixman's:","text":"<pre><code>Pixman is a low-level software library for pixel manipulation, providing features such as image\n</code></pre>"},{"location":"Glossary/#pixman","title":"pixman:","text":"<pre><code>Pixman is a low-level software library for pixel manipulation, providing features such as image\n</code></pre>"},{"location":"Glossary/#pplacers","title":"pplacer's:","text":"<pre><code>Places query sequences on a fixed reference phylogenetic tree\n</code></pre>"},{"location":"Glossary/#pplacer","title":"pplacer:","text":"<pre><code>Places query sequences on a fixed reference phylogenetic tree\n</code></pre>"},{"location":"Glossary/#preseqs","title":"preseq's:","text":"<pre><code>Software for predicting library complexity and genome coverage in high-throughput sequencing.\n</code></pre>"},{"location":"Glossary/#preseq","title":"preseq:","text":"<pre><code>Software for predicting library complexity and genome coverage in high-throughput sequencing.\n</code></pre>"},{"location":"Glossary/#prodigals_1","title":"prodigal's:","text":"<pre><code>Prodigal (Prokaryotic Dynamic Programming Genefinding Algorithm)\n</code></pre>"},{"location":"Glossary/#prodigal_1","title":"prodigal:","text":"<pre><code>Prodigal (Prokaryotic Dynamic Programming Genefinding Algorithm)\n</code></pre>"},{"location":"Glossary/#prodigal-gvs","title":"prodigal-gv's:","text":"<pre><code>A fork of Prodigal meant to improve gene calling\n</code></pre>"},{"location":"Glossary/#prodigal-gv","title":"prodigal-gv:","text":"<pre><code>A fork of Prodigal meant to improve gene calling\n</code></pre>"},{"location":"Glossary/#prokkas","title":"prokka's:","text":"<pre><code>Prokka is a software tool for the rapid annotation of prokaryotic genomes.\n</code></pre>"},{"location":"Glossary/#prokka","title":"prokka:","text":"<pre><code>Prokka is a software tool for the rapid annotation of prokaryotic genomes.\n</code></pre>"},{"location":"Glossary/#proovreads","title":"proovread's:","text":"<pre><code>PacBio hybrid error correction through iterative short read consensus\n</code></pre>"},{"location":"Glossary/#proovread","title":"proovread:","text":"<pre><code>PacBio hybrid error correction through iterative short read consensus\n</code></pre>"},{"location":"Glossary/#protobufs","title":"protobuf's:","text":"<pre><code>Google Protocol Buffers\n</code></pre>"},{"location":"Glossary/#protobuf","title":"protobuf:","text":"<pre><code>Google Protocol Buffers\n</code></pre>"},{"location":"Glossary/#protobuf-pythons","title":"protobuf-python's:","text":"<pre><code>Python Protocol Buffers runtime library.\n</code></pre>"},{"location":"Glossary/#protobuf-python","title":"protobuf-python:","text":"<pre><code>Python Protocol Buffers runtime library.\n</code></pre>"},{"location":"Glossary/#psmcs","title":"psmc's:","text":"<pre><code>Infers population size history from a diploid sequence using the PSMC model.\n</code></pre>"},{"location":"Glossary/#psmc","title":"psmc:","text":"<pre><code>Infers population size history from a diploid sequence using the PSMC model.\n</code></pre>"},{"location":"Glossary/#pstoedits","title":"pstoedit's:","text":"<pre><code>pstoedit translates PostScript and PDF graphics into other vector formats.\n</code></pre>"},{"location":"Glossary/#pstoedit","title":"pstoedit:","text":"<pre><code>pstoedit translates PostScript and PDF graphics into other vector formats.\n</code></pre>"},{"location":"Glossary/#pullseqs","title":"pullseq's:","text":"<pre><code>Utility program for extracting sequences from a fasta/fastq file\n</code></pre>"},{"location":"Glossary/#pullseq","title":"pullseq:","text":"<pre><code>Utility program for extracting sequences from a fasta/fastq file\n</code></pre>"},{"location":"Glossary/#purge_dupss","title":"purge_dups's:","text":"<pre><code>purge haplotigs and overlaps in an assembly based on read depth\n</code></pre>"},{"location":"Glossary/#purge_dups","title":"purge_dups:","text":"<pre><code>purge haplotigs and overlaps in an assembly based on read depth\n</code></pre>"},{"location":"Glossary/#purge_haplotigss","title":"purge_haplotigs's:","text":"<pre><code>Pipeline to help with curating heterozygous diploid genome assemblies\n</code></pre>"},{"location":"Glossary/#purge_haplotigs","title":"purge_haplotigs:","text":"<pre><code>Pipeline to help with curating heterozygous diploid genome assemblies\n</code></pre>"},{"location":"Glossary/#pvs","title":"pv's:","text":"<pre><code>Monitors the progress of data through a unix pipeline.\n</code></pre>"},{"location":"Glossary/#pv","title":"pv:","text":"<pre><code>Monitors the progress of data through a unix pipeline.\n</code></pre>"},{"location":"Glossary/#pyanis","title":"pyani's:","text":"<pre><code>Whole-genome classification using Average Nucleotide Identity\n</code></pre>"},{"location":"Glossary/#pyani","title":"pyani:","text":"<pre><code>Whole-genome classification using Average Nucleotide Identity\n</code></pre>"},{"location":"Glossary/#pycoqcs","title":"pycoQC's:","text":"<pre><code>Computes metrics and generates interactive QC plots for Oxford Nanopore technologies sequencing data.\n</code></pre>"},{"location":"Glossary/#pycoqc","title":"pycoQC:","text":"<pre><code>Computes metrics and generates interactive QC plots for Oxford Nanopore technologies sequencing data.\n</code></pre>"},{"location":"Glossary/#pymol-open-sources","title":"pymol-open-source's:","text":"<pre><code>PyMOL (open source version) molecular visualization system.\n</code></pre>"},{"location":"Glossary/#pymol-open-source","title":"pymol-open-source:","text":"<pre><code>PyMOL (open source version) molecular visualization system.\n</code></pre>"},{"location":"Glossary/#pyspoas","title":"pyspoa's:","text":"<pre><code>Python bindings to spoa.\n</code></pre>"},{"location":"Glossary/#pyspoa","title":"pyspoa:","text":"<pre><code>Python bindings to spoa.\n</code></pre>"},{"location":"Glossary/#qcats","title":"qcat's:","text":"<pre><code>Command-line tool for demultiplexing Oxford Nanopore reads from FASTQ files\n</code></pre>"},{"location":"Glossary/#qcat","title":"qcat:","text":"<pre><code>Command-line tool for demultiplexing Oxford Nanopore reads from FASTQ files\n</code></pre>"},{"location":"Glossary/#rdocks","title":"rDock's:","text":"<pre><code>rDock is a fast and versatile Open Source docking program that\n</code></pre>"},{"location":"Glossary/#rdock","title":"rDock:","text":"<pre><code>rDock is a fast and versatile Open Source docking program that\n</code></pre>"},{"location":"Glossary/#randfolds","title":"randfold's:","text":"<pre><code>Minimum free energy of folding randomization test software\n</code></pre>"},{"location":"Glossary/#randfold","title":"randfold:","text":"<pre><code>Minimum free energy of folding randomization test software\n</code></pre>"},{"location":"Glossary/#rasusas","title":"rasusa's:","text":"<pre><code>Randomly subsample sequencing reads to a specified coverage.\n</code></pre>"},{"location":"Glossary/#rasusa","title":"rasusa:","text":"<pre><code>Randomly subsample sequencing reads to a specified coverage.\n</code></pre>"},{"location":"Glossary/#razers3s","title":"razers3's:","text":"<pre><code>Tool for mapping millions of short genomic reads onto a reference genome.\n</code></pre>"},{"location":"Glossary/#razers3","title":"razers3:","text":"<pre><code>Tool for mapping millions of short genomic reads onto a reference genome.\n</code></pre>"},{"location":"Glossary/#re2cs","title":"re2c's:","text":"<pre><code>re2c is a free and open-source lexer generator for C and C++. Its main goal is generating\n</code></pre>"},{"location":"Glossary/#re2c","title":"re2c:","text":"<pre><code>re2c is a free and open-source lexer generator for C and C++. Its main goal is generating\n</code></pre>"},{"location":"Glossary/#rnaquasts","title":"rnaQUAST's:","text":"<pre><code>Tool for evaluating RNA-Seq assemblies using reference genome and gene database\n</code></pre>"},{"location":"Glossary/#rnaquast","title":"rnaQUAST:","text":"<pre><code>Tool for evaluating RNA-Seq assemblies using reference genome and gene database\n</code></pre>"},{"location":"Glossary/#rsyncs","title":"rsync's:","text":"<pre><code>rsync is an open source utility that provides fast incremental file transfer.  rsync is freely available under the GNU General Public License\n</code></pre>"},{"location":"Glossary/#rsync","title":"rsync:","text":"<pre><code>rsync is an open source utility that provides fast incremental file transfer.  rsync is freely available under the GNU General Public License\n</code></pre>"},{"location":"Glossary/#rust-fmlrcs","title":"rust-fmlrc's:","text":"<pre><code>FM-index Long Read Corrector (Rust implementation)\n</code></pre>"},{"location":"Glossary/#rust-fmlrc","title":"rust-fmlrc:","text":"<pre><code>FM-index Long Read Corrector (Rust implementation)\n</code></pre>"},{"location":"Glossary/#samblasters","title":"samblaster's:","text":"<pre><code>samblaster is a fast and flexible program for marking duplicates in read-id grouped paired-end SAM files.\n</code></pre>"},{"location":"Glossary/#samblaster","title":"samblaster:","text":"<pre><code>samblaster is a fast and flexible program for marking duplicates in read-id grouped paired-end SAM files.\n</code></pre>"},{"location":"Glossary/#samclips","title":"samclip's:","text":"<pre><code>Filter SAM file for soft and hard clipped alignments.\n</code></pre>"},{"location":"Glossary/#samclip","title":"samclip:","text":"<pre><code>Filter SAM file for soft and hard clipped alignments.\n</code></pre>"},{"location":"Glossary/#sbts","title":"sbt's:","text":"<pre><code>sbt is a build tool for Scala, Java, and more.\n</code></pre>"},{"location":"Glossary/#sbt","title":"sbt:","text":"<pre><code>sbt is a build tool for Scala, Java, and more.\n</code></pre>"},{"location":"Glossary/#seqmagicks","title":"seqmagick's:","text":"<pre><code>Seqmagick is a utility built in the spirit of imagemagick to expose the\n</code></pre>"},{"location":"Glossary/#seqmagick","title":"seqmagick:","text":"<pre><code>Seqmagick is a utility built in the spirit of imagemagick to expose the\n</code></pre>"},{"location":"Glossary/#seqtks","title":"seqtk's:","text":"<pre><code>Seqtk is a fast and lightweight tool for processing sequences in the FASTA or FASTQ format.\n</code></pre>"},{"location":"Glossary/#seqtk","title":"seqtk:","text":"<pre><code>Seqtk is a fast and lightweight tool for processing sequences in the FASTA or FASTQ format.\n</code></pre>"},{"location":"Glossary/#shumlibs","title":"shumlib's:","text":"<pre><code>Shumlib is the collective name for a set of libraries which are used by the UM; the UK Met Office's Unified Model, that may be of use to external tools or applications where identical functionality is desired. The hope of the project is to enable developers to quickly and easily access parts of the UM code that are commonly duplicated elsewhere, at the same time benefiting from any improvements or optimisations that might be made in support of the UM itself.\n</code></pre>"},{"location":"Glossary/#shumlib","title":"shumlib:","text":"<pre><code>Shumlib is the collective name for a set of libraries which are used by the UM; the UK Met Office's Unified Model, that may be of use to external tools or applications where identical functionality is desired. The hope of the project is to enable developers to quickly and easily access parts of the UM code that are commonly duplicated elsewhere, at the same time benefiting from any improvements or optimisations that might be made in support of the UM itself.\n</code></pre>"},{"location":"Glossary/#sismonrs","title":"sismonr's:","text":"<pre><code>Simulation of In Silico Multi-Omic Networks R package.\n</code></pre>"},{"location":"Glossary/#sismonr","title":"sismonr:","text":"<pre><code>Simulation of In Silico Multi-Omic Networks R package.\n</code></pre>"},{"location":"Glossary/#slow5toolss","title":"slow5tools's:","text":"<pre><code>Toolkit for converting (FAST5 &lt;-&gt; SLOW5), compressing, viewing, indexing\n</code></pre>"},{"location":"Glossary/#slow5tools","title":"slow5tools:","text":"<pre><code>Toolkit for converting (FAST5 &lt;-&gt; SLOW5), compressing, viewing, indexing\n</code></pre>"},{"location":"Glossary/#smafas","title":"smafa's:","text":"<pre><code>Smafa attempts to align or cluster pre-aligned biological sequences, handling sequences\n</code></pre>"},{"location":"Glossary/#smafa","title":"smafa:","text":"<pre><code>Smafa attempts to align or cluster pre-aligned biological sequences, handling sequences\n</code></pre>"},{"location":"Glossary/#smooves","title":"smoove's:","text":"<pre><code>simplifies and speeds calling and genotyping SVs for short reads.\n</code></pre>"},{"location":"Glossary/#smoove","title":"smoove:","text":"<pre><code>simplifies and speeds calling and genotyping SVs for short reads.\n</code></pre>"},{"location":"Glossary/#snakemakes","title":"snakemake's:","text":"<pre><code>The Snakemake workflow management system is a tool to create reproducible and scalable data analyses.\n</code></pre>"},{"location":"Glossary/#snakemake","title":"snakemake:","text":"<pre><code>The Snakemake workflow management system is a tool to create reproducible and scalable data analyses.\n</code></pre>"},{"location":"Glossary/#snaphus","title":"snaphu's:","text":"<pre><code>SNAPHU is an implementation of the Statistical-cost, Network-flow Algorithm for Phase Unwrapping\n</code></pre>"},{"location":"Glossary/#snaphu","title":"snaphu:","text":"<pre><code>SNAPHU is an implementation of the Statistical-cost, Network-flow Algorithm for Phase Unwrapping\n</code></pre>"},{"location":"Glossary/#snappys","title":"snappy's:","text":"<pre><code>Snappy is a compression/decompression library. It does not aim for maximum compression, or compatibility with any other compression library; instead, it aims for very high speeds and reasonable compression.\n</code></pre>"},{"location":"Glossary/#snappy","title":"snappy:","text":"<pre><code>Snappy is a compression/decompression library. It does not aim for maximum compression, or compatibility with any other compression library; instead, it aims for very high speeds and reasonable compression.\n</code></pre>"},{"location":"Glossary/#snp-sitess","title":"snp-sites's:","text":"<pre><code>Finds SNP sites from a multi-FASTA alignment file.\n</code></pre>"},{"location":"Glossary/#snp-sites","title":"snp-sites:","text":"<pre><code>Finds SNP sites from a multi-FASTA alignment file.\n</code></pre>"},{"location":"Glossary/#snpeffs","title":"snpEff's:","text":"<pre><code>SnpEff is a variant annotation and effect prediction tool.\n</code></pre>"},{"location":"Glossary/#snpeff","title":"snpEff:","text":"<pre><code>SnpEff is a variant annotation and effect prediction tool.\n</code></pre>"},{"location":"Glossary/#somaliers","title":"somalier's:","text":"<pre><code>extract informative sites, evaluate relatedness, and\n</code></pre>"},{"location":"Glossary/#somalier","title":"somalier:","text":"<pre><code>extract informative sites, evaluate relatedness, and\n</code></pre>"},{"location":"Glossary/#spalns","title":"spaln's:","text":"<pre><code>Stand-alone program that maps and aligns a set of cDNA or protein sequences onto a whole genomic sequence in a single job.\n</code></pre>"},{"location":"Glossary/#spaln","title":"spaln:","text":"<pre><code>Stand-alone program that maps and aligns a set of cDNA or protein sequences onto a whole genomic sequence in a single job.\n</code></pre>"},{"location":"Glossary/#spdlogs","title":"spdlog's:","text":"<pre><code>Fast C++ logging library.\n</code></pre>"},{"location":"Glossary/#spdlog","title":"spdlog:","text":"<pre><code>Fast C++ logging library.\n</code></pre>"},{"location":"Glossary/#spglibs","title":"spglib's:","text":"<pre><code>Spglib is a C library for finding and handling crystal symmetries.\n</code></pre>"},{"location":"Glossary/#spglib","title":"spglib:","text":"<pre><code>Spglib is a C library for finding and handling crystal symmetries.\n</code></pre>"},{"location":"Glossary/#splats","title":"splat's:","text":"<pre><code>Splat deploys (builds and installs) the components of heterogenous systems such as cylc suites, described by splat config files, into target directory trees.\n</code></pre>"},{"location":"Glossary/#splat","title":"splat:","text":"<pre><code>Splat deploys (builds and installs) the components of heterogenous systems such as cylc suites, described by splat config files, into target directory trees.\n</code></pre>"},{"location":"Glossary/#spoas","title":"spoa's:","text":"<pre><code>c++ implementation of the partial order alignment (POA) algorithm\n</code></pre>"},{"location":"Glossary/#spoa","title":"spoa:","text":"<pre><code>c++ implementation of the partial order alignment (POA) algorithm\n</code></pre>"},{"location":"Glossary/#sratoolkits","title":"sratoolkit's:","text":"<pre><code>The SRA Toolkit, and the source-code SRA System Development\n</code></pre>"},{"location":"Glossary/#sratoolkit","title":"sratoolkit:","text":"<pre><code>The SRA Toolkit, and the source-code SRA System Development\n</code></pre>"},{"location":"Glossary/#srun-wrappers","title":"srun-wrapper's:","text":"<pre><code>Provides an mpirun script which merely wraps the srun command.\n</code></pre>"},{"location":"Glossary/#srun-wrapper","title":"srun-wrapper:","text":"<pre><code>Provides an mpirun script which merely wraps the srun command.\n</code></pre>"},{"location":"Glossary/#sublimes","title":"sublime's:","text":"<pre><code>Text editor with GUI\n</code></pre>"},{"location":"Glossary/#sublime","title":"sublime:","text":"<pre><code>Text editor with GUI\n</code></pre>"},{"location":"Glossary/#swarms","title":"swarm's:","text":"<pre><code>A robust and fast clustering method for amplicon-based studies.\n</code></pre>"},{"location":"Glossary/#swarm","title":"swarm:","text":"<pre><code>A robust and fast clustering method for amplicon-based studies.\n</code></pre>"},{"location":"Glossary/#swissknifes","title":"swissknife's:","text":"<pre><code>Perl module for reading and writing UniProtKB data in plain text format.\n</code></pre>"},{"location":"Glossary/#swissknife","title":"swissknife:","text":"<pre><code>Perl module for reading and writing UniProtKB data in plain text format.\n</code></pre>"},{"location":"Glossary/#trnascan-ses","title":"tRNAscan-SE's:","text":"<pre><code>Transfer RNA detection\n</code></pre>"},{"location":"Glossary/#trnascan-se","title":"tRNAscan-SE:","text":"<pre><code>Transfer RNA detection\n</code></pre>"},{"location":"Glossary/#tabixs","title":"tabix's:","text":"<pre><code>Generic indexer for TAB-delimited genome position files\n</code></pre>"},{"location":"Glossary/#tabix","title":"tabix:","text":"<pre><code>Generic indexer for TAB-delimited genome position files\n</code></pre>"},{"location":"Glossary/#tabixpps","title":"tabixpp's:","text":"<pre><code>C++ wrapper to tabix indexer\n</code></pre>"},{"location":"Glossary/#tabixpp","title":"tabixpp:","text":"<pre><code>C++ wrapper to tabix indexer\n</code></pre>"},{"location":"Glossary/#tbbs","title":"tbb's:","text":"<pre><code>Intel(R) Threading Building Blocks (Intel(R) TBB) lets you easily write parallel C++ programs that\n</code></pre>"},{"location":"Glossary/#tbb","title":"tbb:","text":"<pre><code>Intel(R) Threading Building Blocks (Intel(R) TBB) lets you easily write parallel C++ programs that\n</code></pre>"},{"location":"Glossary/#tbl2asns","title":"tbl2asn's:","text":"<pre><code>Command-line program that automates the creation of\n</code></pre>"},{"location":"Glossary/#tbl2asn","title":"tbl2asn:","text":"<pre><code>Command-line program that automates the creation of\n</code></pre>"},{"location":"Glossary/#tmuxs","title":"tmux's:","text":"<pre><code>tmux is a terminal multiplexer. It lets you switch easily\n</code></pre>"},{"location":"Glossary/#tmux","title":"tmux:","text":"<pre><code>tmux is a terminal multiplexer. It lets you switch easily\n</code></pre>"},{"location":"Glossary/#trfs","title":"trf's:","text":"<pre><code>Locates tandem repeats in DNA sequences.\n</code></pre>"},{"location":"Glossary/#trf","title":"trf:","text":"<pre><code>Locates tandem repeats in DNA sequences.\n</code></pre>"},{"location":"Glossary/#trimals","title":"trimAl's:","text":"<pre><code>Tool for automated alignment trimming in large-scale phylogenetic analyses\n</code></pre>"},{"location":"Glossary/#trimal","title":"trimAl:","text":"<pre><code>Tool for automated alignment trimming in large-scale phylogenetic analyses\n</code></pre>"},{"location":"Glossary/#um2netcdfs","title":"um2netcdf's:","text":"<pre><code>um2netcdf converts Unified Model output files in fieldsfile format into netCDF format.\n</code></pre>"},{"location":"Glossary/#um2netcdf","title":"um2netcdf:","text":"<pre><code>um2netcdf converts Unified Model output files in fieldsfile format into netCDF format.\n</code></pre>"},{"location":"Glossary/#unimaps","title":"unimap's:","text":"<pre><code>Fork of minimap2 optimized for assembly-to-reference\n</code></pre>"},{"location":"Glossary/#unimap","title":"unimap:","text":"<pre><code>Fork of minimap2 optimized for assembly-to-reference\n</code></pre>"},{"location":"Glossary/#unrars","title":"unrar's:","text":"<pre><code>RAR is a powerful archive manager.\n</code></pre>"},{"location":"Glossary/#unrar","title":"unrar:","text":"<pre><code>RAR is a powerful archive manager.\n</code></pre>"},{"location":"Glossary/#util-linuxs","title":"util-linux's:","text":"<pre><code>Set of Linux utilities\n</code></pre>"},{"location":"Glossary/#util-linux","title":"util-linux:","text":"<pre><code>Set of Linux utilities\n</code></pre>"},{"location":"Glossary/#vcflibs","title":"vcflib's:","text":"<pre><code>Genetic variant detector designed to find polymorphisms smaller than the length of a short-read sequencing alignment.\n</code></pre>"},{"location":"Glossary/#vcflib","title":"vcflib:","text":"<pre><code>Genetic variant detector designed to find polymorphisms smaller than the length of a short-read sequencing alignment.\n</code></pre>"},{"location":"Glossary/#vgs","title":"vg's:","text":"<pre><code>variation graph data structures, interchange formats, alignment, genotyping, and variant calling methods\n</code></pre>"},{"location":"Glossary/#vg","title":"vg:","text":"<pre><code>variation graph data structures, interchange formats, alignment, genotyping, and variant calling methods\n</code></pre>"},{"location":"Glossary/#wgsims","title":"wgsim's:","text":"<pre><code>Wgsim is a small tool for simulating sequence reads from a reference genome.\n</code></pre>"},{"location":"Glossary/#wgsim","title":"wgsim:","text":"<pre><code>Wgsim is a small tool for simulating sequence reads from a reference genome.\n</code></pre>"},{"location":"Glossary/#wheels","title":"wheel's:","text":"<pre><code>A built-package format for Python.\n</code></pre>"},{"location":"Glossary/#wheel","title":"wheel:","text":"<pre><code>A built-package format for Python.\n</code></pre>"},{"location":"Glossary/#wtdbgs","title":"wtdbg's:","text":"<pre><code>de novo sequence assembler for long noisy reads produced by PacBio or Oxford Nanopore Technologies.\n</code></pre>"},{"location":"Glossary/#wtdbg","title":"wtdbg:","text":"<pre><code>de novo sequence assembler for long noisy reads produced by PacBio or Oxford Nanopore Technologies.\n</code></pre>"},{"location":"Glossary/#wxwidgetss","title":"wxWidgets's:","text":"<pre><code>widget toolkit and tools library for creating graphical user interfaces for cross-platform applications.\n</code></pre>"},{"location":"Glossary/#wxwidgets","title":"wxWidgets:","text":"<pre><code>widget toolkit and tools library for creating graphical user interfaces for cross-platform applications.\n</code></pre>"},{"location":"Glossary/#xbitmapss","title":"xbitmaps's:","text":"<pre><code>provides bitmaps for x\n</code></pre>"},{"location":"Glossary/#xbitmaps","title":"xbitmaps:","text":"<pre><code>provides bitmaps for x\n</code></pre>"},{"location":"Glossary/#xkbcommons","title":"xkbcommon's:","text":"<pre><code>keyboard keymap compiler and support library\n</code></pre>"},{"location":"Glossary/#xkbcommon","title":"xkbcommon:","text":"<pre><code>keyboard keymap compiler and support library\n</code></pre>"},{"location":"Glossary/#yacrds","title":"yacrd's:","text":"<pre><code>Chimeric Read Detector for long reads\n</code></pre>"},{"location":"Glossary/#yacrd","title":"yacrd:","text":"<pre><code>Chimeric Read Detector for long reads\n</code></pre>"},{"location":"Glossary/#yajls","title":"yajl's:","text":"<pre><code>Yet Another JSON Library. Why does the world need another C library for parsing JSON? Good question.\n</code></pre>"},{"location":"Glossary/#yajl","title":"yajl:","text":"<pre><code>Yet Another JSON Library. Why does the world need another C library for parsing JSON? Good question.\n</code></pre>"},{"location":"Glossary/#yaks","title":"yak's:","text":"<pre><code>Yet another k-mer analyzer\n</code></pre>"},{"location":"Glossary/#yak","title":"yak:","text":"<pre><code>Yet another k-mer analyzer\n</code></pre>"},{"location":"Glossary/#yaml-cpps","title":"yaml-cpp's:","text":"<pre><code>YAML parser and emitter in C++\n</code></pre>"},{"location":"Glossary/#yaml-cpp","title":"yaml-cpp:","text":"<pre><code>YAML parser and emitter in C++\n</code></pre>"},{"location":"Glossary/#zlibs","title":"zlib's:","text":"<pre><code>zlib is designed to be a free, general-purpose, legally unencumbered -- that is,\n</code></pre>"},{"location":"Glossary/#zlib","title":"zlib:","text":"<pre><code>zlib is designed to be a free, general-purpose, legally unencumbered -- that is,\n</code></pre>"},{"location":"Glossary/#zstds","title":"zstd's:","text":"<pre><code>Zstandard is a real-time compression algorithm, providing high compression ratios.  It offers a very wide range of compression/speed trade-off, while being backed by a very fast decoder.  It also offers a special mode for small data, called dictionary compression, and can create dictionaries  from any sample set.\n</code></pre>"},{"location":"Glossary/#zstd","title":"zstd:","text":"<pre><code>Zstandard is a real-time compression algorithm, providing high compression ratios.  It offers a very wide range of compression/speed trade-off, while being backed by a very fast decoder.  It also offers a special mode for small data, called dictionary compression, and can create dictionaries  from any sample set.\n</code></pre>"},{"location":"format/","title":"Title (H1)","text":"<pre><code># Title (H1)\n</code></pre>"},{"location":"format/#h2","title":"H2","text":"<pre><code>## H2\n</code></pre>"},{"location":"format/#h3","title":"H3","text":"<pre><code>### H3\n</code></pre>"},{"location":"format/#h4","title":"H4","text":"<pre><code>#### H4\n</code></pre> <p>Headers should have a blank line before and after.</p> <p>'H1' (<code>#</code>) is for the page title. Setting a title here will change it in the nav also.</p> <p>Warning</p> <p>Put 2 spaces at the end of a line to force a line break.  If you simply hit enter and don't use 2 spaces it will be considered one line.</p>"},{"location":"format/#text-emphasis","title":"Text Emphasis","text":"<p>bold: <code>**bold**</code></p> <p>italic: <code>_italic</code></p>"},{"location":"format/#tab-containers","title":"Tab Containers","text":"Tab OneTab two <p>someting in the tab</p> <p>something else</p> <pre><code>=== \"Tab One\"\n    someting in the tab\n=== \"Tab two\"\n    something else\n</code></pre>"},{"location":"format/#admonations","title":"Admonations","text":"<p>Warning</p> <p>A warning.</p> <pre><code>!!! warning\n    A warning.\n</code></pre> <p>Optional title</p> <p>Admonation with optional title.</p> <pre><code>!!! info \"Optional title\"\n    Admonation with optional title.\n</code></pre> <p>Any admonation can be made collapsable by replacing the <code>!!!</code> with <code>???</code> (closed), or <code>???+</code> (open)</p> <p>Note for future, once decided which of these we will use, remove the others. And give description of when to use.</p> <p>There are lots of different flavors.</p> <p>Note</p> <p>This is a test admonation.</p> <p>Tip</p> <p>This is a test admonation.</p> <p>Info</p> <p>This is a test admonation.</p> <p>Question</p> <p>This is a test admonation.</p> <p>Warning</p> <p>This is a test admonation.</p> <p>Failure</p> <p>This is a test admonation.</p> <p>Danger</p> <p>This is a test admonation.</p> <p>Bug</p> <p>This is a test admonation.</p> <p>Example</p> <p>This is a test admonation.</p> <p>Quote</p> <p>This is a test admonation.</p> <p>Prerequisite</p> <p>This is a test admonation.</p> <p>Pied-piper</p> <p>this is a test admonation.</p> <p>Desktop-download-24</p> <p>this is a test admonation.</p> <p>Magnifying-glass</p> <p>this is a test admonation.</p> <p>Microscope</p> <p>this is a test admonation.</p> <p>Vial-virus</p> <p>this is a test admonation.</p> <p>Database</p> <p>this is a test admonation.</p> <p>Folder-open</p> <p>this is a test admonation.</p> <p>Backward</p> <p>this is a test admonation.</p> <p>Jupyter</p> <p>this is a test admonation.</p> <p>Terminal</p> <p>this is a test admonation.</p> <p>R-project</p> <p>this is a test admonation.</p> <p>Calendar-days</p> <p>this is a test admonation.</p> <p>Bell</p> <p>this is a test admonation.</p> <p>Comment-dots</p> <p>this is a test admonation.</p> <p>Check-to-slot</p> <p>this is a test admonation.</p> <p>Square-xmark</p> <p>this is a test admonation.</p> <p>Rectangle-list</p> <p>this is a test admonation.</p> <p>Screwdriver-wrench</p> <p>this is a test admonation.</p> <p>Linux</p> <p>this is a test admonation.</p> <p>Code-compare</p> <p>this is a test admonation.</p> <p>Heading</p> <p>this is a test admonation.</p> <p>Space-awesome</p> <p>this is a test admonation.</p> <p>Stethoscope</p> <p>this is a test admonation.</p> <p>Key</p> <p>this is a test admonation.</p> <p>Users-line</p> <p>this is a test admonation.</p> <p>File-code</p> <p>this is a test admonation.</p> <p>Hand-holding-dollar</p> <p>this is a test admonation.</p> <p>Circle-question</p> <p>this is a test admonation.</p> <p>Microphone</p> <p>this is a test admonation.</p> <p>Tower-observation</p> <p>this is a test admonation.</p> <p>Circle-info</p> <p>this is a test admonation.</p> <p>Icon--python</p> <p>this is a test admonation.</p> <p>Quote-right</p> <p>this is a test admonation.</p> <p>Image</p> <p>this is a test admonation.</p> <p>Table</p> <p>this is a test admonation.</p> <p>Glass-chart</p> <p>this is a test admonation.</p> <p>File-export</p> <p>this is a test admonation.</p>"},{"location":"format/#code","title":"Code","text":"<p>Code blocks require a language lexxer in order to do syntax hilighting, e.g. <code>python</code> ,<code>slurm</code>, <code>cuda</code>, <code>json</code>, <code>md</code>. A full list of lexxers can be found in this list.</p> <p>For plain code blocks, still good to use a class as descriptor (e.g. <code>txt</code>, <code>stdout</code>, <code>stderr</code>). May want to add formatting to this later.</p> <pre><code>```stdout\nsome code\n```\n</code></pre> <pre><code>some code \n</code></pre>"},{"location":"format/#block","title":"Block","text":"<pre><code>import somepackage\n\nformatting = True\nif formatting:\n    Print(formatting) # A comment\n</code></pre> <pre><code>```py\nimport somepackage\n\nformatting = True\nif formatting:\n  Print(formatting) # A comment\n```\n</code></pre>"},{"location":"format/#inline","title":"Inline","text":"<p>This is some <code>echo \"Inline Code\"</code>.</p> <pre><code>This is some `echo \"Inline Code\"`.</code></pre>"},{"location":"format/#keyboard","title":"Keyboard","text":"<p>Keyboard keys can be added using the <code>&lt;kbd&gt;</code> tag.</p> <p>Press <code>&lt;kbd&gt;</code>ctrl<code>&lt;/kbd&gt;</code> + <code>&lt;kbd&gt;</code>c<code>&lt;/kbd&gt;</code> to copy text from terminal.</p> <pre><code>Press &lt;kbd&gt;ctrl&lt;/kbd&gt; + &lt;kbd&gt;c&lt;/kbd&gt; to copy text from terminal.\n</code></pre> <p>Note the additional spacing around the <code>+</code> else it will appear cramped.</p>"},{"location":"format/#images","title":"Images","text":"<pre><code>![This is an image](./assets/images/ANSYS_0.png)\n</code></pre>"},{"location":"format/#links","title":"Links","text":"<p>External Link</p> <pre><code>[External Link](\"https://example.com\")\n</code></pre> <p>Internal Link</p> <pre><code>[Internal Link](\"General/FAQs/Password_Expiry\")\n</code></pre> <p>Warning</p> <p>Link paths are relative to current file!!!</p> <p>Anchor Link</p> <pre><code>[Anchor Link](#links)\n</code></pre> <p>snake-case anchors are automatically generated for all headers.</p> <p>For example a header <code>## This is my Header</code> can be linked to with the anchor <code>[Anchor Link](#this-is-my-header)</code></p>"},{"location":"format/#tooltips","title":"Tooltips","text":"<p>Hover over me</p> <pre><code>[Hover over me](https://example.com \"I'm a link with a custom tooltip.\")\n</code></pre> <p>Acroynym should be automatically tooltipped e.g. MPI.</p> <pre><code>Acroynym should be automatically tooltipped e.g. MPI.\n</code></pre>"},{"location":"format/#lists","title":"Lists","text":""},{"location":"format/#unordered-list","title":"Unordered List","text":"<ul> <li>item1</li> <li>item2</li> <li>a</li> <li>item</li> </ul> <pre><code>- item1\n- item2\n- a\n  multiline\n- item\n- nested\n  - items\n    - nesteder\n</code></pre>"},{"location":"format/#ordered-list","title":"Ordered List","text":"<ol> <li>item1</li> <li>a    multiline    item    with multiple    lines</li> <li>nested</li> <li>nested item 1</li> <li>nested item 2<ol> <li>even nesteder</li> </ol> </li> </ol> <pre><code>1. item1\n2. a\n   multiline\n   item\n   with multiple\n   lines\n3. nested\n    1. nested item 1\n    2. nested item 2\n        1. even nesteder\n</code></pre> <p>Note, nested list items use numbers, but will be rendered as whatever the indent is.</p>"},{"location":"format/#tables","title":"Tables","text":"<p>Markdown Table Generator, is a handy tool for complex tables/</p> <p>Tables can be constructed using <code>|</code> to seperate colums, and <code>--</code> to designate headers.</p> <p>Number of dashes has no effect, things dont have to be lined up when in markdown, just looks nice. Leading and trailing <code>|</code> are optional.</p> Head Head Thing1 Thing2 Thing3 Thing3 <pre><code> Head | Head\n --- | -----------\n Thing1 | Thing2\n Thing3 | Thing 3\n</code></pre> <p><code>:</code>'s can be used to align tables.</p> Left Center Right Words Words Words Words Words Words <pre><code>| Syntax      | Description | Test Text     |\n| :---        |    :----:   |          ---: |\n| Header      | Title       | Here's this   |\n| Paragraph   | Text        | And more      |\n</code></pre>"},{"location":"macros/","title":"Info about macros","text":""},{"location":"macros/#macros-plugin-environment","title":"Macros Plugin Environment","text":""},{"location":"macros/#general-list","title":"General List","text":"<p>All available variables and filters within the macros plugin:</p> Variable Type Content extra dict config MkDocsConfig {'config_file_path': '/home/runner/work/support-docs-concept/support-docs-concept/mkdocs.yml', 'site_name': 'NeSI Support Center', 'nav': [{'Getting Started': 'Getting_Started/'}, {'General': 'General/'}, {'Scientific Computing': 'Scientific_Computing/'}, {'Storage': 'Storage/'}, {'NeSI Service Subscriptions': 'NeSI_Service_Subscriptions/'}], 'pages': None, 'exclude_docs': None, 'not_in_nav': None, 'site_url': 'https://github.com/nesi/support-docs-concept/', 'site_description': 'NeSI Support Documentation', 'site_author': None, 'theme': Theme(name='material', dirs=['/home/runner/work/support-docs-concept/support-docs-concept/overrides', '/home/runner/.local/lib/python3.10/site-packages/material/templates', '/home/runner/.local/lib/python3.10/site-packages/mkdocs/templates'], static_templates={'sitemap.xml', '404.html'}, name='material', locale=Locale('en'), language='en', direction=None, features=['navigation.indexes', 'navigation.top', 'navigation.prune', 'content.code.copy', 'content.action.edit', 'navigation.path'], font={'text': 'Roboto', 'code': 'Roboto Mono'}, icon=None, favicon='assets/icons/favicon.ico', logo='assets/icons/logo.svg', palette=[{'primary': 'custom', 'scheme': 'default', 'toggle': {'icon': 'material/lightbulb', 'name': 'Switch to dark mode'}}, {'primary': 'custom', 'scheme': 'slate', 'toggle': {'icon': 'material/lightbulb-outline', 'name': 'Switch to light mode'}}]), 'docs_dir': '/home/runner/work/support-docs-concept/support-docs-concept/docs', 'site_dir': '/home/runner/work/support-docs-concept/support-docs-concept/public', 'copyright': None, 'google_analytics': None, 'dev_addr': _IpAddressValue(host='127.0.0.1', port=8000), 'use_directory_urls': True, 'repo_url': 'https://github.com/nesi/support-docs-concept', 'repo_name': 'GitHub', 'edit_uri_template': None, 'edit_uri': 'edit/main/docs/', 'extra_css': ['assets/stylesheets/neoteroi-mkdocs.css', 'assets/stylesheets/footer.css', 'assets/stylesheets/custom_admonations.css', 'assets/stylesheets/theme.css'], 'extra_javascript': ['assets/javascripts/general.js'], 'extra_templates': [], 'markdown_extensions': ['toc', 'tables', 'fenced_code', 'admonition', 'pymdownx.details', 'pymdownx.superfences', 'pymdownx.highlight', 'pymdownx.inlinehilite', 'pymdownx.tabbed', 'attr_list', 'abbr', 'neoteroi.cards', 'neoteroi.timeline', 'pymdownx.snippets'], 'mdx_configs': {'toc': {'baselevel': 1, 'permalink': True, 'toc_depth': 3}, 'pymdownx.tabbed': {'alternate_style': True}, 'pymdownx.snippets': {'auto_append': ['assets/glossary/snippets.md']}}, 'strict': False, 'remote_branch': 'gh-pages', 'remote_name': 'origin', 'extra': {}, 'plugins': {'material/search': , 'redirects': , 'literate-nav': , 'mkdocs-nav-weight': , 'material/tags': , 'git-revision-date-localized': , 'macros': , 'mkdocs_hooks.py': }, 'hooks': {'mkdocs_hooks.py': }, 'watch': [], 'validation': {'nav': {'omitted_files': 20, 'not_found': 30, 'absolute_links': 20}, 'links': {'not_found': 30, 'absolute_links': 20, 'unrecognized_links': 20}}} environment dict system = 'Linux', system_version = '6.2.0-1018-azure', python_version = '3.10.12', mkdocs_version = '1.5.3', macros_plugin_version = '1.0.5', jinja2_version = '3.0.3' plugin LegacyConfig {'module_name': 'macro_hooks', 'modules': [], 'render_by_default': True, 'include_dir': 'overrides', 'include_yaml': [], 'j2_block_start_string': '', 'j2_block_end_string': '', 'j2_variable_start_string': '', 'j2_variable_end_string': '', 'on_undefined': 'keep', 'on_error_fail': False, 'verbose': False} git dict status = True, date [datetime], short_commit = '738c327b', commit = '738c327b154bcd2c44eb1b7b43328f0fbd1bfed3', tag = '', short_tag = '', author = 'Cal', author_email = '35017184+CallumWalley@users.noreply.github.com', committer = 'GitHub', committer_email = 'noreply@github.com', date_ISO = 'Sat Dec 30 17:16:05 2023 +1300', message = 'Update checks.yml\\n\\nSigned-off-by: Cal &lt;35017184+CallumWalley@users.noreply.github.com&gt;', raw = 'commit 738c327b154bcd2c44eb1b7b43328f0fbd1bfed3\\nAuthor: Cal &lt;35017184+CallumWalley@users.noreply.github.com&gt;\\nDate:   Sat Dec 30 17:16:05 2023 +1300\\n\\n    Update checks.yml\\n    \\n    Signed-off-by: Cal &lt;35017184+CallumWalley@users.noreply.github.com&gt;', root_dir = '/home/runner/work/support-docs-concept/support-docs-concept' applications dict ABAQUS [dict], ABRicate [dict], ABySS [dict], ACTC [dict], AGAT [dict], AGE [dict], AMOS [dict], AMRFinderPlus [dict], ANIcalculator [dict], ANNOVAR [dict], ANSYS [dict], ANTLR [dict], ANTS [dict], ANTs [dict], AOCC [dict], AOCL-BLIS [dict], AOCL-FFTW [dict], AOCL-ScaLAPACK [dict], APR [dict], APR-util [dict], ARCSI [dict], ARIBA [dict], ATK [dict], AUGUSTUS [dict], Abseil [dict], AdapterRemoval [dict], Advisor [dict], AlphaFold [dict], AlphaFold2DB [dict], AlwaysIntelMKL [dict], Amber [dict], Anaconda2 [dict], Anaconda3 [dict], Apptainer [dict], Armadillo [dict], Arrow [dict], Aspera-CLI [dict], AutoDock-GPU [dict], AutoDock_Vina [dict], BBMap [dict], BCFtools [dict], BCL-Convert [dict], BEAST [dict], BEDOPS [dict], BEDTools [dict], BEEF [dict], BLASR [dict], BLAST [dict], BLASTDB [dict], BLAT [dict], BLIS [dict], BOLT-LMM [dict], BRAKER [dict], BUSCO [dict], BWA [dict], BamTools [dict], Bandage [dict], Basilisk [dict], BayPass [dict], BayeScan [dict], BayesAss [dict], Bazel [dict], Beagle [dict], BerkeleyGW [dict], BiG-SCAPE [dict], Bifrost [dict], Bio-DB-BigFile [dict], Bio-DB-HTS [dict], BioPP [dict], Bismark [dict], Bison [dict], BlenderPy [dict], Boost [dict], Bowtie [dict], Bowtie2 [dict], Bpipe [dict], Bracken [dict], BreakDancer [dict], BreakSeq2 [dict], CCL [dict], CD-HIT [dict], CDO [dict], CFITSIO [dict], CGAL [dict], CMake [dict], CNVnator [dict], CNVpytor [dict], COMSOL [dict], CONCOCT [dict], CP2K [dict], CRAMINO [dict], CTPL [dict], CUDA [dict], CUnit [dict], Canu [dict], CapnProto [dict], Catch2 [dict], CellRanger [dict], Centrifuge [dict], Cereal [dict], Charm++ [dict], CheckM [dict], CheckM2 [dict], CheckV [dict], Circlator [dict], Circos [dict], Clair3 [dict], Clustal-Omega [dict], ClustalW2 [dict], Corset [dict], CoverM [dict], CppUnit [dict], CrayCCE [dict], CrayGNU [dict], CrayIntel [dict], CubeGUI [dict], CubeLib [dict], CubeWriter [dict], Cufflinks [dict], Cytoscape [dict], D-Genies [dict], DAS_Tool [dict], DB [dict], DBG2OLC [dict], DBus [dict], DFT-D4 [dict], DIAMOND [dict], DISCOVARdenovo [dict], DRAM [dict], DaliLite [dict], DeconSeq [dict], DeePMD-kit [dict], DeepLabCut [dict], DefaultModules [dict], Delft3D [dict], Delft3D_FM [dict], Delly [dict], Dorado [dict], Doxygen [dict], Dsuite [dict], EDTA [dict], EIGENSOFT [dict], ELPA [dict], EMAN [dict], EMAN2 [dict], EMBOSS [dict], EMIRGE [dict], ENMTML [dict], ESMF [dict], ETE [dict], EasyBuild [dict], Eigen [dict], Elmer [dict], Embree [dict], EnergyPlus [dict], ErlangOTP [dict], EukRep-EukCC [dict], ExaBayes [dict], ExaML [dict], Extrae [dict], FALCON [dict], FASTX-Toolkit [dict], FCM [dict], FDS [dict], FFTW [dict], FFTW.MPI [dict], FFmpeg [dict], FIGARO [dict], FLTK [dict], FTGL [dict], FastANI [dict], FastME [dict], FastQC [dict], FastQ_Screen [dict], FastTree [dict], File-Rename [dict], Filtlong [dict], FimTyper [dict], FlexiBLAS [dict], Flye [dict], FoX [dict], FragGeneScan [dict], FreeBayes [dict], FreeSurfer [dict], FreeXL [dict], FriBidi [dict], GATK [dict], GCC [dict], GCCcore [dict], GD [dict], GDAL [dict], GEMMA [dict], GEOS [dict], GLM [dict], GLPK [dict], GLib [dict], GMAP-GSNAP [dict], GMP [dict], GMT [dict], GOLD [dict], GObject-Introspection [dict], GRADS [dict], GRASS [dict], GRIDSS [dict], GROMACS [dict], GSL [dict], GST-plugins-base [dict], GStreamer [dict], GTDB-Tk [dict], GTK+ [dict], GTS [dict], GUSHR [dict], Gaussian [dict], Gdk-Pixbuf [dict], GeneMark-ES [dict], GenoVi [dict], GenomeThreader [dict], Gerris [dict], GetOrganelle [dict], GlimmerHMM [dict], Go [dict], Graphviz [dict], Gubbins [dict], Guile [dict], HDF [dict], HDF-EOS [dict], HDF-EOS5 [dict], HDF5 [dict], HISAT2 [dict], HMMER [dict], HMMER2 [dict], HOPS [dict], HTSeq [dict], HTSlib [dict], HarfBuzz [dict], HpcGridRunner [dict], Humann [dict], HybPiper [dict], Hypre [dict], ICU [dict], IDBA [dict], IDBA-UD [dict], IDL [dict], IGV [dict], IMPUTE [dict], IQ-TREE [dict], IQmol [dict], IRkernel [dict], ISA-L [dict], ImageMagick [dict], Infernal [dict], Inspector [dict], InterProScan [dict], JAGS [dict], JUnit [dict], JasPer [dict], Java [dict], Jellyfish [dict], JsonCpp [dict], Julia [dict], JupyterLab [dict], KAT [dict], KEALib [dict], KMC [dict], Kaiju [dict], Kent_tools [dict], KmerGenie [dict], KorfSNAP [dict], Kraken2 [dict], KronaTools [dict], KyotoCabinet [dict], LAME [dict], LAMMPS [dict], LAST [dict], LASTZ [dict], LDC [dict], LEfSe [dict], LINKS [dict], LLVM [dict], LMDB [dict], LSD2 [dict], LTR_retriever [dict], LUMPY [dict], LZO [dict], LegacySystemLibs [dict], LibTIFF [dict], Libav [dict], Libint [dict], Liftoff [dict], LittleCMS [dict], Loki [dict], LongStitch [dict], M4 [dict], MAFFT [dict], MAGMA [dict], MAKER [dict], MATIO [dict], MATLAB [dict], MCL [dict], MCR [dict], MEGAHIT [dict], METABOLIC [dict], METIS [dict], MMseqs2 [dict], MOB-suite [dict], MODFLOW [dict], MPFR [dict], MSMC [dict], MUMPS [dict], MUMmer [dict], MUSCLE [dict], MUST [dict], MaSuRCA [dict], Magma [dict], Mamba [dict], MarkerMiner [dict], Mash [dict], MashMap [dict], Maven [dict], MaxBin [dict], Meraculous [dict], Merqury [dict], Mesa [dict], Meson [dict], MetaBAT [dict], MetaEuk [dict], MetaGeneAnnotator [dict], MetaPhlAn [dict], MetaPhlAn2 [dict], MetaSV [dict], MetaVelvet [dict], Metashape [dict], Metaxa2 [dict], Miniconda3 [dict], Minimac3 [dict], Minimac4 [dict], MitoZ [dict], ModDotPlot [dict], Molcas [dict], Molpro [dict], Mono [dict], Monocle3 [dict], Mothur [dict], MotionCorr [dict], MrBayes [dict], Mule [dict], MultiQC [dict], NAMD [dict], NASM [dict], NCARG [dict], NCCL [dict], NCL [dict], NCO [dict], NCVIEW [dict], NECAT [dict], NGS [dict], NLopt [dict], NONMEM [dict], NSPR [dict], NSS [dict], NVHPC [dict], NWChem [dict], NanoComp [dict], NanoLyse [dict], NanoPlot [dict], NanoStat [dict], NewHybrids [dict], Newton-X [dict], NextGenMap [dict], Nextflow [dict], Nim [dict], Ninja [dict], Nsight-Compute [dict], Nsight-Systems [dict], OASIS3-MCT [dict], OBITools [dict], OCI [dict], OMA [dict], OPARI2 [dict], ORCA [dict], OSPRay [dict], OSU-Micro-Benchmarks [dict], OTF2 [dict], Octave [dict], Octopus [dict], OpenBLAS [dict], OpenBabel [dict], OpenCMISS [dict], OpenCV [dict], OpenFAST [dict], OpenFOAM [dict], OpenJPEG [dict], OpenMPI [dict], OpenSSL [dict], OpenSees [dict], OpenSeesPy [dict], OpenSlide [dict], OrfM [dict], OrthoFiller [dict], OrthoFinder [dict], OrthoMCL [dict], PALEOMIX [dict], PAML [dict], PAPI [dict], PBJelly [dict], PCRE [dict], PCRE2 [dict], PDT [dict], PEAR [dict], PEST++ [dict], PETSc [dict], PFFT [dict], PHASIUS [dict], PLINK [dict], PLUMED [dict], PRANK [dict], PROJ [dict], Pango [dict], ParMETIS [dict], ParaView [dict], Parallel [dict], ParallelIO [dict], Paraver [dict], Peregrine [dict], Perl [dict], PhyML [dict], PhyloPhlAn [dict], Pilon [dict], PnetCDF [dict], Porechop [dict], PostgreSQL [dict], Prodigal [dict], ProtHint [dict], Proteinortho [dict], PyOpenGL [dict], PyQt [dict], PyTorch [dict], Python [dict], Python-GPU [dict], Python-Geo [dict], QChem [dict], QIIME2 [dict], QUAST [dict], Qt5 [dict], QuantumESPRESSO [dict], QuickTree [dict], R [dict], R-Geo [dict], R-bundle-Bioconductor [dict], RANGS-GSHHS [dict], RAxML [dict], RAxML-NG [dict], RDP-Classifier [dict], RE2 [dict], RECON [dict], RFPlasmid [dict], RFdiffusion [dict], RMBlast [dict], RNAmmer [dict], ROCm [dict], ROOT [dict], RSEM [dict], RSGISLib [dict], Racon [dict], Ragout [dict], RapidNJ [dict], Raven [dict], Rcorrector [dict], Relion [dict], RepeatMasker [dict], RepeatModeler [dict], RepeatScout [dict], Riskscape [dict], Roary [dict], Rosetta [dict], Rstudio [dict], Ruby [dict], Rust [dict], SAGE [dict], SAMtools [dict], SAS [dict], SCOTCH [dict], SCons [dict], SDL2 [dict], SEPP [dict], SHAPEIT4 [dict], SIONlib [dict], SIP [dict], SKESA [dict], SMRT-Link [dict], SNVoter-NanoMethPhase [dict], SOAPdenovo2 [dict], SOCI [dict], SPAdes [dict], SPECFEM3D [dict], SPIDER [dict], SQLite [dict], SQLplus [dict], SSAHA2 [dict], STAR [dict], STAR-Fusion [dict], SUNDIALS [dict], SURVIVOR [dict], SWIG [dict], Salmon [dict], Sambamba [dict], ScaLAPACK [dict], Scalasca [dict], Score-P [dict], SeqAn [dict], SeqAn3 [dict], SeqKit [dict], SeqMonk [dict], SiBELia [dict], Siesta [dict], Siesta-Optical [dict], SignalP [dict], Singularity [dict], Sniffles [dict], SortMeRNA [dict], SourceTracker [dict], Spack [dict], Spark [dict], SqueezeMeta [dict], Stacks [dict], StringTie [dict], Structure [dict], Subread [dict], Subversion [dict], SuiteSparse [dict], SuperLU [dict], Supernova [dict], Szip [dict], TEtranscripts [dict], TMHMM [dict], TOGA [dict], TSEBRA [dict], TURBOMOLE [dict], TWL-NINJA [dict], Tcl [dict], TensorFlow [dict], TensorRT [dict], Theano [dict], Tk [dict], TopHat [dict], TransDecoder [dict], Transrate [dict], TreeMix [dict], Trilinos [dict], TrimGalore [dict], Trimmomatic [dict], Trinity [dict], Trinotate [dict], Trycycler [dict], TuiView [dict], TurboVNC [dict], UCC [dict], UCX [dict], UDUNITS [dict], USEARCH [dict], Unicycler [dict], VASP [dict], VCF-kit [dict], VCFtools [dict], VEP [dict], VESTA [dict], VIBRANT [dict], VMD [dict], VSEARCH [dict], VTK [dict], VTune [dict], Valgrind [dict], VarScan [dict], Velvet [dict], VelvetOptimiser [dict], ViennaRNA [dict], Vim [dict], VirHostMatcher [dict], VirSorter [dict], VirtualGL [dict], VisIt [dict], WAAFLE [dict], WhatsHap [dict], Winnowmap [dict], Wise2 [dict], XCONV [dict], XGKS [dict], XHMM [dict], XIOS [dict], XMDS2 [dict], XSD [dict], XZ [dict], Xerces-C++ [dict], YAXT [dict], Yade [dict], Yasm [dict], ZeroMQ [dict], Zip [dict], Zonation [dict], abritamr [dict], angsd [dict], ant [dict], antiSMASH [dict], any2fasta [dict], argtable [dict], aria2 [dict], arpack-ng [dict], at-spi2-atk [dict], at-spi2-core [dict], attr [dict], azul-zulu [dict], bamUtil [dict], barrnap [dict], bcl2fastq2 [dict], beagle-lib [dict], best [dict], binutils [dict], bioawk [dict], blasr_libcpp [dict], breseq [dict], bsddb3 [dict], bzip2 [dict], c-ares [dict], cURL [dict], cairo [dict], cdbfasta [dict], cdt [dict], chewBBACA [dict], chopper [dict], compleasm [dict], cromwell [dict], ctags [dict], ctffind [dict], cuDNN [dict], cutadapt [dict], cuteSV [dict], cwltool [dict], cyvcf2 [dict], dadi [dict], dammit [dict], datasets [dict], deepTools [dict], devtools [dict], double-conversion [dict], drep [dict], dtcmp [dict], duphold [dict], duplex-tools [dict], eDNA [dict], ecCodes [dict], eccodes [dict], ectyper [dict], edlib [dict], eggnog-mapper [dict], emmtyper [dict], ensmallen [dict], entrez-direct [dict], exonerate [dict], expat [dict], fastStructure [dict], fastp [dict], fastq-tools [dict], fcGENE [dict], fgbio [dict], fineRADstructure [dict], fineSTRUCTURE [dict], flatbuffers [dict], flex [dict], fmlrc [dict], fmt [dict], fontconfig [dict], forge [dict], foss [dict], freetype [dict], funcx-endpoint [dict], fxtract [dict], g2clib [dict], g2lib [dict], ga4gh [dict], geany [dict], genometools [dict], gettext [dict], gfastats [dict], gffread [dict], giflib [dict], gimkl [dict], gimpi [dict], git [dict], globus-automate-client [dict], globus-compute-endpoint [dict], gmsh [dict], gnuplot [dict], gompi [dict], google-sparsehash [dict], googletest [dict], gperf [dict], grib_api [dict], grive2 [dict], gsort [dict], h5pp [dict], haplocheck [dict], help2man [dict], hifiasm [dict], hunspell [dict], hwloc [dict], hypothesis [dict], icc [dict], iccifort [dict], ifort [dict], iimpi [dict], imkl [dict], imkl-FFTW [dict], impi [dict], intel [dict], intel-compilers [dict], ipyrad [dict], ispc [dict], jbigkit [dict], jcvi [dict], jemalloc [dict], jq [dict], json-c [dict], jvarkit [dict], kalign2 [dict], kallisto [dict], kineto [dict], kma [dict], libFLAME [dict], libGLU [dict], libKML [dict], libStatGen [dict], libarchive [dict], libcircle [dict], libconfig [dict], libdeflate [dict], libdrm [dict], libdwarf [dict], libepoxy [dict], libevent [dict], libffi [dict], libgcrypt [dict], libgd [dict], libgeotiff [dict], libgit2 [dict], libglvnd [dict], libgpg-error [dict], libgpuarray [dict], libgtextutils [dict], libiconv [dict], libjpeg-turbo [dict], libmatheval [dict], libpciaccess [dict], libpmi [dict], libpng [dict], libreadline [dict], libspatialite [dict], libtool [dict], libunistring [dict], libunwind [dict], libvpx [dict], libxc [dict], libxml2 [dict], libxslt [dict], libxsmm [dict], libzstd [dict], lighttpd [dict], likwid [dict], lp_solve [dict], lwgrp [dict], lz4 [dict], magma [dict], makedepend [dict], manta [dict], mapDamage [dict], meRanTK [dict], medaka [dict], megalodon [dict], metaWRAP [dict], miRDeep2 [dict], midl [dict], mimalloc [dict], miniBUSCO [dict], miniasm [dict], minieigen [dict], minimap2 [dict], miniprot [dict], mjpegtools [dict], mlpack [dict], mo_tidl [dict], modbam2bed [dict], mosdepth [dict], mpi4py [dict], mpifileutils [dict], muParser [dict], nano [dict], nanoQC [dict], nanofilt [dict], nanoget [dict], nanomath [dict], nanopolish [dict], ncbi-vdb [dict], nccmp [dict], ncurses [dict], ncview [dict], ne [dict], nearline [dict], netCDF [dict], netCDF-C++ [dict], netCDF-C++4 [dict], netCDF-Fortran [dict], nettle [dict], nodejs [dict], nseg [dict], nsync [dict], nullarbor [dict], numactl [dict], ont-guppy-gpu [dict], padloc [dict], pairtools [dict], panaroo [dict], pandoc [dict], parallel-fastq-dump [dict], parasail [dict], patchelf [dict], pauvre [dict], pfunit [dict], pggb [dict], pgge [dict], phyx [dict], picard [dict], pigz [dict], pixman [dict], pod5 [dict], pplacer [dict], preseq [dict], prodigal [dict], prodigal-gv [dict], prokka [dict], proovread [dict], protobuf [dict], protobuf-python [dict], psmc [dict], pstoedit [dict], pullseq [dict], purge_dups [dict], purge_haplotigs [dict], pv [dict], pyani [dict], pycoQC [dict], pymol-open-source [dict], pyseer [dict], pyspoa [dict], qcat [dict], rDock [dict], randfold [dict], rasusa [dict], razers3 [dict], rclone [dict], re2c [dict], rkcommon [dict], rnaQUAST [dict], rsync [dict], rust-fmlrc [dict], samblaster [dict], samclip [dict], savvy [dict], sbt [dict], sc-RNA [dict], screen_assembly [dict], seqmagick [dict], seqtk [dict], shrinkwrap [dict], shumlib [dict], simuG [dict], sismonr [dict], slow5tools [dict], smafa [dict], smoove [dict], snakemake [dict], snaphu [dict], snappy [dict], snp-sites [dict], snpEff [dict], somalier [dict], spaln [dict], spdlog [dict], spglib [dict], splat [dict], spoa [dict], sratoolkit [dict], srun-wrapper [dict], sublime [dict], swarm [dict], swissknife [dict], tRNAscan-SE [dict], tabix [dict], tabixpp [dict], tbb [dict], tbl2asn [dict], tmux [dict], trf [dict], trimAl [dict], um2netcdf [dict], unimap [dict], unrar [dict], util-linux [dict], vcflib [dict], verkko [dict], vg [dict], wgsim [dict], wheel [dict], wtdbg [dict], wxWidgets [dict], x264 [dict], x265 [dict], xbitmaps [dict], xkbcommon [dict], yacrd [dict], yajl [dict], yak [dict], yaml-cpp [dict], zlib [dict], zstd [dict] macros SuperDict context [function], macros_info [function], now [function], fix_url [function] filters dict pretty [function] filters_builtin dict abs [builtin_function_or_method], attr [function], batch [function], capitalize [function], center [function], count [builtin_function_or_method], d [function], default [function], dictsort [function], e [builtin_function_or_method], escape [builtin_function_or_method], filesizeformat [function], first [function], float [function], forceescape [function], format [function], groupby [function], indent [function], int [function], join [function], last [function], length [builtin_function_or_method], list [function], lower [function], map [function], min [function], max [function], pprint [function], random [function], reject [function], rejectattr [function], replace [function], reverse [function], round [function], safe [function], select [function], selectattr [function], slice [function], sort [function], string [builtin_function_or_method], striptags [function], sum [function], title [function], trim [function], truncate [function], unique [function], upper [function], urlencode [function], urlize [function], wordcount [function], wordwrap [function], xmlattr [function], tojson [function] navigation Navigation files Files page Page Page(title='Info about macros', url='/nesi/support-docs-concept/macros/')"},{"location":"macros/#config-information","title":"Config Information","text":"<p>Standard MkDocs configuration information. Do not try to modify.</p> <p>e.g. <code>{{ config.docs_dir }}</code></p> <p>See also the MkDocs documentation on the config object.</p> Variable Type Content config_file_path str '/home/runner/work/support-docs-concept/support-docs-concept/mkdocs.yml' site_name str 'NeSI Support Center' nav list [{'Getting Started': 'Getting_Started/'}, {'General': 'General/'}, {'Scientific Computing': 'Scientific_Computing/'}, {'Storage': 'Storage/'}, {'NeSI Service Subscriptions': 'NeSI_Service_Subscriptions/'}] pages NoneType None exclude_docs NoneType None not_in_nav NoneType None site_url str 'https://github.com/nesi/support-docs-concept/' site_description str 'NeSI Support Documentation' site_author NoneType None theme Theme Theme(name='material', dirs=['/home/runner/work/support-docs-concept/support-docs-concept/overrides', '/home/runner/.local/lib/python3.10/site-packages/material/templates', '/home/runner/.local/lib/python3.10/site-packages/mkdocs/templates'], static_templates={'sitemap.xml', '404.html'}, name='material', locale=Locale('en'), language='en', direction=None, features=['navigation.indexes', 'navigation.top', 'navigation.prune', 'content.code.copy', 'content.action.edit', 'navigation.path'], font={'text': 'Roboto', 'code': 'Roboto Mono'}, icon=None, favicon='assets/icons/favicon.ico', logo='assets/icons/logo.svg', palette=[{'primary': 'custom', 'scheme': 'default', 'toggle': {'icon': 'material/lightbulb', 'name': 'Switch to dark mode'}}, {'primary': 'custom', 'scheme': 'slate', 'toggle': {'icon': 'material/lightbulb-outline', 'name': 'Switch to light mode'}}]) docs_dir str '/home/runner/work/support-docs-concept/support-docs-concept/docs' site_dir str '/home/runner/work/support-docs-concept/support-docs-concept/public' copyright NoneType None google_analytics NoneType None dev_addr _IpAddressValue _IpAddressValue(host='127.0.0.1', port=8000) use_directory_urls bool True repo_url str 'https://github.com/nesi/support-docs-concept' repo_name str 'GitHub' edit_uri_template NoneType None edit_uri str 'edit/main/docs/' extra_css list ['assets/stylesheets/neoteroi-mkdocs.css', 'assets/stylesheets/footer.css', 'assets/stylesheets/custom_admonations.css', 'assets/stylesheets/theme.css'] extra_javascript list ['assets/javascripts/general.js'] extra_templates list [] markdown_extensions list ['toc', 'tables', 'fenced_code', 'admonition', 'pymdownx.details', 'pymdownx.superfences', 'pymdownx.highlight', 'pymdownx.inlinehilite', 'pymdownx.tabbed', 'attr_list', 'abbr', 'neoteroi.cards', 'neoteroi.timeline', 'pymdownx.snippets'] mdx_configs dict toc [dict], pymdownx.tabbed [dict], pymdownx.snippets [dict] strict bool False remote_branch str 'gh-pages' remote_name str 'origin' extra LegacyConfig {} plugins PluginCollection material/search [SearchPlugin], redirects [RedirectPlugin], literate-nav [LiterateNavPlugin], mkdocs-nav-weight [MkDocsNavWeight], material/tags [TagsPlugin], git-revision-date-localized [GitRevisionDateLocalizedPlugin], macros [MacrosPlugin], mkdocs_hooks.py [module] hooks dict mkdocs_hooks.py [module] watch list [] validation Validation {'nav': {'omitted_files': 20, 'not_found': 30, 'absolute_links': 20}, 'links': {'not_found': 30, 'absolute_links': 20, 'unrecognized_links': 20}}"},{"location":"macros/#macros","title":"Macros","text":"<p>These macros have been defined programmatically for this environment (module or pluglets). </p> Variable Type Content context function (obj, e) <p>Default mkdocs_macro List the defined variables</p> macros_info function () <p>Test/debug function:         list useful documentation on the mkdocs_macro environment.</p> now function () <p>Get the current time (returns a datetime object).          Used alone, it provides a timestamp.         To get the year use <code>now().year</code>, for the month number          <code>now().month</code>, etc.</p> fix_url function (url, r) <p>If url is relative, fix it so that it points to the docs diretory.     This is necessary because relative links in markdown must be adapted     in html ('img/foo.png' =&gt; '../img/img.png').</p>"},{"location":"macros/#git-information","title":"Git Information","text":"<p>Information available on the last commit and the git repository containing the documentation project:</p> <p>e.g. <code>{{ git.message }}</code></p> Variable Type Content status bool True date datetime datetime.datetime(2023, 12, 30, 17, 16, 5, tzinfo=tzoffset(None, 46800)) short_commit str '738c327b' commit str '738c327b154bcd2c44eb1b7b43328f0fbd1bfed3' tag str '' short_tag str '' author str 'Cal' author_email str '35017184+CallumWalley@users.noreply.github.com' committer str 'GitHub' committer_email str 'noreply@github.com' date_ISO str 'Sat Dec 30 17:16:05 2023 +1300' message str 'Update checks.yml\\n\\nSigned-off-by: Cal &lt;35017184+CallumWalley@users.noreply.github.com&gt;' raw str 'commit 738c327b154bcd2c44eb1b7b43328f0fbd1bfed3\\nAuthor: Cal &lt;35017184+CallumWalley@users.noreply.github.com&gt;\\nDate:   Sat Dec 30 17:16:05 2023 +1300\\n\\n    Update checks.yml\\n    \\n    Signed-off-by: Cal &lt;35017184+CallumWalley@users.noreply.github.com&gt;' root_dir str '/home/runner/work/support-docs-concept/support-docs-concept'"},{"location":"macros/#page-attributes","title":"Page Attributes","text":"<p>Provided by MkDocs. These attributes change for every page (the attributes shown are for this page).</p> <p>e.g. <code>{{ page.title }}</code></p> <p>See also the MkDocs documentation on the page object.</p> Variable Type Content file File page [Page], src_uri = 'macros.md', name = 'macros', dest_uri = 'macros/index.html', url = 'macros/', abs_src_path = '/home/runner/work/support-docs-concept/support-docs-concept/docs/macros.md', abs_dest_path = '/home/runner/work/support-docs-concept/support-docs-concept/public/macros/index.html', inclusion [InclusionLevel] children NoneType None previous_page NoneType None next_page NoneType None _Page__active bool False update_date str '2023-12-30' canonical_url str 'https://github.com/nesi/support-docs-concept/macros/' abs_url str '/nesi/support-docs-concept/macros/' edit_url str 'https://github.com/nesi/support-docs-concept/edit/main/docs/macros.md' markdown str '# Info about macros\\n\\n{{ macros_info() }}\\n' _title_from_render NoneType None content NoneType None toc list [] meta dict git_revision_date_localized = 'November 16, 2023', git_revision_date_localized_raw_date = 'November 16, 2023', git_revision_date_localized_raw_datetime = 'November 16, 2023 02:16:32', git_revision_date_localized_raw_iso_date = '2023-11-16', git_revision_date_localized_raw_iso_datetime = '2023-11-16 02:16:32', git_revision_date_localized_raw_timeago = '', git_revision_date_localized_raw_custom = '16. November 2023', git_site_revision_date_localized = 'December 29, 2023', git_site_revision_date_localized_raw_date = 'December 29, 2023', git_site_revision_date_localized_raw_datetime = 'December 29, 2023 22:23:01', git_site_revision_date_localized_raw_iso_date = '2023-12-29', git_site_revision_date_localized_raw_iso_datetime = '2023-12-29 22:23:01', git_site_revision_date_localized_raw_timeago = '', git_site_revision_date_localized_raw_custom = '29. December 2023', git_creation_date_localized = 'November 16, 2023', git_creation_date_localized_raw_date = 'November 16, 2023', git_creation_date_localized_raw_datetime = 'November 16, 2023 02:16:32', git_creation_date_localized_raw_iso_date = '2023-11-16', git_creation_date_localized_raw_iso_datetime = '2023-11-16 02:16:32', git_creation_date_localized_raw_timeago = '', git_creation_date_localized_raw_custom = '16. November 2023' <p>To have all titles of all pages, use:</p> <pre><code>{% for page in navigation.pages %}\n- {{ page.title }}\n{% endfor %}\n</code></pre>"},{"location":"macros/#plugin-filters","title":"Plugin Filters","text":"<p>These filters are provided as a standard by the macros plugin.</p> Variable Type Content pretty function (var_list, rows, header, e) <p>Default mkdocs_macro Prettify a dictionary or object          (used for environment documentation, or debugging).</p>"},{"location":"macros/#builtin-jinja2-filters","title":"Builtin Jinja2 Filters","text":"<p>These filters are provided by Jinja2 as a standard.</p> <p>See also the Jinja2 documentation on builtin filters).</p> Variable Type Content abs builtin_function_or_method <p>Return the absolute value of the argument.</p> attr function (environment, obj, name, value) <p>Get an attribute of an object.  <code>foo|attr(\"bar\")</code> works like     <code>foo.bar</code> just that always an attribute is returned and items are not     looked up.</p> batch function (value, linecount, fill_with, tmp, item) <p>A filter that batches items. It works pretty much like <code>slice</code>     just the other way round. It returns a list of lists with the     given number of items. If you provide a second parameter this     is used to fill up missing items. See this example.</p> capitalize function (s) <p>Capitalize a value. The first character will be uppercase, all others     lowercase.</p> center function (value, width) <p>Centers the value in a field of a given width.</p> count builtin_function_or_method <p>Return the number of items in a container.</p> d function (value, default_value, boolean) <p>If the value is undefined it will return the passed default value,     otherwise the value of the variable.</p> default function (value, default_value, boolean) <p>If the value is undefined it will return the passed default value,     otherwise the value of the variable.</p> dictsort function (value, case_sensitive, by, reverse, sort_func) <p>Sort a dict and yield (key, value) pairs. Python dicts may not     be in the order you want to display them in, so sort them first.</p> e builtin_function_or_method <p>Replace the characters <code>&amp;</code>, <code>&lt;</code>, <code>&gt;</code>, <code>'</code>, and <code>\"</code> in the string with HTML-safe sequences. Use this if you need to display text that might contain such characters in HTML.</p> escape builtin_function_or_method <p>Replace the characters <code>&amp;</code>, <code>&lt;</code>, <code>&gt;</code>, <code>'</code>, and <code>\"</code> in the string with HTML-safe sequences. Use this if you need to display text that might contain such characters in HTML.</p> filesizeformat function (value, binary, bytes, base, prefixes, i, prefix, unit) <p>Format the value like a 'human-readable' file size (i.e. 13 kB,     4.1 MB, 102 Bytes, etc).  Per default decimal prefixes are used (Mega,     Giga, etc.), if the second parameter is set to <code>True</code> the binary     prefixes are used (Mebi, Gibi).</p> first function (args, kwargs, b) <p>Return the first item of a sequence.</p> float function (value, default) <p>Convert the value into a floating point number. If the     conversion doesn't work it will return <code>0.0</code>. You can     override this default using the first parameter.</p> forceescape function (value) <p>Enforce HTML escaping.  This will probably double escape variables.</p> format function (value, args, kwargs) <p>Apply the given values to a <code>printf-style</code>_ format string, like     <code>string % values</code>.</p> groupby function (args, kwargs, b) <p>Group a sequence of objects by an attribute using Python's     :func:<code>itertools.groupby</code>. The attribute can use dot notation for     nested access, like <code>\"address.city\"</code>. Unlike Python's <code>groupby</code>,     the values are sorted first so only one group is returned for each     unique value.</p> indent function (s, width, first, blank, newline, rv, lines) <p>Return a copy of the string with each line indented by 4 spaces. The     first line and blank lines are not indented by default.</p> int function (value, default, base) <p>Convert the value into an integer. If the     conversion doesn't work it will return <code>0</code>. You can     override this default using the first parameter. You     can also override the default base (10) in the second     parameter, which handles input with prefixes such as     0b, 0o and 0x for bases 2, 8 and 16 respectively.     The base is ignored for decimal numbers and non-string values.</p> join function (args, kwargs, b) <p>Return a string which is the concatenation of the strings in the     sequence. The separator between elements is an empty string per     default, you can define it with the optional parameter.</p> last function (environment, seq) <p>Return the last item of a sequence.</p> length builtin_function_or_method <p>Return the number of items in a container.</p> list function (args, kwargs, b) <p>Convert the value into a list.  If it was a string the returned list     will be a list of characters.</p> lower function (s) <p>Convert a value to lowercase.</p> map function (args, kwargs, b) <p>Applies a filter on a sequence of objects or looks up an attribute.     This is useful when dealing with lists of objects but you are really     only interested in a certain value of it.</p> min function (environment, value, case_sensitive, attribute) <p>Return the smallest item from the sequence.</p> max function (environment, value, case_sensitive, attribute) <p>Return the largest item from the sequence.</p> pprint function (value) <p>Pretty print a variable. Useful for debugging.</p> random function (context, seq) <p>Return a random item from the sequence.</p> reject function (args, kwargs, b) <p>Filters a sequence of objects by applying a test to each object,     and rejecting the objects with the test succeeding.</p> rejectattr function (args, kwargs, b) <p>Filters a sequence of objects by applying a test to the specified     attribute of each object, and rejecting the objects with the test     succeeding.</p> replace function (eval_ctx, s, old, new, count) <p>Return a copy of the value with all occurrences of a substring     replaced with a new one. The first argument is the substring     that should be replaced, the second is the replacement string.     If the optional third argument <code>count</code> is given, only the first     <code>count</code> occurrences are replaced.</p> reverse function (value, rv, e) <p>Reverse the object or return an iterator that iterates over it the other     way round.</p> round function (value, precision, method, func) <p>Round the number to a given precision. The first     parameter specifies the precision (default is <code>0</code>), the     second the rounding method.</p> safe function (value) <p>Mark the value as safe which means that in an environment with automatic     escaping enabled this variable will not be escaped.</p> select function (args, kwargs, b) <p>Filters a sequence of objects by applying a test to each object,     and only selecting the objects with the test succeeding.</p> selectattr function (args, kwargs, b) <p>Filters a sequence of objects by applying a test to the specified     attribute of each object, and only selecting the objects with the     test succeeding.</p> slice function (args, kwargs, b) <p>Slice an iterator and return a list of lists containing     those items. Useful if you want to create a div containing     three ul tags that represent columns.</p> sort function (environment, value, reverse, case_sensitive, attribute, key_func) <p>Sort an iterable using Python's :func:<code>sorted</code>.</p> string builtin_function_or_method <p>Convert an object to a string if it isn't already. This preserves a :class:<code>Markup</code> string rather than converting it back to a basic string, so it will still be marked as safe and won't be escaped again.</p> striptags function (value) <p>Strip SGML/XML tags and replace adjacent whitespace by one space.</p> sum function (args, kwargs, b) <p>Returns the sum of a sequence of numbers plus the value of parameter     'start' (which defaults to 0).  When the sequence is empty it returns     start.</p> title function (s) <p>Return a titlecased version of the value. I.e. words will start with     uppercase letters, all remaining characters are lowercase.</p> trim function (value, chars) <p>Strip leading and trailing characters, by default whitespace.</p> truncate function (env, s, length, killwords, end, leeway, result) <p>Return a truncated copy of the string. The length is specified     with the first parameter which defaults to <code>255</code>. If the second     parameter is <code>true</code> the filter will cut the text at length. Otherwise     it will discard the last word. If the text was in fact     truncated it will append an ellipsis sign (<code>\"...\"</code>). If you want a     different ellipsis sign than <code>\"...\"</code> you can specify it using the     third parameter. Strings that only exceed the length by the tolerance     margin given in the fourth parameter will not be truncated.</p> unique function (environment, value, case_sensitive, attribute, getter, seen, item, key) <p>Returns a list of unique items from the given iterable.</p> upper function (s) <p>Convert a value to uppercase.</p> urlencode function (value, items) <p>Quote data for use in a URL path or query using UTF-8.</p> urlize function (eval_ctx, value, trim_url_limit, nofollow, target, rel, extra_schemes, policies, rel_parts, scheme, rv) <p>Convert URLs in text into clickable links.</p> wordcount function (s) <p>Count the words in that string.</p> wordwrap function (environment, s, width, break_long_words, wrapstring, break_on_hyphens) <p>Wrap a string to the given width. Existing newlines are treated     as paragraphs to be wrapped separately.</p> xmlattr function (eval_ctx, d, autospace, rv) <p>Create an SGML/XML attribute string based on the items in a dict.     All values that are neither <code>none</code> nor <code>undefined</code> are automatically     escaped.</p> tojson function (eval_ctx, value, indent, policies, dumps, kwargs) <p>Serialize an object to a string of JSON, and mark it safe to     render in HTML. This filter is only for use in HTML documents.</p>"},{"location":"specification/","title":"Specification","text":""},{"location":"specification/#structure","title":"Structure","text":"<p>Public facing articles are found in the <code>docs</code> folder. Any markdown files inside will be rendered, any directory will be subcategories. Pages can be excluded from being shown in the nav by adding them to <code>mkdocs.yml: not_in_nav</code>, as in the case of <code>includes</code>.</p> <p>By default, all categories are a group only (e.g. they have nothing rendered, only children), However, if the folder contains an <code>index.md</code> file, it will be rendered instead.</p>"},{"location":"specification/#article-namelocation","title":"Article Name/Location","text":"<p>An articles location is determined by its location in the <code>docs</code> directory. Article file can be nested up to two folders deep, and use the title name, in snake_case.</p>"},{"location":"specification/#article-order","title":"Article Order","text":"<p>By default articles will be ordered alphabetically. Article order can be m set using the <code>weight</code> property in the articles front matter, or by using the <code>nav</code> property of <code>mkdocs.yml</code>. Lower weight values will be first in the nav.</p> <p>As <code>weight</code> was inherited from Zendesk, all articles have a weight value post migration. If ordering is not required remove the <code>weight</code> property from the front matter to allow default alphabetical ordering.</p>"},{"location":"specification/#title","title":"Title","text":"<p>Headers should have a blank line before and after.</p>"},{"location":"specification/#succession","title":"Succession","text":"<p>Article title has the follong succession,</p> <ul> <li>A title defined in the 'title' meta-data.</li> <li>A level 1 Markdown header on the first line of the document body.</li> <li>The filename of a document.</li> </ul> <p>Our preference is the opposite (filename &gt; H1 &gt; meta).</p>"},{"location":"specification/#filename-rendering","title":"Filename Rendering","text":"<p>When converting a filename for the nav/title the <code>.md</code> will be dropped and 'snake_case' and 'kebab-case' will both be rendered with spaces, e.g. 'Snake case' and 'Kebab case' respectively.</p> <p>The first letter of the filename will be capitalised, but not any subsiquent words.</p> <p>(Note: Choose only one for naming convention)</p>"},{"location":"specification/#sub-headers","title":"Sub Headers","text":"<p>Unless setting a title, the first 'real' header will be an H2. It's fine to have text before the first header if it is relevent to the entire page.</p> <p>No skipping levels, e.g.</p> <pre><code>H2\nH3\nH4\n</code></pre> <p>never</p> <pre><code>H2\nH4\n</code></pre> <p>Try to avoid only-child headers (e.g shares a parent with at least one other header)</p>"},{"location":"specification/#meta","title":"Meta","text":"<p>Article metadata (or 'front-matter') is yaml format at the top of the page between two <code>---</code>. Content is not rendered.</p>"},{"location":"specification/#mkdocs-parameters","title":"Mkdocs Parameters","text":"<ul> <li><code>template</code> : which template to use.</li> <li><code>title</code>    : title.</li> <li><code>weight</code>   : Used to set page order. Lower comes first. Migrated from Zendesk <code>position</code>.                See page order</li> </ul>"},{"location":"specification/#material-parameters","title":"Material Parameters","text":"<ul> <li><code>description</code> : used for site meta.           : <code>string</code></li> <li><code>icon</code>        : page icon.                    : <code>path</code></li> <li><code>status</code>      : Will dsiplay a symbol on nav  : <code>new</code>, <code>deprecated</code>.</li> <li><code>hide</code>        : Used to turn off features.    : <code>tags</code></li> </ul>"},{"location":"specification/#custom-parameters","title":"Custom Parameters","text":"<ul> <li><code>prereq</code>      : List of prerequisites. Formatted in markdown. Will be rendered inside a admonation.</li> <li><code>postreq</code>     : List of what next. Formatted in markdown. Will be rendered inside a admonation.</li> <li><code>suggested</code>   : Page similar pages to link to. (Not implimented).</li> </ul>"},{"location":"specification/#zendesk-imported","title":"Zendesk Imported","text":"<p>Not used for anything currently. Info imported from Zendesk Page.</p> <ul> <li><code>created_at</code>:</li> <li><code>hidden</code>:</li> <li><code>label_names</code>: []</li> <li><code>vote_count</code>:</li> <li><code>vote_sum</code>:</li> <li><code>zendesk_article_id</code>:</li> <li><code>zendesk_section_id</code>:</li> </ul>"},{"location":"specification/#templates","title":"Templates","text":"<p>Template can be set in article meta.</p> <ul> <li><code>main.html</code>                : Used for regular pages (default).</li> <li><code>application.html</code>         : Used for 'application' pages, will include software details header (and be linked in supported apps page).</li> <li><code>supportedApplication.html</code>: For supported applications page.</li> <li><code>home.html</code>                : Homepage.</li> </ul> <p>By default, the <code>main</code> theme will be used. template of a theme to render Markdown pages. You can use the template meta-data key to define a different template file for that specific page. The template file must be available on the path(s) defined in the theme's environment.</p>"},{"location":"specification/#macros","title":"Macros","text":"<p>Macros allow use of Jinja filter syntax inside the mardown files allowing for much more flexable templating. Details here</p> <p><code>module load ANSYS/2023R2</code></p> <pre><code>`module load ANSYS/{{ applications.ANSYS.machines.mahuika.versions | last }}`\n</code></pre> Fancy Example <p>Our Python modules come prebuilt with the following packages: </p> <p> Package <p>altair-4.1.0</p> <p>ase-3.19.0</p> <p>biom-format-2.1.7</p> <p>biopython-1.76</p> <p>Bottleneck-1.3.1</p> <p>CacheControl-0.12.6</p> <p>click-7.0</p> <p>click-plugins-1.1.1</p> <p>Cython-0.29.15</p> <p>dask-2.10.1</p> <p>dask-jobqueue-0.7.1</p> <p>datashader-0.10.0</p> <p>deap-1.3.0</p> <p>decorator-4.4.1</p> <p>DendroPy-4.4.0</p> <p>docopt-0.6.2</p> <p>emcee-3.0.2</p> <p>future-0.18.2</p> <p>h5py-2.10.0</p> <p>holoviews-1.12.7</p> <p>HTSeq-0.11.2</p> <p>ipython-7.12.0</p> <p>ipywidgets-7.5.1</p> <p>jdcal-1.4.1</p> <p>Jinja2-2.11.1</p> <p>joblib-0.14.1</p> <p>liac-arff-2.4.0</p> <p>line_profiler-3.0.2</p> <p>lockfile-0.12.2</p> <p>lxml-4.5.0</p> <p>Mako-1.1.0</p> <p>matplotlib-3.1.3</p> <p>mpi4py-3.0.3</p> <p>mpmath-1.1.0</p> <p>natsort-7.0.1</p> <p>netCDF4-1.5.3</p> <p>networkx-2.4</p> <p>nose-1.3.7</p> <p>notebook-6.0.3</p> <p>numba-0.48.0</p> <p>numexpr-2.7.1</p> <p>numpy-1.18.1</p> <p>obspy-1.2.0</p> <p>openpyxl-3.0.3</p> <p>pandas-1.0.1</p> <p>param-1.9.2</p> <p>patsy-0.5.1</p> <p>paycheck-1.0.2</p> <p>Pillow-7.0.0</p> <p>pip-20.0.2</p> <p>plotly-4.10.0</p> <p>PrettyTable-0.7.2</p> <p>psycopg2-2.8.4</p> <p>pudb-2019.2</p> <p>Py6S-1.7.2</p> <p>pybedtools-0.8.1</p> <p>pybind11-2.4.3</p> <p>pyct-0.4.6</p> <p>pyparsing-2.4.6</p> <p>pysam-0.15.4</p> <p>Pysolar-0.8</p> <p>pystan-2.19.1.1</p> <p>pytest-5.3.5</p> <p>python-dateutil-2.8.1</p> <p>pytz-2019.3</p> <p>PyYAML-5.3</p> <p>qutip-4.5.0</p> <p>reportlab-3.5.32</p> <p>requests-2.22.0</p> <p>scikit-build-0.10.0</p> <p>scikit-image-0.16.2</p> <p>scikit-umfpack-0.3.2</p> <p>scipy-1.4.1</p> <p>seaborn-0.11.0</p> <p>setuptools-45.2.0</p> <p>six-1.14.0</p> <p>SQLAlchemy-1.3.13</p> <p>statsmodels-0.11.0</p> <p>sympy-1.5.1</p> <p>tabulate-0.8.6</p> <p>toolz-0.10.0</p> <p>tzlocal-2.0.0</p> <p>ujson-1.35</p> <p>xlrd-1.2.0</p> <pre><code>Our Python modules come prebuilt with the following packages: \n{% set pyexts=applications.Python.extensions.split(', ') %}\n&lt;table&gt;\n&lt;tr&gt;&lt;th&gt;Package&lt;/th&gt;&lt;/tr&gt;\n{% for pyext in pyexts %}\n&lt;tr&gt;&lt;td&gt;{{ pyext }}&lt;/td&gt;&lt;/tr&gt;\n{% endfor %}\n\n&lt;/table&gt;\n</code></pre>"},{"location":"specification/#includes","title":"Includes","text":"<p>The macro plugin also allows the use of 'includes',</p> <pre><code>{% include 'snippet.md' %}\n</code></pre> <p>There are a few includes you may want to use.</p> Path content usage <code>{% include \"partials/support_request.html\" %}</code> <code>&lt;a href=\"mailto:support@nesi.org.nz\"&gt;Contact our Support Team&lt;/a&gt;</code> Anywhere the user is told to contact support. <code>{% include \"partials/appHeader.html\" %}</code> Info block At the top of documents about particular software (TODO: elaborate) <code>{% include \"partials/app/app_network_licence.html\" %}</code> List of network licences When dynamic licence info is required (used in <code>appHeader.html</code>) <code>{% include \"partials/app/app_version.html\" %}</code> List of versions and a 'module load' codeblock. When dynamic version info is required"},{"location":"specification/#style-guide","title":"Style Guide","text":""},{"location":"specification/#code-blocks","title":"Code blocks","text":"<p>Don't include prompt or </p>"},{"location":"specification/#links","title":"Links","text":"<p>Try to avoid putting links on ambiguous words, e.g.</p> BadBetter <p>View the software homepage here.</p> <pre><code>View the homepage [here](https://www.example.com).\n</code></pre> <p>View the software homepage.</p> <pre><code>View the [software homepage](https://www.example.com).\n</code></pre>"},{"location":"specification/#accessability-standards","title":"Accessability standards","text":"<ul> <li>nz spec</li> <li>WCAG spec</li> </ul>"},{"location":"specification/#mkdocsyml","title":"mkdocs.yml","text":"<p>When setting a parameter in the YAML, you can use the following syntax.</p> <pre><code>some-param : !ENV [TEST_ENV, false]\n</code></pre> <p>This will use the value of the env variable <code>TEST_ENV</code>, or if unset, false.</p>"},{"location":"General/Announcements/Accessing_NeSI_Support_during_the_holiday_break/","title":"Accessing NeSI Support during the holiday break","text":"<p>As another busy year draws to a close, NeSI\u2019s Support Team will be taking a summer break from 5:00 PM on 21 December 2023 to 9:00 am on 03 January 2024</p> <p>During this time, NeSI platform services will be online and available, but non-critical support requests will be responded to when the team is back on 03 January.</p> <p>Urgent / critical requests will be addressed on a best effort basis. Any changes to system status will be reported via our\u00a0System Status page\u00a0and alerts.</p> <p>A quick reminder of our main support channels as well as other sources of self-service support:</p> <ul> <li> <p>Contact our Support Team     Note: non-emergency requests will be addressed on or after 03 January 2024.</p> </li> <li> <p>Sign up for NeSI system status     updates\u00a0for     advance warning of any system updates or unplanned outages.</p> </li> <li> <p>Consult our User     Documentation\u00a0pages     for instructions and guidelines for using the systems</p> </li> <li> <p>Visit NeSI\u2019s YouTube     channel\u00a0for     introductory training webinars</p> </li> </ul> <p>On behalf of the entire NeSI team, we wish you a safe and relaxing holiday.</p>","tags":["announcement"]},{"location":"General/Announcements/Improved_data_management_and_efficient_use_of_NeSI_HPC_storage/","title":"Improved data management & efficient use of NeSI HPC storage","text":"<p>A growing number of research projects are storing large amounts of data on NeSI systems. To better support this growth, as well as optimise the performance and availability of our filesystems, we are introducing new data management policies and best practices for our HPC facilities.</p> <p>By adopting these measures to regularly audit, clean and manage the amount of data on our filesystems, we\u2019ll ensure they remain high-performing and responsive to your research computing workloads and data science workflows.  </p>","tags":[]},{"location":"General/Announcements/Improved_data_management_and_efficient_use_of_NeSI_HPC_storage/#upcoming-changes-to-data-management-processes-for-project-directories","title":"Upcoming changes to data management processes for project directories","text":"","tags":[]},{"location":"General/Announcements/Improved_data_management_and_efficient_use_of_NeSI_HPC_storage/#4-15-october-2021","title":"4-15 October 2021","text":"<p>The NeSI project filesystem is becoming critically full, however it is currently storing a large amount of dormant data that has not been accessed for more than 12 months. We need your help to free up space on the project filesystem as soon as possible. Please review the data you are currently storing in any \u00a0<code>/nesi/project/</code> directories and delete or relocate any files that are no longer required for ongoing computational and/or analytics work on NeSI.</p> <p>We have started regular audits of data stored in project folders, using the same format as our nobackup auto cleaning (described here). See the file <code>/nesi/project/&lt;project_code&gt;/.policy.test/scan485/latest.summary.txt</code> for a summary of the number and size of files within each project that have not been accessed for more than 485 days (this is ~15 months, and is the draft auto cleaning timeframe under consideration for the project filesystem).</p> <p>If you need assistance with this, Contact our Support Team and we\u2019d be happy to help or answer questions.</p> <p>If you have data that may be used again on NeSI later, [let us know Contact our Support Team and we will consider whether a Nearline storage allocation would be appropriate to manage this.</p>","tags":[]},{"location":"General/Announcements/Improved_data_management_and_efficient_use_of_NeSI_HPC_storage/#18-october-2021","title":"18 October 2021","text":"<p>We will begin a limited roll-out of a new feature to automatically identify inactive files in \u00a0<code>/nesi/project/</code> directories and schedule them for deletion. Generally, we will be looking to identify files that are inactive / untouched for more than 12 months.</p> <p>A selection of active projects will be invited to participate in this first phase of the programme. If you would like to volunteer to be an early tester / participant, please Contact our Support Team. Otherwise, we will be in touch with projects directly to onboard them.</p> <p>Insights from this initial phase will inform the criteria and processes of the programme prior to it being released to the broader user community.</p> <p>Alongside this work, we will also adopt a new policy on how long inactive data may be stored on NeSI systems, particularly once a research project itself becomes inactive.</p>","tags":[]},{"location":"General/Announcements/Improved_data_management_and_efficient_use_of_NeSI_HPC_storage/#january-2022","title":"January 2022","text":"<p>Starting in January 2022, we will expand the\u00a0<code>/nesi/project/</code> directory data management programme to include all active projects on NeSI. Additional Support documentation and user information sessions will be hosted prior to wider implementation, to provide advance notice of the change and to answer any questions you may have around data lifecycle management.</p>","tags":[]},{"location":"General/Announcements/Improved_data_management_and_efficient_use_of_NeSI_HPC_storage/#frequently-asked-questions","title":"Frequently asked questions","text":"","tags":[]},{"location":"General/Announcements/Improved_data_management_and_efficient_use_of_NeSI_HPC_storage/#why-are-you-introducing-these-new-data-management-processes","title":"Why are you introducing these new data management processes?","text":"<p>**We want to avoid our online filesystems reaching critically full levels, as that impacts their performance and availability for users. We also want to ensure our active storage filesystems aren't being used to store inactive data. This new data management feature for\u00a0<code>/nesi/project/</code> directories will complement our existing programme of automatic cleaning of the /nobackup file system.</p>","tags":[]},{"location":"General/Announcements/Improved_data_management_and_efficient_use_of_NeSI_HPC_storage/#can-i-check-how-much-storage-im-currently-using-on-nesi-systems","title":"Can I check how much storage I\u2019m currently using on NeSI systems?**","text":"<p>You can query your actual usage and disk allocations at any time using the following command:</p> <p><code>$ nn_storage_quota</code></p> <p>The values for 'nn_storage_quota' are updated approximately every hour and cached between updates.</p>","tags":[]},{"location":"General/Announcements/Improved_data_management_and_efficient_use_of_NeSI_HPC_storage/#can-i-recover-data-that-i-accidentally-delete-from-my-project-directory","title":"Can I recover data that I accidentally delete from my /project directory?","text":"<p>Perhaps. We regularly make read-only copies of the file system and save them for up to seven days. For more information, refer to our File Recovery page.</p>","tags":[]},{"location":"General/Announcements/Improved_data_management_and_efficient_use_of_NeSI_HPC_storage/#where-should-i-store-my-data-on-nesi-systems","title":"Where should I store my data on NeSI systems?**","text":"<p>In general, the project directory should be used for reference data, tools, and job submission and management scripts. The nobackup directory should be used for holding large reference working datasets (e.g., an extraction of compressed input data) and as a destination for writing and modifying temporary data. The nobackup directory can also be used to build and edit code, provided that the code is under version control and changes are regularly checked into upstream revision control systems. The long-term storage service should be used for larger datasets that you only access occasionally and do not need to change in situ.</p>","tags":[]},{"location":"General/Announcements/Improved_data_management_and_efficient_use_of_NeSI_HPC_storage/#what-should-i-do-if-i-run-out-of-storage-space","title":"What should I do if I run out of storage space?**","text":"<p>There are two tracked resources in the NeSI filesystem,\u00a0disk space\u00a0and\u00a0inodes (number of files). If you run into problems with either of these, refer to this Support page for more information.</p>","tags":[]},{"location":"General/Announcements/Improved_data_management_and_efficient_use_of_NeSI_HPC_storage/#i-have-questions-that-arent-covered-here-who-can-i-talk-to","title":"I have questions that aren\u2019t covered here. Who can I talk to?**","text":"<p>Contact our Support Team. No question is too big or small and our intention is always to work with you to find the best way to manage your research data.</p>","tags":[]},{"location":"General/Announcements/Improved_data_management_and_efficient_use_of_NeSI_HPC_storage/#more-information","title":"More information","text":"<p>This page will continue to be updated in the coming months with more information. If you have questions at any time, don\u2019t hesitate to contact Support Contact our Support Team.</p>","tags":[]},{"location":"General/Announcements/Improvements_to_Fair_Share_job_prioritisation_on_Maui/","title":"Improvements to Fair Share job prioritisation on M\u0101ui","text":"<p>On Thursday 3 September 2020, NeSI updated the way we prioritise jobs on the M\u0101ui HPC platform.</p>","tags":[]},{"location":"General/Announcements/Improvements_to_Fair_Share_job_prioritisation_on_Maui/#background","title":"Background","text":"<p>Since the start of the year, we have been using Slurm's Fair Tree algorithm on M\u0101ui (not yet on Mahuika) to prioritise jobs. This provides a hierarchical structure to Slurm's account management, with the hierarchy representing shares of a total cluster under Slurm's control. This enables control of higher level or aggregate account considerations, such as ensuring a group of projects within a research programme or institution are ensured access to their share of a cluster.</p> <p>Under our Fair Tree implementation, each of NeSI's four collaborating institutions is assigned a percentage share of M\u0101ui, alongside a percentage share for MBIE's Merit allocations (including Postgraduate and Proposal Development allocations), and the remainder as a share to allocations coming from subscriptions.</p> <p>These six shares, or\u00a0what we in NeSI call national pools,\u00a0are then ranked in order, starting with the pool that has been using at the lowest rate compared to its allocated percentage share. See this page (off site) for more details about Slurm's Fair Tree algorithm.</p> <p>Previously, we had given each pool a hard-coded share of M\u0101ui use. These hard-coded shares did not reflect ongoing rounds of allocations given to projects, and so some researchers were suffering from deprioritised jobs. These jobs ended up delayed in the queue, sometimes excessively.</p>","tags":[]},{"location":"General/Announcements/Improvements_to_Fair_Share_job_prioritisation_on_Maui/#what-has-changed","title":"What has changed?","text":"<p>We have now recalculated the shares for each pool to take into account the following:</p> <ul> <li>The investments into HPC platforms by the various collaborating     institutions and by MBIE;</li> <li>The capacity of each HPC platform;</li> <li>The split of requested time (allocations) by project teams between     the M\u0101ui and Mahuika HPC platforms, both overall and within each     institution's pool.</li> </ul> <p>Under this scheme, any job's priority is affected by the behaviour of other workload within the same project team, but also other project teams drawing on the same pool. In particular, even if your project team has been under-using compared to your allocation, your jobs may still be held up if:</p> <ul> <li>Other project teams at your institution (within your pool) have been     over-using compared to their allocations, or</li> <li>Your institution has approved project allocations totalling more     time than it is entitled to within its pool's share.</li> </ul>","tags":[]},{"location":"General/Announcements/Improvements_to_Fair_Share_job_prioritisation_on_Maui/#what-will-i-notice","title":"What will I notice?","text":"<p>If your institution or pool's ranking has not changed, nothing much will immediately change for you.</p> <p>However, if your institution or pool's assigned share of the machine has increased, it will become easier to move up the priority rankings, at least in the short term.</p> <p>Conversely, if your institution or pool's assigned share of the machine has decreased, it will become easier to move down the rankings. This change is one you are more likely to notice over time.</p> <p>Whenever your institution or pool's ranking changes, whether because of usage or because we adjust the assigned shares based on ongoing rounds of allocations, your job priorities will alter almost immediately. Moving up the rankings will increase your job priorities. Moving down the rankings will decrease your job priorities.</p>","tags":[]},{"location":"General/Announcements/Improvements_to_Fair_Share_job_prioritisation_on_Maui/#what-other-changes-are-nesi-planning-on-making","title":"What other changes are NeSI planning on making?","text":"<p>We are looking at introducing Fair Tree on Mahuika as well, though not on M\u0101ui ancillary nodes. We will announce this change well ahead of any planned introduction.</p> <p>We will also adjust the assigned Fair Tree shares on\u00a0M\u0101ui routinely so we don't diverge from allocations across HPC platforms again.</p>","tags":[]},{"location":"General/Announcements/Mahuika%27s_new_Milan_CPU_nodes_open_to_all_NeSI_users/","title":"Mahuika\u2019s new Milan CPU nodes open to all NeSI users","text":"<p>Following a successful early access programme, Mahuika\u2019s newest CPU nodes are now available for use by any projects that have a Mahuika allocation on NeSI's HPC Platform.</p> <p>The production launch of these new nodes is an exciting milestone in NeSI\u2019s strategy to lower the carbon footprint and continually improve the performance and fit-for-purpose design of our platforms to meet your research needs.</p> <p>What\u2019s new</p> <ul> <li> <p>faster, more powerful computing, enabled by AMD 3rd Gen EPYC Milan     architecture</p> </li> <li> <p>specialised high-memory capabilities, allowing rapid simultaneous     processing</p> </li> <li> <p>improved energy efficiency - these nodes are 2.5 times more power     efficient than Mahuika\u2019s original Broadwell nodes</p> </li> </ul> <p>How to access</p> <ul> <li>Visit our Support portal for instructions to get     started     and details of how the Milan nodes differ from Mahuika\u2019s original     Broadwell nodes</li> </ul> <p>Learn more</p> <ul> <li> <p>Watch this webinar sharing a quick     overview of the new resources and some tips for making the most of     the nodes.</p> </li> <li> <p>Bring questions to our weekly Online Office     Hours</p> </li> <li> <p>Contact our Support Team     any time</p> </li> </ul> <p>If you have feedback on the new nodes or suggestions for improving your experience getting started with or using any of our systems, please [get in touch Contact our Support Team.</p>","tags":[]},{"location":"General/Announcements/Mahuika-Core_Dumps_generation_now_disabled_as_default/","title":"Mahuika: Core Dumps generation now disabled as default","text":"<p>A Slurm configuration change has been made on Mahuika so that the\u00a0 maximum size of\u00a0core file\u00a0that can be generated inside a job now defaults to\u00a0<code>0</code>\u00a0bytes rather than\u00a0<code>unlimited</code>.\u00a0</p> <p>You can reenable core dumps with\u00a0<code>ulimit -c unlimited</code>\u00a0.</p>","tags":["mahuika",".core","corefile","coredump"]},{"location":"General/Announcements/Maui_upgrade_is_complete/","title":"M\u0101ui upgrade is complete","text":"<p>The recent upgrade of the M\u0101ui is now complete. The operating system, libraries, and software stack have been upgraded and rebuilt, improving performance and stability and enabling new capabilities.</p> <p>If you encounter any issues, have any questions about the upgrade, need help with getting your software working on the upgraded system, or have a suggestion for our documentation, please Contact our Support Team. We are committed to providing you with the best computing resources possible and will do our best to assist you.</p>","tags":[]},{"location":"General/Announcements/Maui_upgrade_is_complete/#why","title":"Why","text":"<p>This upgrade brings M\u0101ui's operating environment up to the latest supported release available for Cray's XC50 supercomputing platforms, with performance, reliability, and security benefits. This includes more up-to-date tooling and libraries with associated features and performance benefits. This work also enables further upgrades to NeSI's shared HPC storage system.</p>","tags":[]},{"location":"General/Announcements/Maui_upgrade_is_complete/#impact","title":"Impact","text":"<p>Please be aware that this is a major upgrade to M\u0101ui\u2019s operating environment which may impact the compatibility of software compiled with the current toolchains and libraries, as such users should expect to need to test existing applications post-upgrade and in some cases (especially where the application is leveraging software modules on M\u0101ui) rebuilding will be required. Users of applications maintained as software modules in the NeSI software stack can expect NeSI to provide rebuilt and/or updated versions of these applications (though this will be an ongoing effort post-upgrade).</p> <p>The following information will help your transition from the pre-upgrade M\u0101ui environment to the post-upgrade one:\u00a0</p> <ul> <li>The three main toolchains (CrayCCE, CrayGNU and CrayIntel) have all     been updated to release 23.02 (CrayCCE and CrayGNU) and 23.02-19     (CrayIntel). The previously installed versions are no longer     available.</li> <li>Consequently, nearly all of the previously provided environment     modules have been replaced by new versions. You can use the     module avail command to see what versions of those software     packages are now available. If your batch scripts load exact module     versions, they will need updating.</li> <li>The few jobs in the Slurm queue at the start of the upgrade process     have been placed in a \u201cuser hold\u201d state. You have the choice of     cancelling them with scancel &lt;jobid&gt; or releasing them with     scontrol release &lt;jobid&gt;.</li> <li>Be aware that if you have jobs submitted that rely on any software     built before the upgrade, there is a good chance that this software     will not run. We recommend rebuilding any binaries you maintain     before running jobs that utilise those binaries.</li> <li>Note that M\u0101ui login does not require adding a second factor to the     password when authenticating on the M\u0101ui login node after the first     successful login attempt. That is, if you have successfully logged     in using &lt;first factor&gt;&lt;second factor&gt; format, no second     factor part will be required later on.</li> </ul> <p>We have also updated our support documentation for M\u0101ui to reflect the changes, so please review it before starting any new projects.\u00a0</p>","tags":[]},{"location":"General/Announcements/Maui_upgrade_is_complete/#software-changes","title":"Software Changes","text":"<p>Software built on M\u0101ui may not work without recompilation after the upgrade. See the tables below for more detail regarding version changes. If you have any particular concerns about the impact on your work, please contact [NeSI Support Contact our Support Team.</p> <p>The table below outlines the known and expected Cray component changes:</p> <p>CLE Components</p> <p>Version pre-upgrade</p> <p>19.04</p> <p>Version post-upgrade</p> <p>23.02</p> <p>Cray Developer Toolkit</p> <p>19.04</p> <p>23.02</p> <p>Cray Compiling Environment</p> <p>CCE 8.7.10</p> <p>CCE 15.0.1</p> <p>Cray Message Passing Toolkit</p> <p>MPT 7.7.6</p> <p>PMI 5.0.14</p> <p>GA 5.3.0.10</p> <p>Cray OpenSHMEMX 8.0.1</p> <p>MPT 7.7.20</p> <p>PMI 5.0.17</p> <p>Cray OpenSHMEMX 9.1.2</p> <p>Cray Debugging Support Tools</p> <p>ATP 2.13</p> <p>CCDB 3.0.4</p> <p>CTI 2.15.5</p> <p>Gdb4hpc 3.0.10</p> <p>STAT 3.0.1.3</p> <p>Valgrind4hpc 1.0.0</p> <p>ATP 3.14.13</p> <p>CCDB 4.12.13</p> <p>CTI 2.17.2</p> <p>Gdb4hpc 4.14.3</p> <p>STAT 4.11.13</p> <p>Valgrind4hpc 2.12.11</p> <p>Cray Performance Measurement &amp; Analysis Tools \u2013CPMAT (1)</p> <p>Perftools 7.0.6</p> <p>PAPI 5.6.0.6</p> <p>Perftools 23.02.0</p> <p>PAPI 7.0.0.1</p> <p>Cray Scientific and Math Libraries -CSML</p> <p>LibSci 19.02.1</p> <p>LibSci_ACC 18.12.1 (CLE 6)</p> <p>PETSc 3.9.3.0</p> <p>Trilinos 12.12.1.1</p> <p>TPSL 18.06.1</p> <p>FFTW 2.1.5.9</p> <p>FFTW 3.3.8.2</p> <p>Petsc 3.14.5.0</p> <p>TPSL 20.03.2</p> <p>Trilinos 12.18.1.1</p> <p>Cray Environment Setup and Compiling support -CENV</p> <p>craype-installer1.24.5</p> <p>craypkg-gen1.3.7</p> <p>craype 2.5.18</p> <p>cray-modules 3.2.11.1</p> <p>cray-mpich-compat1.0.0-8 (patch)</p> <p>cdt-prgenv 6.0.5</p> <p>craypkg-gen 1.3.26</p> <p>craype 2.7.15</p> <p>Third party products</p> <p>HDF5 1.10.2.0</p> <p>NetCDF 4.6.1.3</p> <p>parallel-NetCDF 1.8.1.4</p> <p>iobuf 2.0.8</p> <p>java jdk 1.8.0_51 (CLE 6)</p> <p>GCC 7.3.0</p> <p>GCC 8.3.0</p> <p>cray-python 2.7.15.3 &amp; 3.6.5.3 (CLE 6)</p> <p>cray-R 3.4.2</p> <p>HDF5 1.12.2.3</p> <p>NetCDF 4.9.0.3</p> <p>Parallel-NetCDF 1.12.3.3</p> <p>iobuf 2.0.10</p> <p>GCC 10.3.0</p> <p>GCC 12.1.0</p> <p>cray-python 3.9.13.2</p> <p>cray-R 4.2.1.1</p> <p>Third Party Licensed Products</p> <p>PGI 18.10 (CLE 6 only)</p> <p>TotalView 2018.3.8</p> <p>Forge 19.0.3.1</p> <p>Forge 21.0.3</p> <p>Totalview 2021.2.14</p> <p>S-2529: XC Series Cray Programming Environment User's Guide</p> <p>S-2559: XC Series Software Installation and Configuration Guide (CLE 7.0.UP04 Rev E)</p> <p>Reference:</p> <p>HPE Cray Programming Environment 21.09 for Cray XC (x86) Systems</p> <p>Cray XC (x86) Programming Environments 19.04</p> <p>Applications supported by NeSI team</p>","tags":[]},{"location":"General/Announcements/New_capabilities_for_Machine_Learning_and_GPU_pricing_updates/","title":"New capabilities for Machine Learning and GPU pricing updates","text":"<p>We\u2019re excited to announce an addition of new GPU capabilities to our platform and some noteworthy changes to resource pricing as a result.</p> <p>New Graphics Processing Units (GPUs)</p> <p>We\u2019ve installed eight NVIDIA A100 GPU cards into the Mahuika HPC system, providing a significant boost in computing performance and an environment particularly suited to machine learning workloads. Over the last few months we\u2019ve worked directly with a group of beta tester researchers to ensure this new capability is fit-for-purpose and tuned to communities' specific software and tool requirements.\u00a0</p> <p>These new A100s, alongside software optimised for data science, are available to researchers using machine learning approaches. If this is you, Contact our Support Team to discuss how these new resources could support your work.</p> <p>Reduced pricing for P100s</p> <p>We\u2019ve recently reviewed our pricing and reduced the price of our existing P100 GPUs to 7.0 compute units per device-hour. The P100 GPUs are available to any project with a Mahuika allocation so if you have an existing allocation on Mahuika, you can access the P100s right away.</p> <p>If you need a larger or new allocation on Mahuika, you can apply for access now, but requests will likely be considered as part of our next allocation call window: 31 August - 01 October.</p> <p>For more technical information about using GPUs on NeSI, click here. If you have questions about allocations or how to access the P100s, [contact NeSI Support Contact our Support Team.</p> <p>Sharing our learning along the way</p> <p>If you\u2019re curious about what it takes to get the best of the A100 cards, you can learn about our experiences in the first post of a new \u2018Tech Insights' blog series: Tech Insights: A behind-the-scenes look at rolling out new GPU resources for NZ researchers.</p> <p>Our inaugural post discusses our first tasks with the A100s: thermal and internal software tests. In future posts, we\u2019ll explore user tests we conducted in the spaces of deep learning and molecular dynamics codes, as well as take a closer look at which codes are suitable to run on GPUs and whether your research project is a fit.</p> <p>Future GPU investments</p> <p>Looking ahead, we\u2019re currently finalising another investment into additional GPU cards later this year. We\u2019re also exploring the A100s' Multi-Instance GPU (MIG) features, which partition the cards\u00a0into several independent instances, giving multiple users access to GPU acceleration at the same time.</p> <p>These activities will enable us to expand GPU access to an even broader community of users, as well as support more advanced and demanding performance needs across domains. If you\u2019re interested in using these A100s for something other than machine learning, let us know by [contacting NeSI Support Contact our Support Team - that way we can keep you up to date on our plans.</p> <p>If you have questions or comments on anything mentioned above, please\u00a0[get in touch Contact our Support Team_request.html\" %}.</p> <p>Thank you,</p> <p>NeSI Support</p>","tags":[]},{"location":"General/Announcements/Slurm_upgrade_to_version_21-8/","title":"Slurm upgrade to version 21.8","text":"<ul> <li>Added\u00a0<code>--me</code>\u00a0option, equivalent to<code>--user=$USER</code>.</li> <li>Added \"pendingtime\" as a option for --Format.</li> <li>Put sorted start times of \"N/A\" or 0 at the end of the list.</li> </ul> <ul> <li>Add time specification: \"now-\" (i.e. subtract from the present)</li> <li>AllocGres and ReqGres were removed. Alloc/ReqTres should be used     instead.\u00a0</li> </ul> <ul> <li>MAGNETIC flag on reservations. Reservations the user doesn't have to     even request.</li> <li>The LicensesUsed line has been removed from\u00a0<code>scontrol show config</code>\u00a0.     Please use updated\u00a0<code>scontrol show licenses</code>\u00a0command as an     alternative.</li> </ul> <ul> <li><code>--threads-per-core</code>\u00a0now influences task layout/binding, not just     allocation.</li> <li><code>--gpus-per-node</code>\u00a0can be used instead of\u00a0<code>--gres=GPU</code></li> <li><code>--hint=nomultithread</code> can now be replaced     with\u00a0<code>--threads-per-core=1</code></li> <li>The inconsistent terminology and environment variable naming for     Heterogeneous Job (\"HetJob\") support has been tidied up.</li> <li>The correct term for these jobs are \"HetJobs\", references to     \"PackJob\"\u00a0\u00a0 have been corrected.</li> <li>The correct term for the separate constituent jobs are     \"components\",\u00a0\u00a0 references to \"packs\" have been corrected.</li> </ul> <ul> <li>Added support for an \"Interactive Step\", designed to be used with     salloc to launch a terminal on an allocated compute node     automatically. Enable by setting \"use_interactive_step\" as part of     LaunchParameters.</li> </ul> <ul> <li>By default, a step started with srun will be granted exclusive (or     non- overlapping) access to the resources assigned to that step. No     other parallel step will be allowed to run on the same resources at     the same time. This replaces one facet of the '--exclusive' option's     behavior, but does not imply the '--exact' option described below.     To get the previous default behavior - which allowed parallel steps     to share all resources - use the new srun '--overlap' option.</li> <li>In conjunction to this non-overlapping step allocation behavior     being the new default, there is an additional new option for step     management '--exact', which will allow a step access to only those     resources requested by the step. This is the second half of the     '--exclusive' behavior. Otherwise, by default all non-gres resources     on each node in the allocation will be used by the step, making it     so no other parallel step will have access to those resources unless     both steps have specified '--overlap'.</li> </ul> <ul> <li>New command which permits crontab-compatible job scripts to be     defined. These scripts will recur automatically (at most) on the     intervals described.</li> </ul>","tags":["general"]},{"location":"General/Announcements/Status_page_subscription_notification_changes/","title":"Status page subscription notification changes\ufeff","text":"<p>NeSI uses its System Status page as a tool to communicate planned and unplanned maintenance and interruptions to service. Our Status page includes a list of components that reflect high-level functional aspects of NeSI's platforms and services, as well as a more specific detailed list covering different platform elements.</p> <p>Recently, we made adjustments to our Status page's default notification settings to balance timely notice of planned and unplanned issues against superfluous or unsolicited noise.</p> <p>Now, instead of automatically subscribing new users for all notifications, we will initially only subscribe you to receive status notices about the following key components:</p> <ul> <li>Submit new HPC Jobs - notices regarding status of login nodes, Slurm scheduler, or filesystem</li> <li>Jobs running on HPC - notices regarding network issues, or status of Slurm scheduler or filesystem</li> <li>Jupyter on NeSI - notices regarding the status of our Jupyter Service for interactive computing</li> <li>HPC Storage - notices regarding the status of storage resources on NeSI systems</li> </ul> <p>Effective Friday 20 October, we adjusted all existing and non-customised Status page subscriptions to match this shorter notification list.</p> <p>This change ensures you receive the most relevant and high value information by push notification, while leaving the option available for you to subscribe to more specific updates.</p>","tags":["announcement"]},{"location":"General/Announcements/Status_page_subscription_notification_changes/#customise-your-notifications","title":"Customise your notifications","text":"<p>You can subscribe to and unsubscribe from components at any time, allowing you to customise your status notifications based on which components you are using or most interested in at the time. Visit our Managing Notification Preferences page for instructions.</p>","tags":["announcement"]},{"location":"General/Announcements/Status_page_subscription_notification_changes/#questions","title":"Questions?","text":"<p>If you have any questions regarding System Status notifications or other NeSI services, contact our Support Team at any time.</p>","tags":["announcement"]},{"location":"General/Announcements/University_of_Auckland_ANSYS_users/","title":"University of Auckland - ANSYS users","text":"<p>On 01/04/2021 afternoon, there was a change to the University ANSYS licences; you may find that your jobs fail with a licence error.</p> <p>The following command should resolve the issue (where <code>-revn 202</code> is replaced with the version you use).</p> <pre><code>module load ANSYS/2020R2\nansysli_util -revn 202 -deleteuserprefs\n</code></pre> <p>The effect this will have on all of the ANSYS products is yet to be determined, so please open a support ticket if you encounter problems.</p>","tags":[]},{"location":"General/Announcements/Upcoming_webinar-Tips_for_making_the_most_of_Mahuika%27s_new_Milan_nodes/","title":"Upcoming webinar: Tips for making the most of Mahuika\u2019s new Milan nodes","text":"<p>In late 2022, the Mahuika cluster was expanded to allow a wider range of research communities to adopt HPC approaches and build digital skills within their research teams.</p> <p>Join us on Thursday 30 March for a short webinar sharing some practical tips and tricks for making the most of these new resources:</p> <p>Making the most of Mahuika's new Milan nodes Thursday 30 March 11:30 am - 12:00 pm Click here to RSVP</p> <p>Background: Following a successful early access programme, Mahuika\u2019s newest CPU nodes are now available for use by any projects that have a Mahuika allocation on NeSI's HPC Platform. The production launch of these new nodes is an exciting milestone in NeSI\u2019s strategy to lower the carbon footprint and continually improve the performance and fit-for-purpose design of our platforms to meet your research needs.</p> <p>What\u2019s new</p> <ul> <li> <p>faster, more powerful computing, enabled by AMD 3rd Gen EPYC Milan     architecture</p> </li> <li> <p>specialised high-memory capabilities, allowing rapid simultaneous     processing</p> </li> <li> <p>improved energy efficiency - these nodes are 2.5 times more power     efficient than Mahuika\u2019s original Broadwell nodes</p> </li> </ul> <p>Come along to this webinar to learn more and to ask questions about how your research project can use these powerful resources.</p> <p>About the speaker</p> <p>Alexander Pletzer\u00a0is a Research Software Engineer working for NeSI at NIWA. Alex helps researchers run better and faster on NeSI platforms.</p> <p>More Information</p> <p>If you're unable to join us for this session but have questions about the Milan nodes or would like more information, come along to one of our weekly Online Office Hours or email support@nesi.org.nz anytime.\u00a0</p>","tags":[]},{"location":"General/FAQs/Can_I_change_my_time_zone_to_New_Zealand_time/","title":"Can I change my time zone to New Zealand time?","text":"<p>The time displayed in your shell is controlled by a system variable called <code>TZ</code>. To change to New Zealand time you need to set the variable as follows:</p> <pre><code>export TZ=\"NZ\"\n</code></pre> <p>This setting will automatically adjust for daylight saving, since the <code>tzdata</code> package is installed at the system level. Our system engineers will keep the <code>tzdata</code> package up to date.</p>","tags":[]},{"location":"General/FAQs/Can_I_change_my_time_zone_to_New_Zealand_time/#making-the-change-persistent","title":"Making the change persistent","text":"<p>You can make your time zone setting persistent by adding the above line to your <code>~/.bashrc</code>. If you do this, we recommend adding the following line to your <code>~/.bash_profile</code>, or to your <code>~/.profile</code> if you have the latter but not the former:</p> <pre><code>test -r ~/.bashrc &amp;&amp; . ~/.bashrc\n</code></pre> <p>Please see the article, \".bashrc or .bash_profile?\" for more information.</p>","tags":[]},{"location":"General/FAQs/Can_I_change_my_time_zone_to_New_Zealand_time/#what-about-cron-jobs","title":"What about cron jobs?","text":"<p>To have the specifications in your crontab file interpreted as NZ times start it with:</p> <pre><code>CRON_TZ=NZ\n</code></pre> <p>Also note that cron does not\u00a0source either <code>~/.bashrc</code> or <code>~/.bash_profile</code>, so most environment variables will not be set, including TZ.</p>","tags":[]},{"location":"General/FAQs/Can_I_use_SSHFS_to_mount_the_cluster_filesystem_on_my_local_machine/","title":"Can I use SSHFS to mount the cluster filesystem on my local machine?","text":"<p>SSHFS allows you to mount a remote filesystem on your local machine. SSHFS relies on SSH underneath, so you should follow the \"Recommended logon procedure\" instructions here to configure SSH first.</p>","tags":[]},{"location":"General/FAQs/Can_I_use_SSHFS_to_mount_the_cluster_filesystem_on_my_local_machine/#linux","title":"Linux","text":"<p>Use the following commands to mount your home directory from Mahuika on your local machine (the same command will work for M\u0101ui, just replace the names):</p> <pre><code># create a mount point and connect\nmkdir -p ~/mahuika-home\nsshfs -oauto_cache,follow_symlinks mahuika: ~/mahuika-home\n</code></pre> <p>Now you should be able to navigate to \"~/mahuika-home\" on your local machine to access your home directory on Mahuika. To unmount the directory run:</p> <pre><code>fusermount -u ~/mahuika-home\n</code></pre> <p>To mount a project directory, you could run:</p> <pre><code># create a mount point and connect\nmkdir -p ~/mahuika-project\nsshfs -oauto_cache,follow_symlinks mahuika:/nesi/project/nesiXXXXX ~/mahuika-project\n</code></pre>","tags":[]},{"location":"General/FAQs/Can_I_use_SSHFS_to_mount_the_cluster_filesystem_on_my_local_machine/#macos","title":"MacOS","text":"<p>We recommend using some extra options with MacOS. The following commands will mount your home directory, make it show up under devices in Finder and give the volume a sensible name:</p> <pre><code># create a mount point and connect\nmkdir -p ~/mahuika-home\nsshfs mahuika: ~/mahuika-home \\\n    -oauto_cache,follow_symlinks \\\n    -ovolname=MahuikaHome,defer_permissions,noappledouble,local \n</code></pre> <p>To unmount the directory on MacOS, either eject from Finder or run:</p> <pre><code>umount ~/mahuika-home\n</code></pre> <p>Note</p> <p>Newer MacOS does not come with SSHFS pre installed. You will have to  install FUSE as SSHFS from here.</p>","tags":[]},{"location":"General/FAQs/Converting_from_Windows_style_to_UNIX_style_line_endings/","title":"Converting from Windows-style to UNIX-style line endings","text":"<p>In a plain text file, to tell the computer that a line of text doesn't continue forever, the end of each line is marked by a sequence of one or more invisible characters, called control characters. While there are many control characters for different purposes, the relevant ones for line endings are the carriage return (CR) and line feed (LF) characters.</p> <p>Unfortunately, the programmers of different operating systems have represented line endings using different sequences:</p> <ul> <li>All versions of Microsoft Windows represent line endings as CR     followed by LF.</li> <li>UNIX and UNIX-like operating systems (including Mac OS X) represent     line endings as LF alone.</li> </ul> <p>Therefore, a text file prepared in a Windows environment will, when copied to a UNIX-like environment such as a NeSI cluster, have an unnecessary carriage return character at the end of each line. To make matters worse, this character will normally be invisible, though in some text editors it will show up as ^M or similar.</p> <p>Many programs, including the Slurm and LoadLeveler batch queue schedulers, will give errors when given a file containing carriage return characters as input.</p> <p>Therefore, you will need to convert any such file so it has only UNIX-style line endings before using it on a NeSI cluster.</p>","tags":[]},{"location":"General/FAQs/Converting_from_Windows_style_to_UNIX_style_line_endings/#the-symptoms","title":"The Symptoms","text":"","tags":[]},{"location":"General/FAQs/Converting_from_Windows_style_to_UNIX_style_line_endings/#in-the-slurm-job-scheduler","title":"In the Slurm job scheduler","text":"<p>If you submit (using <code>sbatch</code>) a Slurm submission script with Windows-style line endings, you will likely receive the following error:</p> <pre><code>sbatch: error: Batch script contains DOS line breaks (\\r\\n) \nsbatch: error: instead of expected UNIX line breaks (\\n).\n</code></pre>","tags":[]},{"location":"General/FAQs/Converting_from_Windows_style_to_UNIX_style_line_endings/#in-other-programs","title":"In other programs","text":"<p>Some UNIX or Linux programs are tolerant to Windows-style line endings, while others give errors. The text of the error is almost infinitely variable, but program behaviours might include the following responses:</p> <ul> <li>Explicitly stating the problem with line endings</li> <li>Complaining more vaguely that the input data is incomplete or corrupt or that there are problems reading it</li> <li>Failing in a more serious way such as a segmentation fault</li> </ul>","tags":[]},{"location":"General/FAQs/Converting_from_Windows_style_to_UNIX_style_line_endings/#checking-a-files-line-ending-format","title":"Checking a file's line ending format","text":"<p>If you have what you think is a text file on the cluster but you don't know whether its line endings are in the correct format or not, you can run the following command:</p> <pre><code>file foo.txt          # Replace foo.txt with the name of your file\n</code></pre> <p>Depending on the contents of <code>foo.txt</code>, the output of this command may vary, but if the output has \"CR\" or \"CRLF\" in it, you will need to convert <code>foo.txt</code> to UNIX format line endings if you want to use it on the cluster.</p>","tags":[]},{"location":"General/FAQs/Converting_from_Windows_style_to_UNIX_style_line_endings/#how-to-convert","title":"How to Convert","text":"","tags":[]},{"location":"General/FAQs/Converting_from_Windows_style_to_UNIX_style_line_endings/#converting-using-notepad","title":"Converting using Notepad++","text":"<p>In the Windows text editing program Notepad++ (not to be confused with ordinary Notepad), there is a function to prepare text files with UNIX-style line endings.</p> <p>To write your file in this way, while you have the file open, go to the Edit menu, select the \"EOL Conversion\" submenu, and from the options that come up select \"UNIX/OSX Format\". The next time you save the file, its line endings will, all going well, be saved with UNIX-style line endings.</p> <p>You can check what format line endings you are currently editing in by looking in the status bar at the bottom of the window. Between the range box (a box containing Ln, Col and Sel entries) and the text encoding box (which will contain UTF-8, ANSI, or some other technical string) will be a box containing the current line ending format.</p> <ul> <li>In most cases, this box will contain the text \"DOS\\Windows\".</li> <li>In a few cases, such as the file having been prepared on a UNIX or     Linux machine or a Mac, it will contain the text \"UNIX\".</li> <li>It is possible, though highly unlikely by now, that the file may     have old-style (pre-OSX) Mac line endings, in which case the box     will contain the text \"Macintosh\".</li> </ul> <p>Please note that if you change a file's line ending style, you must save your changes before copying the file anywhere, including to a cluster.</p>","tags":[]},{"location":"General/FAQs/Converting_from_Windows_style_to_UNIX_style_line_endings/#converting-using-dos2unix","title":"Converting using dos2unix","text":"<p>Suppose, though, that you've copied a text file to the cluster already, and you realise you need to convert it to UNIX format. How do you do that?</p> <p>Simple: Use the program <code>dos2unix</code>.</p> <p>Just give the name of your file to <code>dos2unix</code> as an argument, and it will convert the file's line endings to UNIX format:</p> <pre><code>dos2unix foo.txt      # Replace foo.txt with the name of your file\n</code></pre> <p>There are other options in the rare case that you don't want to just modify your existing file; run <code>man dos2unix</code> for details.</p>","tags":[]},{"location":"General/FAQs/How_can_I_give_read_only_team_members_access_to_my_files/","title":"How can I give read-only team members access to my files?","text":"<p>See also</p> <p>File permissions and  groups</p> <p>Not all projects have read-only groups created by default. If your project has a read-only group created after the project itself was created, you will need to add appropriate access control lists (ACLs) to each of your files and directories within the project or nobackup directory.</p> <p>To do this, you can use the script <code>nn_add_to_acls_recursively</code>. The following commands explain how to do this;\u00a0 when running the commands, replace <code>nesi12345</code> and <code>nesi12345r</code> with your project code and read-only project code respectively.</p> <p>Warning</p> <p>If this process is interrupted part-way through, for example due to  your computer going to sleep and losing its connection to your NeSI  terminal session, your files can end up in a bad way. For this reason  please run all the following commands in a <code>screen</code> or <code>tmux</code>  session.</p> <ol> <li> <p>Prepare a file containing the ACL to add. Ensure you include the     <code>mask</code> line. Note that the script will not remove any of the     existing ACL, except for overwriting existing lines that are the     same, up to the second colon, as one of the new lines you ask to     add.</p> <pre><code>echo \"mask::rwxc\" &gt; acl_to_add.txt\necho \"group:nesi12345r:r-x-\" &gt;&gt; acl_to_add.txt\n</code></pre> </li> <li> <p>Check that the contents of the file are correct.</p> <pre><code>cat acl_to_add.txt\n</code></pre> </li> <li> <p>Carry out the ACL change. You can specify a subdirectory instead if,     as may well be the case, you don't want to trawl through the     entirety of <code>/nesi/project/nesi12345</code> or <code>/nesi/nobackup/nesi12345</code>.</p> <pre><code>nn_add_to_acls_recursively -f acl_to_add.txt /nesi/project/nesi12345\n</code></pre> </li> <li> <p>Check the resulting ACLs, for example:</p> <pre><code>/usr/lpp/mmfs/bin/mmgetacl /nesi/project/nesi12345/some_dir\n/usr/lpp/mmfs/bin/mmgetacl -d /nesi/project/nesi12345/some_dir\n</code></pre> <p>We suggest to check at least one subdirectory, at least one executable file (if there is one) and at least one non-executable file.</p> </li> <li> <p>Repeat steps 3 and 4 for other directories within     <code>/nesi/project/nesi12345</code> and <code>/nesi/nobackup/nesi12345</code>, with the     necessary modifications.</p> </li> <li> <p>Optionally, remove your ACL file.</p> <pre><code>rm acl_to_add.txt\n</code></pre> </li> <li> <p>Optionally, exit the <code>screen</code> or <code>tmux</code> session when you are finished.</p> </li> </ol>","tags":[]},{"location":"General/FAQs/How_can_I_let_my_fellow_project_team_members_read_or_write_my_files/","title":"How can I let my fellow project team members read or write my files?","text":"<p>See also</p> <p>File permissions and  groups</p> <p>If you move or copy a file or directory from one project directory to another, or from somewhere within your home directory to somewhere within a project directory, generally the file, or the directory together with its contents, as the case may be, will keep its original ownership, group and permissions.</p> <p>So, supposing Joe Bloggs moves a file from his home directory to the project directory <code>/nesi/project/nesi99999</code>, his fellow team members won't be able to write to it:</p> <pre><code>$ ls -l README\n-rw-r--r-- 1 bloggsj bloggsj 235 Mar 14  2014 README\n$ mv README /nesi/project/nesi99999/bloggsj/README\n$ ls -l /nesi/project/nesi99999/bloggsj/README\n-rw-r--r-- 1 bloggsj bloggsj 235 Mar 14  2014 /nesi/project/nesi99999/bloggsj/README\n</code></pre> <p>As you can see, the file stays in the group <code>bloggsj</code>, that is Joe Bloggs' personal group, even though it is now inside the project directory.</p> <p>There is, however, a solution involving the <code>rsync</code> command, a more advanced version of <code>scp</code>. <code>rsync</code> is typically used to copy files between two or more machines, but can also be used within the same machine.</p> <p>Warning</p> <p>In both these commands, the\u00a0<code>--no-perms</code>\u00a0and\u00a0<code>--no-group</code>\u00a0options must  both come after\u00a0<code>-a</code>.\u00a0<code>-a</code>\u00a0implicitly asserts\u00a0<code>--perms</code>\u00a0and\u00a0<code>--group</code>,  and will therefore override whichever  of\u00a0<code>--no-perms</code>\u00a0and\u00a0<code>--no-group</code>\u00a0come before it.</p>","tags":[]},{"location":"General/FAQs/How_can_I_let_my_fellow_project_team_members_read_or_write_my_files/#to-copy-a-file-or-directory-and-its-contents-updating-its-group-and-setting-its-permissions","title":"To copy a file (or directory and its contents), updating its group and setting its permissions","text":"<pre><code>rsync -a --no-perms --no-group --chmod=ugo=rwX,Dg+s /path/to/source /path/to/destination\n</code></pre>","tags":[]},{"location":"General/FAQs/How_can_I_let_my_fellow_project_team_members_read_or_write_my_files/#to-move-a-file-or-directory-and-its-contents-updating-its-group-and-setting-its-permissions","title":"To move a file (or directory and its contents), updating its group and setting its permissions","text":"<p>Warning</p> <p>The\u00a0<code>--remove-source-files</code> option is safe only if every source file  is otherwise left intact during the moving process.</p> <pre><code>rsync --remove-source-files -a --no-perms --no-group --chmod=ugo=rwX,Dg+s /path/to/source /path/to/destination\n</code></pre> <p>If you want to set files to executable in all cases, replace\u00a0<code>...ugo=rwX...</code>\u00a0with\u00a0<code>...ugo=rwx...</code>. The capital\u00a0<code>X</code>\u00a0means, \"Preserve whatever executable permissions existed on the source file and aren't masked at the destination.\" A lower case\u00a0<code>x</code>\u00a0on the other hand means, \"Make this entity executable, even if it wasn't so previously, though this may be masked at the destination.\"</p>","tags":[]},{"location":"General/FAQs/How_can_I_let_my_fellow_project_team_members_read_or_write_my_files/#to-fix-the-permissions-of-a-file-or-directory-that-is-already-in-its-intended-place","title":"To fix the permissions of a file or directory that is already in its intended place","text":"<p>Change to the parent directory, which could be a project or nobackup directory, that you want to fix, and find and fix your files. You can do this by means of the following commands.</p> <pre><code># Replace nesi12345 with your desired project code\ngroup=nesi12345\nstartdir=$(pwd)\n# Replace /nesi/project with /nesi/nobackup if needed\ncd /nesi/project/${group}\n# Move all files, directories, etc. owned by yourself into the project group\n# The --no-dereference option updates the group of symbolic links (where permitted)\nfind . -user $(whoami) -print0 | xargs -0 -I {} chgrp --no-dereference ${group} {}\n# Make all files owned by yourself readable and writable by the group\nfind . -user $(whoami) -and -type f -print0 | xargs -0 -I {} chmod g+rw {}\n# Make all directories owned by yourself readable, writable and executable by the group,\n# and set the setgid bit\nfind . -user $(whoami) -and -type d -print0 | xargs -0 -I {} chmod g+rwxs {}\n# Go back to the starting location\ncd ${startdir}\n</code></pre>","tags":[]},{"location":"General/FAQs/How_can_I_see_how_busy_the_cluster_is/","title":"How can I see how busy the cluster is?","text":"<p>You can get the current status of all nodes on a cluster using the command <code>sinfo</code>, you will get a printout like the following.</p> <p>The nodelist column has been truncated for readability</p> <pre><code>PARTITION AVAIL JOB_SIZE TIMELIMIT    CPUS S:C:T    NODES   STATE    NODELIST\nlarge*    up    1-infini 3-00:00:00     72 2:18:2       1   down*      wbn128\nlarge*    up    1-infini 3-00:00:00     72 2:18:2     133   mixed      wbn[009-020...\nlarge*    up    1-infini 3-00:00:00     72 2:18:2       7   allocated  wbn[031,038\nlarge*    up    1-infini 3-00:00:00     72 2:18:2      85   idle       wbn[021,037...\nlong      up    1-infini 21-00:00:0     72 2:18:2      64   mixed      wbn[009-020...\nlong      up    1-infini 21-00:00:0     72 2:18:2       5   allocated  wbn[031,077...\ngpu       up    1-infini 3-00:00:00      8  8:1:1       1   reserved   vgpuwbg004\ngpu       up    1-infini 3-00:00:00      8  8:1:1       3   idle       vgpuwbg[001-003]\nigpu      up    1-infini 3-00:00:00      8  8:1:1       1   reserved   vgpuwbg004\nprepost   up    1-infini    3:00:00     72 2:18:2       2   down*      wbl[003,005]\nprepost   up    1-infini    3:00:00     72 2:18:2       2   mixed      wbl[002,010]\nprepost   up    1-infini    3:00:00     72 2:18:2       5   allocated  wbl[001,004...\nbigmem    up    1-infini 7-00:00:00     72 2:18:2       1   down*      wbl003\nbigmem    up    1-infini 7-00:00:00     72 2:18:2       2   mixed      wbl[002,010]\nbigmem    up    1-infini 7-00:00:00     72 2:18:2       5   allocated  wbl[001,004...\nhugemem   up    1-infini 7-00:00:00    128 4:16:2       1   mixed      wbh001\n</code></pre> <p>Each partition has a row for every state it's nodes are currently in.</p> <p>For example, the <code>large</code> partition currently has\u00a0 1 <code>down</code> node,\u00a0 133 <code>mixed</code> nodes,\u00a0\u00a07 <code>allocated</code> nodes and\u00a0\u00a085 <code>idle</code> nodes.</p> <p>The most common node states you are likely to see are:</p> State Description <code>idle</code> All CPUs on this node are unallocated and available for use. <code>allocated</code> All CPUs on this node are currently allocated. <code>mixed</code> Some CPUs on this node are unallocated, smaller jobs are likely to land here. <code>down</code> The node is unavailable for use <code>reserved</code> This node has been reserved, and is only available for some users (in the case of the igpu partition, please contact NeSI support if you wish to use it). <code>draining</code> Jobs are currently running on this node, but is not available for new jobs. <p>A full list of node states can be found here.</p> <p>If you are interested in the state of one partition in particular you may want to use the command <code>squeue -p &lt;partition&gt;</code> to get the current queue of the partition <code>&lt;partition&gt;</code>.</p>","tags":[]},{"location":"General/FAQs/How_can_I_view_images_generated_on_the_cluster/","title":"How can I view images generated on the cluster?","text":"<p>If for any reason downloading images is impractical you can view them on the cluster using the <code>display</code> command. For example,</p> <pre><code>display myImage.png\n</code></pre> <p>This requires a working X-11 server.</p>","tags":["visualisation","image","x11","view"]},{"location":"General/FAQs/How_do_I_find_out_the_size_of_a_directory/","title":"How do I find out the size of a directory?","text":"<p>To simplify this process, we have written a script, <code>nn_dir_contents</code>. This script can be run in a variety of ways.</p> Command Result <code>nn_dir_contents</code> Shows the size of, and number of directory entries in, the current working directory <code>nn_dir_contents -s</code> Shows the size of the current working directory <code>nn_dir_contents -n</code> Shows the number of directory entries in the current working directory <code>nn_dir_contents &lt;DIR&gt;</code> Shows the size of, and number of directory entries in, the directory <code>DIR</code> <code>nn_dir_contents -s &lt;DIR&gt;</code> Shows the size of the directory <code>DIR</code> <code>nn_dir_contents -n &lt;DIR&gt;</code> Shows the number of directory entries in the directory <code>DIR</code> <code>nn_dir_contents &lt;DIR1&gt; &lt;DIR2&gt; ...</code> Shows the size of, and number of directory entries in, the directories <code>DIR1</code>, <code>DIR2</code>, etc. <code>nn_dir_contents -s &lt;DIR1&gt; &lt;DIR2&gt; ...</code> Shows the sizes of the directories <code>DIR1</code>, <code>DIR2</code>, etc. <code>nn_dir_contents -n &lt;DIR1&gt; &lt;DIR2&gt; ...</code> Shows the numbers of directory entries in the directories <code>DIR1</code>, <code>DIR2</code>, etc. <p>The last three forms of commands work with shell globbing (<code>*</code>, <code>?</code>, etc.), and the last two are particularly useful if you want to find out how much each subdirectory contributes to a directory's total disk space or inode counts. The outputs of the last two commands can easily be piped to <code>sort</code> if you want to get a list of directories from the smallest to the largest (<code>sort -k 2h,2</code> for a human-readable sort), or from the fewest files to the most (<code>sort -k 2n,2</code> for a numeric sort).</p> <p>Only directory arguments are considered by <code>nn_dir_contents</code>, though files do count towards a directory's contents.</p> <p><code>nn_dir_contents</code> is a wrapper for <code>du</code> and is run without any flags that alter the behaviour of <code>du</code> with respect to sparse files. If you think the sparsity of a file is relevant to you, you may need to run <code>du</code> separately on directories that you believe contain sparse files.</p> <p><code>nn_dir_contents</code> relies on two consecutive executions of the <code>find</code> command in order to count the number of files. It does not lock the directory, so if the directory's contents are altered (files created or deleted) while the command is running, the results may be inaccurate or out of date. This is a known limitation of the command.</p>","tags":[]},{"location":"General/FAQs/How_do_I_fix_my_locale_and_language_settings/","title":"How do I fix my locale and language settings?","text":"<p>When logging in to NeSI from some systems, such as Windows Subsystem for Linux, you might get messages like the following while using NeSI (the following message is obtained when running <code>man</code>):</p> <pre><code>man: can't set the locale; make sure $LC_* and $LANG are correct\n</code></pre> <p>To get rid of this message, save a text file called <code>.i18n</code> in your home directory, with the following contents:</p> <pre><code>LC_ALL=\"en_US.UTF-8\"\nLC_ADDRESS=\"en_US.UTF-8\"\nLC_COLLATE=\"en_US.UTF-8\"\nLC_CTYPE=\"en_US.UTF-8\"\nLC_IDENTIFICATION=\"en_US.UTF-8\"\nLC_MEASUREMENT=\"en_US.UTF-8\"\nLC_MESSAGES=\"en_US.UTF-8\"\nLC_MONETARY=\"en_US.UTF-8\"\nLC_NAME=\"en_US.UTF-8\"\nLC_NUMERIC=\"en_US.UTF-8\"\nLC_PAPER=\"en_US.UTF-8\"\nLC_TELEPHONE=\"en_US.UTF-8\"\nLC_TIME=\"en_US.UTF-8\"\n</code></pre> <p>If you know what you're doing, you can replace each instance of \"en_US.UTF-8\" with a different locale. You can get a list of available locales by running <code>locale -a</code>. Use a different locale at your own risk, however.</p>","tags":[]},{"location":"General/FAQs/How_do_I_request_memory/","title":"How do I request memory?","text":"<ul> <li><code>--mem</code>: Memory per node</li> <li><code>--mem-per-cpu</code>: Memory per logical CPU</li> </ul> <p>In most circumstances, you should request memory using <code>--mem</code>. The exception is if you are running an MPI job that could be placed on more than one node, with tasks divided up randomly, in which case <code>--mem-per-cpu</code> is more appropriate. More detail is in the following table, including how you can tell what sort of job you're submitting.</p> Job type Requested tasks (<code>-n</code>, <code>--ntasks</code>) Requested logical CPUs per task (<code>--cpus-per-task</code>) Requested nodes (<code>-N</code>, <code>--nodes</code>) Requested tasks per node (<code>--ntasks-per-node</code>) Preferred memory format Ideal value Serial 1 (or unspecified) 1 (or unspecified) (Irrelevant, but should not be specified)<sup>1</sup> (Irrelevant, but should not be specified)<sup>2</sup> <code>--mem=</code> Peak memory<sup>3</sup> needed by the program Multithreaded (e.g. OpenMP), but not MPI 1 (or unspecified) &gt; 1 (Irrelevant, but should not be specified)<sup>1</sup> (Irrelevant, but should not be specified)<sup>2</sup> <code>--mem=</code> Peak memory<sup>3</sup> needed by the program MPI, evenly split between nodes (recommended method) Unspecified<sup>4</sup> \u2265 1 (or unspecified) \u2265 1<sup>5</sup> \u2265 1<sup>5</sup> <code>--mem=</code> (Peak memory<sup>3</sup> needed per MPI task)\u00a0\u00d7 (number of tasks per node) MPI, evenly split between nodes (discouraged method) &gt; 1 \u2265 1 (or unspecified) Either 1 or the number of tasks<sup>6</sup> (Irrelevant, but should not be specified)<sup>4</sup> <code>--mem=</code> (Peak memory<sup>3</sup> needed per MPI task)\u00a0\u00d7 (number of tasks per node)\u00a0 MPI, randomly placed &gt; 1 \u2265 1 (or unspecified) &gt; 1; &lt; number of tasks<sup>6</sup> (or unspecified) (Irrelevant, but should not be specified)<sup>4</sup> <code>--mem-per-cpu=</code> (Peak memory<sup>3</sup> needed per MPI task)\u00a0\u00f7 (number of logical CPUs per MPI task) <p><sup>1</sup>\u00a0If your job consists of only one task there's no reason to request a specific number of nodes, and requesting more than one node will cause you to be charged too much for your job. A one-task job will be assigned one node by default.</p> <p><sup>2</sup> If you don't request a specific number of nodes, it makes no sense to request a specific number of tasks per node.</p> <p><sup>3</sup> It's usually a good idea to request a little more memory from Slurm than your program absolutely needs, to give your job a buffer in case its behaviour varies slightly from run to run.</p> <p><sup>4</sup> If either <code>-n</code> or <code>--ntasks</code> is used along with <code>--ntasks-per-node</code>, <code>--ntasks-per-node</code> will be silently ignored.</p> <p><sup>5</sup> An MPI job that is evenly split between two or more nodes and that doesn't specify a total number of tasks will need either <code>-N</code> (or <code>--nodes</code>) or <code>--ntasks-per-node</code>, or both, to be greater than 1; and both must be positive integers.</p> <p><sup>6</sup>\u00a0If you set <code>-N</code> (or <code>--nodes</code>) to 1, that is effectively the same as setting<code>--ntasks-per-node</code>\u00a0the same as<code>-n</code> (or <code>--ntasks</code>), and the job is guaranteed to run on a single node. On the other hand, if you request <code>-N</code> (or <code>--nodes</code>) to be the same as <code>-n</code> (or <code>--ntasks</code>), that is effectively the same as setting <code>--ntasks-per-node=1</code>, and the job will be evenly split between nodes. In either of these cases, <code>--mem</code>\u00a0is better than<code>--mem-per-cpu</code>. Meanwhile, requesting more nodes than tasks never makes sense.</p>","tags":[]},{"location":"General/FAQs/How_do_I_run_my_Python_Notebook_through_SLURM/","title":"How do I run my Python Notebook through SLURM?","text":"<p>The first thing you will need to do is to convert your <code>.ipynb</code> (Interactive PYthon Note Book) file into a regular <code>.py</code> python file. There are two ways to do this.</p>","tags":[]},{"location":"General/FAQs/How_do_I_run_my_Python_Notebook_through_SLURM/#nbconvert","title":"nbconvert","text":"<p><code>nbconvert</code> is a tool used to convert notebooks to other formats, it is accessible through the command line if you are logged in through Jupyter.</p> <pre><code>jupyter nbconvert --to script my_notebook.ipynb \n</code></pre> <p>will create a new python script called <code>my_notebook.py</code>.</p>","tags":[]},{"location":"General/FAQs/How_do_I_run_my_Python_Notebook_through_SLURM/#export-notebook","title":"Export Notebook","text":"<p>With your notebook open, select File -&gt; Save and Export Notebook As... -&gt; Executable Script</p> <p>This option might be less convenient as the exporter saves the python file to your local computer, meaning you will have to drag it back into the file explorer in Jupyter from your downloads folder.</p> <p>This script can then be run as a regular python script as described in our Python documentation.</p>","tags":[]},{"location":"General/FAQs/How_to_replace_my_2FA_token/","title":"How to replace my 2FA token","text":"<p>Log in to my.nesi.org.nz and select the\u00a0option 'Manage Two-Factor token' under 'Account'.</p>","tags":["2fa","access","mfa","account","authentication"]},{"location":"General/FAQs/How_to_replace_my_2FA_token/#manage-two-factor-token","title":"Manage Two-Factor token","text":"<p>Once you've created a token, you should see the 'Manage Two-Factor token' option.</p> <p></p>","tags":["2fa","access","mfa","account","authentication"]},{"location":"General/FAQs/How_to_replace_my_2FA_token/#delete-the-two-factor-token","title":"Delete the Two-Factor token","text":"<p>Select the\u00a0option 'Manage Two-Factor token' under 'Account'.</p> <p>You should see the confirmation of the previous token.</p> <p></p> <p>After 'Delete Token' has been selected:</p> <p></p> <p>You will also receive an email confirmation:</p> <p></p>","tags":["2fa","access","mfa","account","authentication"]},{"location":"General/FAQs/How_to_replace_my_2FA_token/#related-content","title":"Related content","text":"<p>Setting Up Two-Factor Authentication</p>","tags":["2fa","access","mfa","account","authentication"]},{"location":"General/FAQs/I_have_not_scanned_the_2FA_QR_code/","title":"I have not scanned the 2FA QR code","text":"<p>The QR code shown during the device registration cannot be regenerated or displayed again. If you do not capture the QR code, or lose the device storing the code (also called a token), you will be unable to access your account.</p> <p>To have your existing token deleted so another can be generated for your account, log in to my.nesi.org.nz and select the\u00a0option 'Manage Two-Factor token' under 'Account'.</p>","tags":["2fa","access","account","authentication"]},{"location":"General/FAQs/I_have_not_scanned_the_2FA_QR_code/#related-content","title":"Related content","text":"<p>How to replace my 2FA token</p>","tags":["2fa","access","account","authentication"]},{"location":"General/FAQs/Ive_run_out_of_storage_space/","title":"I've run out of storage space","text":"<p>There are two tracked resources in the NeSI filesystem, disk space and inodes (number of files).</p> <p>Trying to write to a filesystem over its inode or disk quota will cause an error (and probably kill your job).</p> <p>Current file-count and disk space can be found using\u00a0<code>nn_storage_quota</code>.</p> <pre><code>Filesystem         Available      Used     Use%     Inodes     IUsed     IUse%\nhome_user123             20G    1.957G    9.79%      92160     21052    22.84%\nproject_nesi99999         2T      798G   38.96%     100000     66951    66.95%\nnobackup_nesi99999              6.833T            10000000    2691383   26.91%\n</code></pre> <p>Note</p> <p>There is a delay between making changes to a filesystem and seeing the  change in\u00a0<code>nn_storage_quota</code>, immediate file count and disk space can  be found using the commands\u00a0<code>du --inodes</code>\u00a0and\u00a0<code>du -h</code>\u00a0respectively.</p> <p>There are a few ways to deal with file count problems</p>","tags":["disk quota exceeded"]},{"location":"General/FAQs/Ive_run_out_of_storage_space/#use-the-nobackup-filesystem","title":"Use\u00a0the nobackup filesystem","text":"<p>Your nobackup directories <code>/nesi/nobackup/&lt;projectcode&gt;</code> has a significantly higher inode count and no disk space limits. Files here are not backed up, so best used for intermediary or replaceable data.</p>","tags":["disk quota exceeded"]},{"location":"General/FAQs/Ive_run_out_of_storage_space/#delete-unnecessary-files","title":"Delete unnecessary\u00a0files","text":"<p>Some applications will generate a large number of files during runtime, using the command\u00a0<code>du --inodes -d 1 | sort -hr</code>\u00a0(for inodes) or\u00a0<code>du -h -d 1 | sort -hr</code>\u00a0for disk space.\u00a0\u00a0You can then drill down into the directories with the largest file count deleting files as\u00a0viable.</p>","tags":["disk quota exceeded"]},{"location":"General/FAQs/Ive_run_out_of_storage_space/#squashfs-archive","title":"SquashFS archive","text":"<p>Many files can be compressed into a single SquashFS archive. We have written a utility, <code>nn_archive_files</code>, to help with this process. This utility can be run on M\u0101ui or Mahuika, but not, as yet, on M\u0101ui-ancil; and it can submit the work as a Slurm job, which is preferred. <code>nn_archive_files</code>\u00a0can take, as trailing options, the same options as\u00a0<code>mksquashfs</code>, including choice of compression algorithm; see\u00a0<code>man mksquashfs</code>\u00a0for more details.  </p> <pre><code>nn_archive_files -p &lt;project-code&gt; -n &lt;num-processors&gt; -t &lt;time-limit&gt; --verify -- /path/containing/files /path2/containing/files\u00a0destination.squash\n</code></pre> <p>Then when files need to be accessed again they can be extracted using,</p> <pre><code>/usr/sbin/unsquashfs\u00a0destination.squash\n</code></pre> <p>You can do many other things with SquashFS archives, like quickly list the files in the archive, extract some but not all of the contents, and so on. See <code>man unsquashfs</code> for more details.</p>","tags":["disk quota exceeded"]},{"location":"General/FAQs/Ive_run_out_of_storage_space/#tarball","title":"Tarball","text":"<p>Many files can be compressed into a single 'tarball'</p> <pre><code>tar -czf name.tar /path/containing/files/\n</code></pre> <p>Then when files need to be accessed again they can be un-tarred using,</p> <pre><code>tar -xzf tarname.tar\n</code></pre>","tags":["disk quota exceeded"]},{"location":"General/FAQs/Ive_run_out_of_storage_space/#contact-support","title":"Contact Support","text":"<p>If you are following the recommendations here yet are still concerned about inodes or disk space, Contact our Support Team and we can raise the limit for you.</p>","tags":["disk quota exceeded"]},{"location":"General/FAQs/Login_Troubleshooting/","title":"Login Troubleshooting","text":"<p>Prerequisite</p> <ul> <li>Please make sure you have followed the recommended setup. See Choosing and Configuring Software for Connecting to the Clusters for more information.</li> <li>Most terminals do not give an indication of how many characters have been typed when entering a password.</li> <li>Paste is not usually bound to <code>ctrl</code> + <code>V</code> and will vary based on your method of access.</li> </ul>","tags":[]},{"location":"General/FAQs/Login_Troubleshooting/#repeatedly-asking-for-first-and-second-factor","title":"Repeatedly asking for First and Second Factor","text":"<p>In addition to using an incorrect First/Second factor there are several other issues that will cause a similar looking failure to log in.</p> <pre><code>Login Password:\nLogin Password:\nLogin Password:\n</code></pre> <p>OR</p> <pre><code>Login Password (First Factor): \nAuthenticator Code (Second Factor):\nLogin Password (First Factor): \nAuthenticator Code (Second Factor):\nLogin Password (First Factor): \nAuthenticator Code (Second Factor):\n</code></pre>","tags":[]},{"location":"General/FAQs/Login_Troubleshooting/#try-logging-in-to-lander-directly","title":"Try logging in to <code>lander</code> directly","text":"<p>You can test what part of your connection has failed by first running:</p> <pre><code>ssh &lt;user&gt;@lander.nesi.org.nz\n</code></pre> <p>If this succeeds: Run the following:</p> <pre><code>ssh login.&lt;mahuika/maui&gt;.nesi.org.nz\n</code></pre> <p>If this fails: Are you logging in to the correct cluster? Mahuika/Maui have separate access control, also M\u0101ui requires your password input in a different format, see here.</p> <p>If this succeeds:</p> <ul> <li>If you are using a bash terminal, confirm your .ssh config is set up correctly.</li> <li>If you are using a ssh client like MobaXterm or WinSCP make sure your session is set up correctly.</li> </ul>","tags":[]},{"location":"General/FAQs/Login_Troubleshooting/#check-you-are-a-member-of-an-active-project","title":"Check you are a member of an active project","text":"<p>If you are not a member of an active project, or your project has no active allocation, you will not be able to log in. You should be able to find whether you have any active projects with active allocations\u00a0here.</p>","tags":[]},{"location":"General/FAQs/Login_Troubleshooting/#confirm-you-are-using-the-correct-username-and-password","title":"Confirm you are using the correct username and password","text":"<p>The most common cause of login failure is using incorrect login details. Make sure you are using your NeSI Username and the password you set when first logging into the Lander node. See my.nesi.org.nz.</p>","tags":[]},{"location":"General/FAQs/Login_Troubleshooting/#check-the-time-on-your-device","title":"Check the time on your device","text":"<p>If the device you are using as authentication token is not using NZST/NZDT, or is not keeping the correct time, the second factor code generated will be invalid. Even an error of a few seconds can be enough to invalidate the second factor code.</p> <p>If your device can't keep time properly for whatever reason, please contact the person or team responsible for supporting it.</p>","tags":[]},{"location":"General/FAQs/Login_Troubleshooting/#ensure-youre-not-reusing-the-same-6-digit-code-from-your-token","title":"Ensure you're not reusing the same 6-digit code\u00a0from your token","text":"<p>Login will fail if the same 6-digit code\u00a0is used to access the M\u0101ui or Mahuika login node after it has been used to access the lander node, or for consecutive login attempts to any node. If in doubt, wait 30 seconds for a new token to be generated.</p>","tags":[]},{"location":"General/FAQs/Login_Troubleshooting/#ensure-the-correct-second-factor-token-is-being-used","title":"Ensure the correct Second Factor token is being used","text":"<p>Two-factor authentication is becoming a common security measure. Many people have multiple tokens and occasionally mix them up.</p>","tags":[]},{"location":"General/FAQs/Login_Troubleshooting/#wait-four-hours","title":"Wait four hours","text":"<p>Six failed login attempts within five minutes will trigger a four-hour lockout. Users experiencing login issues can inadvertently trigger the lockout, making diagnosing the original issue much more difficult.\u00a0\u00a0</p>","tags":[]},{"location":"General/FAQs/Login_Troubleshooting/#connection-closed-by-mobaxterm","title":"Connection closed by .... (MobaXterm)","text":"","tags":[]},{"location":"General/FAQs/Login_Troubleshooting/#skip-password-prompts","title":"Skip password prompts","text":"<p>There is a known MobaXterm bug in which a user who has set a second factor and is trying to log in to the lander node will be prompted multiple times for 'Password' before being prompted for 'First Factor'. (On the lander node, you should only be prompted for a 'password' if you have no Second Factor set up.)</p> <p>These initial prompts can be skipped through by pressing 'Enter'. Any input before pressing Enter will cause the login to fail.</p> <p>The expected processes is as follows:</p> <pre><code>ssh &lt;user&gt;@lander.nesi.org.nz \n&lt;user&gt;@lander.nesi.org.nz's password: &lt;Enter&gt;\n&lt;user&gt;@lander.nesi.org.nz's password: &lt;Enter&gt;\n&lt;user&gt;@lander.nesi.org.nz's password: &lt;Enter&gt;\nLogin Password (First Factor): \nAuthenticator Code (Second Factor):\n</code></pre> <p>Note: Sometimes MobaXterm will prompt with a dialogue box.</p>","tags":[]},{"location":"General/FAQs/Login_Troubleshooting/#update-your-mobaxterm-client","title":"Update your MobaXterm client","text":"<p>Occasionally an outdated client can cause errors. MobaXterm can be updated through: 'help&gt;check for updates'</p>","tags":[]},{"location":"General/FAQs/Login_Troubleshooting/#reinstall-your-mobaxterm-client","title":"Reinstall your MobaXterm client","text":"","tags":[]},{"location":"General/FAQs/Login_Troubleshooting/#asked-for-password-instead-of-first-factor","title":"Asked for 'Password' instead of 'First Factor'","text":"","tags":[]},{"location":"General/FAQs/Login_Troubleshooting/#check-password-status","title":"Check password status","text":"<p>Using\u00a0my.nesi.org.nz\u00a0and confirm you have an authentication token registered.</p>","tags":[]},{"location":"General/FAQs/Login_Troubleshooting/#see-above","title":"See above","text":"","tags":[]},{"location":"General/FAQs/Login_Troubleshooting/#authentication-token-manipulation-error","title":"Authentication token manipulation error","text":"<p>This occurs when your authentication token is out of sync. You will have to reset your token though\u00a0my.nesi.org.nz.</p>","tags":[]},{"location":"General/FAQs/Login_Troubleshooting/#nothing-here-has-helped","title":"Nothing here has helped?","text":"<p>Contact our Support Team.</p> <p>Helpful things to include:</p> <ul> <li>The client you are using (WSL, MobaXterm, Mac terminal, Linux,     etc.).</li> <li>The nature of the problem, including the precise text of any error     message you have been receiving.</li> <li>Did you start out having one login problem and are now getting a         different one? If so, when did the change happen, and were you         doing anything in particular related to logging in at the time         things changed?</li> <li>Have you successfully logged in in the past? if so when was the last     time you successfully logged in, and to what NeSI cluster?</li> <li>Has anything administrative and relevant to NeSI access changed     since you last logged in? For example:</li> <li>Have you opened or joined any new NeSI projects, or have any of         your existing NeSI projects closed?<ul> <li>Have any of your NeSI projects been granted new allocations, had     a previously granted new allocation actually start, or had an     existing allocation modified?</li> <li>Have any of your NeSI projects' existing allocations ended?</li> <li>Have any of your NeSI projects had a disk space quota change?</li> <li>Have you changed your institutional username and password, moved     to a different institution, or started a new job at an     institution while also keeping your position at your old     institution? Might NeSI know about any of these changes?</li> </ul> </li> <li>What have you tried so far?</li> <li>Are you on the NIWA network, the NIWA VPN, or neither?</li> </ul>","tags":[]},{"location":"General/FAQs/Mahuika_Maui_Differences/","title":"Mahuika - M\u0101ui Differences","text":"<p>Aside from differences in software stack there are a few other differences between the platforms to be aware of.</p>","tags":[]},{"location":"General/FAQs/Mahuika_Maui_Differences/#logging-in","title":"Logging in","text":"<p>Both Mahuika and M\u0101ui require logging in to the Lander node first.</p> <pre><code>ssh user123@lander.nesi.org.nz\n</code></pre> <p>As you log in to the Lander node, you can expect to receive the following prompts:</p> <pre><code>Login Password (First Factor):\n</code></pre> <pre><code>Authenticator Code (Second Factor):\n</code></pre> <p>Note that being prompted for <code>Authenticator Code (Second Factor)</code> does not prove that the system has accepted your <code>Login Password (First Factor)</code> as correct. If you enter either incorrectly, you will be prompted again for both.</p>","tags":[]},{"location":"General/FAQs/Mahuika_Maui_Differences/#mahuika","title":"Mahuika","text":"<p>Mahuika follows the same procedure as the lander node, except that it doesn't ask for a second factor.</p> <pre><code>ssh login.mahuika.nesi.org.nz\n</code></pre> <p>You will be prompted:</p> <pre><code>Login Password:\n</code></pre> <p>At this prompt, enter only your password (a.k.a. first factor).</p>","tags":[]},{"location":"General/FAQs/Mahuika_Maui_Differences/#maui","title":"M\u0101ui","text":"<p>M\u0101ui differs slightly in how you are authenticated the first time.</p> <pre><code>ssh login.maui.nesi.org.nz\n</code></pre> <p>You will be prompted.</p> <pre><code>Password:\n</code></pre> <p>At this prompt, enter only your password (a.k.a. first factor).</p>","tags":[]},{"location":"General/FAQs/Mahuika_Maui_Differences/#job-limits","title":"Job Limits","text":"<p>Both M\u0101ui and Mahuika have limits on the size and types of jobs you can run, but the limits on each machine is different.</p>","tags":[]},{"location":"General/FAQs/Mahuika_Maui_Differences/#mahuika_1","title":"Mahuika","text":"<p>Mahuika is made up of several partitions\u00a0which have different resources and different limits. A job can request up to 20,000 CPU core hours, running up to 3 weeks with up to 576 CPU cores (equivalent to eight full nodes). Furthermore, there are special nodes available with high memory (up to 6 TB) or GPUs. Depending on what resources you are requesting and for how long, your jobs will be automatically assigned to the most suitable partition. Mahuika allows the submission of jobs with variable numbers of CPUs and amounts of RAM (memory). The nodes your job is running on will probably be shared with other jobs.</p>","tags":[]},{"location":"General/FAQs/Mahuika_Maui_Differences/#maui_1","title":"M\u0101ui","text":"<p>M\u0101ui only has a single partition to which NeSI users are permitted to submit work. For your job, you can request a maximum of 24 hours or a maximum of 240 nodes, however no job may request more than 1,200 M\u0101ui node-hours in total. (This means that if you request more than 50 nodes, your maximum allowed time will start decreasing.) M\u0101ui only allows submission of jobs in units of nodes, so the smallest possible job takes a whole node, and there can never be more than one job on a node at a time.</p> <p>Additionally, projects with valid allocations on M\u0101ui will also have access to M\u0101ui's ancillary nodes, where jobs requiring up to 768 GB of memory or jobs that require GPUs can be run. When submitting a job to the M\u0101ui ancillary nodes you may also request parts of nodes, rather than needing to use the entire node. Because there are relatively few M\u0101ui ancillary nodes, if you require substantial amounts of time on nodes like the M\u0101ui ancillary nodes, we may grant your project an additional allocation on Mahuika. If we do so, we will not forbid you from using the M\u0101ui ancillary nodes while your M\u0101ui allocation remains valid and you are permitted to access NeSI clusters.</p>","tags":[]},{"location":"General/FAQs/Password_Expiry/","title":"Password Expiry","text":"<p>NeSI passwords expire after two years. \u00a0The prompt you will see when that happens is</p> <pre><code>Password expired. Change your password now.\nFirst Factor (Current Password): \nSecond Factor (optional): \nLogin Password: \n</code></pre> <p>however passwords can not be reset this way, instead you should reset your password via the My NeSI Portal.</p>","tags":[]},{"location":"General/FAQs/Skylake_warning_message_on_Maui/","title":"Skylake warning message on M\u0101ui","text":"<p>I get the following warning message, do I need to worry?</p> <pre><code>craype-x86-skylake requires cce/8.6 or later, intel/15.1 or later, or gcc/6.1 or later\n</code></pre>","tags":[]},{"location":"General/FAQs/Skylake_warning_message_on_Maui/#short","title":"Short","text":"<p>No. This is only a warning message from an interim state. And gets resolved immidiately afterwards.</p>","tags":[]},{"location":"General/FAQs/Skylake_warning_message_on_Maui/#more-details","title":"More details","text":"<p>Our software stacks are build with easybuild. There are toolchains defined, which wraps around the Cray <code>PrgEnv-???</code> modules. These toolchains are called <code>CrayIntel</code> , <code>CrayGNU</code> or <code>CrayCCE</code>. Within the procedure of swapping the Programming environments all active PrgEnv get unloaded before loading the new on. In that meantime the module <code>craype-x86-skylake</code> stays loaded and realizes that there is no recent compiler version available. Immediately afterwards the new programming environment get loaded and therewith its compiler, which solve the issue.</p> <p>You can check the actual situation by inspecting <code>module list</code></p> <p>There is a way to prevent the message: Assuming you want to load a certain toolchain or application. Let's say {code}VASP/5.4.4-CrayIntel-18.08{code} which is build with build Intel 18.08, we can first swap into the desired {code}PrgEnv{code}</p> <pre><code>module switch PrgEnv-cray PrgEnv-intel\nmodule load VASP/5.4.4-CrayIntel-23.02-19\n</code></pre> <p>OR</p> <pre><code>module unload craype-x86-skylake\nmodule load CrayIntel        # or any other PrgEnv change you intended to  do\nmodule load craype-x86-skylake\nmodule load VASP/5.4.4-CrayIntel-23.02-19\n</code></pre>","tags":[]},{"location":"General/FAQs/Two_Factor_Authentication_FAQ/","title":"Two Factor Authentication FAQ","text":"","tags":[]},{"location":"General/FAQs/Two_Factor_Authentication_FAQ/#what-is-a-second-factor","title":"What is a Second Factor?","text":"<p>Two Factor Authentication is a method of confirming a user's identity by requiring the user to present two pieces of evidence (\"factors\") to an authentication mechanism. In our system, the first factor is a password (something the user knows), while the second factor is a token generated by software on the user's mobile device (something the user possesses).</p>","tags":[]},{"location":"General/FAQs/Two_Factor_Authentication_FAQ/#what-do-i-need-in-order-to-obtain-a-second-factor-token","title":"What do I need in order to obtain a second factor token?","text":"<p>In order to obtain a second factor token, you will need a mobile device. On that mobile device, you will then need to install Google Authenticator (or another QR code reader application that implements the Time-based One Time Password algorithm) on your mobile device. See also Setting Up Two-Factor Authentication.</p>","tags":[]},{"location":"General/FAQs/Two_Factor_Authentication_FAQ/#can-i-use-an-ssh-key-pair-as-an-alternative-second-factor","title":"Can I use an SSH key pair as an alternative second factor?","text":"<p>We do not support SSH keys as a second factor and we have no plans to implement SSH key-based authentication. Only time-based codes are currently valid.</p>","tags":[]},{"location":"General/FAQs/Two_Factor_Authentication_FAQ/#how-do-i-transfer-my-old-second-factor-onto-my-new-device","title":"How do I transfer my old second factor onto my new device?","text":"","tags":[]},{"location":"General/FAQs/Two_Factor_Authentication_FAQ/#authy","title":"Authy","text":"<p>It should get synchronised automatically.</p>","tags":[]},{"location":"General/FAQs/Two_Factor_Authentication_FAQ/#google-authenticator","title":"Google Authenticator","text":"<p>Open Authenticator on both devices. On the original device tap the three-dot menu icon followed by\u00a0Transfer accounts, then Export accounts, select the accounts you want to keep and then press\u00a0Next. If these options are not present then first update your Authenticator. On the new device press\u00a0Import existing accounts then scan the QR code provided on the old device.</p>","tags":[]},{"location":"General/FAQs/Two_Factor_Authentication_FAQ/#how-do-i-get-a-new-second-factor","title":"How do I get a new Second Factor?","text":"<p>See article here.</p>","tags":[]},{"location":"General/FAQs/Two_Factor_Authentication_FAQ/#can-i-use-the-same-second-factor-again","title":"Can I use the same Second Factor again?","text":"<p>No. You can only use each Second Factor code once and must then wait for a new Second Factor to cycle. Attempting to use the same Second Factor will cause your Authentication to fail.</p>","tags":[]},{"location":"General/FAQs/What_are_my-bashrc_and-bash_profile_for/","title":"What are my .bashrc & .bash_profile for?","text":"<p>If you've been using Linux for a while, you'll have come across resource files. These files are typically read when you start a new instance of your shell, the program that interprets and executes the commands that you type in at your command prompt. But they're somewhat confusing, because there are several, and it's not obvious which are read and when.</p> <p>Warning</p> <p>This documentation is specific to the bash shell, which is our  chosen default shell for all users, and is the default for most Linux  machines. If you have chosen a different default shell, or have  started another shell manually on the command line, these notes will  apply with modifications, or not at all; please consult the  documentation for your shell.</p>","tags":[]},{"location":"General/FAQs/What_are_my-bashrc_and-bash_profile_for/#bashrc","title":"<code>~/.bashrc</code>","text":"<p>In a standard configuration, <code>~/.bashrc</code> is read when your shell session is interactive but not a login session. Because most of your sessions on the cluster will be login sessions, <code>~/.bashrc</code> will not ordinarily be read by default. It will, however, be read if you start an interactive shell within a shell, for instance by executing <code>bash</code> at the command line.</p>","tags":[]},{"location":"General/FAQs/What_are_my-bashrc_and-bash_profile_for/#bash_profile-and-profile","title":"<code>~/.bash_profile</code> (and <code>~/.profile</code>)","text":"<p>In a standard configuration, <code>~/.bash_profile</code> is read when your shell session is a login session. When you log in to the cluster, you will get a login session by default.</p> <p>The equivalent file in the Bourne shell (<code>sh</code>) is called <code>~/.profile</code>. Because the Bash shell is designed to be (mostly) compatible with the Bourne shell, if <code>bash</code> finds <code>~/.profile</code> but not <code>~/.bash_profile</code>, it will source <code>~/.profile</code> as if it were <code>~/.bash_profile</code>. For the rest of this article, however, we will assume that you're using <code>~/.bash_profile</code>.</p>","tags":[]},{"location":"General/FAQs/What_are_my-bashrc_and-bash_profile_for/#can-i-have-the-same-environment-variables-aliases-functions-etc-whether-my-shell-is-a-login-shell-or-not","title":"Can I have the same environment variables, aliases, functions, etc. whether my shell is a login shell or not?","text":"<p>Absolutely. You can define them in both <code>~/.bashrc</code> and <code>~/.bash_profile</code>. But this isn't the best way.</p>","tags":[]},{"location":"General/FAQs/What_are_my-bashrc_and-bash_profile_for/#i-thought-not-i-dont-want-to-repeat-myself","title":"I thought not. I don't want to repeat myself!","text":"<p>A wise choice. Repeating yourself is extra work, and dangerous in that if you later have to make a change, you may forget to do it everywhere it's needed.</p> <p>Fortunately, instead of repeating yourself, you can source one file from another. Most commonly, people source <code>~/.bashrc</code>, if it exists and can be read, from <code>~/.bash_profile</code>:</p> <pre><code># Put this code in your ~/.bash_profile\ntest -r ~/.bashrc &amp;&amp; . ~/.bashrc\n</code></pre> <p>This statement has the effect of ensuring that <code>~/.bashrc</code> will be loaded in all interactive shells (except those launched with special options intended to prevent such loading), instead of only in non-login shells.</p>","tags":[]},{"location":"General/FAQs/What_are_my-bashrc_and-bash_profile_for/#what-should-go-where","title":"What should go where?","text":"<p>That's largely up to you. However, we have found the following to be useful rules of thumb:</p> <ul> <li>Functions and aliases go in <code>~/.bashrc</code></li> <li>Modifications to <code>PATH</code> and <code>LD_LIBRARY_PATH</code> go in     <code>~/.bash_profile</code></li> </ul> <p>These are guidelines only and are subject to your specific working practices and how you expect your shells to behave.</p>","tags":[]},{"location":"General/FAQs/What_are_my-bashrc_and-bash_profile_for/#further-information","title":"Further information","text":"<p>You can find further information in the INVOCATION section of the bash man page, accessible by typing the command <code>man bash</code> at your terminal prompt.</p> <p>Alternatively, or for further commentary, there are many articles on the Internet that address this question in more detail. We do not vouch for the completeness or accuracy of any information published by third parties, and you rely on such information at your own risk.</p>","tags":[]},{"location":"General/FAQs/What_does_oom_kill_mean/","title":"What does \"oom-kill\" mean?","text":"<p>OOM stands for \"Out Of Memory\", and so an\u00a0error such as this:</p> <pre><code>slurmstepd: error: Detected 1 oom-kill event(s) in step 370626.batch cgroup\n</code></pre> <p>indicates that your job attempted to use more memory (RAM) than Slurm reserved for it. \u00a0</p> <p>OOM events can happen even without Slurm's <code>sacct</code> command reporting such a high memory usage, for two reasons:</p> <ul> <li>Unlike the enforcement via cgroups, Slurm's\u00a0accounting system only     records usage every 30 seconds, so sudden spikes in memory usage may     not be recorded, but can still trigger the OOM killer;</li> <li>Slurm's accounting system also does not include any temporary files     the job may have put in the memory-based <code>/tmp</code>\u00a0or <code>$TMPDIR</code>     filesystems.</li> </ul> <p>If you see an OOM event, you have two options. The easier option is to request more memory by increasing the value of the\u00a0<code>--mem</code> argument in your job submission script. The more difficult, but potentially more useful where it is feasible, is to make your job less memory-intensive.</p>","tags":["faq"]},{"location":"General/FAQs/What_is_a_-core_file/","title":"What is a '.core' file?","text":"<p>.core files are created when a program fails in a way that can't be handled by the program's regular error handling. Normally these failures are memory-related, such as the program asking for too much memory or for memory it can't legally access. The creation of a core file is called a 'core dump'.</p> <p>.core files are a record of the working memory at time of failure, and can be used for debugging. MPI jobs will usually create a .core file for each task.</p> <p>As .core files are usually very large, you should delete the ones you don't plan on using them to avoid filling up your\u00a0storage quota.</p>","tags":["corefile","coredump"]},{"location":"General/FAQs/What_software_environments_on_NeSI_are_optimised_for_Machine_Learning_and_data_science/","title":"What software environments on NeSI are optimised for Machine Learning and data science?","text":"<p>When using NeSI's HPC platform, you can bring your own code to install or you can access our extensive software library which is already built and compiled, ready for you to use.</p> <p>Examples of software environments on NeSI optimised for data science include:</p> <ul> <li> <p>R\u00a0and\u00a0Python\u00a0users     can get right into using and exploring the several built-in packages     or create custom code.</p> </li> <li> <p>Jupyter on NeSI     is     particularly well suited to artificial intelligence and machine     learning workloads. R     Studio     and/or Conda can be accessed via Jupyter.</p> </li> <li> <p>Commonly used data science environments and libraries such as     Keras,     LambdaStack,     Tensorflow     and Conda are available to     create comprehensive workflows.</p> </li> </ul> <p>For more information about available software and applications, you can\u00a0browse our catalogue here.</p> <p>As pictured in the screenshot below, you can type keywords into the catalogue's search field to browse by a specific software name or using more broad terms such as \"machine learning\".</p> <p></p> <p>For more information on NeSI's model and approach to application support, refer to our policy for the management of scientific application software.</p> <p>If you need help installing your software or would like to discuss your software needs with us, Contact our Support Team.</p>","tags":[]},{"location":"General/FAQs/Where_should_I_store_my_data_on_NeSI_systems/","title":"Where should I store my data on NeSI systems?","text":"Frequency of data being read Frequency of data being written Recommended option Often Often (at least once every two months) Store in your <code>/nobackup/&lt;projectcode&gt;</code> directory (but ensure key result data is copied to the persistent project directory). Often Seldom Store in your <code>/project/&lt;projectcode&gt;</code> directory. Seldom Seldom Apply for an allocation to use NeSI\u2019s long-term storage service or store elsewhere (e.g. at your institution). <p>In general, the project directory should be used for reference data, tools, and job submission and management scripts. The nobackup directory should be used for holding large reference working datasets (e.g., an extraction of compressed input data) and as a destination for writing and modifying temporary data. The nobackup directory can also be used to build and edit code, provided that the code is under version control and changes are regularly checked into upstream revision control systems. The long-term storage service should be used for larger datasets that you only access occasionally and do not need to change in situ.</p>","tags":[]},{"location":"General/FAQs/Why_am_I_seeing_Account_is_not_ready/","title":"Why am I seeing 'Account is not ready'?","text":"<p>If you don\u2019t see the \u2018Set Password\u2019 button, it means your information on our database is not ready yet, your user account has not yet been created, or you are not a member of an active project. In this case, please email support@nesi.org.nz and wait for a member of our support team to confirm your account and group membership.</p> <p></p>","tags":["access","account","mynesi"]},{"location":"General/FAQs/Why_cant_I_log_in_using_MobaXTerm/","title":"Why can't I log in using MobaXTerm?","text":"<p>Recent changes to our authentication system have caused some problems for people who log in to NeSI HPC systems using MobaXTerm on Windows.</p> <p>To fix these problems, you will need to do the following:</p>","tags":[]},{"location":"General/FAQs/Why_cant_I_log_in_using_MobaXTerm/#upgrade-or-re-install","title":"Upgrade or Re-install","text":"<p>Upgrade your MobaXTerm to the most recent stable version. You can download the most recent stable version from\u00a0https://mobaxterm.mobatek.net.</p>","tags":[]},{"location":"General/FAQs/Why_cant_I_log_in_using_MobaXTerm/#switch-browser-to-scp","title":"Switch Browser to SCP","text":"<p>If you have created saved sessions to connect to NeSI HPC facilities, open the settings for each such saved session and under the \"Advanced SSH settings\" tab, change the SSH browser type from SFTP to something else, such as \"SCP (enhanced speed)\".</p>","tags":[]},{"location":"General/FAQs/Why_cant_I_log_in_using_MobaXTerm/#repeating-password-prompts","title":"Repeating password prompts","text":"<p>If you are prompted multiple times for password (rather than First Factor), this is a bug. Entering any text will cause your login attempt to fail. The expected procedure is as follows.</p> <pre><code>ssh &lt;user&gt;@lander.nesi.org.nz\n&lt;user&gt;@lander.nesi.org.nz's password: &lt;Press Enter&gt; \n&lt;user&gt;@lander.nesi.org.nz's password: &lt;Press Enter&gt; \n&lt;user&gt;@lander.nesi.org.nz's password: &lt;Press Enter&gt;\nLogin Password (First Factor):\nAuthenticator Code (Second Factor):\n</code></pre>","tags":[]},{"location":"General/FAQs/Why_cant_I_log_in_using_MobaXTerm/#delete-saved-credentials","title":"Delete Saved Credentials","text":"<p>It's possible that, even with a fresh install of mobaXterm it is still trying to use your old password from credential manager.</p> <ol> <li>Go to Settings-&gt;Configuration and go to the General tab and click     on MobaXterm password management</li> <li>You should see the credentials for NeSI hosts (<code>lander</code>, <code>mahuika</code>,     <code>maui</code>)</li> <li>Remove all entries.</li> <li>Restart MobaXterm</li> <li>Try logging in again</li> </ol> <p>For more information about how to log in to our HPC facilities, please see this article, and for more login troubleshooting here.</p>","tags":[]},{"location":"General/FAQs/Why_does_my_program_crash/","title":"Why does my program crash?","text":"<p>There are many different reasons why an application could crash. We cannot list all the different possibilities, but we will help you to investigate.</p>","tags":[]},{"location":"General/FAQs/Why_does_my_program_crash/#oom","title":"OOM","text":"<p>One common reason is a limited amount of memory. Then the application could crash with an Out Of Memory exception.</p>","tags":[]},{"location":"General/FAQs/Why_does_my_program_crash/#stack-size","title":"Stack size","text":"<p>On our XC system (M\u0101ui) the stack size is limited. If you application needs more resources on stack, if could result in strange unpredictable crashes. You could get for example an `array index out of bounds` error, not directly pointing to the source of the issue. You can try to unlimit stack size using `ulimit -s unlimited` in your submission script.</p>","tags":[]},{"location":"General/FAQs/Why_does_my_program_crash/#debugger","title":"Debugger","text":"<p>Another common issue is an error in the code. For example an application could (may to unexpected input and missing error handling) call a division by 0. Debugger can help to find the source of the issue. On the NeSI systems are different debuggers available. For serial application the Gnu debugger gdb is available. Furthermore, the ARM DDT debugger is available, which can handle, parallel, serial, applications, written in C/C++, Fortran, and Python (limited support).</p>","tags":[]},{"location":"General/FAQs/Why_is_my_job_taking_a_long_time_to_start/","title":"Why is my job taking a long time to start?","text":"<p>If you think your job is taking unexpectedly long to start running, there are several possible causes.</p> <ul> <li>Scheduled maintenance</li> <li>Delays in the queue<ul> <li>Your job is being beaten by other high-priority jobs<ul> <li>Your project has a low Fair Share score</li> <li>Your project has a high Fair Share score, but there are lots of other jobs from similarly high-priority projects</li> </ul> </li> <li>Your job's resource demands are hard to satisfy</li> <li>Some other problem</li> </ul> </li> </ul>","tags":[]},{"location":"General/FAQs/Why_is_my_job_taking_a_long_time_to_start/#scheduled-maintenance","title":"Scheduled maintenance","text":"<p>First, check our status page\u00a0to look for scheduled maintenance periods. If your job would otherwise run on a cluster during maintenance work affecting that cluster, your job may be delayed until after the maintenance work is completed and the cluster returns to service, depending on the nature of the work to be done.</p>","tags":[]},{"location":"General/FAQs/Why_is_my_job_taking_a_long_time_to_start/#delays-in-the-queue","title":"Delays in the queue","text":"<p>If your job is not supposed to be affected by a maintenance period, you can get more information by running this command:</p> <pre><code>nn_my_queued_jobs\n</code></pre> <p>This command will, for each of your queued jobs, produce an output looking something like this:</p> <pre><code>$ nn_my_queued_jobs \nACCOUNT                JOBID NAME                 SUBMIT_TIME         QOS    NODE CPUS MIN_MEMORY PRIORITY START_TIME          REASON\nnesi99999           12345678 SomeRandomJob        2019-01-01T12:00:00 collab    1    8         2G     1553        N/A          QOSMaxCpuPerJobLimit\n</code></pre> <p>One of the most useful columns to look at, far over on the right hand side, is the \"Reason\" column. This column tells you why the job is delayed. Common answers include \"Priority\", \"Resources\", \"Dependency\", \"ReqNodeNotAvail\", and others.</p> <ul> <li>Priority means that the job just isn't in the front of the queue     yet.</li> <li>Resources means that there are not currently enough free     resources to run the job.</li> <li>Dependency means the job is in some way dependent on another,     and the other job (the dependency) has not yet reached the required     state.</li> <li>ReqNodeNotAvail means that the job has requested some specific     node that is busy working on other jobs, is out of service, or does     not exist.</li> </ul> <p>A more comprehensive list of job reason codes is available here (offsite). As noted on that page, if a job is waiting for execution for several reasons, only one reason will be displayed, and there is not a documented importance of reasons. For example, one job could say Priority and another could say Resources, when in fact both are waiting due to Priority and Resources at the same time.</p>","tags":[]},{"location":"General/FAQs/Why_is_my_job_taking_a_long_time_to_start/#other-high-priority-jobs","title":"Other high-priority jobs","text":"<p>You can check the job's priority relative to other waiting jobs by means of the following command on a Mahuika or M\u0101ui login node (as appropriate):</p> <pre><code>nn_job_priorities\n</code></pre> <p>This command is intended to produce the same results as the native Slurm command\u00a0<code>sprio</code>, but with jobs sorted in order of priority from highest to lowest.</p> <p>The output should look something like this:</p> <pre><code>          JOBID PARTITION   PRIORITY        AGE   FAIRSHARE    JOBSIZE        QOS\n         793492 gpu             1553        504        1000         20         30\n        2008465 long            1107        336         723         18         30\n        2039471 long            1083        312         723         18         30\n        2039456 long            1083        312         723         18         30\n        2039452 long            1083        312         723         18         30\n        2039435 long            1083        312         723         18         30\n        2039399 long            1083        312         723         18         30\n        2039391 long            1083        312         723         18         30\n        2039376 long            1083        312         723         18         30\n        2039371 long            1083        312         723         18         30\n...\n</code></pre> <p>The important aspect of this list is not your job's numeric priority score, but rather its priority ranking compared to other jobs.</p>","tags":[]},{"location":"General/FAQs/Why_is_my_job_taking_a_long_time_to_start/#low-fair-share-score","title":"Low Fair Share score","text":"<p>If, compared to other jobs in the queue, your job's priority (third column) and fair share score (fifth column) are both low, this usually means that your project team has recently been using through CPU core hours faster than expected. See this page for more information on Fair Share, how you can check your project's fair share score, and what you can do about a low project fair share score.</p>","tags":[]},{"location":"General/FAQs/Why_is_my_job_taking_a_long_time_to_start/#queue-congestion","title":"Queue Congestion","text":"<p>In the unlikely scenario that your job's position in the list is low but your job's fair share score is high (i.e. nearly 1,000), you will just have to wait, as this scenario supposes that other jobs like yours are ahead in the queue because they have similar resource needs and similar (or even higher) Fair Share scores but have been waiting for even longer than your job. This condition is called queue congestion, and arises when researchers submit a lot of work at about the same time. Because it is triggered by aggregate researcher behaviour, there isn't much we can do about it.</p>","tags":[]},{"location":"General/FAQs/Why_is_my_job_taking_a_long_time_to_start/#difficult-job","title":"Difficult Job","text":"<p>If your job's priority is high compared to other jobs but the job still won't start,\u00a0make sure that your resource requests (in your Slurm script) are appropriate. If the nature of your work allows, you could try:</p> <ul> <li>Being more flexible about where in the cluster CPU cores come from</li> <li>Asking for less memory (RAM)</li> <li>Asking for a shorter wall time</li> </ul> <p>You can use the <code>scontrol</code> command to reduce the job's requested wall time limit, for example the following command will set the wall time limit of job 12345678 to one hour:</p> <pre><code>scontrol update jobid=12345678 TimeLimit=01:00:00\n</code></pre> <p>The <code>scontrol update</code> command does not print out any message to say that it has succeeded, so you can check its effect using <code>scontrol show</code>:</p> <pre><code>scontrol show job 12345678 | grep TimeLimit\n   RunTime=00:00:00 TimeLimit=00:01:00 TimeMin=N/A\n</code></pre> <p>Note that you can not yourself use <code>scontrol</code> to increase a job's requested wall time.</p> <p>You can not adjust the amount of memory (RAM) using <code>scontrol</code>, and altering the number of tasks, nodes or cores requested is unwise without making corresponding changes in the job submission script, and in some cases the program's input file as well. For this reason, if you wish to change the amount of memory (RAM) or the number or arrangement of cores, you should cancel your queued job using the <code>scancel</code> command and then resubmit it. If your project's fair share score is high, your newly submitted job should progress quickly through the queue.</p>","tags":[]},{"location":"General/FAQs/Why_is_my_job_taking_a_long_time_to_start/#other-problem","title":"Other problem","text":"<p>If your job priority is high, your resource requests are low and your job still won't start, please  Contact our Support Team and we will look into the problem.</p>","tags":[]},{"location":"General/NeSI_Policies/Acceptable_Use_Policy/","title":"Acceptable Use Policy","text":"<p>See Acceptable Use Policy\u00a0for the current version of the NeSI Acceptable Use Policy.</p> <p>The NeSI Acceptable Use Policy is presented to each user when they register. By clicking the \"I accept the Terms and Conditions of Access\" button at registration you are confirming your understanding and willingness to abide by the conditions of usage of the NeSI Services. If you do not agree to these Terms and Conditions, we will not be able to register you or allow you to use the NeSI Services.</p>","tags":[]},{"location":"General/NeSI_Policies/Access_Policy/","title":"Access Policy","text":"<p>See Access Policy\u00a0for the current version of the NeSI Access Policy.</p> <p>Our Access Policy provides essential information for researchers accessing the following NeSI services:</p> <ul> <li>HPC Compute and Analytics \u2013 provides access to     HPC platforms     that host a broad range of high-performance     software applications and libraries.</li> <li>Consultancy and Training \u2013 provides access to     expert scientific software programmers and     training workshops respectively.</li> </ul>","tags":[]},{"location":"General/NeSI_Policies/Account_Requests_for_non_Tuakiri_Members/","title":"Account Requests for non-Tuakiri Members","text":"<p>Most New Zealand universities and Crown Research Institutes are members of the Tuakiri authentication federation, but many other institutions, including private sector organisations and most central and local government agencies, are not. If you are not affiliated with an organisation supported by the Tuakiri federation, you can request access via my.nesi.org.nz/register.</p> <p></p> <p>Prerequisite</p> <p>The email address you use on your application must be your  institutional email address. We do not accept applications using  personal email addresses.</p> <p>We will review your request and, if we approve it, we will create a Tuakiri Virtual Home account for you, which you can use to login to my.nesi.org.nz. Once we have done so, the Tuakiri system will send you an automatically generated email inviting you to activate your account. You will need to activate your account before you can log in to my.nesi.org.nz.</p> <p>What if I don't get the account activation email?</p> <p>Some organisations' email servers are known to block Tuakiri's account  activation emails. If you haven't received your Tuakiri account  activation email by the end of the next business day after you applied  for an account, please check your junk mail and/or quarantine folders.  If you still can't find the email,\u00a0Contact our Support Team.</p> <p>What next?</p> <ul> <li>Project      Eligibility</li> <li>Applying for a new      project.</li> <li>Applying to join an existing      project.</li> </ul>","tags":["onboarding","tuakiri","mynesi"]},{"location":"General/NeSI_Policies/Acknowledgement-Citation_and_Publication/","title":"Acknowledgement, Citation and Publication","text":"<p>See Acknowledgement And Publication.</p>","tags":[]},{"location":"General/NeSI_Policies/Allocation_classes/","title":"Allocation classes","text":"<p>The NeSI access policy, agreed by representatives of the Crown and NeSI's partner institutions, establishes several allocation classes. In general terms, the allocation class granted to your project is used to decide who pays for that aspect of your project's consumption of NeSI services.</p> <p>Allocations Granted:</p> <p>Any time</p> <p>Allocation Period:</p> <p>Up to 6 months</p> <p>Maximum Allocation:</p> <ul> <li>2,000 Mahuika compute units per month</li> <li>40 M\u0101ui node hours per month</li> <li>Online storage dependent on availability</li> <li>Nearline storage dependent on availability</li> </ul> <p>Priority:\u00a0</p> Moderate\u00a0 Allocations Granted: <p>Quarterly</p> <p> </p> <p>A new project's initial allocation may be awarded from this class at any time, but subject to the limits of Proposal Development allocations.</p> Allocation Period: Up to 12 months, renewable indefinitely (subject to continued eligibility and approval) Maximum Allocation: <ul> <li>1,000,000 Mahuika compute units</li> <li>25,000 M\u0101ui node hours</li> <li>Online storage dependent on availability</li> <li>Nearline storage dependent on availability</li> </ul> Priority: Highest Allocations Granted: <p>Quarterly</p> <p> </p> <p>A new project's initial allocation may be awarded from this class at any time, but subject to the limits of Proposal Development allocations.</p> Allocation Period: Up to 12 months, renewable indefinitely (subject to continued eligibility and approval) Maximum Allocation: <ul> <li>500,000 Mahuika compute units</li> <li>12,500 M\u0101ui node hours</li> <li>Online storage dependent on availability</li> <li>Nearline storage dependent on availability</li> </ul> Low Allocations Granted:\u00a0 <p>Any time</p> <p> </p> <p>A new project's initial allocation may be awarded from this class, but subject to the limits of Proposal Development allocations.</p> Allocation Period: Up to 12 months, renewable indefinitely (subject to continued eligibility and approval) Maximum Allocation: Dependent on availability, institutional entitlements and institutional resource allocation decisions Priority: High <p>If you only qualify for a Proposal Development allocation right now, please Contact our Support Team before applying for a new project. Our Engagement Team is happy to talk to you and representatives of your institution about Subscription options. It's best to start talking to us early as Subscription contracts can take a while to set up.</p> <p>Our team is happy to answer any questions you may have throughout the application process.</p> <p>For more information, see how we review applications.</p>","tags":[]},{"location":"General/NeSI_Policies/Allocation_classes/#prop","title":"Proposal Development","text":"<p>A short-term allocation available to researchers from any New Zealand research institution. During this allocation you will:</p> <ul> <li>Find out whether your software works on one of the NeSI platforms.</li> <li>See how your software scales when you run it on more processors.</li> <li>Start estimating how many core hours per year you will need for your project.</li> </ul> <p>Once your Proposal Development allocation is well underway (or has ended), you are welcome to apply for a further allocation, which will be granted from one of our other allocation classes.</p> <p>Depending on your institutional affiliation and your project's satisfaction of eligibility criteria, your project's initial allocation may have the same limits as a Proposal Development allocation but be from one of our other allocation classes.</p>","tags":[]},{"location":"General/NeSI_Policies/Allocation_classes/#merit","title":"Merit","text":"<p>Intended for highly skilled research teams carrying out high quality research funded via a peer review process that supports the\u00a0New Zealand Government's Science Goals.</p> <p>A named investigator on a peer reviewed research grant or contract covering the entire allocation period. Students under the supervision of a named investigator are eligible at NeSI's discretion.</p> <p>If your institution is a NeSI Collaborator, your project's allocation, if approved, will be provided out of your institution's entitlement.</p> <p>Examples likely to qualify:</p> <ul> <li>MBIE managed research funds</li> <li>Health Research Council funding</li> <li>Royal Society of New Zealand funding, e.g. Marsden grants</li> <li>SSIF programme funding (previously known as CRI Core funding)</li> <li>Research programmes forming part of a National Science Challenge</li> <li>Research programmes forming part of a Centre of Research Excellence (CoRE)</li> </ul> <p>Those engaged in privately funded research or research funded by a foreign government are unlikely to qualify.</p>","tags":[]},{"location":"General/NeSI_Policies/Allocation_classes/#postgrad","title":"Postgraduate","text":"<p>Standard allocation for anyone enrolled at a New Zealand degree-granting institution and working on a postgraduate research programme (e.g. PhD or Masters by research) approved by that institution. If your institution is a NeSI Collaborator, your project's allocation, if approved, will be provided out of your institution's entitlement.</p> <p>Applicants in undergraduate programmes (including Honours programmes) or graduate programmes based on coursework are not eligible for Postgraduate allocations.</p>","tags":[]},{"location":"General/NeSI_Policies/Allocation_classes/#institutional","title":"Institutional","text":"<p>Available if you work for or study at one of the institutions that is a NeSI collaborator, or has a subscription to the relevant NeSI service.</p> <p>Allocations from an institution's entitlement are subject to approval by that institution. Some institutions require pre-approval or recover costs for their NeSI usage through internal charging.</p> <p>If you are applying in your capacity as a staff member or student at a NeSI collaborator, your project will be considered for an Institutional allocation rather than a Merit or Postgraduate allocation.</p>","tags":[]},{"location":"General/NeSI_Policies/Allocation_classes/#prerequisites","title":"Warning","text":"<p>If you are employed by and/or study at multiple institutions, it is your responsibility to find out which of those institutions is supporting your proposed work with NeSI. Whether and how your research is eligible to receive allocations of NeSI resources depends on its host institution.</p>","tags":[]},{"location":"General/NeSI_Policies/How_we_review_applications/","title":"How we review applications","text":"<p>When you submit your application through our web site, it will go to our technical support team for review. In general, our review process for new projects is as follows:</p> <ol> <li>Initial check: We see whether your proposal describes a     legitimate research programme and whether your research programme     will need some kind of advanced research computing capability (which     may or may not be high-performance computing). We also check whether     your project team is all assembled and has the skills needed to     start using our systems (for example, basic familiarity with the     Linux command line).     If you are a NIWA researcher, we will also confirm with NIWA's     institutional point of contact that you have followed the     NIWA internal documentation for gaining access to the HPCs.     You will only be able to access the NIWA internal documentation if     you are currently behind the NIWA VPN or on NIWA's internal network.</li> <li>Software check:\u00a0One of our technical experts looks at the     software you say you want to use and determines whether it can run     on any of our systems and whether you are likely to be legally     allowed to run the software on NeSI. This check is intended to cover     both compatibility and licensing matters, as well as whether you are     able and willing to compile or install the software yourself if     necessary.</li> <li>Support check: Some research programmes may have very demanding     support needs. We will consider whether we are able to offer the     kind and amount of support your team is likely to need to progress     your research if we approve it. This check is especially important     if we think you are likely to want or need to change someone else's     code. We may consult with our scientific programmers at this point,     and find out whether your project is likely to be eligible for our     consultancy service.</li> <li>Disk space check: We decide how much disk space your project is     likely to need in the persistent storage (project directory) and     scratch storage (nobackup directory). We may unfortunately have to     reject (or negotiate for less storage) if your disk space needs     would interfere with our ability to provide good service to other     research teams.</li> <li>Facility: Based on the information in your application, we     decide whether your workflow is best suited for Mahuika, M\u0101ui or     both, and also whether your project would benefit from an allocation     of GPU hours or access to ancillary nodes or virtual labs.</li> <li>Decision and notification: If we approve an initial allocation     for your project, we will typically award the project an     allocation of Mahuika compute units, M\u0101ui node hours, or both, and also an online storage allocation,     from one of our allocation classes.     In an case, we will send you an email telling you about our decision.</li> </ol> <p>Our review process for requests for new allocations on existing projects is simpler:</p> <ol> <li>Eligibility check: We look at the information you have given us     (and may ask you more questions) to find out which of our regular     allocation classes     (Merit,     Postgraduate     or     Institutional)     this research programme is eligible to receive. Your research     programme may be eligible for more than one allocation class.</li> <li>Amount and duration: We will calculate the approximate amount of     compute resources you are likely to need based on what kind of     allocation you most recently received and your usage history. We may     suggest an allocation size (i.e. a number of Mahuika compute units     or M\u0101ui node hours) and a duration of up to 12 months, and give you     a chance to provide feedback if you think our suggested allocation     would not meet your needs.</li> <li>Choice of Class and Contention: We will choose from which class     to award your allocation, based on your research programme's     eligibility for the different classes and whether your proposed     allocation would exceed any class-based allocation limits.     We may change this choice depending on which classes, if any, are     under contention.</li> <li>Approval: If we decide that your project should be considered     for an Institutional allocation, the request may need to be approved     by a representative of the project's host institution, which is the     institution where the project owner works or studies.</li> <li>Decision and notification: We will send you an email telling you     about our decision.</li> </ol> <p>From time to time we may have to decline requests for allocations of computing resources. If we can't grant your research programme an allocation because of contention or because the project's host institution has disallowed the allocation, this is not to be taken as a judgement on the merit of your research topic or the quality of your team's work.</p>","tags":["tqp"]},{"location":"General/NeSI_Policies/Institutional_allocations/","title":"Institutional allocations","text":"<p>Your project will typically be considered for an Institutional allocation if you work for or study at one of the institutions that is a NeSI collaborator, or has a subscription to our HPC Compute &amp; Analytics service. If you are unsure whether or not your institution has a current subscription with NeSI, please ask us via Contact our Support Team.</p> <p>Allocations from an institution's entitlement are subject to approval by that institution. Some institutions require pre-approval or recover costs for their HPC usage through internal charging. You are welcome to begin the application process by applying for a new project online.  Our team is happy to answer any questions you may have through the application process, and we will let you know if you are expected to directly seek approval from your institution.</p> <p>If you are a postgraduate student at a NeSI collaborator, your project will likely be considered for an Institutional allocation rather than a Merit or Postgraduate allocation.</p> <p>Read more about how we review applications.</p> <p>To learn more about NeSI Projects or to apply for a new project, please read our article\u00a0Applying for a NeSI Project.</p>","tags":["info","assessment"]},{"location":"General/NeSI_Policies/Merit_allocations/","title":"Merit allocations","text":"<p>This is the highest award given for use of NeSI services. A Merit allocation is intended for highly skilled research teams carrying out high quality research funded via a peer review process that supports the New Zealand Government's Science Goals. Merit allocations may be made for the HPC Compute &amp; Analytics and Consultancy services.</p> <p>To be eligible for consideration for a Merit allocation, the application must meet the following criteria:</p> <ul> <li>The underpinning research programme (that requires access to NeSI     HPC services to achieve the objectives of the research) must support     the Government\u2019s Science     Goals.</li> <li>To demonstrate research quality and alignment with national research     priorities, the research funding must have come from a     peer-reviewed, contestable process at an institutional, regional or     national level.<ul> <li>The following funding sources are likely to qualify:<ul> <li>Research funds managed by the Ministry of Business,     Innovation and Employment (MBIE)</li> <li>Health Research Council funding</li> <li>Royal Society of New Zealand funding, e.g. Marsden grants</li> <li>SSIF programme funding (previously known as CRI Core     funding)</li> <li>Research programmes forming part of a National Science     Challenge</li> <li>Research programmes forming part of a Centre of Research     Excellence (CoRE)</li> <li>Other similar funding sources</li> </ul> </li> <li>The following funding sources are unlikely to qualify:<ul> <li>Privately funded research</li> <li>Research funded by a foreign government</li> </ul> </li> </ul> </li> <li>The research grant or contract must cover the entire period for     which an allocation of NeSI resources is sought.</li> <li>The applicant must be a named investigator on the peer reviewed     research grant or contract. If you are a student, we may at our     discretion consider your application for a Merit award if your     supervisor is a named investigator.</li> </ul> <p>Read more about how we review applications.</p> <p>To learn more about NeSI Projects or to apply for a new project, please read our article\u00a0Applying for a NeSI Project.</p>","tags":["info","allocation","assessment"]},{"location":"General/NeSI_Policies/NeSI_Application_Support_Model/","title":"NeSI Application Support Model","text":"<p>The NeSI policy for management of scientific application software is based on the following principles:</p> <ul> <li>NeSI will install and maintain software in a central location if it     will be useful to a number of users, or if the effort to install it     is small.</li> <li>Users may install software in their <code>/home</code> or <code>/nesi/project/</code>  directories, provided that they have a license for the software (if     needed) that permits the software to be used on NeSI systems.</li> <li>NeSI will provide users with a reasonable amount of help when they     are installing their own software.</li> </ul>","tags":["support","info","application"]},{"location":"General/NeSI_Policies/NeSI_Licence_Policy/","title":"NeSI Licence Policy","text":"<p>With very few exceptions, NeSI does not hold software licences of its own. If you wish to use any of the proprietary software installed on the NeSI cluster, you, or more likely your institution or department, will need to have an appropriate licence.</p> <p>Warning</p> <p>Slurm and many other applications use the American spelling of the  noun, \"license\".</p>","tags":[]},{"location":"General/NeSI_Policies/NeSI_Licence_Policy/#licence-servers","title":"Licence Servers","text":"<p>The most common method of licence control is using 'floating' network licences hosted on an external server. In order for a user on a NeSI cluster to check out a licence, the address of that server must be known, and a firewall exception made for NeSI IP addresses (see below).</p> <p>NeSI's public-facing IP addresses are:</p> <pre><code>202.36.29.252\n202.36.29.253\n103.229.249.252\n103.229.249.253\n</code></pre>","tags":[]},{"location":"General/NeSI_Policies/NeSI_Licence_Policy/#institutional-licences","title":"Institutional Licences","text":"<p>A list of already established licence server connections can be found in the NeSI support documentation for the relevant software. Provided you are a member of the listed institution or faculty you should be able to use the software without any set-up.</p> <p>If you believe you should have access to a licence but do not, or would like to organise remote use of your institution's licence, please Contact our Support Team\u00a0and cc: your own IT services.</p>","tags":[]},{"location":"General/NeSI_Policies/NeSI_Licence_Policy/#personal-or-research-group-licences","title":"Personal or Research Group Licences","text":"<p>You are welcome to make your own licence arrangements, if your institution doesn't have a licence or you need to access extra features of the software that aren't included in your institution's default package. This may involve you setting up an additional licence server at your institution, or on a personal machine. Your licence server must run on a computer that has a static IP address and is visible on the public internet. Setting up your computer in this way and securing it is beyond the scope of this article, but will involve talking to your institution's IT department or to your internet service provider (ISP).</p>","tags":[]},{"location":"General/NeSI_Policies/NeSI_Licence_Policy/#licence-files","title":"Licence Files","text":"<p>An alternative to licence servers, used by some programs, is a licence file that contains a code issued to that user or group during the registration process.</p> <p>This approach is simpler for us to set up, as there is no need to communicate with a remote licence server. However, the onus is on the licensee to provide us with a copy of the licence file. Generally this will be done during installation of the software.</p> <p>It is important for us to know who is eligible to use any particular licence file, so that we don't accidentally allow unauthorised persons to use a given piece of software.</p>","tags":[]},{"location":"General/NeSI_Policies/NeSI_Licence_Policy/#software-without-built-in-licence-management","title":"Software without built-in licence management","text":"<p>Some software packages do not provide their own licence management systems (servers, files, etc.). The owners of these packages rely on us as the system administrators to prevent unauthorised use.</p> <p>Access to such a piece of software is usually by adding people to a NeSI-maintained group of authorised users, which we refer to as a \"software group\". Among NeSI users, only members of the software group for that particular package will be permitted to see or interact with the package.</p> <p>Before adding any particular person to a software group, we may ask to see a licence agreement allowing that person to use the software. We may also check to see whether the licence agreement forbids the person from using the software on NeSI.</p> <p>Prerequisite</p> <p>Some licence agreements are quite restrictive in terms of where, or on  what sort of machine, a licensee may run the program. For example, the  licence may require one or more of the following:  -   The software may only be run on one computer (node) at a time.  -   Any computer on which the software is run must be owned by the      user's employing institution, operated by employees of that      institution, or both.  -   There may be other restrictions, like a limit to the number of      simultaneous tasks or threads you are permitted to run.  We may not have seen your licence agreement, and even if we have,  we're not intellectual property lawyers. Just because we grant you  access to a piece of software it doesn't necessarily mean you're  authorised to use it in the way you intend. It is your  responsibility to ensure that your use of the software on NeSI  complies with the terms of your licence or is otherwise permitted by  law.</p>","tags":[]},{"location":"General/NeSI_Policies/NeSI_Licence_Policy/#slurm-tokens","title":"Slurm Tokens","text":"<p>We encourage the use of Slurm licence tokens in your batch scripts, for example:</p> <pre><code>#SBATCH --licenses ansys_hpc@uoa_foe:60,ansys_r@uoa_foe\n</code></pre> <p>will request 60 'hpc' licences and 1 'research' licence from the University of Auckland Engineering licence server. This will prevent your job starting until the specified number of licences is available.</p> <p>By not including the correct number of Slurm licence tokens you run the risk of a job failing on startup (your job will have to re-queue), or worse, idling until a licence becomes available (wasting CPU hours and likely leading to a timeout).</p> <p>The names of the Slurm licence tokens are included in the application-specific documentation.</p> <p>Note</p> <p>Slurm licence reservations work independently of the licence server.  Not including a Slurm token will not prevent your job from running,  not will including one modify how your job runs (only when it runs).</p>","tags":[]},{"location":"General/NeSI_Policies/NeSI_Password_Policy/","title":"NeSI Password Policy","text":"<p>The NeSI password policy is as follows:</p> <ul> <li>Your password must be at least 12 characters long</li> <li>Your password must contain one or more characters from at least two     of the following classes:<ul> <li>uppercase letters</li> <li>lowercase letters</li> <li>numbers</li> <li>special characters\u00a0(excluding '&amp;&lt;&gt;\\')</li> </ul> </li> <li>Passwords expire after 2 years (730 days)</li> <li>When resetting a password ensure that it is not similar to the     previous password(s) as that will cause the new password to be     rejected.</li> </ul>","tags":[]},{"location":"General/NeSI_Policies/NeSI_Privacy_Policy/","title":"NeSI Privacy Policy","text":"<p>See https://www.nesi.org.nz/nesi-privacy-policy for the current version of the NeSI Privacy Policy.  </p> <p>The Policy outlines what personal information NeSI collects. How it is stored and used. How users can request access, correct or delete information, and under what circumstances NeSI will disclose it.</p>","tags":[]},{"location":"General/NeSI_Policies/Postgraduate_allocations/","title":"Postgraduate allocations","text":"<p>The Postgraduate allocation class supports capability development of postgraduate students enrolled at a New Zealand university.</p> <p>To be considered for a Postgraduate allocation, your application must satisfy the following criteria:</p> <ul> <li>You must be enrolled at a New Zealand degree-granting institution     and working on a postgraduate research programme (e.g. PhD or     Masters by research) approved by that institution. Applicants in     undergraduate programmes (including Honours programmes) or graduate     programmes based on coursework are not eligible.</li> </ul> <p>Even if your application satisfies these criteria, we may not award your project an allocation from the Postgraduate class:</p> <ul> <li>If your institution is a NeSI Collaborator or Subscriber, your     project's allocation will most likely be made from your     institution's entitlement.</li> <li>If you have not used an HPC previously, we may award your project a     Proposal Development allocation first. In this case, your project     may be considered for a Postgraduate allocation after your Proposal     Development allocation is complete.</li> <li>Some allocation requests may be declined, or alternatively postponed  until a later time, if there is insufficient computing capacity     available to meet demand.</li> </ul> <p>Read more about how we review applications.</p> <p>To learn more about NeSI Projects, and to apply please review the content of the section entitled Applying for a NeSI Project.</p>","tags":["info","assessment"]},{"location":"General/NeSI_Policies/Proposal_Development_allocations/","title":"Proposal Development allocations","text":"<p>A Proposal Development allocation is a short-term allocation of up to 2,000 compute units per month (on Mahuika) or 50 node-hours per month (on M\u0101ui) or both, for up to six months. During your Proposal Development allocation you can find out:</p> <ul> <li>whether your software can run on a NeSI HPC,</li> <li>how your software scales to multiple cores or across compute nodes,</li> <li>approximately how many compute units or node hours your research     project is likely to need.</li> </ul> <p>If:</p> <ul> <li>you are new to NeSI or have not run this particular programme of     research on a NeSI system before, and</li> <li>you work at a New Zealand institution that is not a NeSI     collaborating institution, and</li> <li>we decide that your workflow is likely to be a good technical fit     for our facilities,</li> </ul> <p>it is likely that we will initially award your research programme a Proposal Development allocation.</p> <p>Once you have completed your Proposal Development allocation, you are welcome to apply for a further allocation. If you are successful, the project's next allocation will be from another of the\u00a0allocation classes.</p> <p>The How Applications are Reviewed section provides additional important information for applicants.</p> <p>To learn more about NeSI Projects, and to apply please review the content of the section entitled Applying for a NeSI Project.</p>","tags":["info","assessment"]},{"location":"General/NeSI_Policies/Total_HPC_Resources_Available/","title":"Total HPC Resources Available","text":"<p>NeSI resources available for allocation each year combined across both Mahuika and M\u0101ui HPC systems include 152 million x86 CPU Core-hours and 112 thousand GPGPU-hours (equivalent to 400 million Cuda Core-hours) per annum and are divided between Allocation Classes as specified in Table 1, and Table 2.</p> <p>Table 1: NeSI HPC resources (physical core-hours) available per annum. Note: (1) One Node-h on M\u0101ui is equivalent to 40 Core-hs; (2) Allocations on\u00a0Mahuika (cloud) will be available in Q4, 2018.</p> <p>Allocation Class</p> <p>Mahuika HPC Cluster</p> <p>(Core-hs)</p> <p>Mahuika (cloud)</p> <p>(Core-hs)</p> <p>Mahuika (Ancillary Nodes)</p> <p>(Core-hs)</p> <p>M\u0101ui</p> <p>(Node-hs)</p> <p>M\u0101ui (Ancillary nodes)</p> <p>(Core-hs)</p> <p>Merit </p> <p>Proposal Development </p> <p>Post Graduate </p> <p>12,580,834</p> <p>504,922</p> <p>1,122,048</p> <p>362,870</p> <p>1,122,048</p> <p>Subscription</p> <p>12,580,834</p> <p>504,922</p> <p>1,122,048</p> <p>362,870</p> <p>1,122,048</p> <p>Collaborator</p> <p>37,742,399</p> <p>1,514,764</p> <p>3,366,144</p> <p>1,088,611</p> <p>3,366,144</p> <p>Total (core-hours)</p> <p>62,904,067</p> <p>2,524,608</p> <p>5,610,240</p> <p>72,574,040</p> <p>5,610,240</p> <p>Table 2: GPGPU resources available for Allocation per annum. Note: these are the maximum resources available (assuming all GPGPUs are used for computation). Actual total allocations will be lower, as one or more GPGPUs on each HPC may be used to provide remotes visualization services.</p> <p>Allocation Class</p> <p>Mahuika</p> <p>(GPU-hours)</p> <p>M\u0101ui</p> <p>(GPU-hours)</p> <p>Comment</p> <p>Merit </p> <p>Proposal Development </p> <p>Post Graduate </p> <p>13,736</p> <p>8,585</p> <p>1 \u00d7 GPU-hour is equivalent to 3,584 Cuda Core-hours.</p> <p>Subscription</p> <p>13,736</p> <p>8,585</p> <p> </p> <p>Collaborator</p> <p>41,206</p> <p>25,754</p> <p> </p> <p>Total (GPU-hours)</p> <p>68,678</p> <p>42,924</p> <p>Equivalent to 400 million Cuda Core-hours per annum</p>","tags":[]},{"location":"General/Release_Notes/About_the_Release_Notes_section/","title":"About the Release Notes section","text":"<p>NeSI publishes release notes for applications, 3rd party applications and NeSI services. This section will function as a directory to find all published release note articles with the label 'releasenote' .\u00a0</p>","tags":[]},{"location":"General/Release_Notes/About_the_Release_Notes_section/#nesi-applications","title":"NeSI applications","text":"<p>You can find published release notes for NeSI applications in the context of the structure of our documentation.\u00a0 Product context &gt; release notes section &gt; versioned release note</p> <p>Example: Release Notes Long-Term Storage\u00a0can be located under Storage &gt; Long-Term Storage</p>","tags":[]},{"location":"General/Release_Notes/About_the_Release_Notes_section/#3rd-party-applications","title":"3rd party applications","text":"<p>3rd party applications listed under Supported Applications have child pages with details about the available versions on NeSI, and a reference to the vender release notes or documentation.</p>","tags":[]},{"location":"General/Release_Notes/About_the_Release_Notes_section/#nesi-services","title":"NeSI services","text":"<p>Jupyter on NeSI is a recent example of a service composed of\u00a0multiple components and dependencies that NeSI\u00a0maintains.</p>","tags":[]},{"location":"Getting_Started/Accessing_the_HPCs/Choosing_and_Configuring_Software_for_Connecting_to_the_Clusters/","title":"Choosing and Configuring Software for Connecting to the Clusters","text":"<p>Prerequisite</p> <ul> <li>Have an active account and project.</li> <li>Set up your NeSI Account Password.</li> <li>Set up\u00a0Two-Factor    Authentication.</li> </ul> <p>Before you can start submitting work you will need some way of connecting to the NeSI clusters.</p> <p>This is done by establishing an SSH (Secure SHell) connection, giving you access to a command line interface (bash) on the cluster. In order to set up such a connection, you will need a suitable Terminal (or equivalent application). The correct option for you depends on your operating system and level of experience.</p>","tags":["terminal","mobaxterm","gitbash","login"]},{"location":"Getting_Started/Accessing_the_HPCs/Choosing_and_Configuring_Software_for_Connecting_to_the_Clusters/#web-browser","title":"Web Browser","text":"","tags":["terminal","mobaxterm","gitbash","login"]},{"location":"Getting_Started/Accessing_the_HPCs/Choosing_and_Configuring_Software_for_Connecting_to_the_Clusters/#jupyterhub","title":"JupyterHub","text":"<p>JupyterHub is a service providing access to Jupyter Notebooks on  NeSI. A terminal similar to the other setups describe below can be  accessed through the Jupyter Launcher.  </p> <p>What next?</p> <ul> <li>More info on Jupyter Terminal</li> <li>Visit jupyter.nesi.org.nz.</li> </ul>","tags":["terminal","mobaxterm","gitbash","login"]},{"location":"Getting_Started/Accessing_the_HPCs/Choosing_and_Configuring_Software_for_Connecting_to_the_Clusters/#linux-or-mac-os","title":"Linux or Mac OS","text":"","tags":["terminal","mobaxterm","gitbash","login"]},{"location":"Getting_Started/Accessing_the_HPCs/Choosing_and_Configuring_Software_for_Connecting_to_the_Clusters/#terminal","title":"Terminal","text":"<p>On MacOS or Linux you will already have a terminal emulator installed, usually called, \"Terminal.\" To find it, simply search for \"terminal\". Congratulations! You are ready to move to the next step.</p> <p>What next?</p> <ul> <li>Setting up your\u00a0Default    Terminal</li> </ul>","tags":["terminal","mobaxterm","gitbash","login"]},{"location":"Getting_Started/Accessing_the_HPCs/Choosing_and_Configuring_Software_for_Connecting_to_the_Clusters/#windows","title":"Windows","text":"<p>As Windows is not a \"Unix-Like\" operating system, getting access to a functional terminal requires some additional steps. There are several different options, listed in order of preference.</p>","tags":["terminal","mobaxterm","gitbash","login"]},{"location":"Getting_Started/Accessing_the_HPCs/Choosing_and_Configuring_Software_for_Connecting_to_the_Clusters/#ubuntu-terminal-windows-10","title":"Ubuntu Terminal (Windows 10)","text":"<p>Note</p> <p>The Ubuntu Terminal and Windows Subsystem for Linux require administrative privileges to enable and install them. If your institution has not given you such privileges, consider using another option such as MobaXTerm Portable Edition (see below).</p> <p>This is the most functional replication of a Unix terminal available on Windows, and allows users to follow the same set of instructions given to Mac/Linux users. It may be necessary to enable Windows Subsystem for Linux (WSL) first.</p> <p>What next?</p> <ul> <li>Enabling    WSL</li> <li>Setting up the\u00a0Ubuntu    Terminal</li> <li>Setting up    X-Forwarding</li> </ul>","tags":["terminal","mobaxterm","gitbash","login"]},{"location":"Getting_Started/Accessing_the_HPCs/Choosing_and_Configuring_Software_for_Connecting_to_the_Clusters/#mobaxterm","title":"MobaXterm","text":"<p>In addition to being a terminal emulator, MobaXterm also includes  several useful features like multiplexing, X11 forwarding and a file  transfer GUI.</p> <p>MobaXterm can be downloaded from  here.  The portable edition will allow you to use MobaXterm without needing  administrator privileges, however it introduces several bugs so we  highly recommend using the installer edition if you have  administrator privileges on your workstation or if your  institution's IT team supports MobaXTerm.</p> <p>What next?</p> <ul> <li>Setting up    MobaXterm</li> </ul>","tags":["terminal","mobaxterm","gitbash","login"]},{"location":"Getting_Started/Accessing_the_HPCs/Choosing_and_Configuring_Software_for_Connecting_to_the_Clusters/#using-a-virtual-machine","title":"Using a Virtual Machine","text":"<p>In order to avoid the problems of using a Windows environment, it may be advisable to install a Linux Virtual machine. This may be advantageous in other ways as many elements of scientific computing require a Linux environment, also it can provide a more user friendly place to become familiar with command line use.</p> <p>There are multiple free options when it comes to VM software. We recommend\u00a0Oracle VirtualBox.</p> <p>Further instructions on how to set up a virtual machine can be found here.</p> <p>Once you have a working VM you may continue following the instructions as given for Linux/MacOS.</p> <p>What next?</p> <ul> <li>Setting up a Virtual    Machine</li> </ul>","tags":["terminal","mobaxterm","gitbash","login"]},{"location":"Getting_Started/Accessing_the_HPCs/Choosing_and_Configuring_Software_for_Connecting_to_the_Clusters/#winscp","title":"WinSCP","text":"<p>WinSCP has some advantages over MobaXterm (customisable, cleaner interface, open source), and some disadvantages (no built in X-server, additional authentication step). However, WinSCP setup is more involved than with MobaXterm, therefore we do not recommend it for new users.</p> <p>What next?</p> <ul> <li>Setting up    WinSCP</li> </ul>","tags":["terminal","mobaxterm","gitbash","login"]},{"location":"Getting_Started/Accessing_the_HPCs/Choosing_and_Configuring_Software_for_Connecting_to_the_Clusters/#git-bash","title":"Git Bash","text":"<p>If you are using Git for version control you may already have Git Bash installed. If not it can be downloaded from\u00a0here.</p> <p>Git Bash is perfectly adequate for testing your login or setting up your password, but lacks many of the features of MobaXterm or a native Unix-Like terminal. Therefore we do not recommend it as your primary terminal.</p>","tags":["terminal","mobaxterm","gitbash","login"]},{"location":"Getting_Started/Accessing_the_HPCs/Choosing_and_Configuring_Software_for_Connecting_to_the_Clusters/#windows-powershell","title":"Windows PowerShell","text":"<p>All Windows computers have PowerShell installed, however it will only be useful to you if Windows Subsystem for Linux (WSL) is also enabled, instructions here.</p> <p>Like Git Bash, PowerShell is perfectly adequate for testing your login or setting up your password, but lacks many of the features of MobaXterm or a native Unix-Like terminal. Therefore we do not recommend it as your primary terminal.</p>","tags":["terminal","mobaxterm","gitbash","login"]},{"location":"Getting_Started/Accessing_the_HPCs/Port_Forwarding/","title":"Port Forwarding","text":"<p>Prerequisite</p> <ul> <li>Have your connection to the NeSI     cluster     configured.</li> </ul> <p>Some applications only accept connections from internal ports (i.e a port on the same local network), if you are running one such application on the cluster and want to connect to it you will need to set up\u00a0port forwarding.</p> <p>Three values must be known, the local port, the host alias, and the remote port. Chosen port numbers should be between\u00a01024 and 49151 and not be in use by another process.</p> <p>Localhost: The self address of a host (computer), equivalent to\u00a0<code>127.0.0.1</code>. The alias <code>localhost</code> can also be used in most cases.</p> <p>Local Port:\u00a0The port number you will use on your local machine.</p> <p>Host Alias: An alias for the socket of your main connection to the cluster, <code>mahuika</code> or <code>maui</code> if you have set up your ssh config file as described here.</p> <p>Remote Port: The port number you will use on the remote machine (in this case the NeSI cluster)</p> <p>Note</p> <p>The following examples use aliases as set up in standard terminal setup. This allows the forwarding from your local machine to the NeSI cluster, without having to re-tunnel through the lander node.</p>","tags":[]},{"location":"Getting_Started/Accessing_the_HPCs/Port_Forwarding/#command-line-openssh","title":"Command line (OpenSSH)","text":"<p>Works for any Linux terminal, Mac terminal, or Windows with WSL enabled.</p> <p>The command for forwarding a port is</p> <pre><code>ssh -L &lt;local_port&gt;:&lt;destination_host&gt;:&lt;remote_port&gt;\u00a0&lt;ssh_host&gt;\n</code></pre>","tags":[]},{"location":"Getting_Started/Accessing_the_HPCs/Port_Forwarding/#example","title":"Example","text":"<p>A client program on my local machine uses the port 5555 to communicate. I want to connect to a server running on mahuika that is listening on port 6666. In a new terminal on my local machine I enter the command:</p> <pre><code>ssh -L 5555:localhost:6666\u00a0mahuika\u00a0\n</code></pre> <p>Your terminal will now function like a normal connection to mahuika. However if you close this terminal session the port forwarding will end.</p> <p>If there is no existing session on mahuika, you will be prompted for your first and second factor, same as during the regular log in procedure.</p> <p>Note</p> <p>Your local port and remote port do not have to be different numbers. It is generally easier to use the same number for both.</p>","tags":[]},{"location":"Getting_Started/Accessing_the_HPCs/Port_Forwarding/#ssh-config-openssh","title":"SSH Config (OpenSSH)","text":"<p>If you are using port forwarding on a regular basis, and don't want the hassle of opening a new tunnel every time, you can include a port forwarding line in your ssh config file ~/.ssh/config on your local machine.</p> <p>Under the alias for the cluster you want to connect to add the following lines.</p> <pre><code>LocalForward &lt;local_port&gt; &lt;host_alias&gt;:&lt;remote_port&gt;\nExitOnForwardFailure yes\n</code></pre> <p>ExitOnForwardFailure is optional, but it is useful to kill the session if the port fails.</p> <p>For example:</p> <pre><code>  Host mahuika\n      User cwal219\n      Hostname login.mahuika.nesi.org.nz\n      ProxyCommand ssh -W %h:%p lander\n      ForwardX11 yes\n      ForwardX11Trusted yes\n      ServerAliveInterval 300\n      ServerAliveCountMax 2\n      LocalForward 6676 mahuika:6676\n      ExitOnForwardFailure yes\n</code></pre> <p>In the above example, the local and remote ports are the same. This isn't a requirement, but it makes things easier to remember.</p> <p>Now so long as you have a connection to the cluster, your chosen port will be forwarded.</p> <p>Note</p> <ul> <li>If you get a error message     <pre><code>bind: No such file or directory\nunix_listener: cannot bind to path:\n</code></pre>     try to create the following directory:     <pre><code>mkdir -P ~/.ssh/sockets\n</code></pre></li> </ul>","tags":[]},{"location":"Getting_Started/Accessing_the_HPCs/Port_Forwarding/#mobaxterm","title":"MobaXterm","text":"<p>If you have Windows Subsystem for Linux installed, you can use the method described above. This is the recommended method.</p> <p>You can tell if MobaXterm is using WSL as it will appear in the banner when starting a new terminal session.</p> <p></p> <p>You can also set up port forwarding using the MobaXterm tunnelling interface.</p> <p></p> <p>You will need to create two tunnels. One from lander to mahuika. And another from mahuika to itself. (This is what using an alias in the first two examples allows us to avoid).</p> <p>The two tunnels should look like this.</p> <p></p> <p>\u25a0\u00a0local port \u25a0\u00a0remote port \u25a0\u00a0must match \u25a0 doesn't matter</p>","tags":[]},{"location":"Getting_Started/Accessing_the_HPCs/Port_Forwarding/#sshuttle","title":"sshuttle","text":"<p>sshuttle\u00a0is a transparent proxy implementing VPN like traffic forwarding. It is based on Linux or MacOS platforms (unfortunately Windows is not supported). <code>sshuttle</code> allows users to create a VPN connection from a local machine to any remote server that they can connect to via <code>ssh</code>.There is no need to create a separate tunnel for every port to be forwarded, the package routes all traffic, going to the specified subnet, through the tunnel.</p> <p>The command line for <code>sshuttle</code> has the following form:</p> <pre><code>sshuttle [-l [ip:]port] -r &lt;host_alias&gt;[:port] &lt;subnets...&gt;\n</code></pre> <p>More information about specific keys and modifiers for sshuttle commands is available in the online documentation.</p> <p>As an example, this is how to establish a tunnel through Mahuika login node over to a specific virtual machine with IP address <code>192.168.90.5</code>:</p> <pre><code>sshuttle -r mahuika 192.168.0.0/16\n</code></pre> <p>which uses remote SSH host Mahuika to forward all traffic coming to <code>192.16.XXX.XXX</code> subnet through the port forwarder.</p>","tags":[]},{"location":"Getting_Started/Accessing_the_HPCs/Port_Forwarding/#forwarding-to-compute-nodes","title":"Forwarding to Compute Nodes","text":"<p>Ports can also be forwarded from the login node to a compute node.</p> <p>The best way to do this is by creating a reverse tunnel from your slurm job (that way the tunnel doesn't depend on a separate shell, and the tunnel will not outlive the job).</p> <p>The syntax for opening a reverse tunnel is similar the regular tunnel command, <code>-N</code> to not execute a command after connecting, <code>-f</code> to run the connection in the background and <code>-R</code> for a reverse tunnel ( as opposed to <code>-L</code> ).</p> <pre><code>ssh -Nf -R &lt;remote_port&gt;:localhost:&lt;local_port&gt; ${SLURM_SUBMIT_HOST}\n</code></pre> <p>An example Slurm script:</p> <pre><code>#!/bin/bash\n\n#SBATCH --time 00:15:00\n#SBATCH --mem  1G\n\nssh -Nf -R 6676:localhost:6676 ${SLURM_SUBMIT_HOST}\n\n&lt;some process using port 6676&gt;\n</code></pre> <p>What Next?</p> <ul> <li>Using     JupyterLab on the cluster.</li> <li>NiceDCV</li> <li>Paraview</li> </ul>","tags":[]},{"location":"Getting_Started/Accessing_the_HPCs/Setting_Up_Two_Factor_Authentication/","title":"Setting Up Two-Factor Authentication","text":"<p>Prerequisite</p> <ul> <li>Have a\u00a0NeSI account.  </li> <li>Be a member of an active project.  </li> <li>Have set up your NeSI account password.  </li> <li>Have a device with an authentication app.</li> </ul>","tags":["2fa","access","mfa","token"]},{"location":"Getting_Started/Accessing_the_HPCs/Setting_Up_Two_Factor_Authentication/#authentication-app","title":"Authentication App","text":"<p>In order to generate your second factor, which you will require in order to access to any NeSI cluster, you will need a device with an authentication app, such as Authy or Google Authenticator installed (generally the device is a smartphone, but there are also authentication apps which work through the browser like Authy).</p> <p>If you some reason you can't do this, please contact NeSI support.</p>","tags":["2fa","access","mfa","token"]},{"location":"Getting_Started/Accessing_the_HPCs/Setting_Up_Two_Factor_Authentication/#linking-a-device-to-your-account","title":"Linking a device to your account","text":"<ol> <li> <p>Log in to\u00a0My NeSI via your browser.</p> </li> <li> <p>Click My HPC Account on left hand panel\u00a0 and then Setup    Two-Factor Authentication device </p> </li> <li> <p>Click the \"Setup Two-Factor Authentication device\" link. </p> </li> <li> <p>After clicking on \"Continue\" you will retrieve the QR code.</p> </li> <li> <p>Open your Authy or\u00a0Google Authenticator app and click on the add    button and select \"Scan a barcode\". Alternatively, if you are    not able to scan the barcode from your device you can manually enter    the provided authentication code into your authentication app.</p> </li> </ol>","tags":["2fa","access","mfa","token"]},{"location":"Getting_Started/Accessing_the_HPCs/Setting_Up_Two_Factor_Authentication/#the-second-factor-token","title":"The second-factor token","text":"<p>The 6 digit code displayed on your app can now be used as the second factor in the authentication\u00a0process. This code rotates every 30 seconds, and it can only be used once. This means that you can only try logging in to the lander node once every 30 seconds.</p> <p>What next?</p> <ul> <li>Getting access to the   cluster</li> </ul>","tags":["2fa","access","mfa","token"]},{"location":"Getting_Started/Accessing_the_HPCs/Setting_Up_and_Resetting_Your_Password/","title":"Setting Up and Resetting Your Password","text":"<p>Prerequisite</p> <ul> <li>Have a NeSI    account.</li> <li>Be a member of an active project.</li> </ul>","tags":["howto","authentication","mynesi"]},{"location":"Getting_Started/Accessing_the_HPCs/Setting_Up_and_Resetting_Your_Password/#setting-nesi-password","title":"Setting NeSI Password","text":"<ol> <li> <p>Log into the my NeSI portal via your    browser.  </p> </li> <li> <p>Click My HPC Account on left hand panel and then Set    Password (If you are resetting your password this will read    Reset Password).    Note your Username. </p> </li> <li> <p>Enter and verify your new password, making sure it follows the    password    policy. </p> </li> <li> <p>If the password set was successful, following confirmation label    will appear on the same page within few seconds </p> </li> <li> <p>Followed by an email confirmation similar to below    </p> </li> </ol>","tags":["howto","authentication","mynesi"]},{"location":"Getting_Started/Accessing_the_HPCs/Setting_Up_and_Resetting_Your_Password/#resetting-nesi-password-via-my-nesi-portal","title":"Resetting NeSI Password via my NeSI Portal","text":"<ol> <li> <p>Log into the my NeSI portal via your    browser.  </p> </li> <li> <p>Click My HPC Account on left hand panel and then Reset    Password    Note your Username.</p> </li> <li> <p>You can either enter the Old Password first and then set a new one    OR feel free to select Forgot my password         - We recommend Forgot my password option in general</p> </li> <li> <p>If the password reset was successful, following confirmation    label will appear on the same page within few seconds </p> </li> <li> <p>Followed by an email confirmation similar to below </p> </li> </ol> <p>What next?</p> <ul> <li>Set up Second Factor    Authentication.</li> </ul>","tags":["howto","authentication","mynesi"]},{"location":"Getting_Started/Accessing_the_HPCs/X_Forwarding_using_the_Ubuntu_Terminal_on_Windows/","title":"X-Forwarding using the Ubuntu Terminal on Windows","text":"<ol> <li>Download and install Xming from    here. Don't install an SSH    client when prompted during the installation, if you are prompted    for Firewall permissions after installing Xming close the window    without allowing any Firewall permissions.</li> <li>Open your Ubuntu terminal and install x11-apps with the command: <code>sudo apt install x11-apps -y</code>.</li> <li>Restart your terminal, start your Xming (there should be a desktop    icon after installing it). You should now be able to X-Forward    displays from the HPC when you log in (assuming you have completed    the terminal setup instructions found    here).</li> </ol>","tags":["x11","x forwarding","x-forwarding"]},{"location":"Getting_Started/Accounts-Projects_and_Allocations/Applying_for_a_new_NeSI_project/","title":"Applying for a new NeSI project","text":"<p>Prerequisite</p> <ul> <li>Have a NeSI Account profile.</li> <li>NIWA researchers only: read and follow the     NIWA internal documentation for gaining access to the HPCs     (this link is only valid from within the NIWA network or VPN).</li> </ul> <p>Preferred</p> <ul> <li>Assemble your project team.</li> <li>Becoming familiar with the Linux command line.\u00a0There are many      courses and online materials available, such as\u00a0Software      Carpentry, to help      you and your project team gain the necessary skills.</li> <li>Become familiar with foundational HPC skills, for example by      attending a NeSI introductory workshop, one of our weekly      introductory sessions (or watching the      recording),      or having one or more of your project team members do so.</li> <li>Review our allocation classes. If      you don't think you currently qualify for any class other than      Proposal Development, please Contact our Support Team as soon as      possible to discuss your options. Your institution may be in a      position to buy a subscription from us while your Proposal      Development allocation is in effect if they do not already possess      one.</li> </ul> <p>Requests to use NeSI resources are submitted via a web form. The NeSI team will endeavour to approve your project, or contact you for more information, within 3-5 working days of your submitting your project request.</p> <p>Note</p> <p>If you are a member of NIWA please also ensure that you have also read  and followed the  NIWA internal documentation for gaining access to the HPCs  before applying for your NeSI project. You will only be able to  access the link from behind the NIWA VPN.  Other institutions may also put in place, or vary, pre-approval  processes from time to time. If you apply for a new project without  having completed any necessary pre-approval steps, your application  may be delayed more than usual, or we may notify you and ask you to  obtain pre-approval from your institution.</p>","tags":["info","project","mynesi"]},{"location":"Getting_Started/Accounts-Projects_and_Allocations/Applying_for_a_new_NeSI_project/#information-you-will-need-to-provide","title":"Information you will need to provide","text":"<p>During the application process, we will ask you for the following information:</p> <ul> <li>Your name, institutional affiliation (i.e. your employer or place of     study), role at your institution, a contact telephone number, and     work email address</li> <li>The title of your proposed NeSI HPC Project, and a brief abstract     describing your project's goals</li> <li>The scientific domain and field of study (i.e. subdomain) your     project belongs to</li> <li>The date on which you plan to start your computational work on NeSI     (not the start date of the research programme as a whole, or of the     research programme's current or expected funding)</li> <li>Details of how your project is funded (this will help determine     whether you are eligible for an allocation from our     Merit class)</li> <li>Your previous HPC experience</li> <li>Whether you would like expert scientific programming support on your     project</li> <li>Who else will be working on the proposed NeSI HPC Project with you</li> <li>What software you intend to use on the NeSI HPCs.</li> </ul> <p>You will also be given an opportunity to tell us anything else you think is relevant.</p> <p>What Next?</p> <ul> <li>Your NeSI Project proposal will be      reviewed,      after which you will be informed of the outcome.</li> <li>We may contact you if further details are required.</li> <li>When your project is approved you will be able to set your Linux      Password.</li> </ul>","tags":["info","project","mynesi"]},{"location":"Getting_Started/Accounts-Projects_and_Allocations/Applying_to_join_an_existing_NeSI_project/","title":"Applying to join an existing NeSI project","text":"<p>Prerequisite</p> <ul> <li>You must have a NeSI      account.</li> </ul>","tags":["info","project","request_membership","mynesi"]},{"location":"Getting_Started/Accounts-Projects_and_Allocations/Applying_to_join_an_existing_NeSI_project/#how-to-join-an-existing-project-on-nesi","title":"How to join an existing project on NeSI","text":"<ol> <li>Make sure you have been given the project code by the project owner.</li> <li>Log in to\u00a0my.nesi.org.nz.</li> <li>Under the Projects page use     the \"Join Project\" link to request to be added to the project as     a member.</li> </ol> <p>Once submitted you will receive a ticket confirmation via email.</p> <p>What Next?</p> <ul> <li>The project owner will be notified, and asked to approve your      request.</li> <li>Once your request has been approved by the project owner and      processed by us, you will be able to set your NeSI account      password.</li> </ul>","tags":["info","project","request_membership","mynesi"]},{"location":"Getting_Started/Accounts-Projects_and_Allocations/Creating_a_NeSI_Account_Profile/","title":"Creating a NeSI Account Profile","text":"<p>Prerequisite</p> <p>Either an active login at a Tuakiri member institution, or a Tuakiri  Virtual Home account in respect of your current place of work or  study.</p> <ol> <li> <p>Access\u00a0my.nesi.org.nz via your browser and     log in with either your institutional credentials, or your Tuakiri     Virtual Home account, whichever applies.</p> </li> <li> <p>If this is your first time logging in to my.nesi and you do not have     an entry in our database (you have not previously had a NeSI     account) you will be asked to fill out some fields, such as your     role at your institution and contact telephone number, and submit     the online form to us. We will complete your personal profile for     our records.</p> </li> </ol> <p>What next?</p> <ul> <li>Apply for Access,      either submit an application for\u00a0a new project or      join an existing project.</li> </ul>","tags":["onboarding","howto","access","mynesi"]},{"location":"Getting_Started/Accounts-Projects_and_Allocations/Project_Extensions_and_New_Allocations_on_Existing_Projects/","title":"Project Extensions and New Allocations on Existing Projects","text":"<p>NeSI recognises that research programmes often continue over several years before coming to an end. To reduce administrative overhead, we allow most project teams to extend their projects rather than applying for a new project to carry on the same work.</p> <p>We currently offer two sorts of extensions:</p> <ul> <li>A new allocation of computing resources (usually compute units on     Mahuika or node hours on M\u0101ui)</li> <li>A project extension without a new allocation of computing resources.</li> </ul>","tags":[]},{"location":"Getting_Started/Accounts-Projects_and_Allocations/Project_Extensions_and_New_Allocations_on_Existing_Projects/#will-my-project-qualify-for-an-extension","title":"Will my project qualify for an extension?","text":"<p>Usually, yes. There are a few circumstances in which a project will not qualify for an extension:</p> <ul> <li>If there has been a substantial change to the research programme's     goals.</li> <li>If there has been (or is about to be) a substantial change to the     computational methods the project team plans on using to carry out     the project work, such that we decide a new technical assessment is     warranted.</li> <li>If the project team is no longer eligible to receive computing     resources from NeSI. For example, the grant funding the research     programme has come to an end and the host institution does not wish     to purchase computing resources from NeSI (or allocate computing     resources from its subscription or collaborator share if it has     one).</li> <li>If the project's host institution has changed (or is about to     change).</li> <li>If the project's owner is no longer employed by or studying at the     project's host institution, and there is no-one at the host     institution who has agreed to take over project ownership     responsibilities.</li> <li>If the project's owner (or supervisor if the project has one) is not     authorised to access NeSI facilities due to a refusal to accept or     failure to abide by the NeSI Acceptable Use Policy.</li> </ul>","tags":[]},{"location":"Getting_Started/Accounts-Projects_and_Allocations/Project_Extensions_and_New_Allocations_on_Existing_Projects/#who-may-request-a-project-extension","title":"Who may request a project extension?","text":"<p>A request for a project extension should come from the project owner. If the project owner is a student, we will include the supervisor on the extension correspondence. The project supervisor\u00a0may disallow an extension request.</p>","tags":[]},{"location":"Getting_Started/Accounts-Projects_and_Allocations/Project_Extensions_and_New_Allocations_on_Existing_Projects/#how-do-i-request-an-extension","title":"How do I request an extension?","text":"<p>You can submit a request for an extension using https://my.nesi.org.nz or by Contact our Support Team.</p> <p>Please see Requesting to renew an allocation via my.nesi.org.nz for more details.</p> <p>You will receive a series of automated emails inviting you to apply for a new allocation (or, alternatively, clean up your project data) in the following circumstances:</p> <ul> <li>In the lead-up to the end of the call     window     immediately before your currently active allocation is scheduled to     end.</li> <li>In the lead-up to the end of your allocation.</li> <li>If your allocation ends before your project is scheduled to end, in     the lead-up to the end of your project.</li> </ul>","tags":[]},{"location":"Getting_Started/Accounts-Projects_and_Allocations/Project_Extensions_and_New_Allocations_on_Existing_Projects/#requests-for-new-allocations","title":"Requests for new allocations","text":"<p>If you are requesting a new allocation of computing resources, we will look at your usage history and come up with an estimated allocation size and duration based on that history. If you think your rate of usage will be substantially higher or lower than we estimate, you should let us know what you think your rate of usage will be and why you expect it to differ from our forecast.</p> <p>Please be aware that:</p> <ul> <li>First and subsequent allocations are subject to the NeSI allocation     size and duration limits in force at the time they are considered by     our reviewers.</li> <li>An allocation from an institution's entitlement is subject to     approval by that institution.</li> </ul>","tags":[]},{"location":"Getting_Started/Accounts-Projects_and_Allocations/Project_Extensions_and_New_Allocations_on_Existing_Projects/#requests-for-project-extensions-without-a-new-compute-allocation","title":"Requests for project extensions without a new compute allocation","text":"<p>In some circumstances, you may wish to extend your project without receiving a new allocation of compute units or node hours for the project. Typically this occurs in cases like a PhD thesis under examination when the project team would like to keep the research data for a few months while waiting to find out whether more computational work will be needed.</p> <p>If this is your situation, please let us know when you request your project extension. Please note that we are unlikely to let a project continue without a compute allocation for more than six months at a time. If you expect that your project will be suspended for more than six months, we encourage you to enquire about our Long-Term Storage Service\u00a0or to move your research data off our facility and make arrangements with your project's host institution for long-term data storage.</p>","tags":[]},{"location":"Getting_Started/Accounts-Projects_and_Allocations/Quarterly_allocation_periods/","title":"Quarterly allocation periods","text":"<p>Applications for new allocations on existing projects are accepted and assessed in quarterly allocation periods. Under this quarterly call schedule, applications for new allocations on existing projects are due in the following months:</p> <ul> <li>April (for allocations starting 1 June)</li> <li>July (for allocations starting 1 September)</li> <li>October (for allocations starting 1 December)</li> <li>January (for allocations starting 1 March)</li> </ul> <p>Requests will be reviewed in the month following the call period and researchers will be notified by the third Monday of that month. Allocations will start on the first day of the next month and run for one year. The diagram below illustrates how these quarterly call periods are scheduled during the year:</p> <p></p> <p>For example, if you apply for a new allocation on your existing project in the month of October, we will review your application in October or early November, you will be notified of your allocation by the end of November, and your allocation will start on 1 December (as shown in the graphic below).</p> <p></p>","tags":[]},{"location":"Getting_Started/Accounts-Projects_and_Allocations/Quarterly_allocation_periods/#existing-allocations","title":"Existing allocations","text":"<p>If you have an existing allocation, you will be reminded about the upcoming call by email during the second month before your project expires. For example, if your allocation expires at the end of May, you will receive email reminders during the month of April.</p> <p>We aggregate requests and deal with them in batches during the review month.</p> <ul> <li>If you apply for your new allocation early (for example, you apply     in February when your allocation isn\u2019t due to end until the end of     May), we will hold your request until April.</li> <li>If you apply for your new allocation late, your request may be     deprioritised by your institution, or you may suffer an interruption     to service as we have to consider your request separately and later.     It is possible, depending on overall demand, that you may have to     wait for the following call before your request is considered.</li> </ul> <p>If you have questions about the review cycles or other steps involved with getting access to NeSI, Contact our Support Team</p>","tags":[]},{"location":"Getting_Started/Accounts-Projects_and_Allocations/What_is_an_allocation/","title":"What is an allocation?","text":"<p>Because NeSI's resources are limited, we manage access to our resources through allocations. Typically, an allocation is a grant of a certain amount of a resource, or of a rate at which a resource can be consumed, during a defined period of time. Different types of resource have different allocation criteria.</p> <p>An allocation will come from one of our allocation classes. We will decide what class of allocation is most suitable for you and your research programme, however you're welcome to review our article on allocation classes to find out what class you're likely eligible for.</p>","tags":[]},{"location":"Getting_Started/Accounts-Projects_and_Allocations/What_is_an_allocation/#hpc-platform-allocations","title":"HPC Platform allocations","text":"<p>The form of NeSI allocation you may be most familiar with is an allocation of computing power. We currently offer three sorts of compute allocations, of which your project needs at least two (online storage plus one kind of compute allocation) in order to be valid and active.</p> <p>Compute allocations are expressed in terms of a number of units, to be consumed or reserved between a set start date and time and a set end date and time. For allocations of computing power, we use Fair Share to balance work between different projects. NeSI allocations and the relative \"prices\" of resources used by those allocations should not be taken as any indicator of the real NZD costs of purchasing or running the associated infrastructure and services.</p>","tags":[]},{"location":"Getting_Started/Accounts-Projects_and_Allocations/What_is_an_allocation/#mahuika-allocations","title":"Mahuika allocations","text":"<p>Allocations on Mahuika are measured in Mahuika compute units. A job uses one Mahuika compute unit if it runs for one hour on one physical Mahuika CPU core (two logical CPUs), using 3 GB of RAM and no GPU devices. This means a single Mahuika compute unit is equivalent to what we previously called a \"Fair Share adjusted core-hour\" on Mahuika's standard compute nodes.</p> <p>The price of hardware in terms of compute units is shown in the following table.</p> Hardware type Fair Share Price CPU 0.35 compute units per logical-CPU-hour Memory (RAM) 0.10 compute units per GB-hour P100 GPU device 7.0 compute units per device-hour A100 GPU device 18.0 compute units per device-hour A100-1g.5gb GPU device 3.0 compute units per device-hour <p>The total compute unit cost of a job is the sum of these three components. Once the job has finished running, this composite price is what affects your project's Fair Share score. However, whether your institution will be charged based on the composite price or based on your job's CPU core hour consumption alone, or on some other basis, will depend on your contractual arrangements with the NeSI host.</p> <p>Note that the minimum number of logical cores a job can take on Mahuika is two (see\u00a0Hyperthreading\u00a0for details). Therefore:</p> <ul> <li>the lowest possible price for a CPU-only job is 0.70 compute units     per hour, plus memory (RAM).</li> <li>the lowest possible price for a CPU + P100 GPU job is 7.70 compute     units per hour, plus memory (RAM).</li> <li>the lowest possible price for a CPU + A100 GPU job is 18.70 compute     units per hour, plus memory (RAM).</li> </ul> <p>In reality, every job must request at least some RAM.</p>","tags":[]},{"location":"Getting_Started/Accounts-Projects_and_Allocations/What_is_an_allocation/#maui-allocations","title":"M\u0101ui allocations","text":"<p>The compute capacity of the M\u0101ui supercomputer is allocated by node-hours. Though some M\u0101ui nodes have more RAM than others, we do not currently distinguish between low-memory and high-memory nodes for allocation, billing or Fair Share purposes.</p> <p>Each allocation on M\u0101ui includes an entitlement to use the M\u0101ui ancillary nodes equally with other NeSI projects having M\u0101ui allocations at that time.</p> <p>One M\u0101ui node hour is roughly equivalent to 40 Mahuika compute units.</p>","tags":[]},{"location":"Getting_Started/Accounts-Projects_and_Allocations/What_is_an_allocation/#online-storage-allocations","title":"Online storage allocations","text":"<p>An online storage allocation, unlike compute allocations, is more like a lease than a rate of consumption. It is an amount of disk space and, concurrently, a number of inodes (directory entries, i.e. files etc.) that have been made available for your project team to use on our online high-performance file system. An online storage allocation is typically granted to your persistent project directory.</p> <p>We do not yet have a ratio of online storage disk space or inodes to Mahuika compute units.</p>","tags":[]},{"location":"Getting_Started/Accounts-Projects_and_Allocations/What_is_an_allocation/#data-storage-allocations","title":"Data storage allocations","text":"","tags":[]},{"location":"Getting_Started/Accounts-Projects_and_Allocations/What_is_an_allocation/#nearline-storage-allocations","title":"Nearline storage allocations","text":"<p>A nearline storage allocation, like online storage allocations but unlike compute allocations, is more like a lease than a rate of consumption. It is an amount of space and, concurrently, a number of inodes (directory entries, i.e. files etc.) that have been made available for your project team to use on our nearline apparatus.</p> <p>We do not yet have a ratio of nearline storage tape space or inodes to Mahuika compute units.</p>","tags":[]},{"location":"Getting_Started/Accounts-Projects_and_Allocations/What_is_an_allocation/#consultancy-allocations","title":"Consultancy allocations","text":"<p>A consultancy allocation is for a number of scientific programmer hours between two dates, or is sometimes expressed as a fraction of an FTE between the same two dates. This reflects the commitment of NeSI scientific programming expertise to your project.</p> <p>We do not yet have a ratio of consultancy hours to Mahuika compute units.</p>","tags":[]},{"location":"Getting_Started/Cheat_Sheets/Bash-Reference_Sheet/","title":"Bash: Reference Sheet","text":"<p>Regardless of the operating system of your personal computer you will need to know some basic Unix Shell commands since the HPC are Linux machines. If you do not have any experiencing using Unix Shell we would advise going at least the first (3 parts) of the Software Carpentry Unix Shell lessons.</p> Command Description Examples Command <code>ls</code> <code>ls</code> Lists the files in your current directory. <code>ls /path/to/directory/</code> Lists the files in the specified directory. <code>ls -ltra</code> Lists all files, including hidden ones (<code>-a</code>), in long format (<code>-l</code>), in reverse order (<code>-r</code>) of time since edited (<code>-t</code>). <code>pwd</code> <code>pwd</code> Prints the path of your current working directory. <code>cd</code> <code>cd /path/to/directory/</code> Changes your current directory to the specified directory. <code>touch</code> <code>touch file.txt</code> Created an empty file of specified name. <code>nano</code> <code>nano</code> Opens the nano text editor. <code>nano file.txt</code> Opens the specified file in the nano text editor. <code>head</code> <code>head file.txt</code> Prints the top 10 lines of the specified file. <code>head -n 2 file.txt</code> Prints the top n lines of the specified file (in this case 2). <code>tail</code> <code>tail file.txt</code> Prints the bottom 10 lines of the specified file. <code>tail -n 2 file.txt</code> Prints the bottom n lines of the specified file (in this case 2). <code>mv</code> <code>mv file.txt newname.txt</code> rename the file. <code>mv file.txt /path/to/destination/</code> Move the file to the specified directory. <code>mv -r directory/ /path/to/destination/</code> Recursively move the directory and all contained files and directories to the specified path. <code>cp</code> <code>cp file.txt /path/to/destination/</code> Make a copy of the file in the specified directory. <code>cp file.txt /path/to/destination/newname.txt</code> Make a copy of the file in the specified directory with the specified name. <code>cp -r directory/ /path/to/destination/</code> Recursively copy all files and directories of a directory to the specified location. <code>rm</code> <code>rm file.txt</code> Delete the specified file. <code>rm -r directory/</code> Recursively delete the files and directories of the specified directory. <code>mkdir</code> <code>mkdir directory</code> Create a directory of the specified name. <code>man</code> <code>man ls</code> Bring up the manual of a command (in this case ls). <p>Tip</p> <p>Pressing the 'tab' key once will automatically complete the line if it  is the only option. e.g.\u00a0    If there are more than one possible completions, pressing tab again  will show all those options.    Use of the tab key can help navigate the filesystem, spellcheck your  commands and save you time typing.</p>","tags":[]},{"location":"Getting_Started/Cheat_Sheets/Git-Reference_Sheet/","title":"Git: Reference Sheet","text":"<p>Git is the most universally adopted version control software and is often used alongside remote repositories like GitHub and GitLab for developing, managing and distributing code.</p> <p>Full Git documentation can be found\u00a0here, or using <code>man git</code>.</p> <p></p>","tags":["git","github","version control","repository"]},{"location":"Getting_Started/Cheat_Sheets/Git-Reference_Sheet/#authentication","title":"Authentication","text":"<p>In order to pull from a private repo, or push changes to a remote, you need to authenticate yourself on the cluster.</p> <p>Password authentication</p> <p>GitHub removed support for password authentication on August 13, 2021.  Using a SSH key is now the easiest way to set up authentication.</p>","tags":["git","github","version control","repository"]},{"location":"Getting_Started/Cheat_Sheets/Git-Reference_Sheet/#ssh-authentication-github","title":"SSH Authentication (GitHub)","text":"<p>More information can be found in the GitHub documentation.</p> <ul> <li> <p>On the NeSI cluster, run the command</p> <pre><code>ssh-keygen -t ed25519 -C \"your_github_account@example.com\"\n</code></pre> </li> <li> <p>When prompted for a file name, press <code>enter</code>. When prompted for a password, press enter twice more.</p> </li> <li> <p>Open up the newly created .pub key with the command</p> <pre><code>cat ~/.ssh/id_ed25519.pub\n</code></pre> <p>(or whatever you named the key). It should look something like:</p> <pre><code>ssh-ed25519 ABCDEFGKSAfjksjafkjsaLJfakjJF your_github_account@example.com\n</code></pre> <p>Copy the whole key.</p> </li> <li> <p>Now log in to your GitHub account. In the upper-right corner of any     page, click your profile photo click Settings.</p> <p></p> </li> <li> <p>In the \"Access\" section of the sidebar, click SSH and GPG keys.</p> </li> <li> <p>Click New SSH key or Add SSH key.</p> <p></p> </li> <li> <p>In the \"Title\" field, put \"Mahuika\" or \"NeSI\".</p> </li> <li> <p>Paste your key into the \"Key\" field.</p> <p></p> </li> <li> <p>Click Add SSH key.</p> </li> <li> <p>Switching back to your terminal on the cluster, you can test your     connection with the command</p> <pre><code>ssh -T git@github.com\n</code></pre> <p>You may be prompted to authenticate, if so type 'yes' If everything is working, you should see the message</p> <pre><code>Hi User! You've successfully authenticated, but GitHub does not provide shell access.\n</code></pre> </li> </ul>","tags":["git","github","version control","repository"]},{"location":"Getting_Started/Cheat_Sheets/Git-Reference_Sheet/#basics","title":"Basics","text":"<p>You can create a repository with either of the following commands.</p> <code>clone</code> <code>git clone https://github.com/nesi/perf-training.git</code> Copies a remote repository into your current directory. <code>init</code> <code>git init</code> Creates a new empty repo in your current directory. <code>add</code> <code>git add &lt;file1&gt; &lt;file2&gt;</code> Adds <code>&lt;file1&gt;</code> and <code>&lt;file2&gt;</code> to the staging area. <code>git add *.py</code> \u00a0Adds all python files in the current directory to the staging area. <code>status</code> <code>git status</code> Lists changes in working directory, and staged files. <code>commit</code> <code>git commit</code> Records everything in the staging area to your repository. The default text editor will prompt you for a commit message. <code>git commit -m \"Commit message\"</code> Records everything in the staging area to your repository with the commit message \"Commit message\" <code>git commit --amend</code> Modify last commit instead of creating a new one. Useful for fixing small mistakes. <code>log</code> <code>git log</code> Prints commit history of repo. <code>git log &lt;filename&gt;</code> Prints commit history of <code>&lt;filename&gt;</code>. <code>reset</code> <code>git reset</code> Removes all files from staging area. (Opposite of <code>git add</code>) <code>git reset &lt;filename&gt;</code> Removes <code>&lt;filename&gt;</code> from staging area.","tags":["git","github","version control","repository"]},{"location":"Getting_Started/Cheat_Sheets/Git-Reference_Sheet/#remote","title":"Remote","text":"<p>By default, fetch, pull and push will operate on the origin repo. This will be the repo you cloned from, or set manually using <code>git branch --set-upstream-to &lt;origin&gt;</code>.</p> fetch\u00a0 <code>git fetch</code> Gets status of\u00a0'origin'. git fetch does not change your working directory or local repository (see <code>git pull</code>).\u00a0 <code>git fetch &lt;repo&gt; &lt;branch&gt;</code> Get status of <code>&lt;repo&gt;</code> <code>&lt;branch&gt;</code>. pull\u00a0 <code>git pull</code> Incorporates changes from 'origin' into local repo.\u00a0 <code>git pull &lt;repo&gt; &lt;branch&gt;</code> Incorporates changes from <code>&lt;repo&gt;</code> <code>&lt;branch&gt;</code> into local repo. push\u00a0 <code>git push</code> Incorporates changes from local repo into 'origin'.\u00a0 <code>git push &lt;repo&gt; &lt;branch&gt;</code> Incorporates changes from local repo into <code>&lt;repo&gt;</code> <code>&lt;branch&gt;</code> <p>Tip</p> <p>If you are working without collaborators, there should be no reason to  have a conflict between your local and your remote repo. Make sure you  always git pull when starting work on your local and git push when  finished, this will save you wasting time resolving unnecessary  merges.</p>","tags":["git","github","version control","repository"]},{"location":"Getting_Started/Cheat_Sheets/Git-Reference_Sheet/#branches","title":"Branches","text":"<p>At an introductory level, it is best to avoid workflows that lead to multiple branches, or requires merging.</p> branch\u00a0 <code>git branch</code> List branches. <code>git branch &lt;branch-name&gt;</code> Create new branch <code>&lt;branch-name</code> checkout <code>git checkout &lt;branch-name&gt;</code> Switch to editing branch <code>&lt;branch-name&gt;</code> merge <code>git merge &lt;branch-name&gt;</code> Merge <code>&lt;branch-name&gt;</code> into current branch. <p>Other Resources</p> <ul> <li>oshitgit.com</li> </ul>","tags":["git","github","version control","repository"]},{"location":"Getting_Started/Cheat_Sheets/Slurm-Reference_Sheet/","title":"Slurm: Reference Sheet","text":"<p>If you are unsure about using our job scheduler Slurm, more details can be found here.</p>","tags":[]},{"location":"Getting_Started/Cheat_Sheets/Slurm-Reference_Sheet/#slurm-commands","title":"Slurm Commands","text":"<p>A complete list of Slurm commands can be found here, or by entering\u00a0man slurm into a terminal</p> sbatch <code>sbatch submit.sl</code> Submits the Slurm script submit.sl squeue <code>squeue</code> Displays entire queue. <code>squeue --me</code> Displays your queued jobs. <code>squeue -p long</code> Displays queued jobs on the\u00a0long partition. sacct <code>sacct</code> Displays all the jobs run by you that day. <code>sacct -S 2019-01-01</code> Displays all the jobs run by you since the 1st Jan 2019 <code>sacct -j 123456789</code> Displays job 123456789 scancel <code>scancel\u00a0123456789</code> Cancels job 123456789 <code>scancel --me</code> Cancels all your jobs. sshare <code>sshare -U</code> Shows the Fair Share scores for all projects of which\u00a0you\u00a0are a member. sinfo <code>sinfo</code> Shows the current state of our Slurm partitions.","tags":[]},{"location":"Getting_Started/Cheat_Sheets/Slurm-Reference_Sheet/#sbatch-options","title":"`sbatch`` options","text":"<p>A complete list of sbatch options can be found here, or by running man sbatch</p> <p>Options can be provided on the command line or in the batch file as an <code>#SBATCH</code> directive. \u00a0The option name and value can be separated using an '=' sign e.g. <code>#SBATCH --account=nesi99999</code>\u00a0or a space e.g. <code>#SBATCH --account nesi99999</code>.\u00a0But not both!</p>","tags":[]},{"location":"Getting_Started/Cheat_Sheets/Slurm-Reference_Sheet/#general-options","title":"General options","text":"<p>--job-name              <code>#SBATCH --job-name=MyJob</code>               The name that will appear when using squeue or sacct</p> <p>--account               <code>#SBATCH --account=nesi99999</code>            The account your core hours will be 'charged' to.</p> <p>--time                  <code>#SBATCH --time=DD-HH:MM:SS</code>             Job max walltime  </p> <p>--mem                   <code>#SBATCH --mem=512MB</code>                    Memory required per node.</p> <p>--partition             <code>#SBATCH --partition=long</code>               Specified job                                                                    partition.</p> <p>--output                <code>#SBATCH --output=%j_output.out</code>         Path and name of standard output file.</p> <p>--mail-user             <code>#SBATCH --mail-user=bob123@gmail.com</code>   Address to send mail notifications.</p> <p>--mail-type             <code>#SBATCH --mail-type=ALL</code>                Will send a mail notification at <code>BEGIN END FAIL</code></p> <pre><code>                      `#SBATCH --mail-type=TIME_LIMIT_80`      Will send message at *80%* walltime\n</code></pre> <p>--no-requeue            <code>#SBATCH --no-requeue</code>                   Will stop job being requeued in the case of node failure.</p>","tags":[]},{"location":"Getting_Started/Cheat_Sheets/Slurm-Reference_Sheet/#parallel-options","title":"Parallel options","text":"--nodes <code>#SBATCH --nodes=2</code> Will request tasks be run across 2 nodes. --ntasks <code>#SBATCH --ntasks=2</code> Will start 2 MPI tasks. --ntasks-per-node <code>#SBATCH --ntasks-per-node=1</code> Will start 1 task per requested node --cpus-per-task <code>#SBATCH --cpus-per-task=10</code> <p>Will request 10 logical CPUs per task.</p> <p>See Hyperthreading.</p> --mem-per-cpu <code>#SBATCH\u00a0--mem-per-cpu=512MB</code> <p>Memory Per logical CPU.</p> <p><code>--mem</code> Should be used if shared memory job.</p> <p>See How do I request memory?.</p> --array <code>#SBATCH --array=1-5</code> Will submit job 5 times each with a different <code>$SLURM_ARRAY_TASK_ID</code> (1,2,3,4,5) <code>#SBATCH --array=0-20:5</code> Will submit job 5 times each with a different <code>$SLURM_ARRAY_TASK_ID</code> (0,5,10,15,20) <code>#SBATCH --array=1-100%10</code> Will submit 1 though to 100 jobs but no more than 10 at once.","tags":[]},{"location":"Getting_Started/Cheat_Sheets/Slurm-Reference_Sheet/#other","title":"Other","text":"--qos <code>#SBATCH --qos=debug</code> Adding this line gives your job a very high priority. Limited to one job at a time, max 15 minutes. --profile <code>#SBATCH\u00a0--profile=ALL</code> <p>Allows generation of a .h5 file containing job profile information.</p> <p>See Slurm Native Profiling.</p> --dependency <code>#SBATCH\u00a0--dependency=afterok:123456789</code> Will only start after the job 123456789 has completed. --hint <code>#SBATCH\u00a0--hint=nomultithread</code> Disables hyperthreading, be aware that this will significantly change how your job is defined. <p>Tip</p> <p>Many options have a short and long form e.g.  <code>#SBATCH --job-name=MyJob</code> &amp; <code>#SBATCH -J=MyJob</code>.</p> <pre><code>echo \"Completed task ${SLURM_ARRAY_TASK_ID} / ${SLURM_ARRAY_TASK_COUNT} successfully\"\n</code></pre>","tags":[]},{"location":"Getting_Started/Cheat_Sheets/Slurm-Reference_Sheet/#tokens","title":"Tokens","text":"<p>These are predefined variables that can be used in sbatch directives such as the log file name.</p>","tags":[]},{"location":"Getting_Started/Cheat_Sheets/Slurm-Reference_Sheet/#environment-variables","title":"Environment variables","text":"<p>Common examples.</p> <code>$SLURM_JOB_ID</code> Useful for naming output files that won't clash. <code>$SLURM_JOB_NAME</code> Name of the job. <code>$SLURM_ARRAY_TASK_ID</code> The current index of your array job.\u00a0 <code>$SLURM_CPUS_PER_TASK</code> Useful as an input for multi-threaded functions. <code>$SLURM_NTASKS</code> Useful as an input for MPI functions. <code>$SLURM_SUBMIT_DIR</code> Directory where <code>sbatch</code> was called. <p>Tip</p> <p>In order to decrease the chance of a variable being misinterpreted you  should use the syntax <code>${NAME_OF_VARIABLE}</code> and define in strings if  possible. e.g.</p> <pre><code>echo \"Completed task ${SLURM_ARRAY_TASK_ID} / ${SLURM_ARRAY_TASK_COUNT} successfully\"\n</code></pre>","tags":[]},{"location":"Getting_Started/Cheat_Sheets/tmux-Reference_sheet/","title":"tmux: Reference sheet","text":"<p>tmux is a terminal multiplexer.\u00a0 A multiplexer enables the creation and control of multiple terminals from a single screen.\u00a0 It also allows the detachment of a screen to run in the background with the ability to re-attach and start where you left off.</p> <p>Here is an example of starting\u00a0 a tmux session:</p> <pre><code>tmux new -s data_transfer\ncd /nesi/nobackup/nesi99999/myproject\nrsync -av someserver:/projectdata.tgz projectdata.tgz\n</code></pre> <p>then</p> <p>ctrl + b, d</p> <p>The ctrl + b, dkeyboard shortcut \"detaches\" the screen which allows you to logoff .\u00a0 When you are ready to reattach to the session you login and run the following:</p> <pre><code>tmux attach -t data_transfer\n</code></pre> <p>Once reattached your session will be where you left it. \u00a0 You can name the session whatever is most appropriate, such as the task you are performing.\u00a0 You can run as many sessions as you like and they will remain until you terminate the tmux session or the node is rebooted. Also of note, your session will be available even if your laptop/desktop crashes or the network goes down.</p> <p>More information can be found on the web, here are some good references:</p> <p>Shortcut keys and cheat sheet</p> <p>Getting started Guide</p>","tags":[]},{"location":"Getting_Started/Getting_Help/Consultancy/","title":"Consultancy","text":"<p>NeSI's Consultancy service provides scientific and HPC-focussed computational and data science support to research projects across a range of domains.</p>","tags":["help"]},{"location":"Getting_Started/Getting_Help/Consultancy/#need-support-with-your-research-project","title":"Need support with your research project?","text":"<p>If you would like to learn more about NeSI's Consultancy service and how you can work with NeSI's Research Software and Data Science Engineers on a project, please Contact our Support Team\u00a0to set up an initial meeting. We can discuss your needs and\u00a0complete a Consultancy application form together.</p> <p>Researchers from NeSI collaborator institutions (University of Auckland, NIWA, University of Otago and Manaaki Whenua - Landcare Research) and those with Merit projects can usually access consultancy at no cost to themselves, based on their institution's or MBIE's investment into NeSI.</p>","tags":["help"]},{"location":"Getting_Started/Getting_Help/Consultancy/#what-do-we-do","title":"What do we do?","text":"<p>The NeSI team are available to help with any stage of your research software development. We can get involved with designing and developing your software from scratch, or assist with improving software you have already written.</p> <p>The service is completely bespoke and tailored to your requirements. Some examples of outcomes we could assist with (this list is general and non-exhaustive):</p> <ul> <li>Code development<ul> <li>Design and develop research software from scratch</li> <li>Algorithmic improvements</li> <li>Translate Python/R/Matlab code to C/C++/Fortran for faster     execution</li> <li>Accelerate code by offloading computations to a GPU</li> <li>Develop visualisation and post-processing tools (GUIs, dashboards, etc)</li> </ul> </li> <li>Performance improvement<ul> <li>Code optimisation \u2013 profile and improve efficiency (speed and     memory), IO performance</li> <li>Parallelisation \u2013 software (OpenMP, MPI, etc.) and workflow     parallelisation</li> </ul> </li> <li>Improve software sustainability (version control, testing,     continuous integration, etc)</li> <li>Data Science Engineering<ul> <li>Optimise numerical performance of machine learning pipelines</li> <li>Conduct an Exploratory Data Analysis</li> <li>Assist with designing and fitting explanatory and predictive     models</li> </ul> </li> <li>Anything else you can think of ;-)</li> </ul>","tags":["help"]},{"location":"Getting_Started/Getting_Help/Consultancy/#what-can-you-expect-from-us","title":"What can you expect from us?","text":"<p>During a consultancy project we aim to provide:</p> <ul> <li>Expertise and advice</li> <li>An agreed timeline to develop or improve a solution (typical     projects are of the order of 1 day per week for up to 4 months but     this is determined on a case-by-case basis)</li> <li>Training, knowledge transfer and/or capability development</li> <li>A summary document outlining what has been achieved during the     project</li> <li>A case study published on our website after the project has been     completed, to showcase the work you are doing on NeSI</li> </ul>","tags":["help"]},{"location":"Getting_Started/Getting_Help/Consultancy/#what-is-expected-of-you","title":"What is expected of you?","text":"<p>Consultancy projects are intended to be a collaboration and thus some input is required on your part. You should be willing to:</p> <ul> <li>Contribute to a case study upon successful completion of the     consultancy project</li> <li>Complete a short survey to help us measure the impact of our service</li> <li>Attend regular meetings (usually via video conference)</li> <li>Invest time to answer questions, provide code and data as necessary     and make changes to your workflow if needed</li> <li>Acknowledge     NeSI in article and code publications that we have contributed to,     which could include co-authorship if our contribution is deemed     worthy</li> <li>Accept full ownership/maintenance of the work after the project     completes (NeSI's involvement in the project is limited to the     agreed timeline)</li> </ul>","tags":["help"]},{"location":"Getting_Started/Getting_Help/Consultancy/#previous-projects","title":"Previous projects","text":"<p>Listed below are some examples of previous projects we have contributed to:</p> <ul> <li>A quantum casino helps define atoms in the big     chill</li> <li>Using statistical models to help New Zealand prepare for large     earthquakes</li> <li>Improving researchers' ability to access and analyse climate model     data     sets</li> <li>Speeding up the post-processing of a climate model data     pipeline</li> <li>Overcoming data processing overload in scientific web mapping     software</li> <li>Visualising ripple effects in riverbed sediment     transport</li> <li>New Zealand's first national river flow forecasting system for     flooding     resilience</li> <li>A fast model for predicting floods and storm     damage</li> <li>How multithreading and vectorisation can speed up seismic     simulations by     40%</li> <li>Machine learning for marine     mammals</li> <li>Parallel processing for ocean     life</li> <li>NeSI support helps keep NZ rivers     healthy</li> <li>Heating up nanowires with     HPC</li> <li>The development of next generation weather and climate models is     heating     up</li> <li>Understanding the behaviours of     light</li> <li>Getting closer to more accurate climate predictions for New     Zealand</li> <li>Fractal analysis of brain signals for autism spectrum     disorder</li> <li>Optimising tools used for genetic     analysis</li> <li>Investigating climate     sensitivity</li> <li>Tracking coastal precipitation systems in the     tropics</li> <li>Powering global climate     simulations</li> <li>Optimising tools used for genetic     analysis</li> <li>Investigating climate     sensitivity</li> <li>Improving earthquake forecasting     methods</li> <li>Modernising models to diagnose and treat disease and     injury</li> <li>Cataloguing NZ's earthquake     activities</li> <li>Finite element modelling of biological     cells</li> <li>Preparing New Zealand to adapt to climate     change</li> <li>Using GPUs to expand our understanding of the solar     system</li> <li>Speeding up Basilisk with     GPGPUs</li> <li>Helping communities anticipate flood     events</li> </ul>","tags":["help"]},{"location":"Getting_Started/Getting_Help/Introductory_Material/","title":"Introductory Material","text":"<p>If you are new to NeSI, short introductory lectures are available on YouTube here to help get you started on the systems. These videos are open to anyone interested in learning more about High Performance Computing (HPC) using NeSI, prior to getting a NeSI account.</p> <p>The recordings present an overview of NeSI systems and the HPC platforms' capabilities, available software and environment modules, best practice procedures, common commands for submitting jobs, and optimising your work on the system.</p> <p>Slides used in the lectures can be found here.</p>","tags":["introduction","talk","workshop","introductory","office hours"]},{"location":"Getting_Started/Getting_Help/Introductory_Material/#other-ways-to-get-introductory-help","title":"Other ways to get 'Introductory' help","text":"<p>In addition to the material mentioned above, you can also seek help at our weekly \"Office Hours\" or by attending a Workshop. Office hours are drop-in sessions hosted by the NeSI Support Team where any and all questions are welcome. Our Office Hours schedule can be found here.</p> <p>NeSI also participates in and organises Workshops covering a range of topics. Many of these workshops - and particularly \"Introduction to HPC using NeSI\", are designed for new users.</p> <p>Finally, If you wish to schedule a live zoom introductory session covering introductory material (perhaps useful for research groups), please Contact our Support Team and we can advise you on our availability.</p>","tags":["introduction","talk","workshop","introductory","office hours"]},{"location":"Getting_Started/Getting_Help/Job_efficiency_review/","title":"Job efficiency review","text":"","tags":[]},{"location":"Getting_Started/Getting_Help/Job_efficiency_review/#introduction","title":"Introduction","text":"<p>At NeSI we want to help you run your work as efficiently as possible on our platforms, so that you obtain your results in a timely manner and so that we can fit in as much work as possible, hopefully reducing queue times for all users.</p> <p>With this aim in mind, we may ask to perform a \"job efficiency review\" of your work on NeSI, particularly if you have been granted, or are requesting, a large compute allocation. See below for more details about what to expect during this process. If you are interested in us looking into your job efficiency for potential improvements, please Contact our Support Team.</p>","tags":[]},{"location":"Getting_Started/Getting_Help/Job_efficiency_review/#expected-outcomes","title":"Expected outcomes","text":"<p>At the end of a job efficiency review you could expect one of the following outcomes:</p> <ul> <li>We determine that your workflow/jobs are running efficiently on our     platform</li> <li>Some areas for improvement are identified (and agreed with you)<ul> <li>For \"quick wins\" we may be able to achieve these improvements     within the scope of the job efficiency review</li> <li>For larger pieces of work, we would assist you in applying for a     NeSI Consultancy     project, where we would work with you on a longer term project     to implement any agreed changes</li> </ul> </li> </ul>","tags":[]},{"location":"Getting_Started/Getting_Help/Job_efficiency_review/#what-you-can-expect-from-us","title":"What you can expect from us","text":"<p>During a job efficiency review you can expect that we will:</p> <ul> <li>Spend some time (typically up to 10-20 hours) to investigate your     software and workflows that you are running on NeSI, to determine     whether there is an opportunity for optimisation or efficiency     improvements</li> <li>Communicate clearly and pass on any suggestions for improvements     that we identify</li> </ul>","tags":[]},{"location":"Getting_Started/Getting_Help/Job_efficiency_review/#what-we-expect-of-you","title":"What we expect of you","text":"<p>During a job efficiency review, some input will be required from you, such as:</p> <ul> <li>Investing time to answer questions, provide code and input data as     necessary and make changes to your workflow if needed (this may     involve attending some Zoom meetings and/or email communication).</li> <li>Setting up some test configurations that we can use for profiling     and benchmarking your jobs; these should be representative of your     work but don't necessarily need to be complete calculations. For     example, with a simulation code we could choose to reduce the number     of time steps but keep the domain size the same</li> </ul>","tags":[]},{"location":"Getting_Started/Getting_Help/NeSI_wide_area_network_connectivity/","title":"NeSI wide area network connectivity","text":"<p>NeSI's national platform facilities are connected to the REANNZ network, Aotearoa's high-performance national digital network (or NREN). This national network supports collaboration and contributions to data-intensive and complex science and research initiatives in New Zealand and across the globe.</p>","tags":[]},{"location":"Getting_Started/Getting_Help/NeSI_wide_area_network_connectivity/#how-to-verify-the-status-of-external-wide-area-network-wan-connectivity-for-nesi","title":"How to verify the status of external (wide area network - WAN) connectivity for NeSI","text":"<p>A real-time network status page is available on the REANNZ website as a 'weather map', which shows all traffic across its national network:</p> <p>https://weathermap.reannz.co.nz</p> <p>From that main \"weather map\" view, you can drill down to a more detailed view of specific addresses, e.g.</p>","tags":[]},{"location":"Getting_Started/Getting_Help/NeSI_wide_area_network_connectivity/#wellington-wlg-for-niwa","title":"Wellington (WLG) for NIWA","text":"<p>https://weathermap.reannz.co.nz/index.php?src=and39</p>","tags":[]},{"location":"Getting_Started/Getting_Help/NeSI_wide_area_network_connectivity/#mayoral-drive-auckland-mdr","title":"Mayoral Drive, Auckland (MDR)","text":"<p>https://weathermap.reannz.co.nz/index.php?src=and15</p>","tags":[]},{"location":"Getting_Started/Getting_Help/NeSI_wide_area_network_connectivity/#uoa-tamaki-data-centre-tdc","title":"UoA Tamaki Data Centre (TDC)","text":"<p>https://weathermap.reannz.co.nz/index.php?src=red07</p>","tags":[]},{"location":"Getting_Started/Getting_Help/System_status/","title":"System status","text":"<p>See also</p> <p>NeSI wide area network connectivity</p>","tags":["help"]},{"location":"Getting_Started/Getting_Help/System_status/#nesi-system-status-related-notifications","title":"NeSI system status related notifications","text":"<p>All new NeSI users will be automatically subscribed to receive system notifications for all components listed on status.nesi.org.nz (with the option to opt-out). The support.nesi.org.nz homepage shows current incidents and upcoming scheduled events (based on status.nesi.org.nz).</p>","tags":["help"]},{"location":"Getting_Started/Getting_Help/System_status/#how-to-manage-your-subscription-to-notifications","title":"How to manage your subscription to notifications","text":"<p>In order to manage your subscription to notifications, either log into my.nesi or use the link included at the bottom of the notification email message \"Manage your subscription\" or \"Unsubscribe\" to manage your preferences.</p> <p>See also our support article Managing NeSI notification preferences</p> <p></p>","tags":["help"]},{"location":"Getting_Started/Getting_Help/System_status/#statusnesiorgnz","title":"status.nesi.org.nz","text":"<p>NeSI does publish service incidents and scheduled maintenance via status.nesi.org.nz. Interested parties are invited to subscribe to updates (via SMS or email).</p> <p></p>","tags":["help"]},{"location":"Getting_Started/Getting_Help/Weekly_Online_Office_Hours/","title":"Weekly Online Office Hours","text":"<p>Have questions about NeSI services Looking for tips on how to optimise your HPC jobs? Or, simply want to meet some of the team behind NeSI Support?</p> <p>We run regular online Office Hours sessions, hosted via Zoom. These sessions are open to anyone, you don't need to be an existing NeSI user.</p>","tags":[]},{"location":"Getting_Started/Getting_Help/Weekly_Online_Office_Hours/#office-hours-in-december-2023","title":"Office Hours in December 2023","text":"<p>Click on the links below to add the date &amp; Zoom link to your calendar:</p> <ul> <li> <p>13 December (Wednesday): 9:00-10:00 AM</p> </li> <li> <p>20 December (Wednesday): 3:00-4:00 PM</p> </li> </ul> <p>If you are unable to add an Office Hour session to your calendar through these links, please email us at training@nesi.org.nz\u00a0and we can send a calendar invite directly to you.</p>","tags":[]},{"location":"Getting_Started/Getting_Help/Weekly_Online_Office_Hours/#how-does-it-work","title":"How Does It Work","text":"<p>Each session follows a casual 'drop-in / drop-out' format, where you can pop in at any point during the hour and stay for as long or as little as you'd like.</p> <p>Also, don't worry if you have a question or challenge that can't be solved on the spot. We can always use the Office Hours to collect some basic information about your issue and then reconnect with you at a later time to troubleshoot things further.</p>","tags":[]},{"location":"Getting_Started/Getting_Help/Weekly_Online_Office_Hours/#other-ways-to-get-help","title":"Other ways to get help","text":"<p>Remember, you can  Contact our Support Team at any time and we have a team of experts ready to answer any questions you may have, big or small. You can also find helpful user resources, links and documentation elsewhere in our User Support Centre.</p>","tags":[]},{"location":"Getting_Started/Getting_Help/Weekly_Online_Office_Hours/#feedback","title":"Feedback","text":"<p>If you have any suggestions for ways to improve these Office Hours sessions, please fill out this feedback form.</p>","tags":[]},{"location":"Getting_Started/Next_Steps/Finding_Job_Efficiency/","title":"Finding Job Efficiency","text":"","tags":["slurm"]},{"location":"Getting_Started/Next_Steps/Finding_Job_Efficiency/#on-job-completion","title":"On Job Completion","text":"<p>It is good practice to have a look at the resources your job used on completion, this way you can improve your job specifications in the future.</p> <p>Once your job has finished check the relevant details using the tools: <code>nn_seff</code> or <code>sacct</code> For example:  </p>","tags":["slurm"]},{"location":"Getting_Started/Next_Steps/Finding_Job_Efficiency/#using-nn_seff","title":"Using <code>nn_seff</code>","text":"<pre><code>nn_seff 30479534\n</code></pre> <pre><code>Job ID: 1936245\nCluster: mahuika\nUser/Group: user/group\nState: COMPLETED (exit code 0)\nCores: 1\nTasks: 1\nNodes: 1\nJob Wall-time: 7.67% 00:01:09 of 00:15:00 time limit\nCPU Efficiency: 98.55% 00:01:08 of 00:01:09 core-walltime\nMem Efficiency: 10.84% 111.00 MB of 1.00 GB\n</code></pre> <p>Notice that the CPU efficiency was high but the memory efficiency was low and consideration should be given to reducing memory requests for similar jobs.\u00a0 If in doubt, please contact support@nesi.org.nz for guidance.</p>","tags":["slurm"]},{"location":"Getting_Started/Next_Steps/Finding_Job_Efficiency/#using-sacct","title":"Using <code>sacct</code>","text":"<pre><code>sacct --format=\"JobID,JobName,Elapsed,AveCPU,MinCPU,TotalCPU,Alloc,NTask,MaxRSS,State\" -j &lt;jobid&gt;\n</code></pre> <p>Tip</p> <p>If you want to make this your default <code>sacct</code> setting, run; <pre><code>echo 'export SACCT_FORMAT=\"JobID,JobName,Elapsed,AveCPU,MinCPU,TotalCPU,Alloc%2,NTask%2,MaxRSS,State\"' &gt;&gt; ~/.bash_profile\nsource ~/.bash_profile\n</code></pre></p> <p>Below is an output for reference:</p> <pre><code>       JobID    JobName    Elapsed     AveCPU     MinCPU   TotalCPU  AllocCPUS   NTasks     MaxRSS      State\n------------ ---------- ---------- ---------- ---------- ---------- ---------- -------- ---------- ----------\n3007056      rfm_ANSYS+   00:27:07                         03:35:55         16                      COMPLETED\n3007056.bat+      batch   00:27:07   03:35:54   03:35:54   03:35:55         16        1  13658349K  COMPLETED\n3007056.ext+     extern   00:27:07   00:00:00   00:00:00   00:00:00         16        1        89K  COMPLETED\n</code></pre> <p>All of the adjustments below still allow for a degree of variation. There may be factors you have not accounted for.</p>","tags":["slurm"]},{"location":"Getting_Started/Next_Steps/Finding_Job_Efficiency/#walltime","title":"Walltime","text":"<p>From the <code>Elapsed</code> field we may want to update our next run to have a more appropriate walltime.</p> <pre><code>#SBATCH --time=00:40:00\n</code></pre>","tags":["slurm"]},{"location":"Getting_Started/Next_Steps/Finding_Job_Efficiency/#memory","title":"Memory","text":"<p>The <code>MaxRSS</code> field shows the maximum memory used by each of the job steps, so in this case 13 GB. For our next run we may want to set:</p> <pre><code>#SBATCH --mem=15G\n</code></pre>","tags":["slurm"]},{"location":"Getting_Started/Next_Steps/Finding_Job_Efficiency/#cpus","title":"CPUs","text":"<p><code>TotalCPU</code> is the number of computation hours, in the best case scenario the computation hours would be equal to <code>Elapsed</code> x <code>AllocCPUS</code>.</p> <p>In this case our ideal <code>TotalCPU</code> would be 07:12:00, as our job only managed 03:35:55 we can estimate the CPU usage was around 50% It might be worth considering reducing the number of CPUs requested, however bear in mind there are other factors that affect CPU efficiency.</p> <pre><code>#SBATCH --cpus-per-task=10\n</code></pre> <p>Note: When using sacct to determine the amount of memory your job used - in order to reduce memory wastage - please keep in mind that Slurm reports the figure as RSS (Resident Set Size) when in fact the metric being displayed is PSS (Proportional Set Size). This is an issue with Slurm and cannot currently be fixed. PSS is a more accurate measure of memory usage than RSS - RSS shows the sum of memory used including shared libraries, therefore this gives a figure that is more often than not greater than the actual amount of memory used by your job. PSS provides a more accurate measure.</p> <p>Further technical notes for those interested in commonly used memory usage metrics on linux systems:</p> <p>VSS &gt;= RSS &gt;= PSS &gt;= USS VSS-Virtual Set Size - Virtual memory consumption (contains memory consumed by shared libraries) RSS-Resident Set Size - Used physical memory (contains memory consumed by shared libraries) PSS-Proportional Set Size - Actual physical memory used (proportional allocation of memory consumed by shared libraries) USS-Unique Set Size - Process consumed physical memory alone (does not contain the memory occupied by the shared library) <code>PSS = USS + (RSS/# shared processes)</code></p>","tags":["slurm"]},{"location":"Getting_Started/Next_Steps/Finding_Job_Efficiency/#during-runtime","title":"During Runtime","text":"<p>In order to check in on a job that is running, you will need to ssh to the compute node where it it running.</p>","tags":["slurm"]},{"location":"Getting_Started/Next_Steps/Finding_Job_Efficiency/#finding-job-node","title":"Finding Job Node","text":"<p>If 'nodelist' is not one of the fields in the output of your <code>sacct</code> or <code>squeue</code> commands you can find the node a job is running on using the command; <code>squeue -h -o %N -j &lt;jobid&gt;</code>\u00a0The node will look something like <code>wbn123</code> on Mahuika or <code>nid00123</code> on M\u0101ui</p> <p>Note</p> <p>If your job is using MPI it may be running on multiple nodes</p>","tags":["slurm"]},{"location":"Getting_Started/Next_Steps/Finding_Job_Efficiency/#using-htop","title":"Using <code>htop</code>","text":"<pre><code>ssh -t wbn175 htop -u $USER\n</code></pre> <p>If it is your first time connecting to that particular node, you may be prompted:</p> <pre><code>The authenticity of host can't be established\u00a0\nAre you sure you want to continue connecting (yes/no)?\n</code></pre> <p>Reply <code>yes</code>. Y alone (upper or lower case) is not sufficient.</p> <p>Focusing on the lower panel, you will see a printout of all of your current processes running on that node. If you have multiple jobs on the same node, they will all be shown (you can tell them apart by their parent process).</p> <p>Processes in green can be ignored</p> <p></p> <p>RES - Current memory being used (same thing as 'RSS' from sacct)</p> <p>S - State, what the thread is currently doing.</p> <ul> <li>R - Running</li> <li>S - Sleeping, waiting on another thread to finish.</li> <li>D - Sleeping</li> <li>Any other letter - Something has gone wrong!</li> </ul> <p>CPU% - Percentage CPU utilisation.</p> <p>MEM% - Percentage Memory utilisation.</p> <p>Warning</p> <p>If the job finishes, or is killed you will be kicked off the node. If  htop freezes, type <code>reset</code> to clear your terminal.</p>","tags":["slurm"]},{"location":"Getting_Started/Next_Steps/Finding_Job_Efficiency/#limitations-of-using-cpu-efficiency","title":"Limitations of using CPU Efficiency","text":"<p>CPU efficiency, as described here, only represents the percentage of time the CPUs are in use. This is not enough to get a picture of overall job efficiency, as required CPU time may vary by number of CPUs.</p> <p>The only way to get the full context, is to compare walltime performance between jobs at different scale. See\u00a0Job Scaling for more details.</p>","tags":["slurm"]},{"location":"Getting_Started/Next_Steps/Finding_Job_Efficiency/#example","title":"Example","text":"<p>From the above plot of CPU efficiency, you might decide a 5% reduction of CPU efficiency is acceptable and scale your job up to 18 CPU cores .</p> <p></p> <p>However, when looking at a plot of walltime it becomes apparent that performance gains per CPU added drop significantly after 4 CPUs, and in fact absolute performance losses (negative returns) are seen after 8 CPUs.</p>","tags":["slurm"]},{"location":"Getting_Started/Next_Steps/Job_Scaling_Ascertaining_job_dimensions/","title":"Job Scaling - Ascertaining job dimensions","text":"<p>When you run software in an interactive environment such as your ordinary workstation (desktop PC or laptop), the software is able to request from the operating system whatever resources it needs from moment to moment. By contrast, on our HPC platforms, you must request your needed resources when you submit the job,\u00a0so that the scheduler can make sure enough resources are available for your job during the whole time it is running, and also knows what resources will be free for others to use at the same time.</p> <p>The three resources that every single job submitted on the platforms needs to request are:</p> <ul> <li>CPUs (i.e. logical CPU cores), and</li> <li>Memory (RAM), and</li> <li>Time.</li> </ul> <p>Some jobs will also need to request GPUs.</p>","tags":["scaling"]},{"location":"Getting_Started/Next_Steps/Job_Scaling_Ascertaining_job_dimensions/#what-happens-if-i-ask-for-the-wrong-resources","title":"What happens if I ask for the wrong resources?","text":"<p>When you are initially trying to set up your jobs it can be difficult to ascertain how much of each of these resources you will need. Asking for too little or too much, however, can both cause problems: your jobs will be at increased risk of taking a long time in the queue or failing, and your project's fair share score is likely to suffer.\u00a0Your project's fair share score will be reduced in view of compute time spent regardless of whether you obtain a result or not.</p> Resource Asking for too much Not asking for enough CPUs The job may wait in the queue for longer. Your fair share score will fall rapidly (your project will be charged for CPU cores that it reserved but didn't use) The job will run more slowly than expected, and so\u00a0may run out of time and get killed for exceeding its time limit. Memory The job may wait in the queue for longer. Your fair share score will fall more than necessary. Your job will fail, probably with an 'OUT OF MEMORY' error, segmentation fault or bus error. This may not happen immediately. Wall time The job may wait in the queue for longer than necessary The job will run out of time and get killed. <p>See our \"What is an allocation?\" support page for more details on how each resource effects your compute usage.</p> <p>It is therefore important to try and make your jobs resource requests reasonably accurate. In this article we will discuss how you can scale your jobs to help you better estimate your jobs resource needs.</p>","tags":["scaling"]},{"location":"Getting_Started/Next_Steps/Job_Scaling_Ascertaining_job_dimensions/#job-scaling","title":"Job Scaling","text":"<p>Before you start submitting the main bulk of your jobs, it is\u00a0advisable to first submit a test job.</p> <p>A test job should be representative of the main body of your work, scaled down (e.g. a small subset of your data or a low number of job steps). Aim for your test job to run for around 10 minutes, too much shorter and your job will be spending a high proportion of its time on overhead and therefore be less accurate for the purposes of scaling.</p> <p>Keeping your test job small ensures a short queue time, short run time and that minimal resources are expended.</p> <p>When scaling your jobs, one of the most beneficial things you can do is to first scale down your data and calculations to as small as you can. Whether this means only computing on a few rows and columns of your data, or only doing a subset of the calculations you intend to do in the complete jobs, cutting your initial test jobs down in size means that they will both queue faster and run for less time. Also, if one of these jobs fails due to not asking for enough resources, a small scale job will (hopefully) not have waited for hours or days in the queue beforehand.</p> <p>Example</p> <ul> <li>Multithreading Scaling</li> <li>MPI Scaling</li> </ul>","tags":["scaling"]},{"location":"Getting_Started/Next_Steps/MPI_Scaling_Example/","title":"MPI Scaling Example","text":"<p>In the example below we will use Python scripts to demonstrate how you might perform some basic scaling tests, however, the principles outlined in these examples are applicable across software applications. You do not need to know anything about Python to understand this article; it was merely chosen for the purpose of illustration.</p>","tags":[]},{"location":"Getting_Started/Next_Steps/MPI_Scaling_Example/#initial-python-script","title":"Initial Python Script","text":"<pre><code># Imports numpy and mpi4py\nimport numpy as np\nfrom mpi4py import MPI\n\n# Retrieves MPI environment\ncomm = MPI.COMM_WORLD\n# Sets size as the total number of MPI tasks\nsize = comm.Get_size()\n# Sets rank as the specific MPI rank on all MPI tasks\nrank = comm.Get_rank()\n# sets x and y dimensions which will be used for the generated matrix\nmatrix = 1000\nseeds = 60000\n\n# If the rank is 0 (master) then create a list of numbers from 0-4999 and then\n# split those seeds number equally amoung size groups, other set seeds and\n# split_seeds to $\nif rank == 0:\n    seeds = np.arange(seeds)\n    split_seeds = np.array_split(seeds, size, axis = 0)\nelse:\n    seeds = None\n    split_seeds = None\n\n# Scatter the seeds among each MPI task\nrank_seeds = comm.scatter(split_seeds, root = 0)\n# Create a array of zeros of the lenght of the MPI tasks seeds\nrank_data = np.zeros(len(rank_seeds))\n\n# For each number from 0 to the number of the MPI tasks, list of seeds and use\n# one of those seeds to set the random seed (ensuring each random seed is\n# different). Then create an array of random numbers with x and y equal to the\n# matrix variable. Then calculate the dot product of the array with itself.\nfor i in np.arange(len(rank_seeds)):\n    seed = rank_seeds[i]\n    np.random.seed(seed)\n    data = np.random.rand(matrix,matrix)\n    data_mm = np.dot(data, data)\n    rank_data[i] = sum(sum(data_mm))\nrank_sum = sum(rank_data)\n\ndata_gather = comm.gather(rank_sum, root = 0)\n\nif rank == 0:\n    data_sum = sum(data_gather)\n    print('Gathered data:', data_gather)\n    print('Sum:', data_sum)\n</code></pre> <p>You do not need to understand what the above Python script is doing, but for context, the script will create a list of numbers and split them between the available MPI tasks (ranks) then uses those numbers as seeds to create arrays of random numbers. The dot product of each array is then calculated and those numbers are summed together and sent back to the master task (via MPI) and all those numbers are summed together and the results are printed. The script will take the number of MPI tasks from the environment and and run it in parallel with that many ranks. We unfortunately do not know how many CPUs, how much memory (RAM) or how much time to request for this script to complete. This means the first thing we need to do is run a small scale test and see how long that runs for. So we will first try with 5,000 seeds rather than 60,000 seeds.</p>","tags":[]},{"location":"Getting_Started/Next_Steps/MPI_Scaling_Example/#revised-python-script","title":"Revised Python Script","text":"<pre><code># Imports numpy and mpi4py\nimport numpy as np\nfrom mpi4py import MPI\n\n# Retrieves MPI environment\ncomm = MPI.COMM_WORLD\n# Sets size as the total number of MPI tasks\nsize = comm.Get_size()\n# Sets rank as the specific MPI rank on all MPI tasks\nrank = comm.Get_rank()\n# sets x and y dimensions which will be used for the generated matrix\nmatrix = 1000\nseeds = 5000\n\n# If the rank is 0 (master) then create a list of numbers from 0-4999 and then\n# split those seeds number equally amoung size groups, other set seeds and\n# split_seeds to $\nif rank == 0:\n    seeds = np.arange(seeds)\n    split_seeds = np.array_split(seeds, size, axis = 0)\nelse:\n    seeds = None\n    split_seeds = None\n\n# Scatter the seeds among each MPI task\nrank_seeds = comm.scatter(split_seeds, root = 0)\n# Create a array of zeros of the lenght of the MPI tasks seeds\nrank_data = np.zeros(len(rank_seeds))\n\n# For each number from 0 to the number of the MPI tasks, list of seeds and use\n# one of those seeds to set the random seed (ensuring each random seed is\n# different). Then create an array of random numbers with x and y equal to the\n# matrix variable then calculate the dot product of the array with itself.\nfor i in np.arange(len(rank_seeds)):\n    seed = rank_seeds[i]\n    np.random.seed(seed)\n    data = np.random.rand(matrix,matrix)\n    data_mm = np.dot(data, data)\n    rank_data[i] = sum(sum(data_mm))\nrank_sum = sum(rank_data)\n\ndata_gather = comm.gather(rank_sum, root = 0)\n\nif rank == 0:\n    data_sum = sum(data_gather)\n    print('Gathered data:', data_gather)\n    print('Sum:', data_sum)\n</code></pre> <p>Now we need to write a Slurm script to run this job. The wall time, number of logical CPU cores and amount of memory (RAM) you request for this job will ideally be based on how this small-scale test runs on your local workstation, but if that is not possible, make an educated guess, and if the job fails increase the resources requested until is completes.</p> <p>Tip</p> <p>If you can, write your program so that it prints results and timing information out relatively frequently, for example every 100 or 1,000 iterations. That way, even if your job runs out of time or memory and gets killed, you will be able to see how far it got and how long it took to get there.</p>","tags":[]},{"location":"Getting_Started/Next_Steps/MPI_Scaling_Example/#slurm-script","title":"Slurm Script","text":"<pre><code>#!/bin/bash -e\n#SBATCH --job-name=MPIScaling2\n#SBATCH --ntasks=2\n#SBATCH --time=00:30:00\n#SBATCH --mem-per-cpu=512MB\n\nmodule load Python\nsrun python MPIscaling.py\n</code></pre> <p>Let's run our Slurm script with sbatch and look at our output from <code>sacct</code>.</p> <pre><code>         JobID      JobName     Elapsed     TotalCPU Alloc   MaxRSS      State \n-------------- ------------ ----------- ------------ ----- -------- ----------\n6057011        MPIScaling2     00:18:51     01:14:30     4          COMPLETED \n6057011.bat+   batch           00:18:51    00:00.607     4    4316K COMPLETED \n6057011.ext+   extern          00:18:52    00:00.001     4        0 COMPLETED \n6057011.0      python          00:18:46     01:14:30     2  166744K COMPLETED \n</code></pre> <p>Our job performed 5,000 seeds using 2 physical CPU cores (each MPI task will always receive 2 logical CPUs which is equal to 1 physical CPUs. For a more in depth explanation about logical and physical CPU cores see our Hyperthreading article) and a maximum memory of 166,744KB (0.16 GB). In total, the job ran for 18 minutes and 51 seconds.</p> <p>We will initially assume that our job's wall time and memory will scale linearly with the number of iterations. However, we don't know that for certain that this is the case so we will need to understand the scaling behaviour of our job's resource requirements before we can submit our full job and be confident it will succeed.</p> <p>To find out we are going to have to run more tests. Let's try running our script with 2, 3, 4, 5 and 6 MPI tasks/physical CPUs and plot the results:</p> <pre><code>         JobID      JobName     Elapsed     TotalCPU Alloc   MaxRSS      State \n-------------- ------------ ----------- ------------ ----- -------- ----------\n6057011        MPIScaling2     00:18:51     01:14:30     4           COMPLETED \n6057011.bat+   batch           00:18:51    00:00.607     4    4316K  COMPLETED \n6057011.ext+   extern          00:18:52    00:00.001     4        0  COMPLETED \n6057011.0      python          00:18:46     01:14:30     2  166744K  COMPLETED\n6054936        MPIScaling3     00:12:29     01:14:10     6           COMPLETED \n6054936.bat+   batch           00:12:29    00:00.512     2    4424K  COMPLETED \n6054936.ext+   extern          00:12:29    00:00.003     6        0  COMPLETED \n6054936.0      python          00:12:29     01:14:09     3  174948K  COMPLETED \n6054937        MPIScaling4     00:09:29     01:15:04     8           COMPLETED \n6054937.bat+   batch           00:09:29    00:00.658     2    4432K  COMPLETED \n6054937.ext+   extern          00:09:29    00:00.003     8        0  COMPLETED \n6054937.0      python          00:09:28     01:15:04     4  182064K  COMPLETED \n6054938        MPIScaling5     00:07:41     01:15:08    10           COMPLETED \n6054938.bat+   batch           00:07:41    00:00.679     2    4548K  COMPLETED \n6054938.ext+   extern          00:07:41    00:00.005    10        0  COMPLETED \n6054938.0      python          00:07:36     01:15:08     5  173632K  COMPLETED \n6054939        MPIScaling6     00:06:57     01:18:38    12           COMPLETED \n6054939.bat+   batch           00:06:57    00:00.609     2    4612K  COMPLETED \n6054939.ext+   extern          00:06:57    00:00.006    12      44K  COMPLETED \n6054939.0      python          00:06:51     01:18:37     6  174028K  COMPLETED \n</code></pre> <p></p> <p>First, looking at the plot (we used R here, but feel free to use excel or whatever your preferred plotting software) of memory usage per task vs CPUs it would at appears that memory usage per task remains constant, regardless of how many CPUs (equivalent to MPI tasks here) this job uses, because this is an MPI job each task must have its own memory, this is why, if we look back at the script we are using, that we request memory per CPU, rather than just memory with <code>--mem</code>, since MPI jobs will by their nature of having to read everything into memory once for each task, have their memory scale (at least) linearly. But the results of this plot means that we shouldn't have to worry about increasing the memory per CPU.</p> <p>One thing to note about our plot of CPUs versus memory is the fact that memory usage is not measured continuously, it is instead measured every 30 seconds. This means that if your job's memory usage has some spikes,\u00a0<code>sacct</code> will not necessarily detect the maximum memory usage. This is something that you should be aware of when you estimate the memory usage of all your jobs.</p> <p>Looking at the memory usage for an 8 CPU job, it looks like an 8 CPU has a maximum memory requirement of 0.18 GB.</p> <p> </p> <p>The two above plots show the number of CPUs vs time and the Log2 of the CPUs vs time.</p> <p>The reason we have both is that it can often be easier to see the inflection point on the Log2 graph when the speed up from increasing the number of CPUs start to level off, as in the Log2 graph if the jobs scaled perfectly linearly (e.g. doubling the CPU's halves the runtime) the line would be straight. The curving of the line in the Log2 graph represents a loss in efficiency from increasing the number of CPUs.</p> <p>As we can see, increasing the number of CPU cores doesn't linearly increase the job's speed.</p> <p>This non-linear speed-up is mostly caused by Amdahl's Law, which reflects the fact that there is a fixed part of the computation that is inherently serial, that is, some operations can't be started until others have already finished. Additionally, with MPI jobs, another major component of this non-linear speed-up is caused by MPI communication itself not being computationally free, what this means is that you can get to a point where the computational cost of adding additional MPI tasks is greater than the speed-up.</p> <p>Looking at the plot of CPUs vs time we can see the asymptotic speedup and this time the best number of CPUs to use for this job looks to be 5 physical CPUs.</p> <p>Now that we have determined that 5 physical CPUs is the optimal number of CPUs for our jobs we will use this as we will submit three more jobs, using 10,000 15,000 and 20,000 seeds.\u00a0</p> <pre><code>         JobID      JobName     Elapsed     TotalCPU Alloc   MaxRSS      State \n-------------- ------------ ----------- ------------ ----- -------- ----------\n6054938        MPIScaling5k    00:07:41     01:15:08    10           COMPLETED \n6054938.bat+   batch           00:07:41    00:00.679     2    4548K  COMPLETED \n6054938.ext+   extern          00:07:41    00:00.005    10        0  COMPLETED \n6054938.0      python          00:07:36     01:15:08     5  173632K  COMPLETED \n6059931        MPIScaling10k   00:14:57     02:27:36    10           COMPLETED \n6059931.bat+   batch           00:14:57    00:00.624    10    4320K  COMPLETED \n6059931.ext+   extern          00:14:57     00:00:00    10        0  COMPLETED \n6059931.0      python          00:14:56     02:27:36     5  170748K  COMPLETED \n6059939        MPIScaling15k   00:22:39     03:45:13    10           COMPLETED \n6059939.bat+   batch           00:22:39    00:00.631    10    4320K  COMPLETED \n6059939.ext+   extern          00:22:39     00:00:00    10        0  COMPLETED \n6059939.0      python          00:22:38     03:45:13     5  168836K  COMPLETED \n6059945        MPIScaling20k   00:30:34     05:02:42    10           COMPLETED \n6059945.bat+   batch           00:30:34    00:00.646    10    4320K  COMPLETED \n6059945.ext+   extern          00:30:34    00:00.001    10        0  COMPLETED \n6059945.0      python          00:30:32     05:02:41     5  172700K  COMPLETED \n</code></pre> <p>We can see from the <code>sacct</code> output that the wall time seems to be increasing as we add more seeds, but the maximum memory per CPU doesn't seem to change much. Let's try plotting this data to help us better understand what is happening:</p> <p></p> <p>This confirms our assumption of wall-time scaling linearly with number of iterations. Since our 5,000 seed job to 7 minutes and 41 seconds we can estimate that it will take about 12 times longer to run 60,000.</p>","tags":[]},{"location":"Getting_Started/Next_Steps/MPI_Scaling_Example/#estimating-our-total-resource-requirments","title":"Estimating our Total Resource Requirments","text":"<p>Now that we know approximately how our job's CPU, memory and wall time requirements scale, we can try and estimate our total resource requirements for our 60,000 iteration job.</p> <p>From this data we have determined that more than 5 physical CPUs has very limited additional speed up, and 5 CPU should use about 0.17 GB of memory per task (CPU) at most, and that this memory requirement should remain relatively consistent, regardless of the number of seeds. Given this information we can estimate our full size job's resource requirements. Since our 5 physical CPU, 5,000 iteration job took 17 minutes and 41 seconds, our full scale job should take 12 times longer or about 5,532 seconds, 1 hour and 32 minutes (If you want to be more exact you can take the mean of the walltime over the number of seeds, in this case 0.09105, and multiply that by the number of seeds, which works out to be 5,463 seconds, which is very close to our original estimate), and require 0.17 GB of memory per task. To be on the safe side, let's request 1 GB of memory and 2 hours.</p>","tags":[]},{"location":"Getting_Started/Next_Steps/MPI_Scaling_Example/#revised-slurm-script","title":"Revised Slurm Script","text":"<pre><code>#!/bin/bash -e\n#SBATCH --account=nesi99999\n#SBATCH --job-name=MPIScaling60k\n#SBATCH --time=02:00:00\n#SBATCH --mem-per-task=512MB\n#SBATCH --ntasks=5\n\nmodule load Python\nsrun python scaling.R\n</code></pre> <p>Checking on our job with\u00a0<code>sacct</code></p> <pre><code>         JobID      JobName     Elapsed     TotalCPU Alloc   MaxRSS      State \n-------------- ------------ ----------- ------------ ----- -------- ----------\n6061377        MPIScaling60k   01:28:25     14:35:32    10           COMPLETED \n6061377.bat+   batch           01:28:25    00:00.555    10    4320K  COMPLETED \n6061377.ext+   extern          01:28:25     00:00:00    10        0  COMPLETED \n6061377.0      python          01:28:22     14:35:32     5  169060K  COMPLETED \n</code></pre> <p>It looks as though our estimates were accurate in this case, however, when you submit a job it is always a good idea to request about 20% more wall time and memory than you think you are going to need to minimise the chance of your jobs failing due to a lack of resources. Your project's fair share score considers the time actually used by the job, not the time requested by the job.</p>","tags":[]},{"location":"Getting_Started/Next_Steps/Moving_files_to_and_from_the_cluster/","title":"Moving files to and from the cluster","text":"<p>Prerequisite</p> <p>Have an active account and project.</p> <p>Find more information on the different types of directories here.</p>","tags":["scp","transfer","copying","download","upload","mv","cp","move","moving"]},{"location":"Getting_Started/Next_Steps/Moving_files_to_and_from_the_cluster/#using-the-jupyter-interface","title":"Using the Jupyter interface","text":"<p>Many users have found the Jupyter interface very useful for running code on NeSI. The Jupyter interface only requires a web browser; the instructions are same whether your are connecting from a Windows, Mac or a Linux computer.</p> <p>To upload a file, click on the\u00a0</p> <p></p> <p>button, near the top left and generally under the Run button.</p> <p>To download a file, navigate the file browser on the left and right-click on the file to see the menu below,</p> <p></p> <p>The Download button is at the bottom.</p>","tags":["scp","transfer","copying","download","upload","mv","cp","move","moving"]},{"location":"Getting_Started/Next_Steps/Moving_files_to_and_from_the_cluster/#standard-terminal","title":"Standard Terminal","text":"<p>Prerequisite</p> <p>Have SSH configured as recommended</p> <p>In a local terminal the following commands can be used to:</p> <p>Move a file from your local machine to Mahuika.</p> <pre><code>scp &lt;path/filename&gt; mahuika:&lt;path/filename&gt;\n</code></pre> <p>Move a file from Mahuika to your local machine.</p> <pre><code>scp mahuika:&lt;path/filename&gt;\u00a0&lt;path/filename&gt;\n</code></pre> <p>Note</p> <ul> <li>This will only work if you have set up aliases as described in   Terminal Setup.</li> <li>As the terms 'maui' and 'mahuika' are defined locally, the above   commands only works when using a local terminal (i.e. not on Mahuika).</li> <li>If you are using Windows subsystem, the root paths are different   as shown by Windows. e.g. <code>C:</code> is located at <code>/mnt/c/</code></li> </ul> <p><code>scp</code> stands for Secure CoPy and operates in a similar way to regular cp with the source file as the left term and destination on the right.</p> <p>These commands make use of multiplexing,\u00a0this means that if you already have a connection to the cluster you will not be prompted for your password.</p>","tags":["scp","transfer","copying","download","upload","mv","cp","move","moving"]},{"location":"Getting_Started/Next_Steps/Moving_files_to_and_from_the_cluster/#file-managers","title":"File Managers","text":"<p>Most file managers can be used to connect to a remote directory simply by typing in the address bar (provided your have an active connection to the cluster and your ssh config file is set up as described here).</p> <p>For Nautilus (Ubuntu default) just prepend the path you want to connect to with <code>sftp://mahuika</code>. (ctrl + L opens address bar)</p> <p>This does not work for File Explorer (Windows default)</p> <p>This does not work for Finder (Mac default)</p> <p></p> <p>If your default file manager does not support mounting over sftp, see our documentation on\u00a0SSHFS.</p>","tags":["scp","transfer","copying","download","upload","mv","cp","move","moving"]},{"location":"Getting_Started/Next_Steps/Moving_files_to_and_from_the_cluster/#mobaxterm","title":"MobaXterm","text":"<p>Clicking the \"Scp\" tab (located on the left-hand side of the MobaXTerm window) opens up a graphical user interface that can be used for basic file operations. You can drag and drop files in the file explorer or use the up and down arrows on the toolbar to upload and download files.</p> <p></p> <p>You may also transfer files as described under 'Standard Terminal' (provided WSL is enabled).</p>","tags":["scp","transfer","copying","download","upload","mv","cp","move","moving"]},{"location":"Getting_Started/Next_Steps/Moving_files_to_and_from_the_cluster/#winscp","title":"WinSCP","text":"<p>As WinSCP uses multiple tunnels for file transfer you will be required to authenticate again on your first file operation of the session. The second prompt for your 2FA can be skipped, just the same as with login authentication.</p>","tags":["scp","transfer","copying","download","upload","mv","cp","move","moving"]},{"location":"Getting_Started/Next_Steps/Moving_files_to_and_from_the_cluster/#globus","title":"Globus","text":"<p>Globus is available for those with large amounts of data, security concerns, or connection consistency issues. You can find more details on its use on our Globus support page.</p>","tags":["scp","transfer","copying","download","upload","mv","cp","move","moving"]},{"location":"Getting_Started/Next_Steps/Moving_files_to_and_from_the_cluster/#rclone","title":"Rclone","text":"<p>Rclone is available for those that need to transfer data from cloud storage services like Google drive or OneDrive.</p> <p>The basic command syntax of Rclone:</p> <pre><code>rclone subcommand options source:path dest:path\n</code></pre> <p>The most frequently used Rclone subcommands:</p> <ul> <li><code>rclone copy</code> \u2013 Copy files from the source to the destination, skipping what has already been copied.</li> <li><code>rclone sync</code> \u2013 Make the source and destination identical, modifying only the destination.</li> <li><code>rclone mov</code>e \u2013 Move files from the source to the destination.</li> <li><code>rclone delete</code> \u2013 Remove the contents of a path.</li> <li><code>rclone mkdir</code> \u2013 Create the path if it does not already exist.</li> <li><code>rclone rmdir</code> \u2013 Remove the path.</li> <li><code>rclone check</code> \u2013 Check if the files in the source and destination match.</li> <li><code>rclone ls</code> \u2013 List all objects in the path, including size and path.</li> <li><code>rclone lsd</code> \u2013 List all directories/containers/buckets in the path.</li> <li><code>rclone lsl</code> \u2013 List all objects in the path, including size, modification time and path.</li> <li><code>rclone lsf</code> \u2013 List the objects using the virtual directory structure based on the object names.</li> <li><code>rclone cat</code> \u2013 Concatenate files and send them to stdout.</li> <li><code>rclone copyto</code> \u2013 Copy files from the source to the destination, skipping what has already been copied.</li> <li><code>rclone moveto</code> \u2013 Move the file or directory from the source to the destination.</li> <li><code>rclone copyurl</code> \u2013 Copy the URL's content to the destination without saving it in the tmp storage.</li> </ul> <p>A more extensive list can be found on the the Rclone documentation.</p>","tags":["scp","transfer","copying","download","upload","mv","cp","move","moving"]},{"location":"Getting_Started/Next_Steps/Moving_files_to_and_from_the_cluster/#rsync","title":"Rsync","text":"<p>Rsync is an utility that provides fast incremental file transfer and efficient file synchronization between a computer and a storage disk. The basic command syntax of:</p> <pre><code>rsync -options source target\n</code></pre> <p>If the data source or target location is a remote site, it is defined with syntax:</p> <pre><code>userame@server:/path/in/server\n</code></pre> <p>The most frequently used Rsync options:</p> <ul> <li><code>-r</code> \u2013 Recurse into directories.</li> <li><code>-a</code> \u2013 Use archive mode: copy files and directories recursively and preserve access permissions and time stamps.</li> <li><code>-v</code> \u2013 Verbose mode.</li> <li><code>-z</code> \u2013 Compress.</li> <li><code>-e ssh</code> \u2013 Specify the remote shell to use.</li> <li><code>-n</code> \u2013 Show what files would be transferred.</li> <li><code>--partial</code> \u2013 Keep partially transferred files.</li> <li><code>--progress</code> \u2013 Show progress during transfer.</li> </ul> <p>A more extensive list can be found on the the Rsync documentation.</p>","tags":["scp","transfer","copying","download","upload","mv","cp","move","moving"]},{"location":"Getting_Started/Next_Steps/Multithreading_Scaling_Example/","title":"Multithreading Scaling Example","text":"<p>In the example below we will use R scripts to demonstrate how you might perform some basic scaling tests, however, the principles outlined in these examples are applicable across software applications. You do not need to know anything about R to understand this article; it was merely chosen for the purpose of illustration.</p>","tags":[]},{"location":"Getting_Started/Next_Steps/Multithreading_Scaling_Example/#initial-r-script","title":"Initial R Script","text":"<pre><code>library(doParallel)\n\nregisterDoParallel(strtoi(Sys.getenv('SLURM_CPUS_PER_TASK')))\n\n# 60,000 calculations to be done:\nforeach(z=1000000:1060000) %dopar% {\n    x &lt;- sum(rnorm(z))\n}\n</code></pre> <p>You do not need to understand what the above R script is doing, but for context, it will take the sum of z random numbers derived from a normal distribution with a mean of 0 and a standard deviation of 1 (where is z is a value from 1,000,000 to 1,060,000, meaning 60,000 iterations). The script will take the number of CPUs per task from the environment and and run it in parallel with that many threads. We unfortunately do not know how many CPUs, how much memory (RAM), or how much time to request for this script to complete. This means the first thing we need to do is run a small scale test and see how long that runs for. So we will first try with 5,000 iterations rather than 60,000 iterations. So now lets change the number of iterations from 60,000 to 5,000.</p>","tags":[]},{"location":"Getting_Started/Next_Steps/Multithreading_Scaling_Example/#revised-r-script","title":"Revised R Script","text":"<pre><code>library(doParallel)\n\nregisterDoParallel(strtoi(Sys.getenv('SLURM_CPUS_PER_TASK')))\n\n# 5,000 calculations to be done:\nforeach(z=1000000:1005000) %dopar% {\n    x &lt;- sum(rnorm(z))\n}\n</code></pre> <p>Now we need to write a Slurm script to run this job. The wall time, number of logical CPU cores and amount of memory (RAM) you request for this job will ideally be based on how this small-scale test runs on your local workstation, but if that is not possible, make an educated guess, and if the job fails increase the resources requested until is completes.</p> <p>Tip</p> <p>If you can, write your program so that it prints results and timing information out relatively frequently, for example every 100 or 1,000 iterations. That way, even if your job runs out of time or memory and gets killed, you will be able to see how far it got and how long it took to get there.</p>","tags":[]},{"location":"Getting_Started/Next_Steps/Multithreading_Scaling_Example/#slurm-script","title":"Slurm Script","text":"<pre><code>#!/bin/bash -e\n#SBATCH --job-name=Scaling5k\n#SBATCH --time=00:10:00\n#SBATCH --mem=512MB\n#SBATCH --cpus-per-task=4\n\nmodule load R\nRscript scaling.R\n</code></pre> <p>Let's run our Slurm script with sbatch and look at our output from <code>sacct</code>.</p> <pre><code>         JobID      JobName     Elapsed     TotalCPU Alloc   MaxRSS      State \n-------------- ------------ ----------- ------------ ----- -------- ----------\n3106248        Scaling5k       00:03:17    12:51.334     4          COMPLETED\n3106248.batch  batch           00:03:17    00:00.614     4    4213K COMPLETED\n3106248.extern extern          00:03:17     00:00:00     4      86K COMPLETED\n3106248.0      Rscript         00:03:14    12:50.719     4  406516K COMPLETED\n</code></pre> <p>Our job performed 5,000 iterations using four logical CPU cores and a maximum memory of 406,516KB (0.4 GB). In total, the job ran for 3 minutes and 17 seconds.</p> <p>We will initially assume that our job's wall time and memory will scale linearly with the number of iterations. However, we don't know that for certain that this is the case so we will need to understand the scaling behaviour of our job's resource requirements before we can submit our full job and be confident it will succeed.</p> <p>To test this, we will submit three more jobs, using 10,000 15,000 and 20,000 iterations.</p> <pre><code>         JobID      JobName     Elapsed     TotalCPU Alloc   MaxRSS      State \n-------------- ------------ ----------- ------------ ----- -------- ----------\n3106248        Scaling5k       00:03:17    12:51.334     4          COMPLETED\n3106248.batch  batch           00:03:17    00:00.614     4    4213K COMPLETED\n3106248.extern extern          00:03:17     00:00:00     4      86K COMPLETED\n3106248.0      Rscript         00:03:14    12:50.719     4  406516K COMPLETED\n3106249        Scaling10k      00:06:27    25:27.556     4          COMPLETED\n3106249.batch  batch           00:06:27    00:00.553     4    4345K COMPLETED\n3106249.extern extern          00:06:27     00:00:00     4      86K COMPLETED\n3106249.0      Rscript         00:06:24    25:27.002     4  412002K COMPLETED\n3106250        Scaling15k      00:09:37    38:07.395     4          COMPLETED\n3106250.batch  batch           00:09:37    00:00.626     4    4299K COMPLETED\n3106250.extern extern          00:09:37     00:00:00     4      99K COMPLETED\n3106250.0      Rscript         00:09:36    38:06.768     4  421424K COMPLETED\n3106251        Scaling20k      00:12:59    51:34.981     4          COMPLETED\n3106251.batch  batch           00:12:59    00:00.785     4    4147K COMPLETED\n3106251.extern extern          00:12:59     00:00:00     4      89K COMPLETED\n3106251.0      Rscript         00:12:58    51:34.194     4  408163K COMPLETED\n</code></pre> <p>We can see from the <code>sacct</code> output that the wall time seems to be increasing as we add more iterations, but the maximum memory doesn't seem to change much. Let's try plotting this data (we used R here, but feel free to use excel or whatever your preferred plotting software) to help us better understand what is happening:</p> <p> </p> <p>This confirms our assumption of wall-time scaling linearly with number of iterations. However, peak memory usage appears unchanged.</p> <p>Extrapolating from this data, we can estimate\u00a0the full 60,000 iterations will take\u00a012 times longer than 5,000 iterations or about 40 minutes.</p> <p>But suppose we need a result more quickly than that. We are currently using 4 CPU cores, but what if we used more? Could we speed up our job by that means?</p> <p>To find out we are going to have to run more tests. Let's try running our script with 2, 4, 6, 8, 10, 12, 14 and 16 CPUs and plot the results using <code>sacct</code>:</p> <pre><code>         JobID      JobName     Elapsed     TotalCPU Alloc   MaxRSS      State\n-------------- ------------ ----------- ------------ ----- -------- ----------\n3063584        Scaling2        00:06:29    12:49.971     2          COMPLETED\n3063584.batch  batch           00:06:29    00:00.591     2    4208K COMPLETED\n3063584.extern extern          00:06:29    00:00.001     2      86K COMPLETED\n3063584.0      Rscript         00:06:27    12:49.379     2  241718K COMPLETED\n3063585        Scaling4        00:03:15    12:46.159     4          COMPLETED\n3063585.batch  batch           00:03:15    00:00.558     4    4203K COMPLETED\n3063585.extern extern          00:03:15     00:00:00     4      86K COMPLETED\n3063585.0      Rscript         00:03:13    12:45.600     4  314603K COMPLETED\n3063587        Scaling6        00:02:11    12:43.195     6          COMPLETED\n3063587.batch  batch           00:02:11    00:00.569     6    4512K COMPLETED\n3063587.extern extern          00:02:11     00:00:00     6      92K COMPLETED\n3063587.0      Rscript         00:02:09    12:42.624     6  560299K COMPLETED\n3061553        Scaling8        00:01:45    13:10.690     8          COMPLETED\n3061553.batch  batch           00:01:45    00:00.735     8    4296K COMPLETED\n3061553.extern extern          00:01:45    00:00.001     8      93K COMPLETED\n3061553.0      Rscript         00:01:42    13:09.953     8  689376K COMPLETED\n3107288        Scaling10       00:01:20    12:45.238    10          COMPLETED\n3107288.batch  batch           00:01:20    00:00.578    10    4397K COMPLETED\n3107288.extern extern          00:01:20    00:00.001    10      97K COMPLETED\n3107288.0      Rscript         00:01:19    12:44.658    10  549483K COMPLETED\n3107322        Scaling12       00:01:07    12:46.339    12          COMPLETED\n3107322.batch  batch           00:01:07    00:00.525    12    4155K COMPLETED\n3107322.extern extern          00:01:07     00:00:00    12      86K COMPLETED\n3107322.0      Rscript         00:01:06    12:45.812    12  844047K COMPLETED\n3107323        Scaling14       00:01:03    12:32.805    14          COMPLETED\n3107323.batch  batch           00:01:03    00:00.540    14    4112K COMPLETED\n3107323.extern extern          00:01:03     00:00:00    14      87K COMPLETED\n3107323.0      Rscript         00:01:01    12:32.263    14  948918K COMPLETED\n3106181        Scaling16       00:01:00    12:00.619    16          COMPLETED\n3106181.batch  batch           00:01:00    00:00.619    16    4121K COMPLETED\n3106181.extern extern          00:01:00    00:00.001    16      89K COMPLETED\n3106181.0      Rscript         00:00:59    11:59.998    16 1205991K COMPLETED\n</code></pre> <p> </p> <p>The two above plots show the number of CPUs vs time and the Log2 of the CPUs vs time. The reason we have both is that it can often be easier to see the inflection point on the Log2 graph when the speed up from increasing the number of CPUs start to level off, as in the Log2 graph if the jobs scaled perfectly linearly (e.g. doubling the CPU's halves the runtime) the line would be straight. The curving of the line in the Log2 graph represents a loss in efficiency from increasing the number of CPUs.</p> <p>As we can see, increasing the number of CPU cores doesn't linearly increase the job's speed. This non-linear speed-up is called Amdahl's Law, and reflects the fact that there is a fixed part of the computation that is inherently serial, that is, some operations can't be started until others have already finished.</p> <p>Indeed, the difference in speed between 14 and 16 CPU cores is very small. We could try running our script with more than 16 CPU cores, however, in the case of this script we start to have a pretty significant drop in marginal speed-up after eight CPU cores.</p> <p></p> <p>Looking at our jobs' memory use, we can see that as we increase the number of CPUs taken by a job, the job's memory requirements increase approximately linearly. This behaviour isn't necessarily the case for all kinds of jobs, and is most likely to be the case for jobs that use MPI to run in parallel rather than OpenMP.</p> <p>One thing to note about our plot of CPUs versus memory is that our memory usage seems to drop for 10 CPUs, this can likely be explained by the fact that memory usage is not measured continuously, it is instead measured every 30 seconds. This means that if your job's memory usage has some spikes,\u00a0<code>sacct</code> will not necessarily detect the maximum memory usage. This is something that you should be aware of when you estimate the memory usage of all your jobs.</p> <p>Looking at the memory usage for an 8 CPU job, it looks like an 8 CPU has a maximum memory requirement of 0.75 GB.</p>","tags":[]},{"location":"Getting_Started/Next_Steps/Multithreading_Scaling_Example/#estimating-our-total-resource-requirements","title":"Estimating our Total Resource Requirements","text":"<p>Now that we know approximately how our job's CPU, memory and wall requirements scale, we can try and estimate our total resource requirements for our 60,000 iteration job.</p> <p>From this data we have determined that more than 8 CPUs has very limited additional speed and an 8 CPU job should use about 0.75 GB of memory at most, and that this memory requirement should remain relatively consistent, regardless of the number of iterations. Given this information we can estimate our full size job's resource requirements. Since our 8-CPU, 5,000 iteration job took 1 minute and 45 seconds, our full scale job should take 12 times longer (21 minutes) and require 0.75 GB of memory. To be on the safe side, let's request 1 GB of memory and 30 minutes.</p>","tags":[]},{"location":"Getting_Started/Next_Steps/Multithreading_Scaling_Example/#revised-slurm-script","title":"Revised Slurm Script","text":"<pre><code>#!/bin/bash -e\n#SBATCH --account=nesi99999\n#SBATCH --job-name=Scaling60k # Job name (shows up in the queue)\n#SBATCH --time=00:30:00       # Walltime (HH:MM:SS)\n#SBATCH --mem=512MB           # Memory per node\n#SBATCH --cpus-per-task=8     # Number of cores per task (e.g. OpenMP)\n\nmodule load R\nRscript scaling.R\n</code></pre> <p>Checking on our job with\u00a0<code>sacct</code> </p> <pre><code>         JobID      JobName     Elapsed     TotalCPU Alloc   MaxRSS      State \n-------------- ------------ ----------- ------------ ----- -------- ----------\n3119026        Scaling60k      00:20:34     02:41:53     8          COMPLETED\n3119026.batch  batch           00:20:34    00:01.635     8    4197K COMPLETED\n3119026.extern extern          00:20:34    00:00.001     8      89K COMPLETED\n3119026.0      Rscript         00:20:33     02:41:51     8  749083K COMPLETED\n</code></pre> <p>It looks as though our estimates were accurate, but looking at our maximum memory usage it is a good thing that we requested additional memory as we may otherwise have run out.</p> <p>Tip</p> <p>Whenever you submit a job it is always a good idea to request about 20% more wall time and memory than you think you are going to need to minimise the chance of your jobs failing due to a lack of resources. Your project's fair share score considers the time actually used by the job, not the time requested by the job.</p>","tags":[]},{"location":"Getting_Started/Next_Steps/Parallel_Execution/","title":"Parallel Execution","text":"<p>Many scientific software applications are written to take advantage of multiple CPUs in some way. But often this must be specifically requested by the user at the time they run the program, rather than happening automatically.</p> <p>The are three types of parallel execution we will cover are\u00a0Multi-Threading, Distributed (MPI)\u00a0and\u00a0Job Arrays.</p> <p>Note</p> <p>Whenever Slurm mentions CPUs it is referring to logical CPU's (2 logical CPU's = 1 physical\u00a0core). - <code>--cpus-per-task=4</code>\u00a0will give you 4 logical cores. - <code>--mem-per-cpu=512MB</code>\u00a0will give 512 MB of RAM\u00a0per\u00a0logical\u00a0core. - If\u00a0<code>--hint=nomultithread</code> is used then <code>--cpus-per-task</code> will now refer to physical cores, but <code>--mem-per-cpu=512MB</code> still refers to logical cores.</p> <p>See our article on hyperthreading for more information.</p>","tags":[]},{"location":"Getting_Started/Next_Steps/Parallel_Execution/#multi-threading","title":"Multi-threading","text":"<p>Multi-threading is a method of parallelisation whereby the initial single thread of a process forks into a number of parallel threads, generally via a library such as OpenMP (Open MultiProcessing), TBB (Threading Building Blocks), or pthread (POSIX threads).</p> <p> </p> <p> Multi-threading involves dividing the process into multiple 'threads' which can be run across multiple cores.</p> <p>Multi-threading is limited in that it requires shared memory, so all CPU\u00a0cores used must be on the same node. However, because all the CPUs share the same memory environment things only need to be loaded into memory once, meaning that memory requirements will usually not increase proportionally to the number of CPUs.</p> <p>Example script:</p> <pre><code>#!/bin/bash -e\n#SBATCH --job-name=MultithreadingTest    # job name (shows up in the queue)\n#SBATCH --time=00:01:00                  # Walltime (HH:MM:SS)\n#SBATCH --mem=2048MB                     # memory in MB \n#SBATCH --cpus-per-task=4                # 2 physical cores per task.\n\ntaskset -c -p $$                         #Prints which CPUs it can use\n</code></pre> <p>The expected output being</p> <pre><code>pid 13538's current affinity list: 7,9,43,45\n</code></pre>","tags":[]},{"location":"Getting_Started/Next_Steps/Parallel_Execution/#mpi","title":"MPI","text":"<p>MPI stands for Message Passing Interface, and\u00a0is a communication protocol used to achieve distributed parallel computation.</p> <p>Similar in some ways to multi-threading, MPI does not have the limitation of requiring shared memory and thus can be used across multiple nodes, but has higher communication and memory overheads.</p> <p>For MPI jobs you need to set <code>--ntasks</code> to a value larger than 1, or if you want all nodes to run the same number of tasks, set <code>--ntasks-per-node</code> and <code>--nodes</code> instead.</p> <p>MPI programs require a launcher to start the ntasks processes on multiple CPUs, which may belong to different nodes. On Slurm systems like ours, the preferred launcher is <code>srun</code>\u00a0rather than <code>mpi-run</code>.</p> <p>Since the distribution of tasks across different nodes may be unpredictable,\u00a0<code>--mem-per-cpu</code>\u00a0should be used instead of\u00a0<code>--mem</code>.</p> <pre><code>#!/bin/bash -e\n#SBATCH --job-name=MPIJob       # job name (shows up in the queue)\n#SBATCH --time=00:01:00         # Walltime (HH:MM:SS)\n#SBATCH --mem-per-cpu=512MB     # memory/cpu in MB (half the actual required memory)\n#SBATCH --cpus-per-task=4       # 2 Physical cores per task.\n#SBATCH --ntasks=2              # number of tasks (e.g. MPI)\n\nsrun pwd                        # Prints  working directory\n</code></pre> <p>The expected output being</p> <pre><code>/home/user001/demo\n/home/user001/demo\n</code></pre> <p>Warning</p> <p>For non-MPI programs, either set\u00a0<code>--ntasks=1</code> or do not use\u00a0<code>srun</code>\u00a0at all. Using\u00a0<code>srun</code>\u00a0in conjunction with\u00a0<code>--cpus-per-task=1</code>\u00a0will cause\u00a0<code>--ntasks</code>\u00a0to default to 2.</p>","tags":[]},{"location":"Getting_Started/Next_Steps/Parallel_Execution/#job-arrays","title":"Job Arrays","text":"<p>Job arrays are best used for tasks that are completely independent, such as parameter sweeps, permutation analysis or simulation, that could be executed in any order and don't have to run at the same time. This kind of work is often described as embarrassingly parallel. An embarrassingly parallel problem is one that requires no communication or dependency between the tasks (unlike distributed computing problems that need communication between tasks).</p> <p>A job array will submit the same script repeatedly over a designated index using the SBATCH command\u00a0<code>#SBATCH --array</code></p> <p>For example, the following code:</p> <pre><code>#!/bin/bash -e\n#SBATCH --job-name=ArrayJob             # job name (shows up in the queue)\n#SBATCH --time=00:01:00                 # Walltime (HH:MM:SS)\n#SBATCH --mem=512MB                     # Memory\n#SBATCH --array=1-2                     # Array jobs\n\npwd\necho \"This is result ${SLURM_ARRAY_TASK_ID}\"\n</code></pre> <p>will submit, <code>ArrayJob_1</code> and <code>ArrayJob_2</code>, which will return the results <code>This is result 1</code> and <code>This is result 2</code> respectively.</p>","tags":[]},{"location":"Getting_Started/Next_Steps/Parallel_Execution/#using-slurm_array_task_id","title":"Using\u00a0<code>SLURM_ARRAY_TASK_ID</code>","text":"<p>Use of the environment variable\u00a0<code>${SLURM_ARRAY_TASK_ID}</code> is the recommended method of variation between the jobs.</p> <p>Warning</p> <p>Environment variables will not work in the Slurm header. In place of\u00a0<code>${SLURM_ARRAY_TASK_ID}</code>, you can use the token\u00a0<code>%a</code>. This can be useful for sorting your output files e.g.</p> <pre><code>#SBATCH --output=outputs/run_%a/slurm_output.out\n#SBATCH --output=outputs/run_%a/slurm_error.err\n</code></pre>","tags":[]},{"location":"Getting_Started/Next_Steps/Parallel_Execution/#as-a-direct-input-to-a-function","title":"As a direct input to a function","text":"<pre><code>matlab -nodisplay -r \"myFunction(${SLURM_ARRAY_TASK_ID})\"\n</code></pre>","tags":[]},{"location":"Getting_Started/Next_Steps/Parallel_Execution/#as-an-index-to-an-array","title":"As an index to an array","text":"<pre><code>inArray=(1 2 4 8 16 32 64 128)\ninput=${inArray[$SLURM_ARRAY_TASK_ID]}\n</code></pre>","tags":[]},{"location":"Getting_Started/Next_Steps/Parallel_Execution/#for-selecting-input-files","title":"For selecting input files","text":"<pre><code>input=inputs/mesh_${SLURM_ARRAY_TASK_ID}.stl\n</code></pre>","tags":[]},{"location":"Getting_Started/Next_Steps/Parallel_Execution/#as-a-seed-for-a-pseudo-random-number","title":"As a seed for a pseudo-random number","text":"<p>In R</p> <pre><code>task_id = as.numeric(Sys.getenv(\"SLURM_ARRAY_TASK_ID\"))\nset.seed(task_id)\n</code></pre> <p>In MATLAB</p> <pre><code>task_id = str2num(getenv('SLURM_ARRAY_TASK_ID'))\nrng(task_id)\n</code></pre> <p>Using a seed is important, otherwise multiple jobs may receive the same pseudo-random numbers.</p>","tags":[]},{"location":"Getting_Started/Next_Steps/Parallel_Execution/#as-an-index-to-an-array-of-filenames","title":"As an index to an array of filenames","text":"<pre><code>files=( inputs/*.dat )\ninput=${files[SLURM_ARRAY_TASK_ID]}\n</code></pre> <p>Note: If there are 5 <code>.dat</code> files in <code>inputs/</code> you will want to use <code>#SBATCH --array=0-4</code>.</p> <p>This example will submit a job array with each job using a .dat file in 'inputs' as the variable input (in alphabetical order).</p>","tags":[]},{"location":"Getting_Started/Next_Steps/Parallel_Execution/#multidimensional-array-example","title":"Multidimensional array example","text":"<pre><code>#!/bin/bash -e\n\n#SBATCH --open-mode append\n#SBATCH --output week_times.out\n#SBATCH --array 0-167 #This needs to be equal to combinations (in this case 7*24), and zero based.\n\n# Define your dimensions in bash arrays.\narr_time=({00..23})\narr_day=(\"Mon\" \"Tue\" \"Wed\" \"Thur\" \"Fri\" \"Sat\" \"Sun\") \n\n# Index the bash arrays based on the SLURM_ARRAY_TASK)\nn_time=${arr_time[$(($SLURM_ARRAY_TASK_ID%${#arr_time[@]}))]} # '%' for finding remainder.\nn_day=${arr_day[$(($SLURM_ARRAY_TASK_ID/${#arr_time[@]}))]}\n\necho \"$n_day $n_time:00\"\n</code></pre>","tags":[]},{"location":"Getting_Started/Next_Steps/Parallel_Execution/#avoiding-conflicts","title":"Avoiding Conflicts","text":"<p>As all the array jobs could theoretically run at the same time, it is important that all file references are unique and independent.</p> <p>If your program makes use of a working directory make sure you set it e.g.</p> <pre><code>mkdir .tmp/run_${SLURM_ARRAY_TASK_ID}          # Create new directory\nexport TMPDIR=.tmp/run_${SLURM_ARRAY_TASK_ID}\u00a0\u00a0# Set\u00a0TMPDIR\u00a0to\u00a0point\u00a0there\n</code></pre> <p>If you have no control over the name/path of an output used by a program, this can be resolved in a similar manner.</p> <pre><code>mkdir run_${SLURM_ARRAY_TASK_ID}                             # Create new directory\ncd run_${SLURM_ARRAY_TASK_ID}\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0                         # CD to\u00a0new\u00a0directory\n\nbash job.sh\n\nmv\u00a0output.log\u00a0../outputs/output_${SLURM_ARRAY_TASK_ID}.log   # Move and rename output\nrm\u00a0-r\u00a0../run_${SLURM_ARRAY_TASK_ID}\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0# Clear\u00a0directory\n</code></pre> <p>The Slurm documentation on job arrays can be found\u00a0here.</p>","tags":[]},{"location":"Getting_Started/Next_Steps/Submitting_your_first_job/","title":"Submitting your first job","text":"","tags":["slurm","scheduler"]},{"location":"Getting_Started/Next_Steps/Submitting_your_first_job/#environment-modules","title":"Environment Modules","text":"<p>Modules are a convenient \u00a0way to provide access to applications \u00a0on the cluster. They prepare the environment you need to run an application.</p> <p>For a full list of module commands run\u00a0<code>man module</code>.</p> Command Description <code>module spider [ &lt;string&gt; ]</code> List all modules whose names, including version strings, contain <code>&lt;string&gt;</code>. If the <code>&lt;string&gt;</code> argument is not supplied, list all available modules. (only on Mahuika) <code>module show &lt;string&gt;</code> Show the contents of the module given by\u00a0<code>&lt;string&gt;</code>. If only the module name (e.g. <code>Python</code>) is given, show the default module of that name. If both name and version are given, show that particular version module. <code>module load &lt;string&gt;</code> Load the module (name and version) given by\u00a0<code>&lt;string&gt;</code>. If no version is given, load the default version. <code>module list [ &lt;string&gt; ]</code> List all currently loaded modules whose names, including version strings, contain\u00a0<code>&lt;string&gt;</code>. If the\u00a0<code>&lt;string&gt;</code>\u00a0argument is not supplied, list all currently loaded modules.","tags":["slurm","scheduler"]},{"location":"Getting_Started/Next_Steps/Submitting_your_first_job/#slurm","title":"Slurm","text":"<p>Jobs on Mahuika and M\u0101ui are submitted in the form of a batch script containing the code you want to run and a header of information needed by\u00a0our job scheduler Slurm.</p>","tags":["slurm","scheduler"]},{"location":"Getting_Started/Next_Steps/Submitting_your_first_job/#creating-a-batch-script","title":"Creating a batch script","text":"<p>Create a new file and open it with <code>nano myjob.sl</code></p> <pre><code>#!/bin/bash -e\n#SBATCH --job-name=SerialJob # job name (shows up in the queue)\n#SBATCH --time=00:01:00      # Walltime (HH:MM:SS)\n#SBATCH --mem=512MB          # Memory in MB\n#SBATCH --qos=debug          # debug QOS for high priority job tests\n\npwd # Prints working directory\n</code></pre> <p>Copy in the above text and save and exit the text editor with 'ctrl + x'.</p> <p>Note: <code>#!/bin/bash</code> is expected by Slurm.</p> <p>Note: if you are a member of multiple accounts you should add the line</p> <pre><code>#SBATCH --account=&lt;projectcode&gt;\n</code></pre>","tags":["slurm","scheduler"]},{"location":"Getting_Started/Next_Steps/Submitting_your_first_job/#testing","title":"Testing","text":"<p>We recommend testing your job using the debug Quality of Service (QOS). The debug QOS can be gained by adding the <code>sbatch</code> command line option <code>--qos=debug</code>. This adds 5000 to the job priority so raises it above all non-debug jobs, but is limited to one small job per user at a time: no more than 15 minutes and no more than 2 nodes.</p> <p>Warning</p> <p>Please do not run your code on the login node. Any processes running on the login node for long periods of time or using large numbers of CPUs will be terminated.</p>","tags":["slurm","scheduler"]},{"location":"Getting_Started/Next_Steps/Submitting_your_first_job/#submitting","title":"Submitting","text":"<p>Jobs are submitted to the scheduler using:</p> <pre><code>sbatch myjob.sl\n</code></pre> <p>You should receive an output</p> <p>Submitted batch job 1748836</p> <p><code>sbatch</code> can take command line arguments similar to those used in the shell script through SBATCH pragmas</p> <p>You can find more details on its use on the Slurm Documentation</p>","tags":["slurm","scheduler"]},{"location":"Getting_Started/Next_Steps/Submitting_your_first_job/#job-queue","title":"Job Queue","text":"<p>The currently queued jobs can be checked using</p> <pre><code>squeue\n</code></pre> <p>You can filter to just your jobs by adding the flag</p> <pre><code>squeue -u usr9999\n</code></pre> <p>You can also filter to just your jobs using</p> <pre><code>squeue --me\n</code></pre> <p>You can find more details on its use on the Slurm Documentation.</p> <p>You can check all jobs submitted by you in the past day using:</p> <pre><code>sacct\n</code></pre> <p>Or since a specified date using:</p> <pre><code>sacct -S YYYY-MM-DD\n</code></pre> <p>Each job will show as multiple lines,\u00a0one line for the parent job and then additional lines for each job step.</p> <p>Tip</p> <ul> <li><code>sacct -X</code> Only show parent processes.</li> <li><code>sacct --state=PENDING/RUNNING/FAILED/CANCELLED/TIMEOUT</code> Filter jobs by state.</li> </ul> <p>You can find more details on its use on the Slurm Documentation.</p>","tags":["slurm","scheduler"]},{"location":"Getting_Started/Next_Steps/Submitting_your_first_job/#cancelling","title":"Cancelling","text":"<p><code>scancel &lt;jobid&gt;</code> will cancel the job described by <code>&lt;jobid&gt;</code>. You can obtain the job ID by using <code>sacct</code> or <code>squeue</code>.</p> <p>Tip</p> <ul> <li><code>scancel -u [username]</code> Kill all jobs submitted by you.</li> <li><code>scancel {[n1]..[n2]}</code> Kill all jobs with an id between <code>[n1]</code> and <code>[n2]</code>.</li> </ul> <p>You can find more details on its use on the Slurm Documentation.</p>","tags":["slurm","scheduler"]},{"location":"Getting_Started/Next_Steps/Submitting_your_first_job/#job-output","title":"Job Output","text":"<p>When the job completes, or in some cases earlier, two files will be added to the directory in which you were working when you submitted the job:</p> <p><code>slurm-[jobid].out</code>\u00a0containing standard output.</p> <p><code>slurm-[jobid].err</code> containing standard error.</p>","tags":["slurm","scheduler"]},{"location":"Getting_Started/Next_Steps/The_HPC_environment/","title":"The HPC environment","text":"","tags":[]},{"location":"Getting_Started/Next_Steps/The_HPC_environment/#environment-modules","title":"Environment Modules","text":"<p>Modules are a convenient \u00a0way to provide access to applications \u00a0on the cluster. They prepare the environment you need to run an application.</p> <p>For a full list of module commands run\u00a0<code>man module</code> or visit the lmod documentation.</p> Command Description <code>module spider</code> Lists all available modules. (only Mahuika) <code>module spider [module name]</code> Searches available modules for [module name] (only Mahuika) <code>module show [module name]</code> Shows information about\u00a0[module name] <code>module load [module name]</code> Loads\u00a0[module name] <code>module list [module name]</code> Lists currently loaded modules.","tags":[]},{"location":"Getting_Started/Release_Notes_my-nesi-org-nz/my-nesi-org-nz_release_notes_v2-0-1/","title":"my.nesi.org.nz release notes v2.0.1","text":"","tags":["releasenote"]},{"location":"Getting_Started/Release_Notes_my-nesi-org-nz/my-nesi-org-nz_release_notes_v2-0-1/#release-update-28-april-2021","title":"Release Update - 28. April 2021","text":"","tags":["releasenote"]},{"location":"Getting_Started/Release_Notes_my-nesi-org-nz/my-nesi-org-nz_release_notes_v2-0-1/#new-and-improved","title":"New and Improved","text":"<ul> <li> <p>An updated\u00a0web application is introducing a     navigation     in the sidebar and links to important functions</p> </li> <li> <p>Improved project application     form     with automatic draft state so you can\u00a0continue the application at a     later stage without the need to re-enter details</p> </li> <li> <p>Moved the account profile to a dedicated page</p> </li> <li> <p>Replaced backend to support future features</p> </li> </ul>","tags":["releasenote"]},{"location":"Getting_Started/Release_Notes_my-nesi-org-nz/my-nesi-org-nz_release_notes_v2-0-1/#fixes","title":"Fixes","text":"<p>Fixed: Email address validation supports 'modern' domains when requesting a virtual home account.</p>","tags":["releasenote"]},{"location":"Getting_Started/Release_Notes_my-nesi-org-nz/my-nesi-org-nz_release_notes_v2-0-1/#release-update-18-may-2021","title":"Release Update - 18. May 2021","text":"","tags":["releasenote"]},{"location":"Getting_Started/Release_Notes_my-nesi-org-nz/my-nesi-org-nz_release_notes_v2-0-1/#fixes_1","title":"Fixes","text":"<p>Fixed: Email address validation allows uppercase characters to be entered by user when requesting a virtual home account.</p>","tags":["releasenote"]},{"location":"Getting_Started/Release_Notes_my-nesi-org-nz/my-nesi-org-nz_release_notes_v2-0-3/","title":"my.nesi.org.nz release notes v2.0.3","text":"","tags":["releasenote"]},{"location":"Getting_Started/Release_Notes_my-nesi-org-nz/my-nesi-org-nz_release_notes_v2-0-3/#release-update-10-june-2021","title":"Release Update - 10. June 2021","text":"","tags":["releasenote"]},{"location":"Getting_Started/Release_Notes_my-nesi-org-nz/my-nesi-org-nz_release_notes_v2-0-3/#new-and-improved","title":"New and Improved","text":"<ul> <li>Improved the \"Reset NeSI HPC Account Password\" form to clear values     after submission.</li> <li>Lowered the time until a user can reset the password and adjusted     the feedback message to be more meaningful for users, if the change     was not\u00a0successful .</li> </ul>","tags":["releasenote"]},{"location":"Getting_Started/Release_Notes_my-nesi-org-nz/my-nesi-org-nz_release_notes_v2-0-3/#fixes","title":"Fixes","text":"<p>Fixed:\u00a0details in password and 2FA reset email message (IP address, device) are displayed again.</p>","tags":["releasenote"]},{"location":"Getting_Started/Release_Notes_my-nesi-org-nz/my-nesi-org-nz_release_notes_v2-1-0/","title":"my.nesi.org.nz release notes v2.1.0","text":"","tags":["releasenote"]},{"location":"Getting_Started/Release_Notes_my-nesi-org-nz/my-nesi-org-nz_release_notes_v2-1-0/#release-update-24-june-2021","title":"Release Update - 24. June 2021","text":"","tags":["releasenote"]},{"location":"Getting_Started/Release_Notes_my-nesi-org-nz/my-nesi-org-nz_release_notes_v2-1-0/#new-and-improved","title":"New and Improved","text":"<ul> <li>Added ORCID iD field to user account profile.</li> <li>Upgraded components to improve security (web server).</li> </ul>","tags":["releasenote"]},{"location":"Getting_Started/Release_Notes_my-nesi-org-nz/my-nesi-org-nz_release_notes_v2-1-0/#fixes","title":"Fixes","text":"<p>Fixed:\u00a0user affiliation not correct after first login to my.nesi.</p>","tags":["releasenote"]},{"location":"Getting_Started/Release_Notes_my-nesi-org-nz/my-nesi-org-nz_release_notes_v2-10-0/","title":"my.nesi.org.nz release notes v2.10.0","text":"","tags":["releasenote"]},{"location":"Getting_Started/Release_Notes_my-nesi-org-nz/my-nesi-org-nz_release_notes_v2-10-0/#release-update-14-jun-2022","title":"Release Update - 14. Jun 2022","text":"","tags":["releasenote"]},{"location":"Getting_Started/Release_Notes_my-nesi-org-nz/my-nesi-org-nz_release_notes_v2-10-0/#new-and-improved","title":"New and Improved","text":"<ul> <li>Updated dependencies</li> <li>Added release notes to the UI - accessible via the 'hamburger' menu     on the right</li> <li>Added grant/award details for NeSI project in Project view</li> </ul>","tags":["releasenote"]},{"location":"Getting_Started/Release_Notes_my-nesi-org-nz/my-nesi-org-nz_release_notes_v2-11-0/","title":"my.nesi.org.nz release notes v2.11.0","text":"","tags":["releasenote"]},{"location":"Getting_Started/Release_Notes_my-nesi-org-nz/my-nesi-org-nz_release_notes_v2-11-0/#release-update-21-june-2022","title":"Release Update - 21. June 2022","text":"","tags":["releasenote"]},{"location":"Getting_Started/Release_Notes_my-nesi-org-nz/my-nesi-org-nz_release_notes_v2-11-0/#new-and-improved","title":"New and Improved","text":"<ul> <li>Backend security updates</li> </ul>","tags":["releasenote"]},{"location":"Getting_Started/Release_Notes_my-nesi-org-nz/my-nesi-org-nz_release_notes_v2-11-0/#fixes","title":"Fixes","text":"<p>Fixed: usage values for allocation request renewal form</p>","tags":["releasenote"]},{"location":"Getting_Started/Release_Notes_my-nesi-org-nz/my-nesi-org-nz_release_notes_v2-12-0/","title":"my.nesi.org.nz release notes v2.12.0","text":"","tags":["releasenote"]},{"location":"Getting_Started/Release_Notes_my-nesi-org-nz/my-nesi-org-nz_release_notes_v2-12-0/#release-update-7-july-2022","title":"Release Update - 7. July 2022","text":"","tags":["releasenote"]},{"location":"Getting_Started/Release_Notes_my-nesi-org-nz/my-nesi-org-nz_release_notes_v2-12-0/#new-and-improved","title":"New and Improved","text":"<ul> <li>Added a banner to make users aware in case there is already a     current allocation request for the project before raising another     one</li> <li>Added details to make users aware of their HPC account status when     attempting to reset the MFA/2FA token</li> </ul>","tags":["releasenote"]},{"location":"Getting_Started/Release_Notes_my-nesi-org-nz/my-nesi-org-nz_release_notes_v2-12-0/#fixes","title":"Fixes","text":"<p>Fixed: using 'Compute Units' instead of 'Core Hours' for Mahuika</p>","tags":["releasenote"]},{"location":"Getting_Started/Release_Notes_my-nesi-org-nz/my-nesi-org-nz_release_notes_v2-13-0/","title":"my.nesi.org.nz release notes v2.13.0","text":"","tags":["releasenote"]},{"location":"Getting_Started/Release_Notes_my-nesi-org-nz/my-nesi-org-nz_release_notes_v2-13-0/#release-update-28-november-2022","title":"Release Update - 28. November 2022","text":"","tags":["releasenote"]},{"location":"Getting_Started/Release_Notes_my-nesi-org-nz/my-nesi-org-nz_release_notes_v2-13-0/#new-and-improved","title":"New and Improved","text":"<ul> <li> <p>When adding a new allocation request, new option to add the related     grants funding the project if missing.</p> </li> <li> <p>If a project has no compute allocation linked to it, a banner has     been added to make users aware of the missing allocation request and     the visual indicators have been made clearer.</p> </li> <li> <p>Added links for the access policy and the acceptable use policy in     the footer.</p> </li> </ul>","tags":["releasenote"]},{"location":"Getting_Started/Release_Notes_my-nesi-org-nz/my-nesi-org-nz_release_notes_v2-13-0/#fixes","title":"Fixes","text":"<ul> <li>When adding a new allocation request, if an existing allocation     request is present, an alert message is visible: a cross has been     added to allow for the warning message to be closed.</li> </ul>","tags":["releasenote"]},{"location":"Getting_Started/Release_Notes_my-nesi-org-nz/my-nesi-org-nz_release_notes_v2-14-0/","title":"my.nesi.org.nz release notes v2.14.0","text":"","tags":["releasenote"]},{"location":"Getting_Started/Release_Notes_my-nesi-org-nz/my-nesi-org-nz_release_notes_v2-14-0/#release-update-17-january-2023","title":"Release Update - 17. January 2023","text":"","tags":["releasenote"]},{"location":"Getting_Started/Release_Notes_my-nesi-org-nz/my-nesi-org-nz_release_notes_v2-14-0/#new-and-improved","title":"New and Improved","text":"<ul> <li> <p>New option to opt-out of CC'ing all project members when requesting     a renewed allocation.</p> </li> <li> <p>Only sending one notification message of end of allocation per     project covering both Mahuika and M\u0101ui allocations</p> </li> </ul>","tags":["releasenote"]},{"location":"Getting_Started/Release_Notes_my-nesi-org-nz/my-nesi-org-nz_release_notes_v2-14-0/#fixes","title":"Fixes","text":"<ul> <li>Stop forcing authentication on the login page.</li> </ul>","tags":["releasenote"]},{"location":"Getting_Started/Release_Notes_my-nesi-org-nz/my-nesi-org-nz_release_notes_v2-15-0/","title":"my.nesi.org.nz release notes v2.15.0","text":"","tags":["releasenote"]},{"location":"Getting_Started/Release_Notes_my-nesi-org-nz/my-nesi-org-nz_release_notes_v2-15-0/#release-update-3-may-2023","title":"Release Update - 3. May 2023","text":"","tags":["releasenote"]},{"location":"Getting_Started/Release_Notes_my-nesi-org-nz/my-nesi-org-nz_release_notes_v2-15-0/#new-and-improved","title":"New and Improved","text":"<ul> <li> <p>New Allocation Request page has been improved to add field     validations</p> </li> <li> <p>Links to information about notifications have been added under     Account &gt; My Profile, when the Account Profile is edited</p> </li> <li> <p>If my.nesi.org.nz portal cannot connect to NeSI server, a     descriptive error message will be displayed</p> </li> </ul>","tags":["releasenote"]},{"location":"Getting_Started/Release_Notes_my-nesi-org-nz/my-nesi-org-nz_release_notes_v2-15-0/#fixes","title":"Fixes","text":"<ul> <li>New allocation request start date rules updated when the project     does not have a current allocation, today date will be used as the     default value</li> <li>New allocation request interval between the start date and end date     should be around one year but with the quarter splits, it has been     increased to 15 months</li> <li>Inactive users won't receive email notifications when a new     allocation request is created</li> <li>Double clicks have been displayed on submit buttons to avoid     duplicate items being created</li> </ul>","tags":["releasenote"]},{"location":"Getting_Started/Release_Notes_my-nesi-org-nz/my-nesi-org-nz_release_notes_v2-16-0/","title":"my.nesi.org.nz release notes v2.16.0","text":"","tags":["releasenote"]},{"location":"Getting_Started/Release_Notes_my-nesi-org-nz/my-nesi-org-nz_release_notes_v2-16-0/#release-update-15-june-2023","title":"Release Update - 15. June 2023","text":"","tags":["releasenote"]},{"location":"Getting_Started/Release_Notes_my-nesi-org-nz/my-nesi-org-nz_release_notes_v2-16-0/#new-and-improved","title":"New and Improved","text":"<ul> <li>Ability to add new affiliation from Account &gt; My Profile menu</li> <li>Display of the full organisation with department (if relevant) when     creating an new allocation request</li> <li>Addition of progress bars</li> </ul>","tags":["releasenote"]},{"location":"Getting_Started/Release_Notes_my-nesi-org-nz/my-nesi-org-nz_release_notes_v2-16-0/#fixes","title":"Fixes","text":"<ul> <li>Reviewed the logout process from my.nesi</li> <li>Ability to go straight to an individual project page after login</li> </ul>","tags":["releasenote"]},{"location":"Getting_Started/Release_Notes_my-nesi-org-nz/my-nesi-org-nz_release_notes_v2-17-0/","title":"my.nesi.org.nz release notes v2.17.0","text":"","tags":["releasenote"]},{"location":"Getting_Started/Release_Notes_my-nesi-org-nz/my-nesi-org-nz_release_notes_v2-17-0/#release-update-29-august-2023","title":"Release Update - 29. August 2023","text":"","tags":["releasenote"]},{"location":"Getting_Started/Release_Notes_my-nesi-org-nz/my-nesi-org-nz_release_notes_v2-17-0/#new-and-improved","title":"New and Improved","text":"<ul> <li>\u201cProvide Feedback\u201d is now redirected to     support@nesi.org.nz</li> <li>In a project request form, all grants are now visible by default</li> <li>After submitting a new allocation request, the user will be able to     see the Zendesk link in case further comments need to be added</li> <li>Addition of Keycloak packages for future need</li> <li>Make the date and citation fields mandatory for new research output     entries</li> </ul>","tags":["releasenote"]},{"location":"Getting_Started/Release_Notes_my-nesi-org-nz/my-nesi-org-nz_release_notes_v2-17-0/#fixes","title":"Fixes","text":"<ul> <li>Fix the console errors related to the new project form</li> </ul>","tags":["releasenote"]},{"location":"Getting_Started/Release_Notes_my-nesi-org-nz/my-nesi-org-nz_release_notes_v2-18-0/","title":"my.nesi.org.nz release notes v2.18.0","text":"","tags":["releasenote"]},{"location":"Getting_Started/Release_Notes_my-nesi-org-nz/my-nesi-org-nz_release_notes_v2-18-0/#release-update-07-november-2023","title":"Release Update - 07 November 2023","text":"","tags":["releasenote"]},{"location":"Getting_Started/Release_Notes_my-nesi-org-nz/my-nesi-org-nz_release_notes_v2-18-0/#new-and-improved","title":"New and Improved","text":"<ul> <li>A link to NeSI's privacy     policy     has been added to the bottom of all pages of my.nesi environment</li> <li>We've shifted from using Tuakiri's RapidConnect service to Tuakiri's     OpenID Connect bridge to improve overall security of my.nesi's user     authentication process.</li> <li>We've updated the display features of the table showing Merit grants     available to researchers in order to improve our ability to make     changes and future updates to the table's information.</li> </ul>","tags":["releasenote"]},{"location":"Getting_Started/Release_Notes_my-nesi-org-nz/my-nesi-org-nz_release_notes_v2-18-0/#fixes","title":"Fixes","text":"<ul> <li>Fixed a crash that used to occur when a user wanted to join a     project on my.nesi and delete an entry within that project.</li> <li>Fixed a security vulnerability in the my.nesi environment related to     the libwebp library, a code\u00a0library\u00a0used to render and display     images in the\u00a0WebP\u00a0format.</li> <li>Updated the allocation request form's end date message, restricting     allocation requests to no further than one year in the future.</li> <li>Changed which system components from NeSI's System Status page     (https://status.nesi.org.nz/) are default notifications emailed     to users. Users can customise their system status email     notifications at any time. Read more about that     here.</li> </ul> <p>If you have any questions about any of the improvements or fixes, please  Contact our Support Team.</p>","tags":["releasenote"]},{"location":"Getting_Started/Release_Notes_my-nesi-org-nz/my-nesi-org-nz_release_notes_v2-19-0/","title":"my.nesi.org.nz release notes v2.19.0","text":"","tags":["releasenote"]},{"location":"Getting_Started/Release_Notes_my-nesi-org-nz/my-nesi-org-nz_release_notes_v2-19-0/#release-update-23-november-2023","title":"Release Update - 23 November 2023","text":"","tags":["releasenote"]},{"location":"Getting_Started/Release_Notes_my-nesi-org-nz/my-nesi-org-nz_release_notes_v2-19-0/#new-and-improved","title":"New and Improved","text":"<ul> <li>Minor text changes to the About NeSI, Project Application, and Login     pages:<ul> <li>updated the descriptions of NeSI services\u00a0</li> <li>simplified the overview of the project application process</li> <li>updated the list of organisations supported by Tuakiri\u00a0</li> </ul> </li> <li>Improved visibility of Nearline (long-term     storage)     allocations if any exist for your project(s). A project with only     Nearline resources will have a status of Archived.</li> <li>Notification banner created to display holiday hours for NeSI     Support from the 22nd of December 2023 to the 2nd of January 2024.</li> </ul>","tags":["releasenote"]},{"location":"Getting_Started/Release_Notes_my-nesi-org-nz/my-nesi-org-nz_release_notes_v2-19-0/#fixes","title":"Fixes","text":"<ul> <li>If a project didn't have a previous allocation, the submission of a     New Allocation Request would return an error internally. This has     now been fixed.\u00a0</li> </ul> <p>If you have any questions about any of the improvements or fixes, please Contact our Support Team.</p>","tags":["releasenote"]},{"location":"Getting_Started/Release_Notes_my-nesi-org-nz/my-nesi-org-nz_release_notes_v2-2-0/","title":"my.nesi.org.nz release notes v2.2.0","text":"","tags":["releasenote"]},{"location":"Getting_Started/Release_Notes_my-nesi-org-nz/my-nesi-org-nz_release_notes_v2-2-0/#release-update-21-july-2021","title":"Release Update - 21. July 2021","text":"","tags":["releasenote"]},{"location":"Getting_Started/Release_Notes_my-nesi-org-nz/my-nesi-org-nz_release_notes_v2-2-0/#new-and-improved","title":"New and Improved","text":"<ul> <li>Added NeSI allocations list to project details view</li> <li>Improved feedback for users without active projects</li> <li>Improved validation of phone number formats (incl. international     prefix)</li> <li>Improved account profile form to create more clarity about mandatory     fields</li> </ul>","tags":["releasenote"]},{"location":"Getting_Started/Release_Notes_my-nesi-org-nz/my-nesi-org-nz_release_notes_v2-2-0/#fixes","title":"Fixes","text":"<p>Fixed:\u00a0made selection of science domain and study more reliable in the 'apply for access' form.</p>","tags":["releasenote"]},{"location":"Getting_Started/Release_Notes_my-nesi-org-nz/my-nesi-org-nz_release_notes_v2-3-0/","title":"my.nesi.org.nz release notes v2.3.0","text":"","tags":["releasenote"]},{"location":"Getting_Started/Release_Notes_my-nesi-org-nz/my-nesi-org-nz_release_notes_v2-3-0/#release-update-16-august-2021","title":"Release Update - 16. August 2021","text":"","tags":["releasenote"]},{"location":"Getting_Started/Release_Notes_my-nesi-org-nz/my-nesi-org-nz_release_notes_v2-3-0/#new-and-improved","title":"New and Improved","text":"<ul> <li>Added project member list to project details view</li> <li>Added ability to manage\u00a0project members (add/remove, assign     role)\u00a0for project owners</li> <li>Added feedback link to the 'user name' menu</li> </ul>","tags":["releasenote"]},{"location":"Getting_Started/Release_Notes_my-nesi-org-nz/my-nesi-org-nz_release_notes_v2-4-0/","title":"my.nesi.org.nz release notes v2.4.0","text":"","tags":["releasenote"]},{"location":"Getting_Started/Release_Notes_my-nesi-org-nz/my-nesi-org-nz_release_notes_v2-4-0/#release-update-13-september-2021","title":"Release Update - 13. September 2021","text":"","tags":["releasenote"]},{"location":"Getting_Started/Release_Notes_my-nesi-org-nz/my-nesi-org-nz_release_notes_v2-4-0/#new-and-improved","title":"New and Improved","text":"<ul> <li>UI layout changes for project details view</li> <li>Rendering a basic usage data for compute (and storage if quota     information available)</li> <li>Added contextual links for support article references</li> </ul>","tags":["releasenote"]},{"location":"Getting_Started/Release_Notes_my-nesi-org-nz/my-nesi-org-nz_release_notes_v2-4-0/#fixes","title":"Fixes","text":"<p>-</p>","tags":["releasenote"]},{"location":"Getting_Started/Release_Notes_my-nesi-org-nz/my-nesi-org-nz_release_notes_v2-5-0/","title":"my.nesi.org.nz release notes v2.5.0","text":"","tags":["releasenote"]},{"location":"Getting_Started/Release_Notes_my-nesi-org-nz/my-nesi-org-nz_release_notes_v2-5-0/#release-update-03-november-2021","title":"Release Update - 03. November 2021","text":"","tags":["releasenote"]},{"location":"Getting_Started/Release_Notes_my-nesi-org-nz/my-nesi-org-nz_release_notes_v2-5-0/#new-and-improved","title":"New and Improved","text":"<ul> <li>Introducing allocation renewal requests for project owners to be     made from my.nesi.org.nz</li> <li>Apply for Access form changes</li> <li>Allowing password reset regardless of current project membership\u00a0</li> </ul>","tags":["releasenote"]},{"location":"Getting_Started/Release_Notes_my-nesi-org-nz/my-nesi-org-nz_release_notes_v2-5-0/#fixes","title":"Fixes","text":"<ul> <li>Improved reliability to manage project members (so changes will be     reflected in the HPC system in 30-60 minutes)</li> </ul>","tags":["releasenote"]},{"location":"Getting_Started/Release_Notes_my-nesi-org-nz/my-nesi-org-nz_release_notes_v2-6-0/","title":"my.nesi.org.nz release notes v2.6.0","text":"","tags":["releasenote"]},{"location":"Getting_Started/Release_Notes_my-nesi-org-nz/my-nesi-org-nz_release_notes_v2-6-0/#release-update-14-december-2021","title":"Release Update - 14. December 2021","text":"","tags":["releasenote"]},{"location":"Getting_Started/Release_Notes_my-nesi-org-nz/my-nesi-org-nz_release_notes_v2-6-0/#new-and-improved","title":"New and Improved","text":"<ul> <li>Introducing NeSI Notification Preferences to create more     transparency for users</li> <li>Apply for Access form improvements to make naming project members     more consistent</li> </ul>","tags":["releasenote"]},{"location":"Getting_Started/Release_Notes_my-nesi-org-nz/my-nesi-org-nz_release_notes_v2-7-0/","title":"my.nesi.org.nz release notes v2.7.0","text":"","tags":["releasenote"]},{"location":"Getting_Started/Release_Notes_my-nesi-org-nz/my-nesi-org-nz_release_notes_v2-7-0/#release-update-15-march-2022","title":"Release Update - 15. March 2022","text":"","tags":["releasenote"]},{"location":"Getting_Started/Release_Notes_my-nesi-org-nz/my-nesi-org-nz_release_notes_v2-7-0/#new-and-improved","title":"New and Improved","text":"<ul> <li> <p>Notify users mentioned in project applications (if there is no NeSI     account linked to the users email address).</p> </li> <li> <p>Send email notification to affected users when project membership     status is changed (by the project owner).</p> </li> <li> <p>Added optional banner for e.g. holiday announcements.</p> </li> </ul>","tags":["releasenote"]},{"location":"Getting_Started/Release_Notes_my-nesi-org-nz/my-nesi-org-nz_release_notes_v2-8-0/","title":"my.nesi.org.nz release notes v2.8.0","text":"","tags":["releasenote"]},{"location":"Getting_Started/Release_Notes_my-nesi-org-nz/my-nesi-org-nz_release_notes_v2-8-0/#release-update-12-april-2022","title":"Release Update - 12. April 2022","text":"","tags":["releasenote"]},{"location":"Getting_Started/Release_Notes_my-nesi-org-nz/my-nesi-org-nz_release_notes_v2-8-0/#new-and-improved","title":"New and Improved","text":"<ul> <li> <p>Improved NeSI Notification     Preferences     to be project-specific</p> </li> <li> <p>Improved allocation renewal     requests     by providing more context</p> </li> </ul>","tags":["releasenote"]},{"location":"Getting_Started/Release_Notes_my-nesi-org-nz/my-nesi-org-nz_release_notes_v2-9-0/","title":"my.nesi.org.nz release notes v2.9.0","text":"","tags":["releasenote"]},{"location":"Getting_Started/Release_Notes_my-nesi-org-nz/my-nesi-org-nz_release_notes_v2-9-0/#release-update-18-may-2022","title":"Release Update - 18. May 2022","text":"","tags":["releasenote"]},{"location":"Getting_Started/Release_Notes_my-nesi-org-nz/my-nesi-org-nz_release_notes_v2-9-0/#new-and-improved","title":"New and Improved","text":"<ul> <li>Improved\u00a0allocation renewal     requests\u00a0default     organisation selection</li> <li>Added a sub-section to list open allocation requests</li> </ul>","tags":["releasenote"]},{"location":"Getting_Started/my-nesi-org-nz/Logging_in_to_my-nesi-org-nz/","title":"Logging in to my.nesi.org.nz","text":"","tags":[]},{"location":"Getting_Started/my-nesi-org-nz/Logging_in_to_my-nesi-org-nz/#login-credentials","title":"Login credentials","text":"<p>We allow students, academics, alumni and researchers to securely login and create a NeSI account profile using the credentials granted by their home organisation via Tuakiri.</p>","tags":[]},{"location":"Getting_Started/my-nesi-org-nz/Logging_in_to_my-nesi-org-nz/#tuakiri-federated-identity-and-access-management","title":"Tuakiri - federated identity and access management","text":"<p>Most New Zealand universities and Crown Research Institutes are members of the Tuakiri authentication federation, but many other institutions, including private sector organisations and most central and local government agencies, are not.</p> <p>See also Creating a NeSI Account Profile</p>","tags":[]},{"location":"Getting_Started/my-nesi-org-nz/Logging_in_to_my-nesi-org-nz/#support-for-users-outside-the-tuakiri-federation","title":"Support for users outside the Tuakiri federation","text":"<p>In case your organisation is not part of the Tuakiri federated identity management service, a user can still\u00a0request a NeSI Account profile. NeSI will (if approved) provision a so-called \"virtual home account\" on Tuakiri.</p> <p>See also Account Requests for non-Tuakiri Members</p>","tags":[]},{"location":"Getting_Started/my-nesi-org-nz/Logging_in_to_my-nesi-org-nz/#troubleshooting-login-issues","title":"Troubleshooting login issues","text":"<p>Please use the Tuakiri Attribute Validator to verify the details of your account. Contact your identity provider (e.g. institution, university) in case there are details missing or wrong.</p>","tags":[]},{"location":"Getting_Started/my-nesi-org-nz/Logging_in_to_my-nesi-org-nz/#tuakiri-attribute-validator","title":"Tuakiri Attribute Validator","text":"<p>This Tuakiri service is a health check for your managed identity. It checks the attributes provided about you from your identity provider, seeing whether they are provided in a format that is suitable for consumption by Tuakiri connected services. It will report any problems that it finds with your identity record, so that you can talk with the IT department at your identity provider (normally your employer or university) and have the problems fixed.</p> <p>Generally speaking, errors in your Tuakiri Core Attributes, such as empty values, will probably cause problems for you when you try to log in to a service using Tuakiri; while errors in your Tuakiri Optional Attributes are mostly harmless, and not worth bothering your IT department about unless you need a particular Optional Attribute to be correctly set for a specific purpose.</p> <p>To access the Tuakiri Attribute Validator, browse to this page: https://attributes.tuakiri.ac.nz/snapshots/latest\ufeff</p> <p>The primary identifier NeSI consumes is the attribute\u00a0auEduPersonSharedToken. This is a so-called, \"Tuakiri Core Attribute,\" expected to exist for every account.</p> <p>If your institution has issued you an empty or invalid auEduPersonSharedToken (rare), or if there is a difference between the value of your auEduPersonSharedToken as proffered by your institution's identity provision service and its value as recorded in the NeSI database (more common), you will not be able to log in to My NeSI. If you cannot log in, please raise a support ticket with your institutions IT support. For troubleshooting the support team may ask you for a PDF of your Tuakiri attributes. Tuakiri does not include your password in the attribute printout and there is no security risk involved in providing a copy of that PDF.</p>","tags":[]},{"location":"Getting_Started/my-nesi-org-nz/Managing_notification_preferences/","title":"Managing notification preferences","text":"","tags":[]},{"location":"Getting_Started/my-nesi-org-nz/Managing_notification_preferences/#overview","title":"Overview","text":"<p>NeSI aims to keep users informed via various communication channels.\u00a0</p>","tags":[]},{"location":"Getting_Started/my-nesi-org-nz/Managing_notification_preferences/#checking-and-setting-your-preferences","title":"Checking and setting your preferences","text":"<p>Within my.nesi.org.nz you can find a summary of the current subscriptions under NeSI Notification Preferences.\u00a0</p> <p>In order to manage your subscription to notifications, either log into my.nesi or use the link included at the bottom of the notification email message \"Manage your subscription\" or \"Unsubscribe\" to manage your preferences.</p> <p>Use the 'Manage' button provided to open the externally hosted preferences or the checkboxes for the NeSI Project-related notifications.</p> <p></p>","tags":[]},{"location":"Getting_Started/my-nesi-org-nz/Managing_notification_preferences/#see-also","title":"See also","text":"<p>Our support article on the NeSI\u00a0System status.</p>","tags":[]},{"location":"Getting_Started/my-nesi-org-nz/Navigating_the_my-nesi-org-nz_web_interface/","title":"Navigating the my.nesi.org.nz web interface","text":"","tags":[]},{"location":"Getting_Started/my-nesi-org-nz/Navigating_the_my-nesi-org-nz_web_interface/#main-navigation","title":"Main navigation","text":"<p>The main navigation is in the sidebar and links to important functions can be found at the bottom of the sidebar.</p>","tags":[]},{"location":"Getting_Started/my-nesi-org-nz/Navigating_the_my-nesi-org-nz_web_interface/#breadcrumb-navigation","title":"Breadcrumb navigation","text":"<p>A breadcrumb navigation is displayed when viewing sections of the site. Example: Home / Projects / List Project</p>","tags":[]},{"location":"Getting_Started/my-nesi-org-nz/Navigating_the_my-nesi-org-nz_web_interface/#collapsible-elements","title":"Collapsible elements","text":"<p>The triple bar (or hamburger) icons allow elements to be collapsed or revealed. The left icon does collapse the sidebar and therefore hides the navigation elements contained. The triple bar on the right is used for future functions.</p> <p>The &lt; arrow icon on the bottom of the sidebar does minimise the sidebar by reducing the visible content to icons only.</p>","tags":[]},{"location":"Getting_Started/my-nesi-org-nz/Navigating_the_my-nesi-org-nz_web_interface/#closing-the-session","title":"Closing the session","text":"<p>The 'user name menu' on the top right contains the option to logout and close the session.</p>","tags":[]},{"location":"Getting_Started/my-nesi-org-nz/Requesting_to_renew_an_allocation_via_my-nesi-org-nz/","title":"Requesting to renew an allocation via my.nesi.org.nz","text":"","tags":["mynesi"]},{"location":"Getting_Started/my-nesi-org-nz/Requesting_to_renew_an_allocation_via_my-nesi-org-nz/#how-to-raise-a-request-using-mynesiorgnz","title":"How to raise a request using my.nesi.org.nz?","text":"<ol> <li>Login to https://my.nesi.org.nz/projects/list and select a project     from the list. </li> <li>Click the Plus button icon 'action' next to the compute allocation     line item\u00a0 </li> <li>Verify the preset values and add a comment in case you update     some.     Finally, click 'Submit'     </li> </ol>","tags":["mynesi"]},{"location":"Getting_Started/my-nesi-org-nz/Requesting_to_renew_an_allocation_via_my-nesi-org-nz/#can-i-request-any-allocation-size","title":"Can I request any allocation size?","text":"<p>If you are requesting a new allocation of computing resources, we will look at your usage history and come up with an estimated allocation size and duration based on that history. If you think your rate of usage will be substantially higher or lower than we estimate, you should let us know what you think your rate of usage will be and why you expect it to differ from our forecast.</p> <p>Please be aware that:</p> <ul> <li>First and subsequent allocations are subject to the NeSI allocation     size and duration limits in force at the time they are considered by     our reviewers.</li> <li>An allocation from an institution's entitlement is subject to     approval by that institution.</li> </ul> <p>See Project Extensions and New Allocations on Existing Projects for more details.</p>","tags":["mynesi"]},{"location":"Getting_Started/my-nesi-org-nz/The_NeSI_Project_Request_Form/","title":"The NeSI Project Request Form","text":"<p>See Applying for a NeSI project\u00a0 for how to access the form.</p>","tags":[]},{"location":"Getting_Started/my-nesi-org-nz/The_NeSI_Project_Request_Form/#preparing-a-request-to-use-nesi-resources","title":"Preparing a request to use NeSI resources","text":"<p>The procedures for starting a request for a new NeSI project and editing an in-progress request (draft) that you previously started are described below.</p> <ol> <li>Point your web browser to     https://my.nesi.org.nz\u00a0and     login. Select \"Apply for Access\" from the sidebar navigation on the     left. </li> <li>Choose from the following items:<ul> <li>If you are returning to continue work on a draft request you     started earlier, choose the link based on the date/time or title     you've set.</li> <li>For a new project request, select \"Start a new     application. Note, this is the default in case there is not     draft request for your account.</li> </ul> </li> <li>When your request is ready to submit, progress through the form     sections using the 'Next' button at the bottom of the page until you     reach the 'Summary' section. After clicking the 'Submit' button and     passing the validation the request is submitted for review. You will     also receive a confirmation via email.</li> </ol>","tags":[]},{"location":"Getting_Started/my-nesi-org-nz/The_NeSI_Project_Request_Form/#saving-a-request-for-later","title":"Saving a Request for Later","text":"<p>Once you've started filling in details, the system will automatically save a draft.</p>","tags":[]},{"location":"Getting_Started/my-nesi-org-nz/The_NeSI_Project_Request_Form/#asterisks-to-indicate-mandatory-data-next-to-questions","title":"Asterisks to indicate mandatory data next to questions","text":"<p>The request can only be successfully submitted once all mandatory data has been entered. The final section in the form 'Summary' will highlight missing data and allow you to navigate back to the relevant section. \u00a0</p>","tags":[]},{"location":"Getting_Started/my-nesi-org-nz/Tuakiri_Attribute_Validator/","title":"Tuakiri Attribute Validator","text":"","tags":[]},{"location":"Getting_Started/my-nesi-org-nz/Tuakiri_Attribute_Validator/#tuakiri-attribute-validator","title":"Tuakiri Attribute Validator","text":"<p>This Tuakiri service is a health check for your managed identity. It checks the attributes provided about you from your identity provider, seeing whether they are provided in a format that is suitable for consumption by Tuakiri connected services. It will report any problems that it finds with your identity record, so that you can talk with the IT department at your identity provider (normally your employer or university) and have the problems fixed.</p> <p>Generally speaking, errors in your Tuakiri Core Attributes, such as empty values, will probably cause problems for you when you try to log in to a service using Tuakiri; while errors in your Tuakiri Optional Attributes are mostly harmless, and not worth bothering your IT department about unless you need a particular Optional Attribute to be correctly set for a specific purpose.</p> <p>To access the Tuakiri Attribute Validator, browse to this page: https://attributes.tuakiri.ac.nz/snapshots/latest\ufeff</p> <p>The primary identifier NeSI consumes is the attribute\u00a0auEduPersonSharedToken. This is a so-called, \"Tuakiri Core Attribute,\" expected to exist for every account.</p> <p>If your institution has issued you an empty or invalid auEduPersonSharedToken (rare), or if there is a difference between the value of your auEduPersonSharedToken as proffered by your institution's identity provision service and its value as recorded in the NeSI database (more common), you will not be able to log in to My NeSI. If you cannot log in, please raise a support ticket with your institutions IT support.</p> <p>For troubleshooting the support team may ask you for a PDF of your Tuakiri attributes. Tuakiri does not include your password in the attribute printout and there is no security risk involved in providing a copy of that PDF.</p>","tags":[]},{"location":"NeSI_Service_Subscriptions/Contracts_and_billing_processes/Billing_process/","title":"Billing process","text":"<p>Charges for Subscription usage are typically invoiced on a quarterly basis.  </p> <p>If your organisation requires a Purchase Order (PO) Number be used for invoices, the PO Number must be provided to us upon signing your Subscription service agreement.</p> <p>If you have any questions about Subscription billing processes, don\u2019t hesitate to\u00a0get in touch.</p>","tags":[]},{"location":"NeSI_Service_Subscriptions/Contracts_and_billing_processes/Types_of_contracts/","title":"Types of contracts","text":"<p>Typically our Subscription contracts are based on one-year terms and invoiced on a quarterly basis.</p> <ul> <li> <p>HPC Platform:     This is an inclusive package that provides access to all NeSI     services (compute, storage, support, consultancy, etc.) in one     Subscription so that you have the flexibility at any point during     the term of the contract to use any service. You are only charged     for what you use.</p> </li> <li> <p>National Data Transfer Platform - Membership &amp; managed endpoint:     A National Data Transfer Platform is delivered through a     partnership between NeSI, REANNZ, Globus, and research institutions     across the country. Read more here.     As a managed endpoint, your institution can provide secure, reliable, and fast     data transfer to/from NeSI\u2019s HPC Platform as well as to/from other     Globus endpoints across New Zealand and internationally.</p> </li> </ul> <p>We are also happy to discuss other custom options for things like our Training service.</p> <p>If you have any questions or would like to discuss our subscription terms or invoicing schedule in more detail, get in touch.</p> <p>Note</p> <p>National Data Transfer Platform service agreements are invoiced upon contract signing, as the fee is a one-time, annual, up-front fee.</p>","tags":[]},{"location":"NeSI_Service_Subscriptions/Overview/Pricing/","title":"Pricing","text":"<p>We have two categories of pricing for NeSI services through Subscriptions:</p> <ul> <li>Public sector &amp; not-for-profit</li> <li>Commercial</li> </ul> <p>Prices are reviewed annually and subject to change.</p>","tags":[]},{"location":"NeSI_Service_Subscriptions/Overview/Pricing/#current-pricing","title":"Current pricing","text":"<p>Visit our\u00a0Partners &amp; Pricing page\u00a0on the NeSI website for the latest pricing information. The website page will always display the current pricing of a NeSI service.</p> <p>If you have any questions about anything mentioned on this page, don\u2019t hesitate to\u00a0get in touch.</p>","tags":[]},{"location":"NeSI_Service_Subscriptions/Overview/Questions/","title":"Questions?","text":"<p>If you have any questions about anything related to Subscriptions, don\u2019t hesitate to\u00a0get in touch.</p> <p>Visit our Services sections on the NeSI website for more details on the ways we're supporting New Zealand research communities through:</p> <ul> <li> <p>High Performance Computing &amp; Analytics</p> </li> <li> <p>Consultancy</p> </li> <li> <p>Data Services</p> </li> <li> <p>Training</p> </li> </ul>","tags":[]},{"location":"NeSI_Service_Subscriptions/Overview/What_is_a_Subscription/","title":"What is a Subscription?","text":"<p>NeSI has a range of options for research institutions and independent researchers to access our services to build your research capabilities.</p> <p>Subscribing to NeSI's services provides you with:</p> <ul> <li> <p>Managed entitlements on NeSI's HPC platform for your research     projects and programmes. This includes access to:</p> </li> <li> <p>high-capacity CPUs, GPUs and high memory nodes</p> </li> <li> <p>interactive computing via Jupyter Notebooks, containers, and         virtual lab environments</p> </li> <li> <p>an extensive pre-built software library</p> </li> <li> <p>data storage for compute, from the highest performance storage         on NeSI's HPC platform or via NeSI's long-term storage Nearline         Service</p> </li> <li> <p>Personalised support to assist your researchers across the breadth     of their NeSI experience, from applying for projects/allocations, to     getting their projects running on the platform, to general     troubleshooting or experimenting with new tools or techniques</p> </li> <li> <p>Secure, reliable, and fast data transfer to/from NeSI\u2019s HPC Platform     as well as to/from other Globus endpoints across New Zealand and     internationally.</p> </li> <li> <p>Dedicated scientific and HPC-focused computational and data science support.</p> </li> <li> <p>Advice and partnership on capability-building initiatives for   research communities or collaboration on advanced research computing strategies.</p> </li> </ul> <p>If you have any questions about anything mentioned on this page, don\u2019t hesitate to\u00a0get in touch.</p>","tags":[]},{"location":"NeSI_Service_Subscriptions/Service_Governance/Allocation_approvals/","title":"Allocation approvals","text":"<p>Access to NeSI compute and storage resources is managed through allocations. An allocation is a certain amount of a resource, or of a rate at which a resource can be consumed, during a defined period of time (in this case, the term of the Subscription).</p> <p>Depending on the nature of your Subscription, you can choose to directly manage allocation approvals for projects from your institution or leave those decisions to the NeSI Support Team. See examples of some common approval scenarios below.  </p>","tags":[]},{"location":"NeSI_Service_Subscriptions/Service_Governance/Allocation_approvals/#subscription-used-by-a-single-project","title":"Subscription used by a single project","text":"<p>If there is only one project associated with a Subscription, the contract\u2019s full resource entitlement can be allocated to that single project (with the division of resources such as storage vs compute left to the discretion of the project owner).</p>","tags":[]},{"location":"NeSI_Service_Subscriptions/Service_Governance/Allocation_approvals/#subscription-used-by-multiple-projects","title":"Subscription used by multiple projects","text":"<p>If the Subscription contract covers multiple projects, the contract entitlement is split between the projects. The Subscriber can decide how it should be split, or NeSI\u2019s Support Team can review the project requests and make a recommendation based on project requirements.  </p>","tags":[]},{"location":"NeSI_Service_Subscriptions/Service_Governance/Service_Governance_contact/","title":"Service Governance contact","text":"<p>As part of our service agreements, we request you name a Service Governance Contact on behalf of your institution. The role of this person includes:</p> <ul> <li> <p>Acting as a primary liaison / contact person on behalf of your     institution for anything related to the service Subscription.</p> </li> <li> <p>Rpproving allocation requests for projects from your institution.</p> </li> <li> <p>Receiving and reviewing the Subscriber Monthly Usage Reports.</p> </li> </ul> <p>If you have any questions about the role of a Service Governance contact, get in touch.</p>","tags":[]},{"location":"NeSI_Service_Subscriptions/Service_Governance/Subscriber_Monthly_Usage_Reports/","title":"Subscriber Monthly Usage Reports","text":"<p>As part of NeSI\u2019s service agreements, we provide Subscribers with monthly usage reports. They provide a monthly view of chargeable usage (as well as any Merit / non-chargeable usage) across all platform services:</p> <ul> <li> <p>Compute - CPUs</p> </li> <li> <p>Compute - GPUs</p> </li> <li> <p>Storage - Persistent (active project storage)</p> </li> <li> <p>Storage - Nearline (long-term storage)</p> </li> </ul> <p>We send these usage report emails via our NeSI Support portal so that our team has better visibility internally of our monthly check-ins and we can more easily loop in others to answer technical questions or follow up on system-related requests.</p> <p>The monthly emails also include any timely updates on other NeSI news or training events that would be of interest to your research community.</p>","tags":[]},{"location":"NeSI_Service_Subscriptions/Service_Governance/Subscriber_Monthly_Usage_Reports/#how-to-read-your-subscriber-usage-report","title":"How to read your Subscriber Usage Report","text":"<p>A new tab is added to the report for each month, making it easy to view, reference, and compare recent and past usage.</p> <ul> <li> <p>At the top of each tab is a summary of the contract, indicating the:</p> </li> <li> <p>Term of agreement (contract start and end dates)</p> <ul> <li> <p>Maximum contracted value</p> </li> <li> <p>Value of services utilised to date</p> </li> </ul> </li> <li> <p>Usage for each service is shown in the corresponding sections below</p> </li> <li> <p>In cases where a service has differently priced resources (eg.     Compute pricing varies across our CPU and GPU resources), we will     also indicate additional information (eg. \u201cType of CPU\u201d and \u201cType of     GPU\u201d) so you have a breakdown of what usage contributes to the total     chargeable costs that month. See the Pricing section above for more     information on our service pricing.</p> </li> <li> <p>To showcase full value delivered through NeSI services, our reports     will also show usage that is not chargeable (eg. Merit usage). This     is shown simply for information purposes and is not included or     reflected on invoices.</p> </li> </ul> <p>Usage reports generally ready to view by the middle of the following month. So, for example, January usage will appear as a new tab by mid- to late February.</p> <p>If you have any questions about anything mentioned on this page, don\u2019t hesitate to\u00a0get in touch.</p>","tags":[]},{"location":"Scientific_Computing/HPC_Software_Environment/Build_an_Apptainer_container_on_a_Milan_compute_node/","title":"Build an Apptainer container on a Milan compute node","text":"<p>This article describes a technique to build Apptainer containers using Milan compute nodes, via a Slurm job. You can also build Singularity container using this technique.</p>","tags":[]},{"location":"Scientific_Computing/HPC_Software_Environment/Build_an_Apptainer_container_on_a_Milan_compute_node/#building-container-via-slurm","title":"Building container via Slurm","text":"<p>The new Milan compute nodes can be used to build Apptainer containers using the fakeroot feature. This functionality is only available on these nodes at the moment due to their operating system version.</p> <p>To illustrate this functionality, create an example container definition file <code>my_container.def</code> from a shell session on NeSI as follows:</p> <pre><code>cat &lt;&lt; EOF &gt; my_container.def\nBootStrap: docker\nFrom: ubuntu:20.04\n%post\n    apt-get -y update\n    apt-get install -y wget\nEOF\n</code></pre> <p>Then submit the following Slurm job submission script to build the container:</p> <pre><code>#!/bin/bash -e\n#SBATCH --job-name=apptainer_build\n#SBATCH --partition=milan\n#SBATCH --time=0-00:30:00\n#SBATCH --mem=4GB\n#SBATCH --cpus-per-task=2\n\n# load environment module\nmodule purge\nmodule load Apptainer/1.2.2\n\n# recent Apptainer modules set APPTAINER_BIND, which typically breaks\n# container builds, so unset it here\nunset APPTAINER_BIND\n\n# create a build and cache directory on nobackup storage\nexport APPTAINER_CACHEDIR=\"/nesi/nobackup/$SLURM_JOB_ACCOUNT/$USER/apptainer_cache\"\nexport APPTAINER_TMPDIR=\"/nesi/nobackup/$SLURM_JOB_ACCOUNT/$USER/apptainer_tmpdir\"\nmkdir -p $APPTAINER_CACHEDIR $APPTAINER_TMPDIR\nsetfacl -b $APPTAINER_TMPDIR\n\n# build the container\napptainer build --force --fakeroot my_container.sif my_container.def\n</code></pre> <p>Note this script will start an Slurm job for 30 minutes using 2 cores and 4 GB of memory to build the image. Make sure to set these resources correctly, some containers can take hours to build and require tens of GB of memory.</p> <p>Option <code>--force</code> will rebuild my_container.sif even if it already is in the directory.</p> <p>More information about how to submit a Slurm job is available in the Submitting your first job support page.</p> <p>Build environment variables</p> <p>To build containers, you need to ensure that Apptainer has enough  storage space to create intermediate files. It also requires a cache  folder to save images pulled from a different location (e.g.  DockerHub). By default both of these locations are set to <code>/tmp</code> which  has limited space, large builds may exceed this limitation causing the  builder to crash. The environment variables <code>APPTAINER_TMPDIR</code> and  <code>APPTAINER_CACHEDIR</code> are used to overwrite the default location of  these directories.  In this example, the Slurm job submission script creates these folders  using your project <code>nobackup</code> folder.</p>","tags":[]},{"location":"Scientific_Computing/HPC_Software_Environment/Build_an_Apptainer_container_on_a_Milan_compute_node/#known-limitations","title":"Known limitations","text":"<p>If your container uses RPM to install packages, i.e. is based on CentOS or Rocky Linux, you need to disable the <code>APPTAINER_TMPDIR</code> environment variable (use <code>unset APPTAINER_TMPDIR</code>) and request more memory for your Slurm job. Otherwise, RPM will crash due to an incompatibility with the <code>nobackup</code> filesystem.</p> <p>If you encounter the following error when using a base Docker image in your Apptainer definition file</p> <pre><code>While making image from oci registry: error fetching image to cache: while building SIF from layers: conveyor failed to get: unsupported image-specific operation on artifact with type \"application/vnd.docker.container.image.v1+json\"\n</code></pre> <p>it is likely due to an upstream issue (e.g. bad image on Dockerhub). In this case, try an older image version or a different base image.</p> <p>Other limitations</p> <p>This method, using fakeroot, is known to not work for all types of  Apptainer/Singularity containers.  If you encounter an issue, please Contact our Support Team</p>","tags":[]},{"location":"Scientific_Computing/HPC_Software_Environment/Compiling_software_on_Mahuika/","title":"Compiling software on Mahuika","text":"","tags":[]},{"location":"Scientific_Computing/HPC_Software_Environment/Compiling_software_on_Mahuika/#where-to-build","title":"Where to build","text":"<p>You may compile (build) software on the Mahuika login nodes, <code>login.mahuika.nesi.org.nz</code>. Please be aware that these login nodes are limited and shared resources. Please limit the amount of processes on these nodes. For example, use <code>make -j 5</code> instead of <code>make -j</code>. If you require many CPU cores or long run times for your build process, please request these resources through a batch job submitted to Slurm, where you can also ask for a larger amount of compute resources to build your code.</p>","tags":[]},{"location":"Scientific_Computing/HPC_Software_Environment/Compiling_software_on_Mahuika/#compilers-and-toolchains","title":"Compilers and Toolchains","text":"<p>Compilers produced by three different vendors are provided on Mahuika: Cray, GNU and Intel.</p> <p>The GNU and Intel compilers can be accessed by loading one of the toolchains:</p> <ul> <li><code>module load gimkl/2022a</code> - the default toolchain, providing GNU     compilers (version 9.2.0), Intel MPI and Intel MKL</li> <li><code>module load intel/2022a</code> - Intel compilers (version 2020.0.166),     Intel MPI and Intel MKL</li> </ul> <p>A large number of dependencies are built against these toolchains, so they are usually a good place to start when building your own software. However, if for any reason you require a different version of one of these compilers, you can load a compiler module directly, instead of loading a toolchain. For example, the installed versions of the GNU compilers can be listed with the following command:</p> <p><code>module spider GCC</code></p> <p>and version 7.4.0 loaded with the following command:</p> <p><code>module load GCC/7.4.0</code></p> <p>The Cray compilers behave differently to the GNU and Intel compilers, since they are installed as a Cray Programming Environment, and have some special features (see section Cray Programming Environment). The Cray compilers are loaded with:</p> <p><code>module load PrgEnv-cray</code></p>","tags":[]},{"location":"Scientific_Computing/HPC_Software_Environment/Compiling_software_on_Mahuika/#third-party-applications","title":"Third party applications","text":"<p>Installation instructions vary from application to application, and we suggest that you carefully read the instructions provided by the developers of the software you plan to use. Nevertheless, the following should give you an impression which steps you usually need to consider:</p> <ol> <li>Change into your desired source code directory. We suggest you use     <code>/nesi/project/&lt;projectID&gt;</code>, or more typically one of its     subdirectories. You may instead use\u00a0<code>/nesi/nobackup/&lt;projectID&gt;</code>\u00a0(or     one of its subdirectories) if you don't mind the software not being     backed up and prone to automatic deletion in certain circumstances.</li> <li>Download the source code. This could be done via a repository     checkout (<code>git clone &lt;source-url&gt;</code>) or via downloading a tarball     (<code>wget &lt;source-url&gt;</code>).</li> <li>Ensure the tarball is not a tarbomb,     using\u00a0<code>tar tf &lt;source.tar&gt; | sort | less</code> (<code>tar tzf ...</code> if the     source code is a gzipped tarball, <code>tar tjf ...</code> if a bzip2     compressed tarball). If you find that the tarball is in fact a     tarbomb, you will need to handle it using special techniques.</li> <li>Unpack the tarball using <code>tar xf &lt;source.tar&gt;</code>. Change into the     source directory.</li> <li>Load the preferred toolchain (or compiler module) and modules for     any additional required libraries (<code>module load gimkl FFTW</code>)</li> <li>Run the configure script with appropriate options,     e.g.\u00a0<code>./configure --prefix=&lt;desired install directory&gt; --use-fftw=$EBROOTFFTW</code>(options     can usually be listed using <code>./configure --help</code>)</li> <li>In some applications you need to adjust the\u00a0<code>Makefile</code>\u00a0(generated by     <code>configure</code>) to reflect your preferred compiler, and library options     (see below)</li> <li>Compile the code (<code>make</code>)</li> <li>install the binaries and libraries into the specified directory     (<code>make install</code>)\u00a0</li> </ol>","tags":[]},{"location":"Scientific_Computing/HPC_Software_Environment/Compiling_software_on_Mahuika/#compilers","title":"Compilers","text":"<p>Compilers are provided for Fortran, C, and C++. For MPI-parallelised code, different compilers typically need to be used. The different compilers are listed:</p> Language Cray Intel GNU Fortran ftn ifort gfortran Fortran + MPI ftn mpiifort mpif90 C cc icc gcc C + MPI cc mpiicc mpicc C++ CC icpc g++ C++ + MPI CC mpiicpc mpicxx <p>Note, Cray uses compiler wrappers which are described later in more detail.</p> <p>In general you then compile your code using:</p> <p><code>&lt;compiler&gt; &lt;CompilerOptions&gt; &lt;source-file&gt;</code></p> <p>e.g.</p> <p><code>ftn -O3 hello.f90</code></p>","tags":[]},{"location":"Scientific_Computing/HPC_Software_Environment/Compiling_software_on_Mahuika/#compiler-options","title":"Compiler options","text":"<p>Compilers are controlled using different options to control optimizations, output, source and library handling. There options vary between the different compiler vendors. That means you will need to change them if you decide to switch compilers. The following table provides a list of commonly used compiler options for the different compilers:</p> Group Cray Intel GNU Notes Debugging <code>-g</code> or <code>-G{0,1,2,fast}</code> <code>-g</code> or <code>-debug [keyword]</code> <code>-g</code> or <code>-g{0,1,2,3}</code> Set level of debugging information, some levels may disable certain compiler optimisations Light compiler optimisation <code>-O2</code> <code>-O2</code> <code>-O2</code> Aggressive compiler optimisation <code>-O3 -hfp3</code> <code>-O3 -ipo</code> <code>-O3 -ffast-math -funroll-loops</code> This may affect numerical accuracy Architecture specific optimisation Load this module first: <code>module load craype-broadwell</code> <code>-xHost</code> <code>-march=native</code> <code>-mtune=native</code> Build and compute nodes have the same architecture (Broadwell) Vectorisation reports <code>-hlist=m</code> <code>-qopt-report</code> <code>-fopt-info-vec</code> or <code>-fopt-info-missed</code> OpenMP <code>-homp</code> (default) <code>-qopenmp</code> `-fopenmp`` <p>Additional compiler options are documented in the compiler man pages, e.g. <code>man mpicc</code>, which are available after loading the related compiler module. Additional documentation can be also found at the vendor web pages:</p> <ul> <li>Cray Fortran     v8.7,     Cray C and C++     v8.7</li> <li>Intel Parallel Studio XE Cluster     Edition for Linux is     installed on the Mahuika HPC Cluster, Mahuika Ancillary Nodes</li> <li>Intel Developer     Guides</li> <li>GCC Manuals</li> </ul> <p>Note: Cray uses compiler wrappers. To list the compiler options, please consult the man pages for the actual compiler, not the wrapper.</p> <p>For example, the following commands would be used to compile with the gfortran compiler, activate compiler warnings (<code>-Wall</code>), and requiring aggressive compiler optimisation (<code>-O3</code>):</p> <p><code>module load gimkl/2018b</code></p> <p><code>mpif90 -Wall -O3 -o simpleMpi simpleMpi.f90</code></p>","tags":[]},{"location":"Scientific_Computing/HPC_Software_Environment/Compiling_software_on_Mahuika/#linking","title":"Linking","text":"<p>Your application may depend on one or more external software packages, normally libraries, and if so it will need to link against them when you compile the program. In general, to link against an external package, one must specify:</p> <ul> <li>The location of the header files, using the option     <code>-I/path/to/headers</code></li> <li>The location of the compiled library or libraries, using     <code>-L/path/to/lib/</code></li> <li>The name of each library, typically without prefixes and suffixes.     For example, if the full library file name is <code>libfoo.so.1.2.3</code>     (with aliases <code>libfoo.so.1</code> and <code>libfoo.so</code>), the expected entry on     the link line is <code>-lfoo</code>.</li> </ul> <p>Thus the linker expects to find the include headers in the /path/to/headers and the library at /path/to/lib/lib.so (we assume dynamic linking).</p> <p>Note that you may need to list several libraries to link successfully, e.g., <code>-lA -lB</code> for linking against libraries \"A\" and \"B\". The order in which you list libraries matters, as the linker will go through the list in order of appearance. If library \"A\" depends on library \"B\", specifying <code>-lA -lB</code> will work. If library \"B\" depends on \"A\", use <code>-lB -lA</code>. If they depend on each other, use <code>-lA -lB -lA</code> (although such cases are quite rare).</p>","tags":[]},{"location":"Scientific_Computing/HPC_Software_Environment/Compiling_software_on_Mahuika/#external-libraries","title":"External Libraries","text":"<p>There are already many libraries provided on the Mahuika platform. Most of them are provided in modules. You can search them using</p> <p><code>module spider</code></p> <p>and look in the module description using:</p> <p><code>module help &lt;module-name&gt;</code></p> <p>Sometimes modules provide multiple libraries, e.g. cray-libsci.</p> <p>Most libraries are provided using the EasyBuild software management system, that NeSI/NIWA use to provide modules. Easybuild automatically defines environment variables <code>$EBROOT&lt;library name in upper case&gt;</code> when a module is loaded, which help pointing the compiler and linker to include files and libraries as in the example above. Thus, you can keep your Makefile independent of library versions, by defining e.g. <code>-L$EBROOT&lt;library name in upper case&gt;/lib</code>. Therewith you can use another version by only swapping modules. If you are unsure which <code>$EBROOT&lt;...&gt;</code> variables are available, use</p> <p><code>module show &lt;module-name&gt;</code></p> <p>to find out.</p> <p>Note that specifying search paths with <code>-I</code> and <code>-L</code> is not strictly necessary in case of the GNU and Intel compilers, which will use the contents of <code>CPATH</code>, <code>LIRARY_PATH</code>, and <code>LD_LIBRARY_PATH</code> provided by the NeSI/NIWA module. This will not work with the Cray compiler.</p> <p>Important note: Make sure that you load the correct variant of a library, depending on your choice of compiler. Switching compiler environment will not switch NeSI/NIWA modules automatically. Furthermore, loading a NeSI/NIWA module may switch programming environment if it was built with a different compiler. In general, the used library should be build with the same compiler.</p> <p>Note: the mentioned MPI compilers are practically compiler wrappers adding the location to the MPI library. This can be observed calling e.g. <code>mpif90 -showme</code></p>","tags":[]},{"location":"Scientific_Computing/HPC_Software_Environment/Compiling_software_on_Mahuika/#common-linker-problems","title":"Common Linker Problems","text":"<p>Linking can easily go wrong. Most often, you will see linker errors about \"missing symbols\" when the linker could not find a function used in your program or in one of the libraries that you linked against. To resolve this problem, have a closer look at the function names that the linker reported:</p> <ul> <li>Are you missing some object code files (these are compiled source     files and have suffix <code>.o</code>) that should appear on the linker line?     This can happen if the build system was not configured correctly or     has a bug. Try running the linking step manually with all source     files and debug the build system (which can be a lengthy and     cumbersome process, unfortunately).</li> <li>Do the missing functions have names that contain \"mp\" or \"omp\"? This     could mean that some of your source files or external libraries were     built with OpenMP support, which requires you to set an OpenMP flag     (<code>-fopenmp</code> for GNU compilers, <code>-qopenmp</code> for Intel) in your linker     command. For the Cray compilers, OpenMP is enabled by default and     can be controlled using <code>-h[no]omp</code>.</li> <li>Do you see a very long list of complex-looking function names, and     does your source code or external library dependency include C++     code? You may need to explicitly link against the C++ standard     library (<code>-lstdc++</code> for GNU and Cray compilers, <code>-cxxlib</code> for Intel     compilers); this is a particularly common problem for statically     linked code.</li> <li>Do the function names end with an underscore (\"_\")? You might be     missing some Fortran code, either from your own sources or from a     library that was written in Fortran, or parts of your Fortran code     were built with flags such as <code>-assume nounderscore</code> (Intel) or     <code>-fno-underscoring</code> (GNU), while others were using different flags     (note that the Cray compiler always uses underscores).</li> <li>Do the function names end with double underscores (\"__\")? Fortran     compilers offer an option to add double underscores to Fortran     subroutine names for compatibility reasons     (<code>-h [no]second_underscore</code>, <code>-assume [no]2underscores</code>,     <code>-f[no-]second-underscore</code>) which you may have to add or remove.</li> <li>Compiler not necessarily enable preprocessing, which could result in     <code>#ifndef VAR; Warning: Illegal preprocessor directive</code>. For example,     using preprocessor directives in <code>.f</code> files with gfortran requires     the <code>-cpp</code> option.</li> </ul> <p>Note that the linker requires that function names match exactly, so any variation in function name in your code will lead to a \"missing symbols\" error (with the exception of character case in Fortran source code).</p>","tags":[]},{"location":"Scientific_Computing/HPC_Software_Environment/Compiling_software_on_Mahuika/#cray-programming-environment","title":"Cray Programming Environment","text":"<p>The Cray Programming Environment includes the Cray compiler, various libraries and tools. These work nicely together and provide certain user-friendly features by using compiler wrappers. This works very similar as the Cray XC environment, provided on M\u0101ui, and is described in detail on the page Building Code on M\u0101ui.</p> <p>Note:\u00a0on M\u0101ui the three compilers (Cray, Gnu and Intel) are provided in this special environment, which provides support for both dynamic and static linking. In contrast to that, on Mahuika only the Cray compiler is provided in this environment. Furthermore, it only provides support for dynamic linking.</p>","tags":[]},{"location":"Scientific_Computing/HPC_Software_Environment/Compiling_software_on_Maui/","title":"Compiling software on M\u0101ui","text":"<p>Warning</p> <p>This page has been automatically migrated and may contain formatting errors.</p> <p>Building Fortran, C, or C++ code on the XC50 platform requires using the Cray programming environment. From a user perspective, the programming environment consists of a set of environment modules that select a compiler, essential libraries such as the MPI library, a CPU target, and more. The build process on the XC50 thus differs slightly from a standard Linux system. Non-compiled code, such as Python or R programs, do not use the programming environment. Note, however, that loading a module provided by NeSI/NIWA to get access to, e.g., the \u201cRegCM\u201d code, may change the Cray programming environment in that Cray environment modules may be swapped.</p> <p>Important:</p> <ul> <li>It is essential to use a Programming Environment (PrgEnv-cray, PrgEnv-intel or PrgEnv-gnu) when you build code, otherwise it is very likely that problems at build time or run time appear</li> <li> Never use <code>module purge</code> on the XC50 platform, this will render the programming environment unusable, and you will have to log out and log back in</li> <li>Code that was built on the XC50 platform is unlikely to run on M\u0101ui\u2019s CS500 platform or on Mahuika\u2019s CS400 platform; please rebuild your code when you change platform</li> </ul>","tags":[]},{"location":"Scientific_Computing/HPC_Software_Environment/Compiling_software_on_Maui/#the-build-node","title":"The build node","text":"<p>M\u0101ui has a dedicated build node, <code>login.maui.nesi.org.nz</code>, which should be used for building code. Please do not build code on the compute nodes by submitting a build job through SLURM:</p> <ul> <li>The compute nodes only run a thin operating system with very few command line utilities, it is thus likely that your build will fail</li> <li>The file system on XC50 compute nodes is optimised for handling large block IO, small block IO that is typical for a build job is inefficient</li> <li>Submitting a job will allocate entire nodes. This is a waste of compute resources, especially if only one core or a few cores are used</li> </ul> <p>Furthermore, please keep in mind that the build node is a shared resource. Instead of using as many parallel build processes as possible (with <code>make -j</code>), please limit the amount of processes (<code>make -j 5</code> for example).</p>","tags":[]},{"location":"Scientific_Computing/HPC_Software_Environment/Compiling_software_on_Maui/#choosing-a-programming-environment","title":"Choosing a programming environment","text":"<p>The following Programming Environments are provided on M\u0101ui, named after the underlying compiler suite:</p> <ol> <li><code>PrgEnv-cray</code></li> <li><code>PrgEnv-intel</code></li> <li><code>PrgEnv-gnu</code></li> </ol> <p>The <code>PrgEnv-cray</code> environment is the default. If you want to change programming environment to use the Intel or GNU compilers, run</p> <pre><code>module swap PrgEnv-cray PrgEnv-intel\n</code></pre> <p>or</p> <pre><code>module swap PrgEnv-cray PrgEnv-gnu\n</code></pre> <p>Note that several compiler versions are currently installed, in case of GNU for example:</p> <pre><code>&gt; module avail gcc\n-------------------------------------- /opt/modulefiles --------------------------------------\ngcc/10.3.0 gcc/11.2.0 gcc/12.1.0(default)\n</code></pre> <p>To change GCC version, run for example</p> <pre><code>module swap gcc gcc/11.2.0\n</code></pre> <p>GCC v6.1.0 or later is required to build code that can make use of the Intel Skylake microarchitecture and its advanced capabilities, such as AVX-512, on the XC50 platform.</p> <p>Note: There is not the best compiler. Depending on your application/algorithms, different compilers can optimise the code better. Keep in mind trying different compilers.</p>","tags":[]},{"location":"Scientific_Computing/HPC_Software_Environment/Compiling_software_on_Maui/#targetting-a-cpu","title":"Targeting a CPU","text":"<p>Compiling a program translates source code into machine instructions. It is important to let the compiler know for which CPU (\u201ctarget\u201d) the executable shall be build, to make best use of that CPU\u2019s capabilities. M\u0101ui uses Intel Skylake microprocessors on all XC50 build and compute nodes, which come with AVX-512 vector instructions, enabling better performance for some codes.</p> <p>CPU targets can be set by loading a module. By default, module <code>craype-x86-skylake</code> is loaded. In the rare case that you encounter problems with the Skylake target at build time or run time, try target for \u201cBroadwell\u201d processors instead:</p> <pre><code>module swap craype-x86-skylake craype-broadwell\n</code></pre> <p>Choosing the \u201cBroadwell\u201d target is also necessary if you want to build code using the older GCC compilers prior to GCC 6.1.0, which were released before Skylake became available. If you see the error message</p> <pre><code>craype-x86-skylake requires cce/8.6 or later, intel/15.1 or later, or gcc/6.1 or later\n</code></pre> <p>when trying to swap to the <code>PrgEnv-gnu</code> environment, or an error message of the kind</p> <pre><code>f951: error: bad value (skylake-avx512) for -march= switch\n</code></pre> <p>when you compile a program with a GNU compiler, run</p> <pre><code>module swap craype-x86-skylake craype-broadwell\n</code></pre> <p>and try again.</p> <p>Make sure that a target is always set. If you do not set a target, the compilers will produce generic code that runs on many processors of the \u201cx86-64\u201d family, and the program will thus not be able to benefit from capabilities such as AVX-512. You will see the following warning message when you run a compiler:</p> <pre><code>No supported cpu target is set, CRAY_CPU_TARGET=x86-64 will be used.\nLoad a valid targeting module or set CRAY_CPU_TARGET\n</code></pre>","tags":[]},{"location":"Scientific_Computing/HPC_Software_Environment/Compiling_software_on_Maui/#using-the-compiler-drivers","title":"Using the compiler drivers","text":"<p>The programming environment provides compiler drivers for compiling Fortran, C, and C++ code. This means that you will need to use the following commands instead of the actual compilers:</p> <pre><code>ftn -o simpleMpi simpleMpi.f90 # compile Fortran code\ncc  -o simpleMpi simpleMpi.c    # compile C code\nCC  -o simpleMpi simpleMpi.cxx  # compile C++ code\n</code></pre> <p>The drivers will ensure correct linking of your code with compiler runtime libraries, and with Cray-supported libraries (such as Cray\u2019s \u201clibsci\u201d scientific library, or Cray\u2019s version of netCDF). It is therefore not recommended to use the compilers directly, there is a good chance that the executable will fail to build or run correctly.</p> <p>The compiler drivers automatically add necessary compile and link flags to the compile/link line for the selected hardware and Cray-supported libraries. If you are interested in seeing what the compiler driver does, add the <code>-craype-verbose</code> flag:</p> <pre><code>ftn -craype-verbose -o simpleMpi simpleMpi.f90\n</code></pre> <p>Further compiler driver options can be found on their man pages:</p> <pre><code>man ftn\nman cc\nman CC</code></pre>","tags":[]},{"location":"Scientific_Computing/HPC_Software_Environment/Compiling_software_on_Maui/#compiling_and_running_mpi_code","title":"Compiling and Running MPI code","text":"<p>The compiler drivers will also automatically build MPI codes correctly, there is no need to use special compilers or add additional compiler or linker flags.</p> <p>Note that running an MPI code on the build node (<code>login.maui.nesi.org.nz</code>) using</p> <pre><code>./simpleMPI\n</code></pre> <p>will fail with an error message, as there is no MPI runtime environment:</p> <pre><code>[Wed Oct 18 02:00:14 2017] [c0-0c0s3n1] Fatal error in MPI_Init: Other MPI error, error stack:\nMPIR_Init_thread(537):\nMPID_Init(247).......: channel initialization failed\nMPID_Init(636).......:  PMI2 init failed: 1\n</code></pre> <p>If you want to run a short test of your build, use SLURM\u2019s srun command that submits your program to a compute node on the fly, e.g.,</p> <pre><code>SLURM_PARTITION=nesi_research srun -n 6 simpleMPI\n</code></pre>","tags":[]},{"location":"Scientific_Computing/HPC_Software_Environment/Compiling_software_on_Maui/#common-compiler-options","title":"Common compiler options","text":"<p>Although the compiler drivers <code>ftn</code>, <code>cc</code> and <code>CC</code> have a few options of their own, such as the <code>-craype-verbose</code> flag, they will pass through any additional compiler options to the underlying compiler. This means that you will still need to choose compiler flags that are specific to the Cray, Intel, or GNU compilers, and you will need to change them if you decide to switch compilers.</p> <p>For example, if you wanted to use the gfortran compiler, activate compiler warnings (<code>-Wall</code>), and require aggressive compiler optimisation (<code>-O3</code>), you would use the following commands:</p> <pre><code>module swap PrgEnv-cray PrgEnv-gnu\nftn -Wall -O3 -o simpleMpi simpleMpi.f90\n</code></pre> <p>The following table provides a list of commonly used compiler options:</p> Group Cray Intel GNU Notes Debugging <code>-g</code> or <code>-G{0,1,2,fast}</code> <code>-g</code> or <code>-debug [keyword]</code> <code>-g or -g{0,1,2,3}</code> Set level of debugging information, some levels may disable certain compiler optimisations Light compiler optimisation <code>-O2</code> <code>-O2</code> <code>-O2</code> Aggressive compiler optimisation <code>-O3 -hfp3</code> <code>-O3 -ipo</code> <code>-O3 -ffast-math -funroll-loops</code> This may affect numerical accuracy Vectorisation reports <code>-hlist=m</code> <code>-qopt-report</code> <code>-fopt-info-vec</code> or <code>-fopt-info-missed</code> OpenMP <code>-homp</code> (default) <code>-openmp</code> <code>-fopenmp</code> <p>Additional compiler options are documented on the compiler man pages, which are accessible after loading the corresponding programming environment:</p> language cray intel gnu Fortran man crayftn man ifort man gfortran C man craycc man icc man gcc C++ man crayCC man icpc man g++ <p> </p>","tags":[]},{"location":"Scientific_Computing/HPC_Software_Environment/Compiling_software_on_Maui/#building_code_that_depends_on_external_libraries_","title":"Building Code that Depends on External Libraries","text":"<p>While linking external libraries, one need to pay attention to the correct compiler and linker setup. This, depends on the correct library version (working properly with the compiler and the link type) and the used link options. These depend on whether the libraries have been provided by Cray, by NeSI/NIWA, or if you built them yourself.</p>","tags":[]},{"location":"Scientific_Computing/HPC_Software_Environment/Configuring_Dask_MPI_jobs/","title":"Configuring Dask-MPI jobs","text":"<p>Start simple</p> <p>The technique explained in this page should be considered after  trying simpler single node options (e.g.  Dask Distributed LocalCluster),  if</p> <ul> <li>you need more cores than what is available on a single node,</li> <li>or your queuing time is too long.</li> </ul> <p>Note that using MPI to distribute computations on multiple nodes can  have an impact on performances, compared to a single node setting.</p> <p>Dask is a popular Python package for parallelising workflows. It can use a variety of parallelisation backends, including Python multiprocessing and multithreading. A separate Dask-MPI package is provided for distributed high-performance computation using the MPI (Message Passing Interface) backend, which can achieve scalability across many nodes and integrates well into an HPC environment.</p> <p>Installing the Dask-MPI package and configuring jobs requires careful consideration to work reliably and efficiently. Internally it relies on the mpi4py package that provides an interface to the MPI library. MPI itself is implemented by different freely available distributions, including MPICH and OpenMPI, as well as a variety of vendor-specific distributions, such as Intel MPI and Cray MPI.</p> <p>While some of the MPI distributions should be compatible with each other, it is advisable to use the same MPI distribution as the host HPC system for reliability. The Mahuika and\u00a0M\u0101ui Ancil clusters use Intel MPI.</p>","tags":[]},{"location":"Scientific_Computing/HPC_Software_Environment/Configuring_Dask_MPI_jobs/#using-dask-mpi-on-mahuika","title":"Using Dask-MPI on Mahuika","text":"<p>Dask-MPI can be readily used with the more recent Python modules available on Mahuika that come with the mpi4py package, e.g.</p> <pre><code>module load Python/3.9.9-gimkl-2020a\n</code></pre>","tags":[]},{"location":"Scientific_Computing/HPC_Software_Environment/Configuring_Dask_MPI_jobs/#installing-dask-mpi-with-conda-on-mahuika-and-maui-ancil","title":"Installing Dask-MPI with Conda on Mahuika and M\u0101ui Ancil","text":"<p>Load an Anaconda3 or Miniconda3 module and use the following commands to install mpi4py with the Intel MPI distribution before installing the Dask-MPI package:</p> <pre><code>conda install -c intel mpi4py\nconda install -c conda-forge dask-mpi\n</code></pre> <p>If you use an environment file, add the <code>intel</code> channel at the end of the list (so that it will not take priority over other channels) and request mpi4py with the Intel MPI distribution as follows:</p> <pre><code>name: myenvironment\nchannels:\n  - myfavouritechannel\n  - intel\ndependencies:\n  - mypackage\n  - anotherpackage\n  - intel::mpi4py\n  - dask-mpi\n</code></pre> <p>See also</p> <p>See the  Miniconda3  page for more information on how to create and manage Miniconda  environments on NeSI.</p>","tags":[]},{"location":"Scientific_Computing/HPC_Software_Environment/Configuring_Dask_MPI_jobs/#configuring-slurm","title":"Configuring Slurm","text":"<p>At runtime, Slurm will launch a number of Python processes as requested in the Slurm configuration script. Each process is given an ID (or \"rank\") starting at rank 0. Dask-MPI then assigns different roles to the different ranks:</p> <ul> <li>Rank 0 becomes the scheduler that coordinates work and communication</li> <li>Rank 1 becomes the worker that executes the main Python program and   hands out workloads</li> <li>Ranks 2 and above become additional workers that run workloads</li> </ul> <p>This implies that Dask-MPI jobs must be launched on at least 3 MPI ranks! Ranks 0 and 1 often perform much less work than the other ranks, it can therefore be beneficial to use Hyperthreading to place these two ranks onto a single physical core. Ensure that activating hyperthreading does not slow down the worker ranks by running a short test workload with and without hyperthreading.</p> <p>In the following, two cases will be discussed:</p> <ol> <li>The worker ranks use little memory and they do not use    parallelisation themselves</li> <li>The worker ranks use a lot of memory and/or parallelisation</li> </ol> <p>Note that Slurm will place different MPI ranks on different nodes on the HPC by default - this has the advantage of much reduced queuing times as Slurm can use gaps in node utilisation, and this should not affect performance, unless individual work items are very small (e.g., if a given work item only takes a few seconds or less to run).</p>","tags":[]},{"location":"Scientific_Computing/HPC_Software_Environment/Configuring_Dask_MPI_jobs/#dask-workers-have-low-memory-usage-and-no-parallelisation","title":"Dask workers have low memory usage and no parallelisation","text":"<p>This case is straightforward to set up. Use the following example to run a workload with 1 scheduler rank and 6 worker ranks. Each rank will be given 1 GB of memory and a single (logical) core.</p> <pre><code>#!/bin/bash\n#SBATCH --time=01:00:00\n#SBATCH --ntasks=6\n#SBATCH --cpus-per-task=1\n#SBATCH --mem-per-cpu=1G\n\nmodule purge\nmodule load Python/3.9.9-gimkl-2020a\n\nsrun python mydaskprogram.py\n</code></pre>","tags":[]},{"location":"Scientific_Computing/HPC_Software_Environment/Configuring_Dask_MPI_jobs/#dask-workers-have-high-memory-usage-andor-parallelisation","title":"Dask workers have high memory usage and/or parallelisation","text":"<p>This case is more complex to set up and uses Slurm \"job packs\" to handle the heterogeneous configuration. In the following example, the scheduler and first worker rank will be given 1 GB of memory and a single (logical) core each, while the remaining worker ranks will be given 4*3 GB = 12 GB of memory and 4 (logical) cores per rank.</p> <pre><code>#!/bin/bash\n#SBATCH --time=01:00:00\n#SBATCH --ntasks=2 --mem-per-cpu=1G --cpus-per-task=1\n#SBATCH hetjob\n#SBATCH --ntasks=3 --mem-per-cpu=3G --cpus-per-task=4\n\nmodule purge\nmodule load Python/3.9.9-gimkl-2020a\n\nsrun --het-group=0-1 python mydaskprogram.py\n</code></pre> <p>The <code>--het-group</code> flag asks <code>srun</code> to launch both job packs together.</p>","tags":[]},{"location":"Scientific_Computing/HPC_Software_Environment/Configuring_Dask_MPI_jobs/#example","title":"Example","text":"<p>The following example illustrates how to run Dask-MPI on the HPC. It is based on the Dask Futures tutorial on the Dask examples webpage.</p>","tags":[]},{"location":"Scientific_Computing/HPC_Software_Environment/Configuring_Dask_MPI_jobs/#python-program","title":"Python program","text":"<pre><code>import os\nimport dask_mpi as dm\nimport dask.distributed as dd\n\n# Initialise Dask cluster and store worker files in current work directory\ndm.initialize(local_directory=os.getcwd())\n\n# Define two simple test functions\ndef inc(x):\n    return x + 1\n\ndef add(x, y):\n    return x + y\n\nclient = dd.Client()\n\n# Submit chain of computations using futures\na = client.submit(inc, 1)\nb = client.submit(inc, 2)\nc = client.submit(add, a, b)\n\n# Expect the same answer\nprint(\"Dask result:\", c.result())\nprint(\"Local result:\", add(inc(1), inc(2)))\n</code></pre>","tags":[]},{"location":"Scientific_Computing/HPC_Software_Environment/Configuring_Dask_MPI_jobs/#slurm-script","title":"Slurm script","text":"<p>Replace <code>PROJECTID</code> with your project ID number and use the <code>sbatch</code> command to submit this Slurm script and run the test code on 3 MPI ranks:</p> <pre><code>#!/bin/bash\n#SBATCH --account=PROJECTID\n#SBATCH --time=00:01:00\n#SBATCH --ntasks=3\n#SBATCH --cpus-per-task=1\n#SBATCH --mem-per-cpu=512M\n\nmodule purge\nmodule load Python/3.9.9-gimkl-2020a\n\nsrun python dask_example.py\n</code></pre> <p>The Slurm output file should contain some status information from Dask-MPI, along with program output</p> <pre><code>Dask result: 5\nLocal result: 5\n</code></pre>","tags":[]},{"location":"Scientific_Computing/HPC_Software_Environment/Configuring_Dask_MPI_jobs/#running-dask-mpi-inside-a-singularity-container","title":"Running Dask-MPI inside a Singularity container","text":"<p>It is straightforward to run a Dask-MPI workload inside a Singularity container on the HPC. For reliable and efficient execution it is best to use the same MPI distribution inside and outside the container. This restricts choices to Intel MPI on the Mahuika and M\u0101ui Ancil clusters; see section Installing Dask-MPI with Conda above for instructions. It will also reduce container portability between platforms that use different MPI distributions.</p>","tags":[]},{"location":"Scientific_Computing/HPC_Software_Environment/Configuring_Dask_MPI_jobs/#container-configuration","title":"Container configuration","text":"<p>While it is impossible to cover every possible scenario, the following guidelines should help with configuring the container correctly.</p> <ol> <li>Make sure that the Intel MPI version of the \"mpi4py\" package is    installed with Dask-MPI</li> <li>The correct version of Python and the Intel MPI distribution need to    be loaded at runtime.</li> </ol> <p>Here is an example of a minimal Singularity container definition file:</p> <pre><code>Bootstrap: docker\nFrom: continuumio/miniconda3:latest\n\n%post\n    conda install -y -n base -c intel mpi4py\n    conda install -y -n base -c conda-forge dask-mpi\n\n%runscript\n    . $(conda info --base)/etc/profile.d/conda.sh\n    conda activate base\n    python \"$@\"\n</code></pre> <p>where the <code>%runscript</code> section ensures that the Python script passed to <code>singularity run</code> is executed using the Python interpreter of the base Conda environment inside the container.</p> <p>Note</p> <p>You can build this container on NeSI, using the Mahuika Extension  nodes, following the instructions from the dedicated support  page.</p>","tags":[]},{"location":"Scientific_Computing/HPC_Software_Environment/Configuring_Dask_MPI_jobs/#slurm-configuration","title":"Slurm configuration","text":"<p>Slurm configuration is identical to the case without Singularity, see section Configuring Slurm above. The Slurm job submission script needs to be slightly modified to setup and launch the container runtime environment, ensuring that Intel MPI finds Slurm's PMI-2 library on the host.</p> <p>In the first case with low worker memory consumption and no parallelisation, use for example</p> <pre><code>module load Singularity\nexport I_MPI_PMI_LIBRARY=\"/opt/slurm/lib64/libpmi2.so\"\nexport SINGULARITY_BIND=\"/opt/slurm/lib64\"\nsrun singularity run my_container.sif dask_example.py\n</code></pre> <p>In the second case with high worker memory consumption and/or parallelisation, use for example</p> <pre><code>module load Singularity\nexport I_MPI_PMI_LIBRARY=\"/opt/slurm/lib64/libpmi2.so\"\nexport SINGULARITY_BIND=\"/opt/slurm/lib64\"\nsrun --het-group=0-1 singularity run my_container.sif dask_example.py\n</code></pre> <p>Note: You may need to append more folders to <code>SINGULARITY_BIND</code> to make your script accessible in the container, e.g. <code>$PWD</code></p>","tags":[]},{"location":"Scientific_Computing/HPC_Software_Environment/Finding_Software/","title":"Finding Software","text":"","tags":[]},{"location":"Scientific_Computing/HPC_Software_Environment/Finding_Software/#environment-modules","title":"Environment Modules","text":"<p>NeSI uses environment modules to manage installed software.</p> <p>Using the <code>module</code> command you can:</p> <ul> <li>View loaded modules:</li> <li>List all available modules</li> <li>Load a module:</li> <li>Switch out a loaded module for a different version:</li> </ul>","tags":[]},{"location":"Scientific_Computing/HPC_Software_Environment/Finding_Software/#lmod-on-mahuika","title":"Lmod on Mahuika","text":"<p>As on Pan, Mahuika uses an enhanced version of modules called Lmod .</p> <p>Lmod extends the basic environment modules by adding simple shortcuts and a more powerful search capability. The <code>ml</code> shortcut can be used in place of <code>module</code>. With Lmod you can:</p> <ul> <li>View loaded modules:</li> <li>List all available modules:</li> <li>Use \u201cspider\u201d to search for modules, e.g. \u201cPython\u201d modules:</li> <li>Load a module:</li> <li>Prefix a module with \u201c-\u201c to unload it, e.g. switch from Python 2 to   Python 3:</li> <li>To get a fresh environment, we recommend that you log out and log in     again. By logging out and logging in again you will revert to not     only the default set of modules, but also the default set of     environment variables.</li> </ul> <p>Further information can be found in the online User Guide for Lmod.</p>","tags":[]},{"location":"Scientific_Computing/HPC_Software_Environment/Finding_Software/#modules-on-maui","title":"Modules on M\u0101ui","text":"<p>On M\u0101ui and M\u0101ui_Ancil we use top level modules to provide the different software stacks. Per default the \"NeSI\" module is loaded, which provides access to the different NeSI software stacks.</p> <p>On M\u0101ui XC nodes an improved version of the modules framework is provided. Therewith you can also search for modules using a sub-string using the \"-S\" option, e.g.</p> <p>as a result you will also find modules having the substring \"netcdf\" in name, e.g. cray-netcdf.</p> <p>NOTE: The substring search will be soon implemented by default, then you do not need to specify the -S anymore. Furthermore, this improvement should be also ported to the\u00a0M\u0101ui_Ancil part.</p> <p>Tip</p> <p>You can create your own modules. This is described here.</p>","tags":[]},{"location":"Scientific_Computing/HPC_Software_Environment/Installing_Third_Party_applications/","title":"Installing (Third Party) applications","text":"<p>NeSI provides a long list of various applications on its systems. Nevertheless, if you need additional applications or libraries (below called package), we distinguish:</p> <ul> <li>you need a newer version of an already installed package:  Contact our Support Team for     an update</li> <li>you need an older version of an installed package: please use     the Easybuild installation procedure (below) to install it into your     working space</li> <li>you want to test a new (not installed) package: below we     collected some hints, how you can install it in your user space.</li> </ul> <p>In any case, if you have issues, do not hesitate to Contact our Support Team.</p>","tags":[]},{"location":"Scientific_Computing/HPC_Software_Environment/Installing_Third_Party_applications/#additional-packages-for-python-r-etc","title":"Additional Packages for Python, R, etc.","text":"<p>See\u00a0Python\u00a0or\u00a0R, or for other languages check if we have additional documentation for it in our application documentation.</p>","tags":[]},{"location":"Scientific_Computing/HPC_Software_Environment/Installing_Third_Party_applications/#third-party-applications","title":"Third party applications","text":"<p>Installation instruction vary from application to application. In any case we suggest to read the provided installing instructions. Nevertheless, the following should give you an impression which steps you usually need to consider:</p> <ul> <li>Change into a desired source code directory. We suggest to use     <code>/nesi/nobackup/&lt;projectID&gt;</code> or <code>/nesi/project/&lt;projectID&gt;</code></li> <li>download the source code. This could be done via a repository     checkout (<code>git clone &lt;URL to the application source repository&gt;</code>) or     via downloading a tarball (<code>wget &lt;URL to the tarball&gt;</code>). Unpack the     tarball using <code>tar xf &lt;tar file name&gt;</code>. Change into source     directory.</li> <li>load compiler module and modules for additional libraries     (<code>module load gimkl FFTW</code>)</li> <li>run the configure with appropriate options     <code>./configure --prefix=&lt;desired install directory&gt; --use-fftw=$EBROOTFFTW</code>(options     can be listed using <code>./configure --help</code>)</li> <li>In other applications you need to adjust the provided <code>Makefile</code> to     reflect compiler, and library options (see below)</li> <li>compile code (<code>make</code>)</li> <li>install the binaries and libraries into the specified directory     (<code>make install</code>)</li> </ul>","tags":[]},{"location":"Scientific_Computing/HPC_Software_Environment/Installing_Third_Party_applications/#create-your-own-modules-optional","title":"Create your own modules (Optional)","text":"<p>You can create personalised module environments, which can load modules and set up environment variables. For example, you could define a modules in a project directory <code>/nesi/project/&lt;projectID&gt;/modulefiles/ProdXY</code> as the following:</p> <p>In the first lines, we can set conflicts with other modules (here named ProdABC). Then we load some dependency modules and provide some description. The additional lines depend on your requirements for the module. With set you can define internal variables (within this module file). The command setenv defines a environment variable. And prepend-path and append-path extend an environment variable at the front or end.</p> <p>There are common environment variables like:</p> <ul> <li>PATH for providing executabl,</li> <li>LD_LIBRARY_PATH for self created libraries,</li> <li>PYTHONPATH for providing Python modules,</li> <li>CONDA_ENVS_PATH for providing Conda environments,</li> <li>etc.</li> </ul> <p>And others which are very application specific.</p> <p>To use the module (or all in that directory and sub-directories) we need to register that directory to the module environment. This can be done by setting the following environment variable:</p> <p>by adding that line to your <code>$HOME/.bashrc</code> you will have the modules always available.</p> <p>The module then can be loaded by:</p> <p>These modules can easily be shared with collaborators. They just need to specify the last two steps.</p>","tags":[]},{"location":"Scientific_Computing/HPC_Software_Environment/NICE_DCV_Setup/","title":"NICE DCV Setup","text":"<p>NICE DCV is a virtual desktop solution that enables users to run graphics-intensive OpenGL applications, such as 3D visualisation, remotely on the HPC. You get full access to your data on the high-performance file systems, as well as the advanced CPU and GPU capabilities of the Cray CS clusters Mahuika and M\u0101ui Ancil.</p> <p>NICE DCV sessions can persist for as long as your SLURM resource allocation is valid - you could launch a session in the morning, start a complex visualisation job with ParaView or VisIt, disconnect while the machine is still rendering the graphics, and reconnect in the afternoon to check how your job is coming along.</p> <p>Follow the instructions below to set up a new session and connect to it.</p>","tags":["support","application","visualisation"]},{"location":"Scientific_Computing/HPC_Software_Environment/NICE_DCV_Setup/#internet-connection","title":"Internet Connection","text":"<p>NICE DCV uses image compression mechanisms to reduce the amount of data that needs to be transmitted from the HPC to your laptop or desktop computer. However, data volumes can still become significant for longer sessions. Please keep this in mind if your internet provider charges by data volume.</p> <p>While NICE DCV works reasonably well over a WiFi connection, for best performance we suggest you use a wired (Ethernet) connection if possible.</p>","tags":["support","application","visualisation"]},{"location":"Scientific_Computing/HPC_Software_Environment/NICE_DCV_Setup/#creating-a-new-nice-dcv-server-session","title":"Creating a new NICE DCV server session","text":"","tags":["support","application","visualisation"]},{"location":"Scientific_Computing/HPC_Software_Environment/NICE_DCV_Setup/#logging-in","title":"Logging in","text":"","tags":["support","application","visualisation"]},{"location":"Scientific_Computing/HPC_Software_Environment/NICE_DCV_Setup/#on-maui","title":"On M\u0101ui","text":"<pre><code>1. Connect to the lander node following the instructions\n    [here](https://support.nesi.org.nz/hc/en-gb/sections/360000034315-Accessing-the-HPCs).\n    For example:\n\n    ``` sl\n    ssh lander\n    ```\n\n2. Connect from the lander node to one of the NICE DCV server\n    nodes:\n\n    ``` sl\n    ssh w-ndcv01\n    ```\n</code></pre>","tags":["support","application","visualisation"]},{"location":"Scientific_Computing/HPC_Software_Environment/NICE_DCV_Setup/#on-mahuika","title":"On Mahuika","text":"<pre><code>1. Connect to the Mahuika login node:\n\n    ``` sl\n    ssh mahuika\n    ```\n\n2. Connect to the NICE DCV server node (not yet available):\n\n    ``` sl\n    ssh vgpuwbg005\n    ```\n</code></pre>","tags":["support","application","visualisation"]},{"location":"Scientific_Computing/HPC_Software_Environment/NICE_DCV_Setup/#create-a-new-nice-dcv-session","title":"Create a new NICE DCV session","text":"<p>Replace <code>&lt;session name&gt;</code> with a     session name of your choice:</p> <pre><code>``` sl\ndcv create-session &lt;session name&gt;\n```\n</code></pre>","tags":["support","application","visualisation"]},{"location":"Scientific_Computing/HPC_Software_Environment/NICE_DCV_Setup/#establishing-an-ssh-tunnel","title":"Establishing an SSH tunnel","text":"<p>For security reasons, the NICE DCV nodes are not on the public internet and so are not directly accessible from your workstation. Therefore, we must create an SSH tunnel through the NeSI lander node.</p>","tags":["support","application","visualisation"]},{"location":"Scientific_Computing/HPC_Software_Environment/NICE_DCV_Setup/#linux-mac-or-windows-subsystem-for-linux","title":"Linux, Mac, or Windows Subsystem for Linux","text":"<p>Warning</p> <p>If successful, commands to open SSH tunnels will look like they are  doing nothing (hanging) but it is important to leave them running.  Once you kill a relevant SSH tunnel connection (e.g. <code>Ctrl-c</code>) you  will no longer be able to connect to your NICE DCV session.</p> <ol> <li>On your machine run the following command in your Linux terminal     emulator (assuming you added the     recommended     sections to your <code>~/.ssh/config</code> file). This command opens an SSH     tunnel through the NeSI lander node to the SSH port on w-ndcv01.</li> </ol>","tags":["support","application","visualisation"]},{"location":"Scientific_Computing/HPC_Software_Environment/NICE_DCV_Setup/#to-connect-to-maui","title":"To connect to M\u0101ui","text":"<pre><code>``` sl\n# The first port number (22222 in this example) can be anything you like &gt; 1024,\n# so long as it's not in use by another service.\n# We have picked 22222 because it's easy to remember, the SSH port being 22.\nssh -L 22222:w-ndcv01.maui.niwa.co.nz:22 -o ExitOnForwardFailure=yes -N lander\n```\n\nIf you don't already have another open connection to or through the\nNeSI lander node, you will at this point be prompted for your\npassword and your second factor. Enter them in the usual manner.\n</code></pre>","tags":["support","application","visualisation"]},{"location":"Scientific_Computing/HPC_Software_Environment/NICE_DCV_Setup/#to-connect-to-mahuika","title":"To connect to Mahuika","text":"<pre><code>1.  Open an SSH tunnel through the lander node to the Mahuika login\n    node.\n\n    ``` sl\n    # The tunnel port numbers (10022 in this example) can be anything you like &gt; 1024,\n    # so long as neither of them is in use by another service.\n    # We have picked 10022 because it's easy to remember, the SSH port being 22.\n    ssh -L 10022:login.mahuika.nesi.org.nz:22 -o ExitOnForwardFailure=yes -N lander\n    ```\n\n    If you don't already have another open connection to or through\n    the NeSI lander node to the Mahuika login node, you will at this\n    point be prompted for your password and your second factor.\n    Enter them in the usual manner.\n\n2.  In a new terminal, open an SSH tunnel through this existing\n    tunnel to Mahuika's NICE DCV node.\n\n    ``` sl\n    # The tunnel port numbers (22222 in this example) can be anything you like &gt; 1024,\n    # so long as neither of them is in use by another service.\n    # We have picked 22222 because it's easy to remember, the SSH port being 22.\n    ssh -L 22222:vgpuwbg005:22 -o ExitOnForwardFailure=yes -N -p 10022 -l &lt;nesi_linux_username&gt; localhost\n    ```\n\n    If prompted for a first factor, enter it in the usual manner.\n    The second factor is optional (you can just press Enter), but if\n    you provide a second factor it must be correct.\n</code></pre> <ol> <li> <p>Open a second terminal session, and run the following command in it.</p> <pre><code># The first port number (28443 in this example) can be anything you like &gt; 1024,\n# so long as it's not in use by another service.\n# We have picked 28443 because it's easy to remember, the NICE DCV port being 8443.\nssh -L 28443:localhost:8443 -o ExitOnForwardFailure=yes -N -p 22222 -l &lt;nesi_linux_username&gt; localhost\n</code></pre> <p>You will probably be prompted for a first factor and an optional second factor.</p> <p>Like the above command, this command will apparently hang if successful. Do not interrupt it as it is necessary to hold the port open for the server.</p> </li> </ol>","tags":["support","application","visualisation"]},{"location":"Scientific_Computing/HPC_Software_Environment/NICE_DCV_Setup/#mobaxterm-on-windows","title":"MobaXTerm on Windows","text":"<p>If using MobaXTerm on Windows, set up and then start port forwarding connections to look like this:</p>","tags":["support","application","visualisation"]},{"location":"Scientific_Computing/HPC_Software_Environment/NICE_DCV_Setup/#to-connect-to-maui_1","title":"To connect to M\u0101ui","text":"<p> When setting up and using the connections, note the following:</p>","tags":["support","application","visualisation"]},{"location":"Scientific_Computing/HPC_Software_Environment/NICE_DCV_Setup/#to-connect-to-mahuika_1","title":"To connect to Mahuika","text":"<p>A picture is still to come.</p> <ul> <li>The numbers of the forward ports (fourth column) are arbitrary so     long as you set them to be greater than 1024, but the SSH server     port for the second connection must be the same as the forward port     for the first connection.</li> <li>The destination port for the first tunnel must be\u00a0<code>22</code>\u00a0and that for     the second tunnel must be\u00a0<code>8443</code>.</li> <li>The server port for the first tunnel must be\u00a0<code>22</code>.</li> <li>The tunnel through the lander node must be started before the tunnel     through localhost can be started.</li> <li>The destination server for the tunnel through the lander node must     be the NeSI login node where your NICE DCV server is running.</li> </ul>","tags":["support","application","visualisation"]},{"location":"Scientific_Computing/HPC_Software_Environment/NICE_DCV_Setup/#connecting-to-a-session","title":"Connecting to a session","text":"<p>NICE DCV comes with a client for Windows and Linux systems, which can be downloaded from the NICE web pages. If you use MacOS, or if you do not want to install the client, you can also connect to your NICE DCV session with a modern browser.</p> <p>Before you proceed, make sure that you have a valid SLURM allocation on the HPC and that a session has been created.</p>","tags":["support","application","visualisation"]},{"location":"Scientific_Computing/HPC_Software_Environment/NICE_DCV_Setup/#connecting-with-a-client","title":"Connecting with a Client","text":"<p>To connect with the NICE DCV client software:</p> <ol> <li>Launch the client on your laptop or desktop computer.</li> <li>Enter the server and session name in the login screen using the     format <code>localhost:28443#&lt;session_name&gt;</code>, or whatever port number you     used for the second SSH tunnel as an alternative to 28443.</li> <li>Click on \"Connect\".</li> <li>Enter your NeSI Linux username and password.</li> <li>Click on \"Login\".</li> </ol>","tags":["support","application","visualisation"]},{"location":"Scientific_Computing/HPC_Software_Environment/NICE_DCV_Setup/#connecting-with-a-browser","title":"Connecting with a Browser","text":"<p>To connect with a browser:</p> <ol> <li>Launch the browser or open a new tab</li> <li>Enter \"https://localhost:28443/#&lt;session name&gt;\" in the URL     bar. If you used a port other than 28443 when creating the second     SSH tunnel, make the necessary modifications to this URL.</li> <li>You may need to accept the insecure certificate in your browser     before you can proceed</li> <li>Enter your HPC account credentials (first factor)</li> <li>Click on \"Login\"</li> </ol>","tags":["support","application","visualisation"]},{"location":"Scientific_Computing/HPC_Software_Environment/NICE_DCV_Setup/#using-the-desktop-environment","title":"Using the Desktop Environment","text":"<p>You should be presented with a Linux desktop environment after successful login with the client or browser. You can then use the application launcher to start an application. You can also launch the terminal application by right-clicking on the desktop and selecting \"Konsole\". This will give you access to the NeSI software stack that includes various visualisation software solutions.</p>","tags":["support","application","visualisation"]},{"location":"Scientific_Computing/HPC_Software_Environment/NICE_DCV_Setup/#disconnecting-and-stopping-a-session","title":"Disconnecting and Stopping a Session","text":"<p>Sessions can persist on the HPC for as long as the SLURM resource allocation is valid. You can disconnect and reconnect to the session as often as you like.</p>","tags":["support","application","visualisation"]},{"location":"Scientific_Computing/HPC_Software_Environment/NICE_DCV_Setup/#disconnecting-from-a-session-without-stopping-it","title":"Disconnecting from a session without stopping it","text":"<ol> <li>Click on the machine URL in the top-right corner of the NICE DCV     window</li> <li>Select \"Disconnect\"</li> <li>Close the NICE DCV client or browser window</li> </ol>","tags":["support","application","visualisation"]},{"location":"Scientific_Computing/HPC_Software_Environment/NICE_DCV_Setup/#disconnecting-and-stopping-a-session_1","title":"Disconnecting and stopping a session","text":"<ol> <li>Click on the application launcher icon in the top-left corner of the     virtual desktop</li> <li>Click on \"Leave\"</li> <li>Click on \"Log out\"</li> <li>Confirm the logout in the dialog box that appears</li> <li>Close the NICE DCV client or browser window</li> </ol> <p>A running session will automatically stop if your SLURM resource allocation finishes.</p>","tags":["support","application","visualisation"]},{"location":"Scientific_Computing/HPC_Software_Environment/NVIDIA_GPU_Containers/","title":"NVIDIA GPU Containers","text":"<p>NVIDIA provides access to GPU accelerated software through their NGC container registry.</p> <p>NGC offers a comprehensive catalog of GPU-accelerated software for deep learning, machine learning, and HPC. NGC containers deliver powerful and easy-to-deploy software proven to deliver the fastest results. By taking care of the plumbing, NGC enables users to focus on building lean models, producing optimal solutions and gathering faster insights.</p> <p>Many of these containers are able to run under Apptainer, which is supported on the NeSI platform. NVIDIA also specifies the GPU requirements for each container, i.e. whether it will run on our Pascal (sm60) GPUs.</p> <p>There are instructions for converting their Docker images to Apptainer images on the NVIDIA site but some small changes are required to these instructions on NeSI. As an example, here we show the steps required for running the NAMD image on NeSI, based on the NVIDIA instructions here.</p> <ol> <li> <p>Download the APOA1 benchmark data:</p> <pre><code>wget -O - https://gitlab.com/NVHPC/ngc-examples/raw/master/namd/3.0/get_apoa1.sh | bash\ncd apoa1\n</code></pre> </li> <li> <p>Load the Apptainer module:</p> <pre><code>module load Apptainer\n</code></pre> </li> <li> <p>Build the Apptainer image. This step differs from the NVIDIA    instructions because instead of using \"build\" we \"pull\" the image    directly, which does not require root access:</p> <p>Note</p> <p>Please do refer Build Environment Variables prior to running the following <code>pull</code> command.</p> <pre><code>apptainer pull namd-3.0-alpha9-singlenode.sif docker://nvcr.io/hpc/namd:3.0-alpha9-singlenode\n</code></pre> </li> <li> <p>Copy the following into a Slurm script named run.sl:</p> <pre><code>#!/bin/bash -e\n#SBATCH --job-name=namdgpu\n#SBATCH --time=00:10:00\n#SBATCH --ntasks=1\n#SBATCH --cpus-per-task=8\n#SBATCH --gpus-per-node P100:1\n#SBATCH --mem=1G\n\nmodule purge\nmodule load Apptainer\n\n# name of the NAMD input file, tag, etc\nNAMD_INPUT=\"apoa1_nve_cuda.namd\"\nNAMD_SIF=\"namd-3.0-alpha9-singlenode.sif\"\nNAMD_EXE=namd3\n\n# apptainer command with required arguments\nAPPTAINER=\"apptainer exec --nv -B $(pwd):/host_pwd --pwd /host_pwd ${NAMD_SIF}\"\n\n# run NAMD\n${APPTAINER} ${NAMD_EXE} +ppn ${SLURM_CPUS_PER_TASK} +idlepoll ${NAMD_INPUT}\n</code></pre> </li> <li> <p>Submit the job:</p> <pre><code>sbatch run.sl\n</code></pre> </li> <li> <p>View the standard output from the simulation in the Slurm .out file.</p> </li> </ol> <p>We expect similar steps to work for other NGC containers.</p>","tags":[]},{"location":"Scientific_Computing/HPC_Software_Environment/Offloading_to_GPU_with_OpenACC/","title":"Offloading to GPU with OpenACC","text":"<p>Many codes can be accelerated significantly by offloading computations to a GPU. Some NeSI Mahuika nodes have GPUs attached to them. If you want your code to run faster, if you're developing your own code or if you have access to the source code and you feel comfortable editing the code, read on.</p> <p>Here we show how to tell the compiler which part of your algorithm you want to run a GPU. We'll use OpenACC, which adds directives to your source code. The advantages of OpenACC over other approaches is that the source code changes are generally small and your code remains portable, i.e. it will run on both CPU and GPU. The main disadvantage of OpenACC is that only a few compilers support it.</p> <p>More information about OpenACC can be found here.</p>","tags":[]},{"location":"Scientific_Computing/HPC_Software_Environment/Offloading_to_GPU_with_OpenACC/#example","title":"Example","text":"<p>In the following we show how to achieve this in the case of a reduction operation involving a large loop in C++ (a similar example can be written in Fortran):</p> <pre><code>#include &lt;iostream&gt;\n#include &lt;cmath&gt;\nint main() {\n  double total = 0;\n  int i, n = 1000000000;\n  #pragma acc parallel loop copy(total) copyin(n) reduction(+:total)\n  for (i = 0; i &lt; n; ++i) {\n    total += exp(sin(M_PI * (double) i/12345.6789));\n  }\n  std::cout &lt;&lt; \"total is \" &lt;&lt; total &lt;&lt; '\\n';\n}\n</code></pre> <p>Save the above code in file total.cxx.</p> <p>Note the pragma</p> <pre><code>#pragma acc parallel loop copy(total) copyin(n) reduction(+:total)\n</code></pre> <p>We're telling the compiler that the loop following this pragma should be executed in parallel on the GPU. Since GPUs have hundreds or more threads, the speedup can be significant. Also note that <code>total</code> is initialised on the CPU (above the pragma) and should be copied to the GPU and back to the CPU after completing the loop. (It is also possible to initialise this variable on the GPU.) Likewise the number of iterations <code>n</code> should be copied from the CPU\u00a0 to the GPU.</p>","tags":[]},{"location":"Scientific_Computing/HPC_Software_Environment/Offloading_to_GPU_with_OpenACC/#compile","title":"Compile","text":"<p>We can use the NVIDIA compiler</p> <pre><code>module load NVHPC\n</code></pre> <p>and type</p> <pre><code>nvc++ -Minfo=all -acc -o totalAccNv total.cxx\n</code></pre> <p>to compile the example.</p> <p>Alternatively, we can use\u00a0the Cray C++ compiler to build the executable but first we need to load a few modules:</p> <pre><code>module load craype-broadwell\nmodule load cray-libsci_acc \nmodule load craype-accel-nvidia60 \nmodule load PrgEnv-cray\n</code></pre> <p>(Ignore warning \"cudatoolkit &gt;= 8.0 is required\"). Furthermore, you may need to load <code>cuda/fft</code> or <code>cuda/blas</code> </p> <p>To compare the execution times between the CPU and GPU version, we build two executables:</p> <pre><code>CC -h noacc -o total total.cxx\nCC -o totalAccGpu total.cxx\n</code></pre> <p>with executable <code>total</code> compiled with <code>-h noacc</code>, i.e. OpenACC turned off.</p>","tags":[]},{"location":"Scientific_Computing/HPC_Software_Environment/Offloading_to_GPU_with_OpenACC/#run","title":"Run","text":"<p>The following commands will submit the runs to the Mahuika queue (note <code>--gpus-per-node=P100:1</code> in the case of the executable that offloads to the GPU):</p> <pre><code>time srun --ntasks=1 --cpus-per-task=1 ./total\ntime srun --ntasks=1 --cpus-per-task=1 --gpus-per-node=P100:1 ./totalAccGpu\n</code></pre> executable time [s] total 7.6 totalAccGpu 0.41 <p>Check out this page to find out how you can offload computations to a GPU using OpenMP.</p>","tags":[]},{"location":"Scientific_Computing/HPC_Software_Environment/Offloading_to_GPU_with_OpenMP/","title":"Offloading to GPU with OpenMP","text":"<p>With OpenMP 4.5, it has become possible to offload computations from the CPU to a GPU, see\u00a0https://www.openmp.org/wp-content/uploads/SC18-BoothTalks-Jost.pdf</p>","tags":[]},{"location":"Scientific_Computing/HPC_Software_Environment/Offloading_to_GPU_with_OpenMP/#example","title":"Example","text":"<p>In the following we show how to achieve this in the case of a reduction operation involving a large loop:</p> <pre><code>#include &lt;iostream&gt;\n#include &lt;cmath&gt;\nint main() {\n  int n = 1000000000;\n  double total = 0;\n  #pragma omp target teams distribute parallel for map(tofrom: total) \\\n    map(to: n) reduction(+:total)\n  for (int i = 0; i &lt; n; ++i) {\n    total += exp(sin(M_PI * (double) i/12345.6789));\n  }\n  std::cout &lt;&lt; \"total is \" &lt;&lt; total &lt;&lt; '\\n';\n}\n</code></pre> <p>Save the above code in file total.cxx.</p> <p>Note the pragma</p> <pre><code>#pragma omp target teams distribute parallel for map(tofrom: total) \\\n  map(to: n) reduction(+:total)\n</code></pre> <p>which moves variables <code>total</code> and <code>n</code> to the GPU and creates teams of threads to perform the sum operation in parallel.\u00a0</p>","tags":[]},{"location":"Scientific_Computing/HPC_Software_Environment/Offloading_to_GPU_with_OpenMP/#compile","title":"Compile","text":"<p>We'll use the Cray C++ compiler to build the executable but first we need to load a few modules:</p> <pre><code>module load cray-libsci_acc/18.06.1 craype-accel-nvidia60 \\\n PrgEnv-cray/1.0.4 cuda92/blas/9.2.88 cuda92/toolkit/9.2.88\n</code></pre> <p>(Ignore warning \"cudatoolkit &gt;= 8.0 is required\").</p> <p>To compare the execution times between the CPU and GPU version, we build two executables:</p> <pre><code>CC -h noomp -o total total.cxx\nCC -o totalOmpGpu total.cxx\n</code></pre> <p>with executable <code>total</code> compiled with <code>-h noomp</code>, i.e. OpenMP turned off.</p>","tags":[]},{"location":"Scientific_Computing/HPC_Software_Environment/Offloading_to_GPU_with_OpenMP/#run","title":"Run","text":"<p>The following commands will submit the runs to the Mahuika queue (note <code>--partition=gpu --gres=gpu:1</code> in the case of the executable that offloads to the GPU):</p> <pre><code>time srun --ntasks=1 --cpus-per-task=1 ./total\ntime srun --ntasks=1 --cpus-per-task=1 --partition=gpu --gres=gpu:1 ./totalOmpGpu\n</code></pre> executable time [s] total 10.9 totalOmpGpu 0.45","tags":[]},{"location":"Scientific_Computing/HPC_Software_Environment/OpenMP_settings/","title":"OpenMP settings","text":"<p>OpenMP is an application programming interface that lets you write parallel programs on shared memory platforms. In a parallel section, OpenMP code can create multiple threads that run on separate cores, executing their shares of the total workload concurrently. OpenMP is suited for the Mahuika and M\u0101ui HPCs as each platform has 36 and 40 physical cores per node respectively.\u00a0 Each physical core can handle up to two threads in parallel using Hyperthreading. Therefore you can run up to 72 threads on Mahuika and 80 threads on M\u0101ui</p> <p>The environment variable that controls the number of threads is <code>OMP_NUM_THREADS</code>, e.g.,</p> <pre><code>export OMP_NUM_THREADS=16\n</code></pre> <p>allows OpenMP code to fork 16 threads. To make sure that resources requested from Slurm and used by your program are consistent, is usually a good idea to set</p> <pre><code>#SBATCH --cpus-per-task=16\n[...]\nexport OMP_NUM_THREADS=$SLURM_CPUS_PER_TASK\n</code></pre> <p>in your Slurm script - although this can sometimes be more complicated, e.g., with TensorFlow on CPUs.</p> <p>On Mahuika, you will be charged for the number of physical cores that you requested - the second logical core on a physical core is free, although Slurm always reports both cores. On M\u0101ui you will be charged for full nodes.</p> <p>In order to achieve good and consistent parallel scaling, additional settings may be required. This is particularly true on Mahuika whose nodes are shared between different Slurm jobs. Following are some settings that can help improve scaling and/or make your timings more consistent, additional information can be found in our article Thread Placement and Thread Affinity.</p> <ol> <li> <p><code>--hint=nomultithread</code>. Set this in conjunction with srun or sbatch to tell Slurm that you don't want to use hyperthreads. Your program will only be presented with physical cores. Inversely, --hint=multithread will request two threads per physical core. If --hint is not set, Slurm will currently assume --hint=multithread by default.</p> </li> <li> <p><code>OMP_PROC_BIND</code>. Set this to \"true\" to pin the threads down during program execution. By default, threads may migrate from one core to another, depending on the load on the node. In an HPC setting, it is generally advisable to pin the threads to avoid delays caused by thread migration.</p> </li> <li> <p><code>OMP_PLACES</code>. Set this to \"cores\" if you want to pin the threads to physical cores, or to \"threads\" if you want to use hyperthreading.\u00a0</p> </li> </ol> <p>The effect of each setting is illustrated below. In this experiment we measured the execution time twice of the finite difference code\u00a0upwindCxx -numCells 256 -numSteps 10.\u00a0The code was built with the gimpi/2018b toolchain on Mahuika.</p> Number of physical cores <code>--hint</code> not used, <code>OMP_PROC_BIND</code>, <code>OMP_PLACES</code> unset <code>--hint=nomultithread</code>, <code>OMP_PROC_BIND=true</code>, <code>OMP_PLACES=cores</code> <code>--hint=multithread</code>, <code>OMP_PROC_BIND=true</code>. <code>OMP_PLACES=threads</code> 1 1m43s, 1m42s 1m42s, 1m42s 1m30s, 1m30s 2 1m30s, 1m31s 1m03, 55s 56s, 56s 4 58s, 1m27s 45s, 41s 27s, 28s 8 24s, 27s 18s, 17s 16s, 16s","tags":[]},{"location":"Scientific_Computing/HPC_Software_Environment/OpenMP_settings/#results","title":"Results","text":"<p>In the default case, <code>--hint</code> was not used and the environment variables <code>OMP_PROC_BIND</code> and <code>OMP_PLACES</code> were not set. Significant variations of execution times are sometimes observed due to the random placement of threads, which may or may not share a physical core.\u00a0</p> <p>The third column shows the settings for the case with no multithreading. The fourth column places 2 threads per physical cores (i.e. <code>OMP_NUM_THREADS=2</code> times the number of physical cores) and this often gives the best performance.</p>","tags":[]},{"location":"Scientific_Computing/HPC_Software_Environment/OpenMP_settings/#bottom-line","title":"Bottom line","text":"<p>Be explicit by using <code>--hint</code> and setting <code>OMP_PROC_BIND</code> and <code>OMP_PLACES</code>. In many cases we expect one of the following to work best:</p> threads per core <code>--hint=</code> <code>OMP_PROC_BIND=</code> <code>OMP_PLACES=</code> 1 nomultithread true cores 2 multithread true threads <p>Let us know if you find other settings to work better for you.</p>","tags":[]},{"location":"Scientific_Computing/HPC_Software_Environment/Per_job_temporary_directories/","title":"Per job temporary directories","text":"<p>Most programs which create temporary files will put those files in the directory specified by the environment variable <code>TMPDIR</code> if that is set, or <code>/tmp</code> otherwise. This is also true in Slurm, such that whenever a job starts, a temporary directory is created for that job and <code>TMPDIR</code> set. When the job ends our Slurm epilog ensures that the directory is deleted.</p> <p>By default, this job-specific temporary directory is placed in <code>/dev/shm</code>, which is a \u201ctmpfs\u201d filesystem and so actually sits in ordinary RAM.\u00a0 As a consequence, your job\u2019s memory request should include enough to cover the size of any temporary files.</p> <p>On the <code>milan</code> and <code>hgx</code> partitions, however, you have the option of specifying <code>#SBATCH --gres=ssd</code> in your job script which will place <code>TMPDIR</code> on a 1.5 TB NVMe SSD attached to the node rather than in RAM. When <code>--gres=ssd</code> is set your job\u2019s memory request does not need to include enough to cover the size of any temporary files (as this is a separate resource). These SSDs give the job a slower but very much larger temporary directory. They are allocated exclusively to jobs, so there can only be one such job per node at a time. This gives the job all the available bandwidth of the SSD device but does limit the number of such jobs.</p> <p>Alternatively you can ignore the provided directory and set <code>TMPDIR</code> yourself, typically to a location in <code>/nesi/nobackup</code>.\u00a0 This will be the slowest option with the largest capacity. Also if set to <code>nobackup</code> the files will remain after the job finishes, so be weary of how much space your jobs temporary files use. An example of how <code>TMPDIR</code> may be set yourself is shown below,</p> <pre><code>export TMPDIR=/nesi/nobackup/$SLURM_ACCOUNT/tmp/$SLURM_JOB_ID\n</code></pre>","tags":[]},{"location":"Scientific_Computing/HPC_Software_Environment/Per_job_temporary_directories/#example-of-copying-data-into-the-per-job-temporary-directories-for-use-mid-job","title":"Example of copying data into the per job temporary directories for use mid-job","text":"<p>The per job temporary directory can also be used to store data that needs to be accessed as the job runs. For example you may wish to read the standard database of Kraken2 (located in <code>/opt/nesi/db/Kraken2/standard-2018-09</code>) from the <code>milan</code> SSDs instead of <code>/opt</code>. To do this, request the NVMe SSD on <code>milan</code> as described above. Then, after loading the Kraken2 module in your Slurm script, copy the database onto the SSD,</p> <pre><code>cp -r /opt/nesi/db/Kraken2/standard-2018-09/* $TMPDIR\n</code></pre> <p>To get Kraken2 to read the DB from the SSDs (and not from <code>/opt</code>), change the <code>KRAKEN2_DEFAULT_DB</code> variable,</p> <pre><code>export KRAKEN2_DEFAULT_DB=$TMPDIR\n</code></pre> <p>The variable <code>KRAKEN2_DEFAULT_DB</code> simply points to the database and is found by <code>module show Kraken2</code>.</p>","tags":[]},{"location":"Scientific_Computing/HPC_Software_Environment/Programming_environment_differences_between_Maui_and_Mahuika/","title":"Programming environment differences between Maui and Mahuika","text":"<p>Warning</p> <p>This page has been automatically migrated and may contain formatting errors.</p> <p>The compile environment of M\u0101ui and Mahuika have various similarities, but also significant differences. Both systems are configured with the Cray Programming Environment (CPE), but these vary in detail. In general we distinguish, between the XC50 part of M\u0101ui and the CS (Mahuika, Mahuika Ancillary Nodes, and M\u0101ui Ancillary nodes) systems.</p> <p>Table 1: The Cray Programming Environment on M\u0101ui and Mahuika. Black text indicates components common to both systems, green to components only available on Mahuika, and blue to components only available on M\u0101ui XC part.  </p> <p>Programming Languages</p> <p>Programming Models</p> <p>Compilers</p> <p>Tools</p> <p>Optimised Scientific Libraries</p> <p>I/O Libraries</p> <p>Fortran</p> <p>C</p> <p>C++</p> <p>Chapel</p> <p>Distributed Memory:</p> <p>MPI Support:</p> <p>\u00b7 Intel MPI<sup>1</sup></p> <p>\u00b7\u00a0MVAPICH2<sup>1</sup></p> <p>\u00b7 OpenMPI<sup>1</sup></p> <p>\u00b7 MPICH<sup>1</sup></p> <p>\u00b7 Cray-MVAPICH2<sup>1</sup> </p> <p>Cray MPT<sup>2</sup>:</p> <p>\u00b7 MPI</p> <p>\u00b7 Cray Compiling Environment (CCE)</p> <p>\u00b7 GNU</p> <p>\u00b7 Intel</p> <p>Performance Analysis:</p> <p>\u00b7 CrayPat &amp; Cray Apprentice2</p> <p>\u00b7 Allinea MAP</p> <p>\u00b7 Intel Vtune Amplifier XE, Advisor, Trace Analyser &amp; Collector</p> <p>Dense:</p> <p>\u00b7\u00a0BLAS</p> <p>\u00b7\u00a0LAPACK</p> <p>\u00b7\u00a0ScaLAPACK</p> <p>\u00b7\u00a0Iterative Refinement Tool</p> <p>NetCDF<sup>2</sup></p> <p>HDF<sup>2</sup></p> <p>Shared Memory</p> <p>\u00b7\u00a0OpenMP 4.0</p> <p>\u00b7\u00a0OpenACC 2.0</p> <p>Environment Setup</p> <p>\u00b7\u00a0Modules</p> <p>\u00b7\u00a0Lmod<sup>1</sup></p> <p> </p> <p>Porting Tools:</p> <p>\u00b7\u00a0Reveal</p> <p>\u00b7\u00a0CCDB</p> <p>FFT:</p> <p>\u00b7\u00a0FFTW</p> <p>PGAS</p> <p>\u00b7 UPC</p> <p>\u00b7 CAF</p> <p>\u00b7 CoArray C++</p> <p> </p> <p>Debuggers:</p> <p>\u00b7\u00a0lgdb</p> <p>\u00b7\u00a0Allinea DDT</p> <p>\u00b7\u00a0ATP<sup>2</sup></p> <p>\u00b7\u00a0STAT<sup>2</sup></p> <p>Sparse:</p> <p>\u00b7\u00a0Cray PETSc (with CASK)<sup>2</sup></p> <p>\u00b7\u00a0Cray Trilinos (with CASK)<sup>2</sup></p> <p> </p> <p> </p> <p>Data Analytics</p> <p>\u00b7\u00a0Urika XC Data Analytics<sup>2</sup></p> <p>\u00b7\u00a0Cray Graph Engine<sup>2</sup></p> <p> </p> <p>Notes:</p> <ol> <li><sup>1</sup>Only available on Mahuika HPC Cluster, Mahuika Ancillary     Nodes and M\u0101ui Ancillary nodes</li> <li><sup>2</sup>Only available on M\u0101ui Supercomputer.</li> <li>On M\u0101ui (XC50) the Modules framework is used to simplify access to     the various compiler suites and libraries. To access a particular     compiler suite, you simply load (or switch to) the appropriate     programming environment module using the command PrgEnv-X (where X     is one of gnu, intel, or cray). This facility is not available on     the Mahuika HPC Cluster, Mahuika Ancillary Nodes and M\u0101ui Ancillary     nodes.</li> <li>Intel Parallel Studio XE Cluster     Edition for Linux     will be installed on the Mahuika HPC Cluster, Mahuika Ancillary     Nodes and M\u0101ui Ancillary nodes.</li> <li>Intel Parallel Studio XE Professional Edition for CLE will be     installed installed on M\u0101ui.</li> </ol>","tags":["info","software","application","cs400","XC50"]},{"location":"Scientific_Computing/HPC_Software_Environment/Programming_environment_differences_between_Maui_and_Mahuika/#key-similarities-between-cpe-on-xc50-and-cs400500s","title":"Key Similarities\u00a0 between CPE on XC50 and CS400/500s","text":"<p>As shown in the table above, Cray provides a list of tools, libraries, and compilers for both platforms. The Cray compiler environment comes with the compiler, basic numeric libraries, automatically including compile and link flags for system architecture and libraries (enabled/disabled by loading modules), and the Cray performance analysis tools (CrayPAT)</p>","tags":["info","software","application","cs400","XC50"]},{"location":"Scientific_Computing/HPC_Software_Environment/Programming_environment_differences_between_Maui_and_Mahuika/#key-differences-between-cpe-on-xc50-and-cs400500s","title":"Key Differences between CPE on XC50 and CS400/500s","text":"<p>There are many similarities between the XC and CS programming environments (compilers and many tools and libraries are the same or at least similar), but also some important differences that affect how a user interacts with the system when building an application code:</p> <ul> <li>The XC platform uses compiler drivers (\u201cftn\u201d, \u201ccc\u201d, \u201cCC\u201d), users     should not use compilers directly. The CS platforms have compiler     drivers only for Cray compiler. For GNU and Intel compilers, users     run \u201cgfortran\u201d, \u201cifort\u201d,\u00a0\u201cgcc\u201d, \u201cicc\u201d etc.;</li> <li>On the XC platform, a compiler is chosen by switching to its     corresponding \u201cPrgEnv-xxx\u201d module. This will also switch     automatically the version of the loaded Cray provided libraries,     e.g., the cray-netcdf and cray-fftw library modules \u2013 no equivalent     is available on the CS platforms; On the CS platforms the main     software stack is based on Easybuild toolchains. The default one is     \u201cgimkl\u201d, including GCC, Intel MPI, and Intel MKL.</li> <li>The XC platform requires everyone to use Cray-MPI, but on the CS     platform, users can choose to use various MPI libraries;</li> <li>Getting rid of all modules via \u201cmodule purge\u201d renders an XC session     unusable (a list of ~20 modules are necessary to guarantee     operation). On CS there are only few modules necessary, the main one     is called \u201cNeSI\u201d, providing the NeSI software stack and slurm     module;</li> <li>The XC platform defaults to static linking, the CS platform to     dynamic linking;</li> </ul> <p>In summary, compilers, as well as various tools and libraries are common across both platforms. However, there are important differences in how the programming environment is used, requiring users to familiarise themselves with each platform. For more information see the specific user guides for Mahuika (and Ancillary nodes) and M\u0101ui XC50.</p>","tags":["info","software","application","cs400","XC50"]},{"location":"Scientific_Computing/HPC_Software_Environment/Software_Version_Management/","title":"Software Version Management","text":"<p>Much of the software installed on the NeSI cluster have multiple versions available as shown on the supported applications page or by using the <code>module avail</code> or <code>module spider</code> commands.</p> <p>If only the application name is given a default version will be chosen, generally the most recent one. However it is good practice to load modules using the specific version so you can ensure consistent execution of your job even after the default version has been changed.</p> <p>If you need a specific version of software, feel free to ask support and we may install it.</p>","tags":["software","versions"]},{"location":"Scientific_Computing/HPC_Software_Environment/Software_Version_Management/#example","title":"Example","text":"<pre><code>module load ANSYS\n</code></pre> <p>Will load the default version of ANSYS, in this case latest, however this may change.</p> <pre><code>module load ANSYS/18.1\n</code></pre> <p>Will always load that version specifically.</p>","tags":["software","versions"]},{"location":"Scientific_Computing/HPC_Software_Environment/Thread_Placement_and_Thread_Affinity/","title":"Thread Placement and Thread Affinity","text":"<p>Multithreading with OpenMP and other threading libraries is an important way to parallelise scientific software for faster execution (see our article on Parallel Execution for an introduction). Care needs to be taken when running multiple threads on the HPC to achieve best performance - getting it wrong can easily increase compute times by tens of percents, sometimes even more. This is particularly important for programs that can use a large number of threads, such as TensorFlow and other software that uses Intel oneMKL.</p> <p>Understanding thread placement and affinity, and the interplay with the Slurm workload scheduler requires some background knowledge of modern multicore HPC nodes. After explaining a few key aspects, the article will demonstrate how to configure thread placement and affinity using the Intel OpenMP library which offers detailed runtime reports and its configurations also apply to Intel oneMKL. However, the same concepts apply to other threading libraries as well, such as the GOMP library used by the GCC compiler family.</p>","tags":[]},{"location":"Scientific_Computing/HPC_Software_Environment/Thread_Placement_and_Thread_Affinity/#nodes-sockets-and-physical-cores-and-logical-cores","title":"Nodes, Sockets, and Physical Cores, and Logical Cores","text":"<p>To run parallel software effectively, a modern HPC consists of many nodes, with multiple processors on each node. The processors are often simply referred to as sockets when it comes to performance, as a socket connects the processor to its RAM and other processors. A processor in each socket consists of multiple physical cores, and each physical core is split into two logical cores using a technology called Hyperthreading).</p> <p>A processor also includes caches - a cache is very fast memory that stores data that will be needed for the next computations, which avoids that the processor has to wait for data coming from the much slower RAM.</p> <p>The picture below shows a simplified view of a two-socket node with two cores per socket. Most HPC nodes will have two sockets, but a lot more cores (our current HPCs have 18 to 20 cores). Each core can also be further divided into two logical cores (or hyperthreads, as mentioned before).</p> <p></p> <p>It is very important to note the following:</p> <ul> <li>Each socket only has access to its own RAM - it will need to ask the     processor in the other socket if it wants to access that RAM space,     and that takes longer (this is called a     NUMA     architecture)</li> <li>Each socket has a fast cache that is shared between all cores in     that socket</li> <li>Each core has its own private fast cache as well</li> </ul> <p>For a thread that runs on a given core, this means:</p> <ul> <li>Data is \"local\" when it is stored in RAM or cache close to that core     and can be accessed very quickly</li> <li>Data is \"remote\" when it is stored elsewhere and takes extra time to     access</li> </ul>","tags":[]},{"location":"Scientific_Computing/HPC_Software_Environment/Thread_Placement_and_Thread_Affinity/#thread-placement-and-affinity","title":"Thread Placement and Affinity","text":"<p>Given the arrangement of node, sockets, and cores, with different access to RAM and caches, we need to to make sure that our threads are located as close as possible to their data, and as close as possible to each other if they need to work on the same piece of data. Threads can even share the data in a cache for maximum performance.</p> <p>This configuration is is called thread placement.</p> <p>The operating system on a computer with multiple cores can normally freely move processes and threads around between the different cores, to make sure that the overall workload is spread evenly across the available cores. This is very useful on general purpose computers like laptops that run a great number of applications at the same time.</p> <p>However, moving processes and threads can cause performance problems on an HPC, where we usually want to run only a single processe or threads per core for best performance. The problem is that every time a process or thread moves from one core to another, registers and caches need to be flushed and reloaded. This can become very costly if it happens often, and our threads may also no longer be close to their data, or be able to share data in a cache.</p> <p>The mechanism that keeps our threads on their cores is called thread affinity.</p>","tags":[]},{"location":"Scientific_Computing/HPC_Software_Environment/Thread_Placement_and_Thread_Affinity/#example-program","title":"Example Program","text":"<p>We will use the Intel OpenMP library in the following examples. The same configurations can be used for all software that is compiled with the Intel compiler, or uses Intel oneMKL. Other OpenMP libraries such as GOMP have similar configurations.</p> <p>Use a text editor to save the following test program in a text file called \"hello_world.c\":</p> <pre><code>#include &lt;stdio.h&gt;\n#include &lt;omp.h&gt;\nint main()\n{\n    #pragma omp parallel\n    printf(\"Hello World from Thread %i!\\n\", omp_get_thread_num());\n    return 0;\n}\n</code></pre> <p>On Mahuika or M\u0101ui Ancil, compile the program using the commands</p> <pre><code>module load intel/2018b\nicc -o hello_world.x -qopenmp hello_world.c\n</code></pre> <p>Running the program with two threads should return the following output (although the order of threads may be different):</p> <pre><code>OMP_NUM_THREADS=2 ./hello_world.x\n</code></pre> <pre><code>Hello World from Thread 0!\nHello World from Thread 1!\n</code></pre>","tags":[]},{"location":"Scientific_Computing/HPC_Software_Environment/Thread_Placement_and_Thread_Affinity/#configuring-slurm","title":"Configuring Slurm","text":"<p>The Slurm scheduler reserves resources on compute nodes according to our requests. Unless we ask for a full node, we will get a subset of the available logical cores. Let us start by asking for 1 node, and 1 process with 3 threads using only physical cores (no hyperthreading):</p> <pre><code>#!/bin/bash -e\n#SBATCH --job-name=threads\n#SBATCH --time=00:00:30\n#SBATCH --nodes=1\n#SBATCH --ntasks=1               # We will run on a single process\n#SBATCH --cpus-per-task=3        # ... and with 3 threads\n#SBATCH --hint=nomultithread     # No hyperthreading\n\nexport KMP_AFFINITY=verbose      # Get detailed output\nmodule load intel/2018b\nsrun hello_world.x\n</code></pre> <p>Running the script should present you with output similar to this, although the number of \"packages\" (sockets) and cores may deviate if Slurm allocates cores on more than one socket (note also that \"threads\" means what we called logical cores earlier on):</p> <pre><code>OMP: Info #209: KMP_AFFINITY: decoding x2APIC ids.\nOMP: Info #207: KMP_AFFINITY: Affinity capable, using global cpuid leaf 11 info\nOMP: Info #154: KMP_AFFINITY: Initial OS proc set respected: {0,6,8}\nOMP: Info #156: KMP_AFFINITY: 3 available OS procs\nOMP: Info #157: KMP_AFFINITY: Uniform topology\nOMP: Info #179: KMP_AFFINITY: 1 packages x 3 cores/pkg x 1 threads/core (3 total cores)\nOMP: Info #247: KMP_AFFINITY: pid 156318 tid 156318 thread 0 bound to OS proc set {0,6,8}\nOMP: Info #247: KMP_AFFINITY: pid 156318 tid 156320 thread 1 bound to OS proc set {0,6,8}\nOMP: Info #247: KMP_AFFINITY: pid 156318 tid 156321 thread 2 bound to OS proc set {0,6,8}\nHello World from Thread 0!\nHello World from Thread 1!\nHello World from Thread 2!\n</code></pre> <p>The runtime library tells us that:</p> <ul> <li>Slurm provided 3 physical cores with only 1 logical core (\"thread\")     per physical core - no hyperthreading</li> <li>We got the cores with IDs 0, 6, 8 in this particular example - these     happen to be on the same socket, but that is not guaranteed!</li> <li>All our threads are \"bound\" to all 3 cores at once - this means that     no affinity setup has been made, and the threads are free to move     from one core to another</li> </ul> <p>Setting \"--hint=multithread\" instead to activate hyperthreading should result in output similar to this:</p> <pre><code>OMP: Info #209: KMP_AFFINITY: decoding x2APIC ids.\nOMP: Info #207: KMP_AFFINITY: Affinity capable, using global cpuid leaf 11 info\nOMP: Info #154: KMP_AFFINITY: Initial OS proc set respected: {6,8,46}\nOMP: Info #156: KMP_AFFINITY: 3 available OS procs\nOMP: Info #158: KMP_AFFINITY: Nonuniform topology\nOMP: Info #179: KMP_AFFINITY: 1 packages x 2 cores/pkg x 2 threads/core (2 total cores)\nOMP: Info #247: KMP_AFFINITY: pid 158044 tid 158044 thread 0 bound to OS proc set {6,8,46}\nOMP: Info #247: KMP_AFFINITY: pid 158044 tid 158046 thread 1 bound to OS proc set {6,8,46}\nOMP: Info #247: KMP_AFFINITY: pid 158044 tid 158047 thread 2 bound to OS proc set {6,8,46}\nHello World from Thread 0!\nHello World from Thread 1!\nHello World from Thread 2!\n</code></pre> <ul> <li>Slurm provided 2 physical cores with 2 logical cores (\"threads\")     each and 3 logical cores in total (we don't get the remaining     logical core on the second physical core, even though that logical     core will not be given to other jobs)</li> <li>Notice that we now get logical core IDs 6, 8, 46 - IDs 6 and 46 are     the first and second logical core inside the first physical core,     while ID 8 is a logical core in the second physical core</li> </ul>","tags":[]},{"location":"Scientific_Computing/HPC_Software_Environment/Thread_Placement_and_Thread_Affinity/#setting-up-thread-placement-and-affinity","title":"Setting up thread placement and affinity","text":"<p>We will now place our threads on cores in a specific order and bind them to these cores, so that they can no longer move to another core during execution.</p> <p>Warning</p> <p>Slurm does NOT guarantee that all threads will be placed on the same socket even if our job would fit entirely within one socket. This needs to be taken into account when optimising threading setup.</p> <p>Let us start with the following setup:</p> <ul> <li>Run with \"--hint=multithread\" so that our program can access all     available logical cores</li> <li>Bind threads to physical cores (\"granularity=core\") - they are still     free to move between the two logical cores inside a given physical     core</li> <li>Place threads close together (\"compact\") - although this has little     significance here as we use all available cores anyway, we still     need to specify this to activate thread affinity</li> <li>Bind thread IDs to logical core IDs in simple numerical order by     setting permute and offset specifiers to 0</li> </ul> <pre><code>#!/bin/bash -e\n#SBATCH --job-name=thread_placement_affinity\n#SBATCH --time=00:00:30\n#SBATCH --nodes=1\n#SBATCH --ntasks=1\n#SBATCH --cpus-per-task=4\n#SBATCH --hint=multithread\n\nexport KMP_AFFINITY=verbose,granularity=core,compact,0,0\nmodule load intel/2018b\nsrun hello_world.x\n</code></pre> <p>You should get this result:</p> <pre><code>OMP: Info #209: KMP_AFFINITY: decoding x2APIC ids.\nOMP: Info #207: KMP_AFFINITY: Affinity capable, using global cpuid leaf 11 info\nOMP: Info #154: KMP_AFFINITY: Initial OS proc set respected: {2,7,42,47}\nOMP: Info #156: KMP_AFFINITY: 4 available OS procs\nOMP: Info #157: KMP_AFFINITY: Uniform topology\nOMP: Info #179: KMP_AFFINITY: 1 packages x 2 cores/pkg x 2 threads/core (2 total cores)\nOMP: Info #211: KMP_AFFINITY: OS proc to physical thread map:\nOMP: Info #171: KMP_AFFINITY: OS proc 2 maps to package 0 core 2 thread 0\nOMP: Info #171: KMP_AFFINITY: OS proc 42 maps to package 0 core 2 thread 1\nOMP: Info #171: KMP_AFFINITY: OS proc 7 maps to package 0 core 10 thread 0\nOMP: Info #171: KMP_AFFINITY: OS proc 47 maps to package 0 core 10 thread 1\nOMP: Info #144: KMP_AFFINITY: Threads may migrate across 1 innermost levels of machine\nOMP: Info #247: KMP_AFFINITY: pid 177638 tid 177638 thread 0 bound to OS proc set {2,42}\nOMP: Info #247: KMP_AFFINITY: pid 177638 tid 177640 thread 1 bound to OS proc set {2,42}\nOMP: Info #247: KMP_AFFINITY: pid 177638 tid 177641 thread 2 bound to OS proc set {7,47}\nOMP: Info #247: KMP_AFFINITY: pid 177638 tid 177642 thread 3 bound to OS proc set {7,47}\nHello World from Thread 0!\nHello World from Thread 1!\nHello World from Thread 2!\nHello World from Thread 3!\n</code></pre> <p>As requested, pairs of threads are now bound to both logical cores inside a given physical core and can move between those two.</p> <p>Choosing \"granularity=fine\" instead of \"granularity=core\" will bind each thread to a single logical core, and threads can no longer move at all:</p> <pre><code>[...]\nOMP: Info #247: KMP_AFFINITY: pid 178055 tid 178055 thread 0 bound to OS proc set {2}\nOMP: Info #247: KMP_AFFINITY: pid 178055 tid 178057 thread 1 bound to OS proc set {42}\nOMP: Info #247: KMP_AFFINITY: pid 178055 tid 178058 thread 2 bound to OS proc set {7}\nOMP: Info #247: KMP_AFFINITY: pid 178055 tid 178059 thread 3 bound to OS proc set {47}\n[...]\n</code></pre> <p>Note in the output of the last example how threads 0 and 1 fill up the first and second logical core (IDs 2 and 42) of the first physical core, while threads 3 and 4 are placed on the second physical core (IDs 7 and 47). We can influence placement by manipulating the \"permute\" and \"offset\" values. Choosing \"1,0\" results in:</p> <pre><code>[...]\nOMP: Info #247: KMP_AFFINITY: pid 178741 tid 178741 thread 0 bound to OS proc set {2}\nOMP: Info #247: KMP_AFFINITY: pid 178741 tid 178743 thread 1 bound to OS proc set {7}\nOMP: Info #247: KMP_AFFINITY: pid 178741 tid 178744 thread 2 bound to OS proc set {42}\nOMP: Info #247: KMP_AFFINITY: pid 178741 tid 178745 thread 3 bound to OS proc set {47}\n[...]\n</code></pre> <p>Threads 0 and 1 are now placed on the first logical cores of each physical core, threads 2 and 3 on the second logical cores.</p> <p>We can also choose an offset - setting \"0,1\" shifts placement of thread IDs onto logical core IDs by 1:</p> <pre><code>[...]\nOMP: Info #171: KMP_AFFINITY: OS proc 2 maps to package 0 core 2 thread 0\nOMP: Info #171: KMP_AFFINITY: OS proc 42 maps to package 0 core 2 thread 1\nOMP: Info #171: KMP_AFFINITY: OS proc 3 maps to package 0 core 3 thread 0\nOMP: Info #171: KMP_AFFINITY: OS proc 43 maps to package 0 core 3 thread 1\nOMP: Info #247: KMP_AFFINITY: pid 180198 tid 180198 thread 0 bound to OS proc set {42}\nOMP: Info #247: KMP_AFFINITY: pid 180198 tid 180200 thread 1 bound to OS proc set {3}\nOMP: Info #247: KMP_AFFINITY: pid 180198 tid 180201 thread 2 bound to OS proc set {43}\nOMP: Info #247: KMP_AFFINITY: pid 180198 tid 180202 thread 3 bound to OS proc set {2}\n[...]\n</code></pre> <p>Please refer to the Intel documentation for further information on \"KMP_AFFINITY\".</p>","tags":[]},{"location":"Scientific_Computing/HPC_Software_Environment/Thread_Placement_and_Thread_Affinity/#tips","title":"Tips","text":"<p>Unfortunately, there is no single best choice for setting up thread placement and affinity, it depends on the application. Also keep in mind that:</p> <ul> <li>Job runtimes can be affected by other jobs that are running on the     same node and share network access, memory bus, and some caches on     the same socket</li> <li>The operating system on a node will also still need to run its own     processes and threads</li> </ul> <p>This can lead to a trade-off between restricting thread movement for better performance while allowing some flexibility for threads that are held up by other jobs or system processes. It is therefore worth trying out different affinity setups to see which one works best for your program.</p> <p>It is usually a good idea to start without hyperthreading and activate thread affinity by choosing:</p> <pre><code>#SBATCH --hint=nomultithread\nexport KMP_AFFINITY=granularity=fine,compact,0,0\n</code></pre> <p>You can now try out other configurations and compare runtimes.</p>","tags":[]},{"location":"Scientific_Computing/HPC_Software_Environment/Visualisation_software/","title":"Visualisation software","text":"<p>NeSI provides a variety of visualisation software solutions via modules. Available software and software versions vary between the Mahuika and M\u0101ui Ancil systems. Additional software can be provided upon request, see section Supported Applications.</p> <p>Most software only requires a CPU node to run, but some software, in particular high-performance 3D visualisation software, can utilise a GPU node for better performance. Where possible, both CPU and GPU variants of the same software are provided for maximum flexibility; please make sure to select the correct module version as described below.</p> <p>Apart from interactive operation, many visualisation solutions also support batch mode (\"headless\") operation, allowing users to submit large visualisation jobs to the HPC.</p> <p>In the following installed packages are listed for:</p> <ul> <li> <p>Scripting Languages with Visualisation Capabilities</p> </li> <li> <p>2D Visualisation Software</p> </li> <li> <p>3D Visualisation Software</p> </li> </ul>","tags":["support","application","visualisation"]},{"location":"Scientific_Computing/HPC_Software_Environment/Visualisation_software/#scripting-languages-with-visualisation-capabilities","title":"Scripting Languages with Visualisation Capabilities","text":"","tags":["support","application","visualisation"]},{"location":"Scientific_Computing/HPC_Software_Environment/Visualisation_software/#python","title":"Python","text":"<p>The Python language comes with packages such as \"matplotlib\" for general-purpose 2D visualisation, or \"vtk\" (see VTK section on this page) for 3D visualisation.</p>","tags":["support","application","visualisation"]},{"location":"Scientific_Computing/HPC_Software_Environment/Visualisation_software/#available-modules","title":"Available Modules","text":"Module Name Mahuika M\u0101ui Ancil Comment Anaconda2/5.2.0-GCC-7.1.0 \u2714 Anaconda distribution of Python 2 Anaconda2/2019.10-GCC-7.1.0 \u2714 Anaconda distribution of Python 2 Anaconda3/5.2.0-GCC-7.1.0 \u2714 Anaconda distribution of Python 3 Anaconda3/2019.03 \u2714 Anaconda distribution of Python 3 Anaconda3/2019.07-gimkl-2018b \u2714 Anaconda distribution of Python 3 Anaconda3/2020.02-GCC-7.1.0 \u2714 Anaconda distribution of Python 3 Miniconda3/4.4.10 \u2714 Minimal Anaconda distribution of Python 3 Miniconda3/4.5.12 \u2714 Minimal Anaconda distribution of Python 3 Miniconda3/4.7.10 \u2714 Minimal Anaconda distribution of Python 3 Miniconda3/4.8.2 \u2714 Minimal Anaconda distribution of Python 3 Miniconda3/4.8.3 \u2714 Minimal Anaconda distribution of Python 3 Miniconda3/4.9.2 \u2714 Minimal Anaconda distribution of Python 3 Python-GPU/3.6.3-gimkl-2017a \u2714 GPU-enabled Python 3 Python-Geo/2.7.14-gimkl-2017a \u2714 Python 2 for geospatial applications Python-Geo/2.7.16-gimkl-2018b \u2714 Python 2 for geospatial applications Python-Geo/3.6.3-gimkl-2017a \u2714 Python 3 for geospatial applications Python-Geo/3.7.3-gimkl-2018b \u2714 Python 3 for geospatial applications Python-Geo/3.8.2-gimkl-2020a \u2714 Python 3 for geospatial applications Python/2.7.14-gimkl-2017a \u2714 Python 2 base packages Python/2.7.16-gimkl-2018b \u2714 Python 2 base packages Python/2.7.16-intel-2018b \u2714 Python 2 base packages Python/2.7.18-gimkl-2020a \u2714 Python 2 base packages Python/3.6.3-gimkl-2017a \u2714 Python 3 base packages Python/3.7.3-gimkl-2018b \u2714 Python 3 base packages Python/3.8.1-gimkl-2018b \u2714 Python 3 base packages Python/3.8.2-gimkl-2020a \u2714 Python 3 base packages Python/3.9.5-gimkl-2020a \u2714 Python 3 base packages","tags":["support","application","visualisation"]},{"location":"Scientific_Computing/HPC_Software_Environment/Visualisation_software/#r","title":"R","text":"<p>The R language comes with built-in 2D plotting capabilities that can be extended with additional packages.</p>","tags":["support","application","visualisation"]},{"location":"Scientific_Computing/HPC_Software_Environment/Visualisation_software/#available-modules_1","title":"Available Modules","text":"Module Name Mahuika M\u0101ui Ancil Comment R/3.4.2-gimkl-2017a \u00a0\u2714 R base package R/3.5.0-gimkl-2017a \u00a0\u2714 R base package R/3.5.1-gimkl-2017a \u2714 R base package R/3.5.3-gimkl-2018b \u2714 R base package R/3.6.1-gimkl-2018b \u2714 R base package R/3.6.2-gimkl-2020a \u2714 R base package R/4.0.1-gimkl-2020a \u2714 R base package R/3.5.1-gimkl-2018b \u2714 R base package R/3.6.1-gimkl-2018b \u2714 R base package R-Geo/3.6.1-gimkl-2018b \u2714 R for geospatial applications R-Geo/3.6.2-gimkl-2020a \u2714 R for geospatial applications R-Geo/4.0.1-gimkl-2020a \u2714 R for geospatial applications Rstudio/1.1.456 \u2714 Rstudio IDE","tags":["support","application","visualisation"]},{"location":"Scientific_Computing/HPC_Software_Environment/Visualisation_software/#ncl","title":"NCL","text":"<p>The NCAR Command Language provides visualisation capabilities which are mostly used in the weather and climate fields.</p>","tags":["support","application","visualisation"]},{"location":"Scientific_Computing/HPC_Software_Environment/Visualisation_software/#available-modules_2","title":"Available Modules","text":"Module Name Mahuika M\u0101ui Ancil Comment NCL/6.2.0-GCC-7.1.0 \u00a0\u2714 NCL base package NCL/6.4.0-GCC-7.1.0 \u00a0\u2714 NCL base package NCL/6.6.2-intel-2018b \u00a0\u2714 NCL base package","tags":["support","application","visualisation"]},{"location":"Scientific_Computing/HPC_Software_Environment/Visualisation_software/#matlab","title":"MATLAB","text":"<p>The MATLAB programming language comes with built-in visualisation capabilities.</p>","tags":["support","application","visualisation"]},{"location":"Scientific_Computing/HPC_Software_Environment/Visualisation_software/#available-modules_3","title":"Available Modules","text":"Module Name Mahuika M\u0101ui Ancil Comment MATLAB/2016b \u2714 \u2714 MATLAB/2017b \u2714 \u2714 MATLAB/2018b \u2714 \u2714 MATLAB/2019b \u2714 \u2714 MATLAB/2020a \u2714 MATLAB/2020b \u2714 MATLAB/2021a \u2714","tags":["support","application","visualisation"]},{"location":"Scientific_Computing/HPC_Software_Environment/Visualisation_software/#2d-visualisation-software","title":"2D Visualisation Software","text":"","tags":["support","application","visualisation"]},{"location":"Scientific_Computing/HPC_Software_Environment/Visualisation_software/#iris","title":"IRIS","text":"<p>IRIS is a Python-based visualisation tool that is mainly used in the weather and climate fields.</p>","tags":["support","application","visualisation"]},{"location":"Scientific_Computing/HPC_Software_Environment/Visualisation_software/#available-modules_4","title":"Available Modules","text":"Module Name Mahuika M\u0101ui Ancil Comment Anaconda2/5.2.0-GCC-7.1.0 \u00a0\u2714 IRIS v1.13.0 Anaconda2/2019.10-GCC-7.1.0 \u2714 IRIS v2.2.1.dev0 Anaconda3/5.2.0-GCC-7.1.0 \u00a0\u2714 IRIS v1.13.0\u00a0 Anaconda3/2019.03 \u2714 IRIS v1.13.0 Anaconda3/2020.02-GCC-7.1.0 \u2714 IRIS v2.4.0","tags":["support","application","visualisation"]},{"location":"Scientific_Computing/HPC_Software_Environment/Visualisation_software/#ncview","title":"Ncview","text":"<p>Ncview is a visual browser for netCDF files.</p>","tags":["support","application","visualisation"]},{"location":"Scientific_Computing/HPC_Software_Environment/Visualisation_software/#available-modules_5","title":"Available Modules","text":"Module Name Mahuika M\u0101ui Ancil Comment ncview/2.1.7-gimkl-2018b \u2714 NCVIEW/2.1.8-GCC-7.1.0 \u00a0\u2714","tags":["support","application","visualisation"]},{"location":"Scientific_Computing/HPC_Software_Environment/Visualisation_software/#xconv","title":"XCONV","text":"<p>XCONV is a visual browser for netCDF and Unified Model format files.</p>","tags":["support","application","visualisation"]},{"location":"Scientific_Computing/HPC_Software_Environment/Visualisation_software/#available-modules_6","title":"Available Modules","text":"Module Name Mahuika M\u0101ui Ancil Comment XCONV/1.93 \u00a0\u2714","tags":["support","application","visualisation"]},{"location":"Scientific_Computing/HPC_Software_Environment/Visualisation_software/#3d-visualisation-software","title":"3D Visualisation Software","text":"","tags":["support","application","visualisation"]},{"location":"Scientific_Computing/HPC_Software_Environment/Visualisation_software/#paraview","title":"ParaView","text":"<p>ParaView is a high-performance 3D visualisation tool. The headless versions only provide ParaView Server, which can operate in batch mode, as well as in client-server operation.</p> <p>For interactive sessions, it is highly recommended to use ParaView with the NICE DCV cloud visualisation software for best performance. Client-server mode, where a ParaView GUI runs on a local machine and connects to a ParaView server on the HPC, is also an option but requires much more network bandwidth than NICE DCV and thus may be slower.</p>","tags":["support","application","visualisation"]},{"location":"Scientific_Computing/HPC_Software_Environment/Visualisation_software/#parallelisation","title":"Parallelisation","text":"<p>The CPU based versions of ParaView use the OpenSWR rasteriser as well as the OSPRay ray tracer for rendering graphics. These support parallel operation for better performance, but are configured to only use a single core by default. Run the following commands before launching ParaView GUI or ParaView Server if you want to use more cores (depending on the number of cores available in your session):</p> <pre><code>export KNOB_MAX_WORKER_THREADS=&lt;number of cores&gt;\nexport OSPRAY_THREADS=&lt;number of cores&gt;\n</code></pre> <p>ParaView Server also supports parallel execution using MPI, see \"Setting up Client-Server Mode\" below.</p>","tags":["support","application","visualisation"]},{"location":"Scientific_Computing/HPC_Software_Environment/Visualisation_software/#available-modules_7","title":"Available Modules","text":"Module Name Mahuika M\u0101ui Ancil Comment ParaView/5.3.0-gimkl-2017a \u2714 ParaView/5.4.1-gimkl-2017a-Python-2.7.14 \u2714 ParaView/5.4.1-gimkl-2018b-Python-2.7.16 \u2714 ParaView/5.4.1-gimpi-2018b \u2714 ParaView/5.5.2-gimpi-2018a-Server-EGL \u2714 Headless version for GPUs, no GUI operation ParaView/5.5.2-gimpi-2018b-GUI-Mesa \u2714 GUI version for CPUs, no headless operation ParaView/5.5.2-gimpi-2018b-Server-OSMesa \u2714 Headless version for CPUs, no GUI operation ParaView/5.6.0-gimkl-2018b-Python-3.7.3 ParaView/5.6.0-gimpi-2017a-Server-OSMesa \u2714 Headless version for CPUs, no GUI operation ParaView/5.6.0-gimpi-2018b \u2714 ParaView/5.6.0-gimkl-2018b-Python-3.7.3 \u2714 ParaView/5.9.0-gimkl-2020a-Python-3.8.2 \u2714","tags":["support","application","visualisation"]},{"location":"Scientific_Computing/HPC_Software_Environment/Visualisation_software/#setting-up-client-server-mode","title":"Setting up Client-Server Mode","text":"<p>If you want to use ParaView in client-server mode, use the following setup:</p> <ul> <li>Load one of the ParaView Server modules listed above and launch the     server in your interactive visualisation session on the HPC:</li> </ul> <pre><code>mpiexec -np &lt;number of MPI ranks&gt; pvserver\n</code></pre> <ul> <li>Create an SSH tunnel for port \"11111\" from the HPC to your local     machine using, e.g., the ssh program (Linux and MacOS) or MobaXterm     (Windows)</li> <li>Launch the ParaView GUI on your local machine and go to \"File &gt;     Connect\"</li> <li>Click on \"Add Server\", choose server type \"Client / Server\", host     \"localhost\" (as we will be using the SSH tunnel), and port \"11111\",     then click on \"Configure\"</li> <li>Select the new server and click on \"Connect\"</li> </ul>","tags":["support","application","visualisation"]},{"location":"Scientific_Computing/HPC_Software_Environment/Visualisation_software/#visit","title":"VisIt","text":"<p>VisIt is a high-performance 3D visualisation tool. At this point, only GUI-based interactive sessions on CPUs via NICE DCV are supported, GPU support and client-server operation will be added later.</p>","tags":["support","application","visualisation"]},{"location":"Scientific_Computing/HPC_Software_Environment/Visualisation_software/#available-modules_8","title":"Available Modules","text":"Module Name Mahuika M\u0101ui Ancil Comment VisIt/2.13.3-gimpi-2018b-GUI-Mesa \u2714 GUI version for CPUs","tags":["support","application","visualisation"]},{"location":"Scientific_Computing/HPC_Software_Environment/Visualisation_software/#vtk","title":"VTK","text":"<p>The Visualization ToolKit (VTK) can be used for 3D visualisation in various programming languages, in particular with the Python scripting language.</p>","tags":["support","application","visualisation"]},{"location":"Scientific_Computing/HPC_Software_Environment/Visualisation_software/#available-modules_9","title":"Available Modules","text":"Module Name Mahuika M\u0101ui Ancil Comment VTK/6.3.0-gimkl-2017a-Python-2.7.14 \u2714 VTK6 with Python bindings VTK/7.1.1-gimkl-2018b-Python-2.7.16 \u2714 VTK7 with Python bindings VTK/8.1.1-GCC-7.1.0-Anaconda2-5.2.0 \u2714 VTK8 with Python bindings","tags":["support","application","visualisation"]},{"location":"Scientific_Computing/Interactive_computing_using_Jupyter/Jupyter_kernels_Manual_management/","title":"Jupyter kernels - Manual management","text":"","tags":[]},{"location":"Scientific_Computing/Interactive_computing_using_Jupyter/Jupyter_kernels_Manual_management/#introduction","title":"Introduction","text":"<p>Jupyter kernels execute the code that you write. The following Jupyter kernels are installed by default and can be selected from the Launcher:</p> <ul> <li>Python 3.8.2</li> <li>Python 3.8.1</li> <li>Python 3.7.3</li> <li>Anaconda3</li> <li>R 4.0.1</li> <li>R 3.6.1</li> </ul> <p>Many packages are preinstalled in our default Python and R environments and these can be extended further as described on the Python\u00a0and R\u00a0support pages.</p>","tags":[]},{"location":"Scientific_Computing/Interactive_computing_using_Jupyter/Jupyter_kernels_Manual_management/#adding-a-custom-python-kernel","title":"Adding a custom Python kernel","text":"<p>see also</p> <p>See the\u00a0Jupyter kernels - Tool-assisted  management  page for the preferred way to register kernels, which uses the  <code>nesi-add-kernel</code> command line tool to automate most of these manual  steps.</p> <p>You can configure custom Python kernels for running your Jupyter notebooks. This could be necessary and/or recommended in some situations, including:</p> <ul> <li>if you wish to load a different combination of environment modules     than those we load in our default kernels</li> <li>if you would like to activate a virtual environment or conda     environment before launching the kernel</li> </ul> <p>The following example will create a custom kernel based on the Miniconda3 environment module (but applies to other environment modules too).</p> <p>In a terminal run the following commands to load a Miniconda environment module:</p> <pre><code>module purge\nmodule load Miniconda3/4.8.2\n</code></pre> <p>Now create a conda environment named \"my-conda-env\" using Python 3.6. The ipykernel\u00a0Python package is required but you can change the names of the environment, version of Python and install other Python packages as required.</p> <pre><code>conda create --name my-conda-env python=3.6\nsource $(conda info --base)/etc/profile.d/conda.sh\nconda activate my-conda-env\nconda install ipykernel\n# you can pip/conda install other packages here too\n</code></pre> <p>Now create a Jupyter kernel based on your new conda environment:</p> <pre><code>python -m ipykernel install --user --name my-conda-env --display-name=\"My Conda Env\"\n</code></pre> <p>We must now edit the kernel to load the required NeSI environment modules before the kernel is launched. Change to the directory the kernelspec was installed to <code>~/.local/share/jupyter/kernels/my-conda-env</code>, (assuming you kept <code>--name my-conda-env</code> in the above command):</p> <pre><code>cd ~/.local/share/jupyter/kernels/my-conda-env\n</code></pre> <p>Now create a wrapper script, called <code>wrapper.sh</code>, with the following contents:</p> <pre><code>#!/usr/bin/env bash\n\n# load required modules here\nmodule purge\nmodule load Miniconda3/4.8.2\n\n# activate conda environment\nsource $(conda info --base)/etc/profile.d/conda.sh \nconda deactivate  # workaround for https://github.com/conda/conda/issues/9392\nconda activate my-conda-env\n\n# run the kernel\nexec python $@\n</code></pre> <p>Make the wrapper script executable:</p> <pre><code>chmod +x wrapper.sh\n</code></pre> <p>Next edit the kernel.json to change the first element of the argv list to point to the wrapper script we just created. The file should look like this (change &lt;username&gt; to your NeSI username):</p> <pre><code>{\n \"argv\": [\n \"/home/&lt;username&gt;/.local/share/jupyter/kernels/my-conda-env/wrapper.sh\",\n \"-m\",\n \"ipykernel_launcher\",\n \"-f\",\n \"{connection_file}\"\n ],\n \"display_name\": \"My Conda Env\",\n \"language\": \"python\"\n}\n</code></pre> <p>After refreshing JupyterLab your new kernel should show up in the Launcher as \"My Conda Env\".</p>","tags":[]},{"location":"Scientific_Computing/Interactive_computing_using_Jupyter/Jupyter_kernels_Manual_management/#sharing-a-python-kernel-with-your-project-team-members","title":"Sharing a Python kernel with your project team members","text":"<p>You can also configure a shared Python kernel that others with access to the same NeSI project will be able to load. If this kernel is based on a Python virtual environment, Conda environment or similar, you must make sure it also exists in a shared location (other users cannot see your home directory).</p> <p>The example below shows creating a shared Python kernel based on the <code>Python/3.8.2-gimkl-2020a</code> module and also loads the <code>ETE/3.1.1-gimkl-2020a-Python-3.8.2</code> module.</p> <p>In a terminal run the following commands to load the Python and ETE environment modules:</p> <pre><code>module purge\nmodule load Python/3.8.2-gimkl-2020a\nmodule load ETE/3.1.1-gimkl-2020a-Python-3.8.2\n</code></pre> <p>Now create a Jupyter kernel within your project directory, based on your new virtual environment:</p> <pre><code>python -m ipykernel install --prefix=/nesi/project/&lt;project_code&gt;/.jupyter --name shared-ete-env --display-name=\"Shared ETE Env\"\n</code></pre> <p>Next change to the kernel directory, which for the above command would be:</p> <pre><code>cd /nesi/project/&lt;project_code&gt;/.jupyter/share/jupyter/kernels/shared-ete-env\n</code></pre> <p>Create a wrapper script,\u00a0wrapper.sh, with the following contents:</p> <pre><code>#!/usr/bin/env bash\n\n# load necessary modules here\nmodule purge\nmodule load Python/3.8.2-gimkl-2020a\nmodule load ETE/3.1.1-gimkl-2020a-Python-3.8.2\n\n# run the kernel\nexec python $@\n</code></pre> <p>Note we also load the ETE module so that we can use that from our kernel.</p> <p>Make the wrapper script executable:</p> <pre><code>chmod +x wrapper.sh\n</code></pre> <p>Next, edit the kernel.json to change the first element of the argv list to point to the wrapper script we just created. The file should look like this (change &lt;project_code&gt; to your NeSI project code):</p> <pre><code>{\n \"argv\": [\n \"/nesi/project/&lt;project_code&gt;/.jupyter/share/jupyter/kernels/shared-ete-env/wrapper.sh\",\n \"-m\",\n \"ipykernel_launcher\",\n \"-f\",\n \"{connection_file}\"\n ],\n \"display_name\": \"Shared Conda Env\",\n \"language\": \"python\"\n}\n</code></pre> <p>After refreshing JupyterLab your new kernel should show up in the Launcher as \"Shared Virtual Env\".</p>","tags":[]},{"location":"Scientific_Computing/Interactive_computing_using_Jupyter/Jupyter_kernels_Manual_management/#custom-kernel-in-a-singularity-container","title":"Custom kernel in a Singularity container","text":"<p>An example showing setting up a custom kernel running in a Singularity container can be found on our Lambda Stack support page.</p>","tags":[]},{"location":"Scientific_Computing/Interactive_computing_using_Jupyter/Jupyter_kernels_Manual_management/#adding-a-custom-r-kernel","title":"Adding a custom R kernel","text":"<p>You can configure custom R kernels for running your Jupyter notebooks. The following example will create a custom kernel based on the R/3.6.2-gimkl-2020a environment module and will additionally load an MPFR environment module (e.g. if you wanted to load the Rmpfr package).</p> <p>In a terminal run the following commands to load the required environment modules:</p> <pre><code>module purge\nmodule load IRkernel/1.1.1-gimkl-2020a-R-3.6.2\nmodule load Python/3.8.2-gimkl-2020a\n</code></pre> <p>The IRkernel module loads the R module as a dependency and provides the R kernel for Jupyter. Python is required to install the kernel (since Jupyter is written in Python).</p> <p>Now create an R Jupyter kernel based on your new conda environment:</p> <pre><code>R -e \"IRkernel::installspec(name='myrwithmpfr', displayname = 'R with MPFR', user = TRUE)\"\n</code></pre> <p>We must now to edit the kernel to load the required NeSI environment modules when the kernel is launched. Change to the directory the kernelspec was installed to (~/.local/share/jupyter/kernels/myrwithmpfr, assuming you kept --name myrwithmpfr\u00a0in the above command):</p> <pre><code>cd ~/.local/share/jupyter/kernels/myrwithmpfr\n</code></pre> <p>Now create a wrapper script in that directory, called wrapper.sh, with the following contents:</p> <pre><code>#!/usr/bin/env bash\n\n# load required modules here\nmodule purge\nmodule load MPFR/4.0.2-GCCcore-9.2.0\nmodule load IRkernel/1.1.1-gimkl-2020a-R-3.6.2\n\n# run the kernel\nexec R $@\n</code></pre> <p>Make the wrapper script executable:</p> <pre><code>chmod +x wrapper.sh\n</code></pre> <p>Next edit the kernel.json to change the first element of the argv list to point to the wrapper script we just created. The file should look something like this (change &lt;username&gt; to your NeSI username):</p> <pre><code>{\n \"argv\": [\n \"/home/&lt;username&gt;/.local/share/jupyter/kernels/myrwithmpfr/wrapper.sh\",\n \"--slave\",\n \"-e\",\n \"IRkernel::main()\",\n \"--args\",\n \"{connection_file}\"\n ],\n \"display_name\": \"R with MPFR\",\n \"language\": \"R\"\n}\n</code></pre> <p>After refreshing JupyterLab your new R kernel should show up in the Launcher as \"R with MPFR\".</p>","tags":[]},{"location":"Scientific_Computing/Interactive_computing_using_Jupyter/Jupyter_kernels_Manual_management/#spark","title":"Spark","text":"<p>At the time of writing, the latest stable version of Spark does not support Python 3.8. If you wish to use Spark (e.g. PySpark) make sure you select one of our Python 3.7.3 or Anaconda3 kernels.</p>","tags":[]},{"location":"Scientific_Computing/Interactive_computing_using_Jupyter/Jupyter_kernels_Tool_assisted_management/","title":"Jupyter kernels - Tool-assisted management","text":"","tags":[]},{"location":"Scientific_Computing/Interactive_computing_using_Jupyter/Jupyter_kernels_Tool_assisted_management/#introduction","title":"Introduction","text":"<p>Jupyter can execute code in different computing environments using kernels. Some kernels are provided by default (Python, R, etc.) but you may want to register your computing environment to use it in notebooks. For example, you may want to load a specific environment module in your kernel or use a Conda environment.</p> <p>To register a Jupyter kernel, you can follow the steps highlighted in the Jupyter kernels - Manual management or use the <code>nesi-add-kernel</code> tool provided on Jupyter on NeSI service. This page details the latter option, which we recommend.</p>","tags":[]},{"location":"Scientific_Computing/Interactive_computing_using_Jupyter/Jupyter_kernels_Tool_assisted_management/#getting-started","title":"Getting started","text":"<p>First you need to open a terminal. It can be from a session on Jupyter on NeSI or from a regular ssh connection on Mahuika login node. If you use the ssh option, make sure to load the JupyterLab module to have access to the <code>nesi-add-kernel</code> tool:</p> <pre><code>module purge  # remove all previously loaded modules\nmodule load JupyterLab\n</code></pre> <p>Then, to list all available options, use the <code>-h</code> or <code>--help</code> options as follows:</p> <pre><code>nesi-add-kernel --help\n</code></pre> <p>Here is an example to add a TensorFlow kernel, using NeSI\u2019s module:</p> <pre><code>nesi-add-kernel tf_kernel TensorFlow/2.8.2-gimkl-2022a-Python-3.10.5\n</code></pre> <p>and to share the kernel with other members of your NeSI project:</p> <pre><code>nesi-add-kernel --shared tf_kernel_shared TensorFlow/2.8.2-gimkl-2022a-Python-3.10.5 \n</code></pre> <p>To list all the installed kernels, use the following command:</p> <pre><code>jupyter-kernelspec list\n</code></pre> <p>and to delete a specific kernel:</p> <pre><code>jupyter-kernelspec remove &lt;kernel_name&gt;\n</code></pre> <p>where <code>&lt;kernel_name&gt;</code> stands for the name of the kernel to delete.</p>","tags":[]},{"location":"Scientific_Computing/Interactive_computing_using_Jupyter/Jupyter_kernels_Tool_assisted_management/#conda-environment","title":"Conda environment","text":"<p>First, make sure the <code>JupyterLab</code> module is loaded:</p> <pre><code>module purge\nmodule load JupyterLab\n</code></pre> <p>To add a Conda environment created using <code>conda create -p &lt;conda_env_path&gt;</code>, use:</p> <pre><code>nesi-add-kernel my_conda_env -p &lt;conda_env_path&gt;\n</code></pre> <p>otherwise if created using <code>conda create -n &lt;conda_env_name&gt;</code>, use:</p> <pre><code>nesi-add-kernel my_conda_env -n &lt;conda_env_name&gt;\n</code></pre>","tags":[]},{"location":"Scientific_Computing/Interactive_computing_using_Jupyter/Jupyter_kernels_Tool_assisted_management/#virtual-environment","title":"Virtual environment","text":"<p>If you want to use a Python virtual environment, don\u2019t forget to specify which Python module you used to create it.</p> <p>For example, if we create a virtual environment named <code>my_test_venv</code> using Python 3.10.5:</p> <pre><code>module purge\nmodule load Python/3.10.5-gimkl-2022a\npython -m venv my_test_venv\n</code></pre> <p>to create the corresponding <code>my_test_kernel</code> kernel, we need to use the command:</p> <pre><code>module purge\nmodule load JupyterLab\nnesi-add-kernel my_test_kernel Python/3.10.5-gimkl-2022a --venv my_test_venv\n</code></pre>","tags":[]},{"location":"Scientific_Computing/Interactive_computing_using_Jupyter/Jupyter_kernels_Tool_assisted_management/#singularity-container","title":"Singularity container","text":"<p>To use a Singularity container, use the <code>-c</code> or <code>--container</code> options as follows:</p> <pre><code>module purge\nmodule load JupyterLab\nnesi-add-kernel my_test_kernel -c &lt;container_image.sif&gt;\n</code></pre> <p>where <code>&lt;container_image.sif&gt;</code> is a path to your container image.</p> <p>Note that your container must have the <code>ipykernel</code> Python package installed in it to be able to work as a Jupyter kernel.</p> <p>Additionally, you can use the <code>--container-args</code> option to pass more arguments to the <code>singularity exec</code> command used to instantiate the kernel.</p> <p>Here is an example instantiating a NVIDIA NGC container as a kernel. First, we need to pull the container:</p> <pre><code>module purge\nmodule load Singularity/3.11.3\nsingularity pull nvidia_tf.sif docker://nvcr.io/nvidia/tensorflow:21.07-tf2-py3\n</code></pre> <p>then we can instantiate the kernel, using the <code>--nv</code> singularity flag to ensure that the GPU will be found at runtime (assuming our Jupyter session has access to a GPU):</p> <pre><code>module purge\nmodule load JupyterLab\nnesi-add-kernel nvidia_tf -c nvidia_tf.sif --container-args \"'--nv'\"\n</code></pre> <p>Note that the double-quoting of <code>--nv</code> is needed to properly pass the options to <code>singularity exec</code>.</p>","tags":[]},{"location":"Scientific_Computing/Interactive_computing_using_Jupyter/Jupyter_on_NeSI/","title":"Jupyter on NeSI","text":"<p>Note</p> <p>This service is available for users with a current allocation on  Mahuika only.  Please Contact our Support Team to request a suitable  allocation.</p>","tags":["jupyter","hub","home","lab","notebook"]},{"location":"Scientific_Computing/Interactive_computing_using_Jupyter/Jupyter_on_NeSI/#introduction","title":"Introduction","text":"<p>NeSI supports the use of Jupyter for interactive computing. Jupyter allows you to create notebooks that contain live code, equations, visualisations and explanatory text.\u00a0There are many uses for Jupyter, including data cleaning, analytics and visualisation, machine learning, numerical simulation, managing Slurm job submissions and workflows and much more.</p> <p>See also</p> <ul> <li>See the RStudio via Jupyter on NeSI      page for launching an RStudio instance.</li> <li>See the MATLAB via Jupyter on NeSI      page for launching MATLAB via Jupyter</li> <li>See the Virtual Desktop via Jupyter on NeSI      page for launching a virtual desktop via Jupyter.</li> <li>See the\u00a0Jupyter kernels - Tool-assisted management      (recommended) and Jupyter kernels - Manual management      pages for adding kernels.</li> </ul>","tags":["jupyter","hub","home","lab","notebook"]},{"location":"Scientific_Computing/Interactive_computing_using_Jupyter/Jupyter_on_NeSI/#accessing-jupyter-on-nesi","title":"Accessing Jupyter on NeSI","text":"<p>Jupyter at NeSI is powered by JupyterHub, a multi-user hub that spawns, manages and proxies multiple instances of the single-user Jupyter server.</p>","tags":["jupyter","hub","home","lab","notebook"]},{"location":"Scientific_Computing/Interactive_computing_using_Jupyter/Jupyter_on_NeSI/#access-nesis-jupyterhub-here","title":"Access NeSI's JupyterHub here","text":"<p>ttps://jupyter.nesi.org.nz</p> <p>When you log in with your NeSI credentials you will be taken to the \"Server Options\" page, where typical job configuration options can be selected to allocate the resources that will be used to run Jupyter. Typical jobs, not requesting a GPU, should be up and running within one to two minutes. Requesting a GPU can increase this time significantly as there are only a small number of GPUs available at NeSI.</p> <p>Tip</p> <p>If your server appears to not have started within 3 minutes please  reload the browser window and check again, otherwise contact   Contact our Support Team.</p>","tags":["jupyter","hub","home","lab","notebook"]},{"location":"Scientific_Computing/Interactive_computing_using_Jupyter/Jupyter_on_NeSI/#known-issues","title":"Known issues","text":"<ul> <li> <p>When using srun in a Jupyter terminal you may see messages like     those shown below. The \"error\" messages are actually just warnings     and can be ignored; the srun command should still work.     Alternatively, you could run unset TMPDIR in the terminal before     running srun to avoid these warnings.</p> <pre><code>srun --pty bash\n</code></pre> <pre><code>srun: job 28560743 queued and waiting for resources\nsrun: job 28560743 has been allocated resources\nslurmstepd: error: Unable to create TMPDIR [/dev/shm/jobs/28560712]: Permission denied\nslurmstepd: error: Setting TMPDIR to /tmp\n</code></pre> </li> </ul>","tags":["jupyter","hub","home","lab","notebook"]},{"location":"Scientific_Computing/Interactive_computing_using_Jupyter/Jupyter_on_NeSI/#jupyter-user-interface","title":"Jupyter user interface","text":"","tags":["jupyter","hub","home","lab","notebook"]},{"location":"Scientific_Computing/Interactive_computing_using_Jupyter/Jupyter_on_NeSI/#jupyterlab","title":"JupyterLab","text":"<p>Once your server has started you will be redirected to JupyterLab. JupyterLab is the next generation of the Jupyter user interface and provides a way to use notebooks, text editor, terminals and custom components together. If you would prefer to use the classic Notebook interface, then select \"Launch Classic Notebook\" from the JupyterLab Help menu, or change the URL from /lab to /tree once the server is running.</p>","tags":["jupyter","hub","home","lab","notebook"]},{"location":"Scientific_Computing/Interactive_computing_using_Jupyter/Jupyter_on_NeSI/#file-systems","title":"File systems","text":"<p>Your Jupyter server will start in a new directory created within your home directory for that specific Jupyter job. Within that directory, you will find symbolic links to your home directory and to the project and nobackup directories of your active projects. We do not recommend that you store files in this initial directory because next time you launch Jupyter you will be starting in a different directory, instead switch to one of your home, project or nobackup directories first.</p>","tags":["jupyter","hub","home","lab","notebook"]},{"location":"Scientific_Computing/Interactive_computing_using_Jupyter/Jupyter_on_NeSI/#jupyter-terminal","title":"Jupyter terminal","text":"<p>JupyterLab provides a terminal that can be an alternative means of gaining command line access to NeSI systems instead of using an SSH client. Some things to note are:</p> <ul> <li>when you launch the terminal application some environment modules     are already loaded, so you may want to run <code>module purge</code></li> <li>processes launched directly in the JupyterLab terminal will probably     be killed when you Jupyter session times out</li> </ul>","tags":["jupyter","hub","home","lab","notebook"]},{"location":"Scientific_Computing/Interactive_computing_using_Jupyter/Jupyter_on_NeSI/#ending-your-interactive-session-and-logging-out","title":"Ending your interactive session and logging out","text":"<p>To end a JupyterLab session, please select \"Hub Control Panel\" under the File menu then \"Stop My Server\". Finally, click on \"Log Out\".</p> <p> </p> <p>If you click \"Log Out\" without stopping your server, the server will continue to run until the Slurm job reaches its maximum wall time.</p> <p>This means that if you wish to have a session lasting, say, 4 hours (which is not offered in the \"Select walltime\" drop-down) then you can start a 8 hour session and end the job as described above when you are finished. Alternatively, you can cancel your Jupyter job by running <code>scancel 'job_id'</code> from within the Jupyter terminal when you are done. Note this will make the page unresponsive as it now has no compute powering it.</p>","tags":["jupyter","hub","home","lab","notebook"]},{"location":"Scientific_Computing/Interactive_computing_using_Jupyter/Jupyter_on_NeSI/#installing-jupyterlab-extensions","title":"Installing JupyterLab extensions","text":"<p>JupyterLab supports many extensions that enhance its functionality. At NeSI we package some extensions into the default JupyterLab environment. Keep reading if you need to install extensions yourself.</p> <p>Note, there were some changes related to extensions in JupyterLab 3.0 and there are now multiple methods to install extensions. More details about JupyterLab extensions can be found here. Check the extension's documentation to find out the supported installation method for that particular extension.</p>","tags":["jupyter","hub","home","lab","notebook"]},{"location":"Scientific_Computing/Interactive_computing_using_Jupyter/Jupyter_on_NeSI/#installing-prebuilt-extensions","title":"Installing prebuilt extensions","text":"<p>If the extension is packaged as a prebuilt extension (e.g. as a pip package), then you can install it from the JupyterLab terminal by running:</p> <pre><code>pip install --user &lt;packagename&gt;\n</code></pre> <p>For example, the Dask extension can be installed with the following:</p> <pre><code>pip install --user dask-labextension\n</code></pre>","tags":["jupyter","hub","home","lab","notebook"]},{"location":"Scientific_Computing/Interactive_computing_using_Jupyter/Jupyter_on_NeSI/#installing-source-extensions","title":"Installing source extensions","text":"<p>Installing source extensions requires a rebuild of the JupyterLab web application. Since this requires write permissions, you will need to set the JupyterLab application directory to a location that you can write to. To do this you need to create a file named ~/.jupyterlab3_dir in your home directory with the full path to your desired JupyterLab application directory and then run some commands to initialise the JupyterLab application directory.</p> <p>Running the following commands will create the JupyterLab application directory in your home directory:</p> <pre><code>module load JupyterLab\necho $HOME/.local/share/jupyter/lab &gt; ~/.jupyterlab3_dir\nexport JUPYTERLAB_DIR=$HOME/.local/share/jupyter/lab\njupyter lab build\n</code></pre> <p>These changes will only take effect after relaunching your Jupyter server and then you should be able to install JupyterLab extensions as you please.</p> <p>Note</p> <p>The above commands will put the JupyterLab application directory in  your home directory. The application directory often requires at least  1-2GB of disk space and 30,000 inodes (file count), so make sure you  have space available in your home directory first (see  NeSI File Systems and Quotas)  or request a larger quota.</p> <p>You could change the path to point to a location in your project directory, especially if multiple people on your project will share the same JupyterLab application directory, e.g.:</p> <pre><code>module load JupyterLab\necho /nesi/project/&lt;project_code&gt;/$USER/jupyter/lab &gt; ~/.jupyterlab_dir\nexport JUPYTERLAB_DIR=/nesi/project/&lt;project_code&gt;/$USER/jupyter/lab\njupyter lab build\n</code></pre>","tags":["jupyter","hub","home","lab","notebook"]},{"location":"Scientific_Computing/Interactive_computing_using_Jupyter/Jupyter_on_NeSI/#log-files","title":"Log files","text":"<p>The log file of a Jupyter server session is saved either in the project directory of the project you selected on the \"Server Options\" JupyterHub page, or in your home directory, and is named <code>.jupyterhub_&lt;username&gt;_&lt;job_id&gt;.log</code> (note the leading <code>.</code> which means the log file is hidden). If you encounter problems with your Jupyter session, the contents of this file can be a good first clue to debug the issue.</p>","tags":["jupyter","hub","home","lab","notebook"]},{"location":"Scientific_Computing/Interactive_computing_using_Jupyter/Jupyter_on_NeSI/#external-documentation","title":"External documentation","text":"<ul> <li>Jupyter</li> <li>JupyterHub</li> <li>JupyterLab</li> </ul>","tags":["jupyter","hub","home","lab","notebook"]},{"location":"Scientific_Computing/Interactive_computing_using_Jupyter/MATLAB_via_Jupyter_on_NeSI/","title":"MATLAB via Jupyter on NeSI","text":"<p>Warning</p> <p>This functionality is experimental and developing, which may introduce  breaking changes in the future.  If you would like to report a bug or propose a change see the GitHub  repo  https://github.com/nesi/jupyter-matlab-proxy  or Contact our Support Team.</p>","tags":[]},{"location":"Scientific_Computing/Interactive_computing_using_Jupyter/MATLAB_via_Jupyter_on_NeSI/#getting-started","title":"Getting started","text":"<p>MATLAB can be accessed as a web application via Jupyter on NeSI.</p> <p>In the JupyterLab interface, MATLAB can be started using the corresponding entry in the launcher.</p>","tags":[]},{"location":"Scientific_Computing/Interactive_computing_using_Jupyter/MATLAB_via_Jupyter_on_NeSI/#_1","title":"MATLAB via Jupyter on NeSI","text":"<p>Clicking on this entry will open a separate tab in your web browser, where you will see the following status information page.</p>","tags":[]},{"location":"Scientific_Computing/Interactive_computing_using_Jupyter/MATLAB_via_Jupyter_on_NeSI/#_2","title":"MATLAB via Jupyter on NeSI","text":"<p>MATLAB may take a few minutes to load, once it does you will be put straight into the MATLAB environment.\u00a0</p> <p>You can open the status page at any time by clicking the  button.</p> <p>Warning</p> <p>Your license must be valid for MATLAB 2021b or newer.</p>","tags":[]},{"location":"Scientific_Computing/Interactive_computing_using_Jupyter/MATLAB_via_Jupyter_on_NeSI/#licensing","title":"Licensing","text":"<p>If you are a member of an institution that has access to MATLAB, the corresponding network license will be selected. You can confirm this in the info panel.</p> <p>If you do not wish to use a network license you can click the 'Unset License Server Address' button.</p>","tags":[]},{"location":"Scientific_Computing/Interactive_computing_using_Jupyter/MATLAB_via_Jupyter_on_NeSI/#_3","title":"MATLAB via Jupyter on NeSI","text":"<p>If you have no licence address set you can instead authenticate using a MathWorks email address, provided you have a valid license associated to your account.</p>","tags":[]},{"location":"Scientific_Computing/Interactive_computing_using_Jupyter/MATLAB_via_Jupyter_on_NeSI/#_4","title":"MATLAB via Jupyter on NeSI","text":"","tags":[]},{"location":"Scientific_Computing/Interactive_computing_using_Jupyter/MATLAB_via_Jupyter_on_NeSI/#troubleshooting","title":"Troubleshooting","text":"<p>As MATLAB via Jupyter on NeSI uses MATLAB 2021a, you will see a glibc warning whenever you run a system command, and some system commands will not work as intended.</p> <p>For more details see MATLAB#known_bugs.</p>","tags":[]},{"location":"Scientific_Computing/Interactive_computing_using_Jupyter/RStudio_via_Jupyter_on_NeSI/","title":"RStudio via Jupyter on NeSI","text":"<p>Note</p> <p>This functionality is experimental and may introduce breaking changes  in the future. These notes should be read in conjunction with NeSI's  main R support page  Your feedback is welcome, please don't hesitate Contact our Support Team to make suggestions.</p>","tags":[]},{"location":"Scientific_Computing/Interactive_computing_using_Jupyter/RStudio_via_Jupyter_on_NeSI/#getting-started","title":"Getting started","text":"<p>RStudio can be accessed as a web application via Jupyter on NeSI.</p> <p>In the JupyterLab interface, RStudio can be started using the corresponding entry in the launcher.</p> <p></p> <p>Clicking on this entry will open a separate tab in your web browser, where RStudio will be accessible.</p> <p>Once RStudio is launched, you should briefly see a login screen. It will be auto-filled using a pre-generated password, unless you disabled javascript in your web browser.</p>","tags":[]},{"location":"Scientific_Computing/Interactive_computing_using_Jupyter/RStudio_via_Jupyter_on_NeSI/#changing-r-version","title":"Changing R version","text":"<p>You can configure a set of environment modules to preload before starting RStudio. This can be useful if you want to change the version of the R interpreter or use NeSI's R-Geo or R-bundle-Bioconductor modules.</p> <p>The module needs to be entered in the configuration file <code>~/.config/rstudio_on_nesi/prelude.bash</code>.</p> <p>In the following example, we use the module that is built for R/4.2.1</p> <pre><code>echo \"module load R/4.2.1-gimkl-2022a\" &gt; ~/.config/rstudio_on_nesi/prelude.bash\n</code></pre> <p>Once your configuration file is ready, make sure to restart your Jupyter session and re-launch RStudio for these changes to be taken into account. Check that the correct version of R has loaded and that the correct Library Paths are available. For R/4.2.1 the command <code>.libPaths()</code> will return the following:</p> <pre><code>.libPaths()\n</code></pre> <pre><code>[1] \"/home/YOUR_USER_NAME/R/gimkl-2022a/4.2\"                            \n[2] \"/opt/nesi/CS400_centos7_bdw/R/4.2.1-gimkl-2022a/lib64/R/library\"\n</code></pre>","tags":[]},{"location":"Scientific_Computing/Interactive_computing_using_Jupyter/RStudio_via_Jupyter_on_NeSI/#package-installation","title":"Package Installation","text":"<p>To avoid read/write issues with a small temorary directory filling up, in a terminal run the following two lines of code. These will setup a larger directory that will allow for packages to be installed to your personal library. NOTE: this is not creating a library.</p> <pre><code>mkdir -p /nesi/nobackup/&lt;projectID&gt;/rstudio_tmp\necho \"TMP=/nesi/nobackup/&lt;projectID&gt;/rstudio_tmp\" &gt; .Renviron\n</code></pre> <p>Within RStudio run the command `tempdir()` which should return the following (below), where `Rtmpjp2rm8` is a randomly generated folder name, and is emptied with each new session. So will not fill up your home directory.</p> <pre><code>tempdir()\n</code></pre> <pre><code>[1] \"/nesi/nobackup/&lt;projectID&gt;/rstudio_tmp/Rtmpjp2rm8\"\n</code></pre> <p>The alternative is to install packages in a terminal session</p>","tags":[]},{"location":"Scientific_Computing/Interactive_computing_using_Jupyter/RStudio_via_Jupyter_on_NeSI/#advanced-usage","title":"Advanced usage","text":"<p>RStudio runs in a Singularity container prepared by the NeSI team to run on jupyter.nesi.org.nz. The related code is hosted on GitHub, in the rstudio_on_nesi repository.</p> <p>To modify the content of the container, you need to adapt the Singularity definition file, found in the <code>conf</code> folder of the repository, and then rebuild the container.</p> <p>Once your container is ready, upload it on NeSI and use the configuration file <code>~/.config/rstudio_on_nesi/singularity_image_path</code> to indicate the path of your container to the RStudio-on-NeSI plugin:</p> <pre><code>echo PATH_TO_CONTAINER &gt; ~/.config/rstudio_on_nesi/singularity_image_path\n</code></pre> <p>Then restart your Jupyter session and launch a new RStudio session to make use of your container.</p> <p>If your RStudio session does not start, try to reload the page, in case the reported failure is just due to the container taking too much time to start.</p> <p>If this does not work, you will need to investigate the errors. A good place to start is looking at the log file from jupyter, for the current session:</p> <pre><code>cat ~/.jupyter/.jupyterhub_${USER}_${SLURM_JOB_ID}.log\n</code></pre>","tags":[]},{"location":"Scientific_Computing/Interactive_computing_using_Jupyter/RStudio_via_Jupyter_on_NeSI/#troubleshooting","title":"Troubleshooting","text":"<p>If you get an error 500 after clicking on the launcher icon, this could be due to RStudio taking too much time to start, which is interpreted as a failure by JupyterLab. Please try to start RStudio again from the launcher. If the problem persists, Contact our Support Team.</p> <p></p> <p>If you have disabled javascript in your web browser, you will need to enter your password manually in the RStudio login screen. To retrieve the password, open a terminal in JupyterLab and enter the following to print the password:</p> <pre><code>$ cat ~/.config/rstudio_on_nesi/server_password\n</code></pre>","tags":[]},{"location":"Scientific_Computing/Interactive_computing_using_Jupyter/Virtual_Desktop_via_Jupyter_on_NeSI/","title":"Virtual Desktop via Jupyter on NeSI","text":"<p>A virtual desktop provides a graphical interface to using the cluster. Desktops are hosted within Singularity containers, so not all of the NeSI software stack is supported. If you would like to build your own desktop containers with the code here.</p> <p>Rendering is done cluster-side, and compressed before being sent to your local machine. This means any rendering should be significantly more responsive than when using X11 on its own (approximately 40 times faster).</p> <p>The quickest and easiest way to get started with a desktop is through Jupyter on NeSI, connect here.</p>","tags":[]},{"location":"Scientific_Computing/Interactive_computing_using_Jupyter/Virtual_Desktop_via_Jupyter_on_NeSI/#connecting","title":"Connecting","text":"<p>Click the icon labelled 'VirtualDesktop', The desktop instance will last as long as your Jupyter session.</p>","tags":[]},{"location":"Scientific_Computing/Interactive_computing_using_Jupyter/Virtual_Desktop_via_Jupyter_on_NeSI/#customisation","title":"Customisation","text":"<p>Most of the customisation of the desktop can be done from within, panels, desktop, software preferences.</p>","tags":[]},{"location":"Scientific_Computing/Interactive_computing_using_Jupyter/Virtual_Desktop_via_Jupyter_on_NeSI/#prebash","title":"<code>pre.bash</code>","text":"<p>Enviroment set in <code>singularity_wrapper.bash</code> can be changed by creating a file <code>$XDG_CONFIG_HOME/vdt/pre.bash</code> Anything you want to run *before* launching the container put in here.</p> <pre><code>export VDT_BASE_IMAGE=\"~/my_custom_container.sif\" # Use a different image file.\nexport VDT_RUNSCRIPT=\"~/my_custom_runscript\" # Use a different runscript.\n\nexport OVERLAY=\"TRUE\"\nexport BROWSER=\"chrome\" # Desktop session will inherit this\n\nmodule load ANSYS/2021R2 # Any modules you want to be loaded in main instance go here.\n</code></pre>","tags":[]},{"location":"Scientific_Computing/Interactive_computing_using_Jupyter/Virtual_Desktop_via_Jupyter_on_NeSI/#postbash","title":"`post.bash`","text":"<p>Environment set in <code>runscript_wrapper.bash</code> can be changed by creating a file <code>$XDG_CONFIG_HOME/vdt/post.bash</code></p> <p>Things you may wish to set here are: <code>VDT_WEBSOCKOPTS</code>, <code>VDT_VNCOPTS</code>, any changes to the wm environment, any changes to path, this include module files.</p> <pre><code>export VDT_VNCOPTS=\"-depth 16\" # This will start a 16bit desktop\nexport BROWSER=\"chrome\" # Desktop session will inherit this.\n\nmodule load ANSYS/2021R2 # Any modules you want to be loaded in main instance go here.\n</code></pre>","tags":[]},{"location":"Scientific_Computing/Interactive_computing_using_Jupyter/Virtual_Desktop_via_Jupyter_on_NeSI/#custom-container","title":"Custom container","text":"<p>You can build your own container bootstrapping off <code>vdt_base.sif</code>/<code>rocky8vis.sif</code> and then overwrite the default by setting <code>VDT_BASE_IMAGE</code> in <code>pre.bash</code>.</p> <p>You can help contribute to this project\u00a0here.</p>","tags":[]},{"location":"Scientific_Computing/Manuals_and_User_Guides/Manuals/","title":"Manuals","text":"<p>The following links will provide access to reference manuals and other guides.</p> <ul> <li>Cray maintains a comprehensive technical documentation library     accessible here, providing access to     Language Reference manuals, User guides, Performance analysis tools     and Cray Applications.</li> <li>Cray Fortran v8.7,     Cray C and C++ v8.7</li> <li>Intel     C/C++     and     Fortran,     Intel Parallel Studio XE Cluster Edition,\u00a0Intel     Developer     Guides</li> <li>Allinea     Forge     (includes DDT and MAP, now called Arm Forge)</li> <li>Nvidia Documentation</li> <li>cuda-gdb debugger</li> <li>cuda-memcheck memory     checker.</li> <li>GCC Manuals</li> </ul>","tags":[]},{"location":"Scientific_Computing/Manuals_and_User_Guides/Troubleshooting_on_NeSI/","title":"Troubleshooting on NeSI","text":"","tags":["webinar","video"]},{"location":"Scientific_Computing/Manuals_and_User_Guides/XC50_Aries_Network_Architecture/","title":"XC50 Aries Network Architecture","text":"<p>There are 4 dual socket nodes on blade, connected to a single Aries (switch) chip, and there are 16 Aries chips in a chassis connected to the backplane. On M\u0101ui, this implies each chassis contains 64 nodes, or 2,560 Skylake cores. There are 3 chassis in an XC50 cabinet, and two XC50 cabinets are an Electrical \"group\". M\u0101ui has 1.5 groups.</p> <p></p> <p>The performance characteristics are:</p> <ol> <li>Intra-Chassis<ol> <li>Backplane</li> <li>15 links in the backplane</li> <li>Rank 1 (green) Network</li> <li>14 Gbps</li> </ol> </li> <li>Intra-group<ol> <li>Copper cables</li> <li>15 links in 5 connectors</li> <li>Rank 2 (black) Network</li> <li>14 Gbps</li> </ol> </li> <li>Inter-group links<ol> <li>Optical</li> <li>10 links in 5 connectors</li> <li>Rank 3 (blue) Network</li> <li>12.5 Gbps</li> </ol> </li> </ol> <p>The centrepiece of the Aries network is dynamic routing through a large variety of different routes from Aries A to Aries B. Therewith the effective bandwidth is increased significantly. These dynamic routing on alternative paths is applied on all 3 levels of the network.</p>","tags":["asadasdad"]},{"location":"Scientific_Computing/Profiling_and_Debugging/Debugging/","title":"Debugging","text":"<p>There are many reasons why a program might crash. Some Slurm job states such as <code>TIMEOUT</code> or <code>OUT_OF_MEMORY</code> can indicate a clear reason, but when the job state is simply FAILED and the error message in the job's log file simply states \"Segmentation Fault\" or \"Illegal instruction\", investigations become more complicated. In the following, a selection of tools is presented which might assist. Most of the following approaches require the application being built with debugging symbols (e.g. <code>-g</code> compile option), otherwise only nameless memory address will be provided.</p>","tags":[]},{"location":"Scientific_Computing/Profiling_and_Debugging/Debugging/#tracing-job-scripts","title":"Tracing job scripts","text":"<p>Complex job scripts may contain several commands. To located the failing executable and keep track of performed settings and operations the option <code>set -x</code> (in bash) may help. It lists line by line called commands together with the related output.</p>","tags":[]},{"location":"Scientific_Computing/Profiling_and_Debugging/Debugging/#analysing-core-files-with-gdb","title":"Analysing core files with GDB","text":"<p>In some cases a memory dump at the time of crash can be generated by the operating system. These core files are usually provided one for each process. The GNU debugger (<code>gdb</code>) can extract information from them. For example, a stack trace could be obtained by running the following command for ONE core file:</p> <pre><code>gdb -c core.12345 /path/to/bin/exe\n</code></pre> <pre><code>bt\n</code></pre> <p>This assumes that the crashing job used the executable <code>/path/to/bin/exe</code>, which should be built with debugging symbols. This may already point you to the failing location. With the command <code>bt</code> you can request a stack trace.</p> <p>On Mahuika the default maximum core file size is zero, so no core files are produced. \u00a0To enable them do:</p> <pre><code>ulimit -S -c unlimited\n</code></pre> <p>For more detailed information see the GDB Manual.</p>","tags":[]},{"location":"Scientific_Computing/Profiling_and_Debugging/Debugging/#arm-ddt","title":"ARM DDT","text":"<p>The parallel debugger DDT is a commercial product from ARM (formerly Allinea) and can handle various kinds of applications, serial, parallel, compiled from different kind of sources (C, C++, Fortran, Python). To work properly DDT needs to have debug symbols provided by the application binary (compiled with e.g. <code>-g</code>\u00a0 option). DDT can be used using the module <code>forge</code>. There are basically 2 ways to use the debugger, interactive using the GUI and on the command line (bash script) using the so called \"offline\" mode.</p>","tags":[]},{"location":"Scientific_Computing/Profiling_and_Debugging/Debugging/#ddt-offline-mode","title":"DDT offline mode","text":"<p>You can start and configure DDT directly on the command line in your job scripts without a GUI. Which is useful especially if you have long lasting jobs to debug or long queuing times. To use this so called \"offline mode\" you just need to add <code>ddt --offline</code> in front of the srun statement. You can add more arguments for example to print the values of variables.</p> <pre><code>ddt --offline --break-at=fail.c:14 --evaluate=\"k;n\" srun -n 4 &lt;application&gt; &lt;arguments&gt;\n</code></pre> <p>As a result some basic information, stack traces and more requested information are provided into the application stdout and a HTML file is created. Thus this could also be a handy alternative for print statements without touching the code.</p> <p></p> <p>See full example page here.</p> <p>For more detailed information see DDT manual.</p>","tags":[]},{"location":"Scientific_Computing/Profiling_and_Debugging/Debugging/#ddt-graphical-user-interface","title":"DDT graphical user interface","text":"<p>The DDT GUI can be opened using:</p> <pre><code>module load forge\nddt\n</code></pre> <p>Note</p> <p>You can also install forge locally and connect to the machine remotely.</p> <p>In the start window you can select between attaching DDT to a running application (ATTACH), open an existing core file (OPEN CORE) and launching an application with DDT (RUN).</p> <p>In the RUN menu the different settings for the executable need to be specified.</p> <p></p> <p>Beside Application location and name, we need to specify arguments, working directory, MPI and OpenMP settings. If we have no interactive slurm session, we do need to specify the workload manager settings in the \"Submit to queue\" section. For your first time, you net to open the Configure menu and select in the \"Job submission\" tab the <code>nesi_slurm.ptf</code> template file. You can add necessary Slurm parameters there, e.g. hyperthreading options, accounts and QoS. In the Environment Variables section you can load necessary modules.</p> <p>After submitting the task, DDT launches the application (wait for the workload manager if necessary) and opens the following window.</p> <p></p> <p>In the top part the processes and threads can be selected. The application is paused at the initialization phase, giving the user the opportunity to set break/watch points, and define the type execution (in/over/out of functions or just until next break point). For more detailed information see the DDT manual</p>","tags":[]},{"location":"Scientific_Computing/Profiling_and_Debugging/Debugging/#atp-cray-abnormal-termination-processing","title":"ATP (Cray Abnormal Termination Processing)","text":"<p>Warning</p> <p>This tool is only available on M\u0101ui.</p> <p>Abnormal Termination Processing (ATP) is a system that monitors Cray XC System (Maui) user applications, and should an application take a system trap, ATP preforms analysis on the dying application. All of the stack backtraces of the application processes are gathered into a merged stack back trace tree and written to disk as the file <code>atpMergedBT.dot</code>. The stack back trace for the first process to die is sent to stderr as is the number of the signal that caused the death. If the core file size limit (<code>RLIMIT_CORE</code>) is non-zero, a heuristically selected set of processes dump their core.</p> <p>An example output looks like:</p> <pre><code>Application 427046 is crashing. ATP analysis proceeding...\n\nATP Stack walkback for Rank 0 starting:\n _start@start.S:118\n __libc_start_main@libc-start.c:289\n main@fail.c:65\n m_routine@fail.c:38\n calculation@fail.c:31\n do_task@fail.c:25\nATP Stack walkback for Rank 0 done\nProcess died with signal 8: 'Floating point exception'\nForcing core dumps of ranks 0, 1\nView application merged backtrace tree with: stat-view atpMergedBT.dot\nYou may need to: module load stat\n</code></pre>","tags":[]},{"location":"Scientific_Computing/Profiling_and_Debugging/Profiler-ARM_MAP/","title":"Profiler: ARM MAP","text":"<p>The ARM (previously known as Allinea) provides a package called forge, which consists of a debugger, DDT and a profiler, MAP.</p>","tags":[]},{"location":"Scientific_Computing/Profiling_and_Debugging/Profiler-ARM_MAP/#introduction-to-profiling","title":"Introduction to profiling","text":"<p>Profiling tools help you understand how much resources are consumed during run time. This can be time, memory, or MPI communication. One main goal is to understand in which parts of your code most time is spent. Depending on the profiler and the applied methods, profiles can be gathered on basis of functions, loops within functions, or source code lines. Profiling information is important for optimising code, as it enables you to focus your efforts on improving the parts of the code that will result in the biggest gains in performance.</p>","tags":[]},{"location":"Scientific_Computing/Profiling_and_Debugging/Profiler-ARM_MAP/#profiling-test-cases","title":"Profiling test cases","text":"<p>During a optimisation process profiling will be used regularly to monitor the behaviour and change in behaviour of the code. Therefore, it is advisable to have a representative but reasonable short test case. It should trigger all the desired features of the code. But keep in mind that with a reduced run time (e.g. fewer iterations) other parts of the code could become more dominant (e.g. initialisation phase). Furthermore, due to possible overhead from the profiling tool, the code could run slower than normal.</p>","tags":[]},{"location":"Scientific_Computing/Profiling_and_Debugging/Profiler-ARM_MAP/#map-profiler","title":"MAP profiler","text":"<p>MAP is a commercial product, which can profile parallel, multi-threaded and single-threaded C/C++, Fortran, as well as Python code. MAP supports codes with OpenMP threads and/or MPI communication and can be used without code modification.</p> <p>MAP can be launched with a graphical user interface (GUI Launch) or without GUI (Express Launch). With the GUI the user specifies all the parameters including executables, options and parallelisation parameters in a guided form, while with the Express Launch a pre-existing script is modified. The \u201cExpress Launch\u201d, is preferable especially for jobs in complex scripts and workflows.</p> <p>In either case, the analysis of the profiling data will be undertaken in the MAP GUI. This conveniently provides access to different metrics and allows the user to navigate through different levels of details, browsing through the code, and focusing on specific functions, loops and source code lines.\u00a0 See section MAP profile below.</p> <p>There is also an Arm Forge Client you can download to your local machine. Therewith you can browse through your downloaded profiling data or even start your profiling/debug tasks from remote (not described in detail here).</p> <p>Note</p> <p>If you want to use the GUI on the NeSI systems, please remember to start your ssh session using X11 forwarding (e.g. using the <code>ssh -Y</code> option).</p>","tags":[]},{"location":"Scientific_Computing/Profiling_and_Debugging/Profiler-ARM_MAP/#map-express-launch","title":"MAP \u201cExpress Launch\u201d","text":"<p>To use MAP we need to load the forge module in our batch script and add <code>map --profile</code> in front of the parallel run statements. For example:</p> <p>Upon execution, a <code>.map</code>\u00a0file will be generated. The results can be viewed, for instance, with</p> <p>(the <code>.map</code> file name will vary with each run.) See section MAP profile below for how to interpret the results.</p>","tags":[]},{"location":"Scientific_Computing/Profiling_and_Debugging/Profiler-ARM_MAP/#map-gui-launch","title":"MAP GUI launch","text":"<p>The GUI can be started after loading <code>module load forge</code> and launching</p> <p></p> <p>Click on \u201cPROFILE\u201d.</p> <p></p> <p>In the profile menu we need to specify the executable/application (in this case <code>python</code>), the arguments (here <code>scatter.py</code> and any additional options and arguments if necessary). Select the working directory, number of MPI processes, and OpenMP threads. Furthermore, the \u201csubmit to queue\u201d parameter needs to be checked, for example the <code>--hint=nomultithread</code> can be specified there. In the environment Variables block, e.g. modules could be loaded or variables defined.</p> <p>After submitting, MAP will wait until the job is allocated, connect to the processes, run the program, gather all the data and present the profile information.</p>","tags":[]},{"location":"Scientific_Computing/Profiling_and_Debugging/Profiler-ARM_MAP/#map-profile","title":"MAP Profile","text":"<p>By default the profile window is divided into the following three main sections (click on picture to enlarge).</p> <p></p> <p>On top, various metrics can be selected in the \u201cMetrics\u201d menu. In the middle part, a source code navigator connects line by line source code with its profiling data. Most interesting is the profiling table at the bottom, which sorts the most time consuming parts of the program, providing function names, source code and line numbers.</p> <p>There are various options to get even more information.</p> <p>In the top part the metrics graphs can be changed to:</p> <ul> <li>Activity timeline</li> <li>CPU instructions</li> <li>CPU Time</li> <li>IO</li> <li>Memory</li> <li>MPI</li> </ul> <p>using the Metrics Menu.</p> <p>As an example, \u201cCPU instructions\u201d presents the usage of different instruction sets during the program run time.</p> <p></p> <p>The lower part can also be used to check the application output or show statistics on basis of files or functions.</p> <p>Note</p> <p>Often you can get additional information while hovering with your mouse over a certain item.</p>","tags":[]},{"location":"Scientific_Computing/Profiling_and_Debugging/Profiler-VTune/","title":"Profiler: VTune","text":"","tags":[]},{"location":"Scientific_Computing/Profiling_and_Debugging/Profiler-VTune/#what-is-vtune","title":"What is VTune?","text":"<p>VTune is a performance analysis tool. It can be used to identify and analyse various aspects in both serial and parallel programs and can be used for both OpenMP and MPI applications. It can be used with a command line interface (CLI) or a graphical user interface (GUI).  </p>","tags":[]},{"location":"Scientific_Computing/Profiling_and_Debugging/Profiler-VTune/#where-to-find-more-resources-on-vtune","title":"Where to find more resources on VTune?","text":"<ul> <li>Main page is at     VTune Profiler</li> <li>Tutorials are available at VTune Tutorials.</li> </ul>","tags":[]},{"location":"Scientific_Computing/Profiling_and_Debugging/Profiler-VTune/#how-to-use-vtune","title":"How to use VTune?","text":"<p>VTune is available on Mahuika by loading the VTune module.</p> <pre><code>module spider VTune\n</code></pre> <pre><code>Versions:\nVTune/2019_update4\nVTune/2019_update8\n</code></pre> <p>First login to Mahuika and then run:</p> <pre><code>module load VTune\n</code></pre> <p>To use the VTune amplifier command line tool help run:</p> <pre><code>amplxe-cl -help\n</code></pre>","tags":[]},{"location":"Scientific_Computing/Profiling_and_Debugging/Profiler-VTune/#how-do-i-profile-an-application-with-vtune","title":"How do I profile an application with VTune?","text":"<p>The hot-spot analysis is the most commonly used analysis and generally the first approach to optimizing an application.</p> <ul> <li>Example on Mahuika with the matrix sample.     The matrix sample is composed of a pre-built matrix in C++ for     matrix multiplication.</li> </ul> <pre><code>module load VTune/2019_update8\ncp -r /opt/nesi/mahuika/VTune/2019_update8/samples_2019/en/vtune_amplifier/C++/matrix .\ncd matrix\namplxe-cl -collect hotspots ./matrix\n</code></pre> <p>The <code>amplxe-cl</code> command collects hotspots data.</p> <p>The option <code>-collect</code> specifies the collection experiment to run. The option <code>hotspots</code> is to collect basic hotspots to have a general performance overview.  </p> <p>This is the type of output you are going to get:</p> <pre><code>amplxe-cl -collect hotspots ./matrix\n</code></pre> <pre><code>amplxe: Collection started.\nTo stop the collection, either press CTRL-C or enter from another console window: amplxe-cl -r /scale_wlg_persistent/filesets/home/asav179/o/matrix/r000hs -command stop.\nAddr of buf1 = 0x2aaab8e08010\nOffs of buf1 = 0x2aaab8e08180\nAddr of buf2 = 0x2aaabae09010\nOffs of buf2 = 0x2aaabae091c0\nAddr of buf3 = 0x2aaabce0a010\nOffs of buf3 = 0x2aaabce0a100\nAddr of buf4 = 0x2aaabee0b010\nOffs of buf4 = 0x2aaabee0b140\nThreads #: 16 Pthreads\nMatrix size: 2048\nUsing multiply kernel: multiply1\nFreq = 2.101000 GHz\nExecution time = 2.515 seconds\namplxe: Collection stopped.\namplxe: Using result path `/scale_wlg_persistent/filesets/home/asav179/o/matrix/r000hs'\namplxe: Executing actions 19 % Resolving information for `libpthread.so.0'\namplxe: Warning: Cannot locate debugging information for file `/lib64/libpthread.so.0'.\namplxe: Executing actions 19 % Resolving information for `matrix'\namplxe: Warning: Cannot locate debugging information for file `/lib64/libc.so.6'.\namplxe: Executing actions 75 % Generating a report Elapsed Time: 2.552s\nCPU Time: 36.440s\nEffective Time: 36.440s\nIdle: 0s\nPoor: 36.440s\nOk: 0s\nIdeal: 0s\nOver: 0s\nSpin Time: 0s\nOverhead Time: 0s\nTotal Thread Count: 17\nPaused Time: 0s\nTop Hotspots\nFunction Module CPU Time\n--------- ------ --------\nmultiply1 matrix 36.420s\ninit_arr matrix 0.010s\ninit_arr matrix 0.010s\nEffective Physical Core Utilization: 85.4% (30.750 out of 36)\nEffective Logical Core Utilization: 46.1% (33.194 out of 72)\n| The metric value is low, which may signal a poor utilization of logical\n| CPU cores while the utilization of physical cores is acceptable. Consider\n| using logical cores, which in some cases can improve processor throughput\n| and overall performance of multi-threaded applications.\nCollection and Platform Info\nApplication Command Line: ./matrix\nOperating System: 3.10.0-693.2.2.el7.x86_64 NAME=\"CentOS Linux\" VERSION=\"7 (Core)\" ID=\"centos\" ID_LIKE=\"rhel fedora\" VERSION_ID=\"7\" PRETTY_NAME=\"CentOS Linux 7 (Core)\" ANSI_COLOR=\"0;31\" CPE_NAME=\"cpe:/o:centos:centos:7\" HOME_URL=\"https://www.centos.org/\" BUG_REPORT_URL=\"https://bugs.centos.org/\" CENTOS_MANTISBT_PROJECT=\"CentOS-7\" CENTOS_MANTISBT_PROJECT_VERSION=\"7\" REDHAT_SUPPORT_PRODUCT=\"centos\" REDHAT_SUPPORT_PRODUCT_VERSION=\"7\"\nComputer Name: mahuika01\nResult Size: 3 MB\nCollection start time: 05:35:42 15/09/2021 UTC\nCollection stop time: 05:35:45 15/09/2021 UTC\nCollector Type: Event-based counting driver,User-mode sampling and tracing\nCPU\nName: Intel(R) Xeon(R) Processor code named Broadwell\nFrequency: 2.095 GHz\nLogical CPU Count: 72\namplxe: Executing actions 100 % done\n</code></pre> <p>The output one receives the overall elapsed and idle times as well as the CPU times of the individual functions in descending order (list of hotspots). The utilization of the CPUs is also analyzed and judged.</p> <p>Note</p> <p>You can run the VTune GUI via Jupyter+VDT to visualise profiling results.</p> <ol> <li>First launch VDT then open a terminal and cd to directory.</li> <li>Then run:</li> </ol> <pre><code>module load VTune\namplxe-gui --path-to-open &lt;vtune_result_dir&gt;\n</code></pre>","tags":[]},{"location":"Scientific_Computing/Profiling_and_Debugging/Slurm_Native_Profiling/","title":"Slurm Native Profiling","text":"<p>Job resource usage can be determined on job completion by checking the following sacct columns;</p> <ul> <li>MaxRSS - Peak memory usage.</li> <li>TotalCPU - Check\u00a0Elapsed x Alloc\u2248TotalCPU</li> </ul> <p>However if you want to examine resource usage over the run-time of your job, the line\u00a0<code>#SBATCH --profile task</code> can be added to your script.</p> <p>That will cause profile data to be recorded every 30 seconds throughout the job. For jobs which take much less/more than a day to run we recommend increasing/decreasing that sampling frequency, so for example when profiling a job of less than 1 hour it would be OK to sample every second by adding <code>#SBATCH --acctg-freq=1</code>, and for a week long job the rate should be reduced to once every 5 minutes:\u00a0<code>#SBATCH --acctg-freq=300</code>.  </p> <p>On completion of your job, collate the data into an HDF5 file using <code>sh5util -j &lt;jobid&gt;</code>, this will collect the results from the nodes where your job ran and write into an HDF5 file named: <code>job_&lt;jobid&gt;.h5</code></p> <p>You can plot the contents of this file with the command <code>nn_profile_plot job_&lt;jobid&gt;.h5</code>, this will generate a file named <code>job_&lt;jobid&gt;_profile.png</code>.</p> <p>Alternatively you could use one of the following scripts.</p> <ul> <li>Python</li> <li>MATLAB</li> </ul> <p>Any GPU usage will also be recorded in the profile, so long as the process was executed via srun.</p>","tags":["slurm","profiling"]},{"location":"Scientific_Computing/Release_Notes_jupyter-nesi-org-nz/jupyter-nesi-org-nz_release_notes_02-02-2023/","title":"jupyter.nesi.org.nz release notes 02/02/2023","text":"","tags":["releasenote"]},{"location":"Scientific_Computing/Release_Notes_jupyter-nesi-org-nz/jupyter-nesi-org-nz_release_notes_02-02-2023/#new-and-improved","title":"New and Improved","text":"<ul> <li>Updated JupyterHub to v2.3.1</li> <li>Updated JupyterLab to v3.5.3</li> <li>Switched to Python 3.10 for running JupyterLab (kernels are     unaffected)<ul> <li>Note: if you have previously installed Python packages in your     home directory using Python 3.10, we recommend cleaning out your     ~/.local/Python-3.10-gimkl-2022a directory, as it could     conflict with our JupyterLab installation, and consider     Installing packages in a Python virtual     environment     instead</li> </ul> </li> </ul>","tags":["releasenote"]},{"location":"Scientific_Computing/Release_Notes_jupyter-nesi-org-nz/jupyter-nesi-org-nz_release_notes_02-06-2022/","title":"jupyter.nesi.org.nz release notes 02/06/2022","text":"","tags":["releasenote"]},{"location":"Scientific_Computing/Release_Notes_jupyter-nesi-org-nz/jupyter-nesi-org-nz_release_notes_02-06-2022/#release-update-2-june-2022","title":"Release Update - 2. June 2022","text":"","tags":["releasenote"]},{"location":"Scientific_Computing/Release_Notes_jupyter-nesi-org-nz/jupyter-nesi-org-nz_release_notes_02-06-2022/#new-and-improved","title":"New and Improved","text":"<ul> <li>Updated JupyterLab version to v3.4.2</li> <li>Updated RStudio-on-NeSI (v0.22.5): fix library path when using NeSI R package in RStudio (e.g. R-bundle-Bioconductor)</li> <li>Plotly extension re-added (missing in the previous release)</li> <li>Added papermill extension</li> <li>Updated NeSI Virtual Desktop to v2.4.1<ul> <li>Image changes<ul> <li>Update default Firefox version.</li> <li>Update to use singularity 3.8.5.</li> <li>Switched to rocky8 image.</li> <li>Added chrome, strace, sview and xfce-terminal to image.</li> <li>Added some libraries need for ANSYS</li> <li>Added missing GLX libraries.</li> </ul> </li> <li>Bug fixes<ul> <li>Fixed faulty startup messages</li> <li>Fixed entrypoint duplication issue.</li> <li>unset <code>SLURM_EXPORT_ENV</code> before starting desktop.</li> </ul> </li> <li>Refactoring<ul> <li>Removed dependency on system vdt repo.</li> <li>Removed faulty &amp; unneeded bind paths.</li> <li>Removed debug by default and hardcoded verbose.</li> <li>replaced <code>VDT_HOME</code> with XDG equiv</li> </ul> </li> </ul> </li> </ul>","tags":["releasenote"]},{"location":"Scientific_Computing/Release_Notes_jupyter-nesi-org-nz/jupyter-nesi-org-nz_release_notes_02-11-2021/","title":"jupyter.nesi.org.nz release notes 02/11/2021","text":"","tags":["releasenote"]},{"location":"Scientific_Computing/Release_Notes_jupyter-nesi-org-nz/jupyter-nesi-org-nz_release_notes_02-11-2021/#release-update-02-november-2021","title":"Release Update - 02. November 2021","text":"","tags":["releasenote"]},{"location":"Scientific_Computing/Release_Notes_jupyter-nesi-org-nz/jupyter-nesi-org-nz_release_notes_02-11-2021/#new-and-improved","title":"New and Improved","text":"<ul> <li>Enabled jupyter server proxy to forward requests to a different host     (compute node).</li> </ul>","tags":["releasenote"]},{"location":"Scientific_Computing/Release_Notes_jupyter-nesi-org-nz/jupyter-nesi-org-nz_release_notes_12-05-2021/","title":"jupyter.nesi.org.nz release notes 12/05/2021","text":"","tags":["releasenote"]},{"location":"Scientific_Computing/Release_Notes_jupyter-nesi-org-nz/jupyter-nesi-org-nz_release_notes_12-05-2021/#release-update-12-may-2021","title":"Release Update - 12. May 2021","text":"","tags":["releasenote"]},{"location":"Scientific_Computing/Release_Notes_jupyter-nesi-org-nz/jupyter-nesi-org-nz_release_notes_12-05-2021/#new-and-improved","title":"New and Improved","text":"<ul> <li>JupyterLab upgrade to v3.0.15.     Read more on user-facing     changes     and the installation of extensions here:\u00a0     https://jupyterlab.readthedocs.io/en/stable/user/extensions.html</li> </ul>","tags":["releasenote"]},{"location":"Scientific_Computing/Release_Notes_jupyter-nesi-org-nz/jupyter-nesi-org-nz_release_notes_12-07-2022/","title":"jupyter.nesi.org.nz release notes 12/07/2022","text":"","tags":["releasenote"]},{"location":"Scientific_Computing/Release_Notes_jupyter-nesi-org-nz/jupyter-nesi-org-nz_release_notes_12-07-2022/#new-and-improved","title":"New and Improved","text":"<ul> <li>Added the\u00a0<code>pyviz_comms</code>\u00a0package to allow fully interactive usage of     HoloViz tools within notebooks\u00a0(in     particular Panel and HoloViews).</li> </ul>","tags":["releasenote"]},{"location":"Scientific_Computing/Release_Notes_jupyter-nesi-org-nz/jupyter-nesi-org-nz_release_notes_14-10-2021/","title":"jupyter.nesi.org.nz release notes 14/10/2021","text":"","tags":["releasenote"]},{"location":"Scientific_Computing/Release_Notes_jupyter-nesi-org-nz/jupyter-nesi-org-nz_release_notes_14-10-2021/#release-update-14-october-2021","title":"Release Update - 14. October 2021","text":"","tags":["releasenote"]},{"location":"Scientific_Computing/Release_Notes_jupyter-nesi-org-nz/jupyter-nesi-org-nz_release_notes_14-10-2021/#new-and-improved","title":"New and Improved","text":"<ul> <li>Changed hub session timeout to 16 hours. Users will be prompted to     login again after 16 hrs. aligned with max. wall time for JupyterLab     instances.\u00a0</li> <li>JupyterHub fixed: improvements to avoid 403 errors</li> </ul>","tags":["releasenote"]},{"location":"Scientific_Computing/Release_Notes_jupyter-nesi-org-nz/jupyter-nesi-org-nz_release_notes_14-11-2023/","title":"jupyter.nesi.org.nz release notes 14/11/2023","text":"","tags":[]},{"location":"Scientific_Computing/Release_Notes_jupyter-nesi-org-nz/jupyter-nesi-org-nz_release_notes_14-11-2023/#new-and-improved","title":"New and Improved","text":"<ul> <li>Adding extra logging when the Jupyter Health Check fails</li> </ul>","tags":[]},{"location":"Scientific_Computing/Release_Notes_jupyter-nesi-org-nz/jupyter-nesi-org-nz_release_notes_14-11-2023/#fixed","title":"Fixed","text":"<ul> <li>We are now closing user session when the corresponding Jupyter     server is stopped, to avoid idle sessions to linger on the host</li> </ul> <p>If you have any questions about any of the improvements or fixes, please  Contact our Support Team.</p>","tags":[]},{"location":"Scientific_Computing/Release_Notes_jupyter-nesi-org-nz/jupyter-nesi-org-nz_release_notes_15-06-2023/","title":"jupyter.nesi.org.nz release notes 15/06/2023","text":"","tags":["releasenote"]},{"location":"Scientific_Computing/Release_Notes_jupyter-nesi-org-nz/jupyter-nesi-org-nz_release_notes_15-06-2023/#new-and-improved","title":"New and Improved","text":"<ul> <li>If\u00a0jupyter.nesi.org.nz\u00a0portal cannot     connect to the NeSI server, a descriptive error message will be     displayed instead of internal error 500</li> </ul>","tags":["releasenote"]},{"location":"Scientific_Computing/Release_Notes_jupyter-nesi-org-nz/jupyter-nesi-org-nz_release_notes_15-06-2023/#fixed","title":"Fixed","text":"<ul> <li>Update to ignore the low level variable SRUN_CPUS_PER_TASK</li> </ul>","tags":["releasenote"]},{"location":"Scientific_Computing/Release_Notes_jupyter-nesi-org-nz/jupyter-nesi-org-nz_release_notes_16-09-2021/","title":"jupyter.nesi.org.nz release notes 16/09/2021","text":"","tags":["releasenote"]},{"location":"Scientific_Computing/Release_Notes_jupyter-nesi-org-nz/jupyter-nesi-org-nz_release_notes_16-09-2021/#release-update-16-september-2021","title":"Release Update - 16. September 2021","text":"","tags":["releasenote"]},{"location":"Scientific_Computing/Release_Notes_jupyter-nesi-org-nz/jupyter-nesi-org-nz_release_notes_16-09-2021/#new-and-improved","title":"New and Improved","text":"<ul> <li>JupyterLab upgrade to v3.1.9 (Python updated from v3.8 to v3.9)     Read more on changes and bug     fixes</li> <li>Updated to JupyterHub 1.4.2</li> <li>Rendering time remaining, CPU and Memory usage in the top menu bar </li> <li>Confirmed JupyterLab extension for version control using Git     working     See https://pypi.org/project/jupyterlab-git/</li> </ul>","tags":["releasenote"]},{"location":"Scientific_Computing/Release_Notes_jupyter-nesi-org-nz/jupyter-nesi-org-nz_release_notes_19-05-2023/","title":"jupyter.nesi.org.nz release notes 19/05/2023","text":"","tags":["releasenote"]},{"location":"Scientific_Computing/Release_Notes_jupyter-nesi-org-nz/jupyter-nesi-org-nz_release_notes_19-05-2023/#fixed","title":"Fixed","text":"<ul> <li>Updated some Python packages in the Python 3.10 kernel to fix an     issue with ipywidgets not working properly in notebooks</li> </ul>","tags":["releasenote"]},{"location":"Scientific_Computing/Release_Notes_jupyter-nesi-org-nz/jupyter-nesi-org-nz_release_notes_22-11-2023/","title":"jupyter.nesi.org.nz release notes 22/11/2023","text":"","tags":["releasenote"]},{"location":"Scientific_Computing/Release_Notes_jupyter-nesi-org-nz/jupyter-nesi-org-nz_release_notes_22-11-2023/#fixed","title":"Fixed","text":"<ul> <li>We are now closing user sessions when the corresponding Jupyter     server is stopped, to avoid idle sessions to linger on the host. We     missed one case during the last release.</li> </ul> <p>If you have any questions about any of the improvements or fixes, please Contact our Support Team.</p>","tags":["releasenote"]},{"location":"Scientific_Computing/Release_Notes_jupyter-nesi-org-nz/jupyter-nesi-org-nz_release_notes_24-09-2021/","title":"jupyter.nesi.org.nz release notes 24/09/2021","text":"","tags":["releasenote"]},{"location":"Scientific_Computing/Release_Notes_jupyter-nesi-org-nz/jupyter-nesi-org-nz_release_notes_24-09-2021/#release-update-24-september-2021","title":"Release Update - 24. September 2021","text":"","tags":["releasenote"]},{"location":"Scientific_Computing/Release_Notes_jupyter-nesi-org-nz/jupyter-nesi-org-nz_release_notes_24-09-2021/#new-and-improved","title":"New and Improved","text":"<ul> <li>Fixed Singularity version for RStudio and VirtualDesktop kernels</li> <li>Fixed pywidgets installation</li> <li>JupyterHub fixed: in case a job takes more than 300 seconds, don't     start the job to avoid 'ghost' instances of JupyterLab</li> </ul>","tags":["releasenote"]},{"location":"Scientific_Computing/Release_Notes_jupyter-nesi-org-nz/jupyter-nesi-org-nz_release_notes_25-08-2022/","title":"jupyter.nesi.org.nz release notes 25/08/2022","text":"","tags":["releasenote"]},{"location":"Scientific_Computing/Release_Notes_jupyter-nesi-org-nz/jupyter-nesi-org-nz_release_notes_25-08-2022/#new-and-improved","title":"New and Improved","text":"<ul> <li>Updated\u00a0RStudio-on-NeSI     to v0.24.0</li> <li>RStudio server v2022.07.1</li> <li>Allow usage of NeSI environment modules in RStudio terminal (beta)</li> <li>Allow usage of Slurm commands in RStudio terminal (beta)</li> <li>Updated\u00a0NeSI Virtual     Desktop     to v2.4.3  </li> <li>Utilising latest version of         Singularity </li> </ul>","tags":["releasenote"]},{"location":"Scientific_Computing/Release_Notes_jupyter-nesi-org-nz/jupyter-nesi-org-nz_release_notes_25-08-2022/#fixed","title":"Fixed","text":"<ul> <li>RStudio</li> <li>Addressed issue preventing user installation of rmarkdown when using R/4.1.0-gimkl-2020a</li> <li>Addressed knitr PDF compilation when using R/4.2.1-gimkl-2022a</li> <li>NeSI Virtual Desktop</li> <li>Added dependencies to fix OpenGL related issues</li> <li>Internal refactoring for maintenance purpose of the permission         with skeleton files in container build</li> </ul>","tags":["releasenote"]},{"location":"Scientific_Computing/Release_Notes_jupyter-nesi-org-nz/jupyter-nesi-org-nz_release_notes_28-06-2022/","title":"jupyter.nesi.org.nz release notes 28/06/2022","text":"","tags":["releasenote"]},{"location":"Scientific_Computing/Release_Notes_jupyter-nesi-org-nz/jupyter-nesi-org-nz_release_notes_28-06-2022/#release-update-28-june-2022","title":"Release Update - 28. June 2022","text":"","tags":["releasenote"]},{"location":"Scientific_Computing/Release_Notes_jupyter-nesi-org-nz/jupyter-nesi-org-nz_release_notes_28-06-2022/#new-and-improved","title":"New and Improved","text":"<ul> <li>Updated JupyterLab version to v3.4.3</li> </ul>","tags":["releasenote"]},{"location":"Scientific_Computing/Release_Notes_jupyter-nesi-org-nz/jupyter-nesi-org-nz_release_notes_28-06-2022/#fixed","title":"Fixed","text":"<ul> <li>Addressed issue handling the \"slurm job id\" with some Python modules that depend on MPI</li> </ul>","tags":["releasenote"]},{"location":"Scientific_Computing/Release_Notes_jupyter-nesi-org-nz/jupyter-nesi-org-nz_release_notes_31-03-2022/","title":"jupyter.nesi.org.nz release notes 31/03/2022","text":"","tags":["releasenote"]},{"location":"Scientific_Computing/Release_Notes_jupyter-nesi-org-nz/jupyter-nesi-org-nz_release_notes_31-03-2022/#release-update-31-march-2022","title":"Release Update - 31. March 2022","text":"","tags":["releasenote"]},{"location":"Scientific_Computing/Release_Notes_jupyter-nesi-org-nz/jupyter-nesi-org-nz_release_notes_31-03-2022/#new-and-improved","title":"New and Improved","text":"<ul> <li>Updated JupyterLab version     to\u00a0<code>JupyterLab/.2022.2.0-gimkl-2020a-3.2.8</code></li> <li>Added user guidance on options (when launching a server instance)</li> <li>Updated available GPU options</li> <li>Added links to NeSI documentation</li> </ul>","tags":["releasenote"]},{"location":"Scientific_Computing/Running_Jobs_on_Maui_and_Mahuika/Checking_your_projects_usage_using_nn_corehour_usage/","title":"Checking your project's usage using nn_corehour_usage","text":"<p>Warning</p> <p>This page has been automatically migrated and may contain formatting errors.</p> <p>To check your project's usage of Slurm-managed resources, you can use the command <code>nn_corehour_usage</code>. This command displays usage of cluster resources by a specific project, computed from the Slurm program <code>sreport</code>.</p>","tags":[]},{"location":"Scientific_Computing/Running_Jobs_on_Maui_and_Mahuika/Checking_your_projects_usage_using_nn_corehour_usage/#synopsis","title":"Synopsis","text":"<pre><code>nn_corehour_usage [OPTIONS...] PROJECT_CODE...\n</code></pre>","tags":[]},{"location":"Scientific_Computing/Running_Jobs_on_Maui_and_Mahuika/Checking_your_projects_usage_using_nn_corehour_usage/#description","title":"Description","text":"<p><code>nn_corehour_usage</code> shows a month-by-month breakdown of how the specified project or projects have used Slurm resources. Some resources, like disk space, are not managed by Slurm and so are excluded. Included resources are CPU time, RAM time (also known as memory time) and GPU time.</p>","tags":[]},{"location":"Scientific_Computing/Running_Jobs_on_Maui_and_Mahuika/Checking_your_projects_usage_using_nn_corehour_usage/#options","title":"Options","text":"<p><code>-c</code>, <code>--calendar-months</code></p> <p>Break usage down so that the time periods are the first and last days of the calendar months, instead of working back a month at a time from today.</p> <p><code>-n</code>, <code>--number-of-months=NUM</code></p> <p>Show <code>NUM</code> months back from today. Used in conjunction with <code>-c</code>, will show <code>NUM</code> complete months plus the partial month up to when the command is run. Default is 12 months. The results will not go further back than when the cluster commenced operations.</p> <p><code>-u</code>, <code>--user=USERNAME</code></p> <p>Display results for the user <code>USERNAME</code>. The default user is the current user.</p> <p>Treat all subsequent entries on the command line, including those starting with a dash (<code>-</code>), as arguments instead of as options.</p>","tags":[]},{"location":"Scientific_Computing/Running_Jobs_on_Maui_and_Mahuika/Checking_your_projects_usage_using_nn_corehour_usage/#examples","title":"Examples","text":"<p>To print the last year of project nesi12345:</p> <pre><code>nn_corehour_usage nesi12345\n</code></pre> <p>To print the last six complete calendar months of project nesi12345:</p> <pre><code>nn_corehour_usage -c -n 6 nesi12345\n</code></pre>","tags":[]},{"location":"Scientific_Computing/Running_Jobs_on_Maui_and_Mahuika/Checksums/","title":"Checksums","text":"<p>Warning</p> <p>This page has been automatically migrated and may contain formatting errors.</p> <p>Applying a checksum function to a file will return its checksum. Checksum functions are a type of hash function, and will always return the same hash for any particular file contents, making them useful for file validation. There are many different checksum functions. The most commonly used are listed in the table below.</p> <p>Checksums can be used to check for minor errors that may have been introduced into a dataset. For example:</p> <ul> <li>After downloading a file (compare your generated checksum with the     checksum provided by the vendor).</li> <li>When copying a file onto the cluster (generate a checksum on your     local machine and another on the cluster).</li> <li>Verifying your results/workflow. (making a checksum of a results     file can be a quick way to confirm nothing has changed).</li> <li>Corroborate files when working in a team.</li> </ul> <p>While not necessary to do in every case, every time, file integrity should be one of the first things you check when troubleshooting.</p>","tags":[]},{"location":"Scientific_Computing/Running_Jobs_on_Maui_and_Mahuika/Checksums/#example","title":"Example","text":"<p>The file '<code>corrupt.bin</code>' has had 1 byte changed, yet on inspection would appear identical.\u00a0</p> <pre><code>-rw-rw-r--\u00a0 1\u00a0 393315\u00a0 copy.bin\n-rw-rw-r--\u00a0 1\u00a0 393315\u00a0 corrupt.bin\n-rw-rw-r--\u00a0 1\u00a0 393315\u00a0 original.bin\n</code></pre> <p>By using a MD5 checksum (<code>md5sum *</code>) we can see that '<code>corrupt.bin</code>' has diverged from the original, while '<code>copy.bin</code>' has not.</p> <pre><code>002c33835b3921d92d8074f3b392ef65 copy.bin\nef749eb4110c2a3b3c747390095d0b76 corrupt.bin\n002c33835b3921d92d8074f3b392ef65 original.bin\n</code></pre> <p>Note that filename, path, permissions or any other metadata does not affect the checksum.</p> <p>Prerequisite</p> <p>Checksum functions are designed so that similar files will not  produce similar hashes.  You will only need to compare a few characters of the string to  confirm validity.</p>","tags":[]},{"location":"Scientific_Computing/Running_Jobs_on_Maui_and_Mahuika/Checksums/#commands","title":"Commands","text":"<p>The checksum for file 'filename.txt' can be found with the following commands.</p> Linux Windows(CMD/Powershell) Mac SH1 <code>sha1sum</code><code>filename.txt</code> <code>certUtil -hashfile</code><code>filename.txt</code> <code>shasum</code><code>filename.txt</code> SHA256 <code>sha256sum</code><code>filename.txt</code> <code>certUtil -hashfile</code><code>filename.txt</code><code>sha256</code> <code>shasum -a 256</code><code>filename.txt</code> MD5 <code>md5sum</code><code>filename.txt</code> <code>certUtil -hashfile</code><code>filename.txt</code><code>md5</code> <code>md5</code><code>filename.txt</code>","tags":[]},{"location":"Scientific_Computing/Running_Jobs_on_Maui_and_Mahuika/Fair_Share/","title":"Fair Share","text":"<p>Warning</p> <p>This page has been automatically migrated and may contain formatting errors.</p> <p>The fair-share system is designed to encourage users to balance their use of resources over their allocation period. Fair-share is the largest factor in determining priority, but not the only one. For more details see Job Prioritisation.</p>","tags":[]},{"location":"Scientific_Computing/Running_Jobs_on_Maui_and_Mahuika/Fair_Share/#fair-share-score","title":"Fair Share Score","text":"<p>Your Fair Share score is a number between 0 and 1. Projects with a larger Fair Share score receive a higher priority in the queue.</p> <p>A project is given an\u00a0allocation\u00a0of compute units over a given\u00a0period. An institution also has a percentage Fair Share\u00a0entitlement of each machine's deliverable capacity over that same period.</p> <p>Prerequisite</p> <p>Although we use the term \"Fair Share entitlement\" in this article, it  bears only a loose relationship to an institution's contractual  entitlement to receive allocations from the NeSI HPC Compute &amp;  Analytics service. The Fair Share entitlement is managed separately  for each cluster, and is adjusted as needed by NeSI staff so that each  institution can receive, as nearly as possible, its contractual  entitlement to the service as a whole, as well as a mix of cluster  hours that corresponds closely to the needs of that institution's  various project teams.</p> <ul> <li>Your project's expected rate of use\u00a0= (your\u00a0institution's Fair     Share entitlement\u00a0\u00d7 your project's\u00a0allocation)\u00a0/ (sum of     your institution's allocations\u00a0\u00d7\u00a0period)</li> <li>Your institution's expected rate of use = your institution's     Fair Share\u00a0entitlement on that machine</li> </ul> <p>If an entity\u00a0\u2014 an institution or project team\u00a0\u2014 is using the machine more slowly than expected, for Fair Share purposes it is considered a light user. By contrast, one using the machine faster than expected is a heavy user.</p> <ul> <li>Projects at lightly using institutions get a higher Fair Share score     than those at heavily using institutions.</li> <li>Within each institution, lightly using projects get a higher Fair     Share score than heavily using projects.</li> <li>Using faster than your\u00a0expected rate of usage\u00a0will usually     cause your Fair Share score to\u00a0decrease. The more extreme the     overuse, the more severe the likely drop.</li> <li>Using slower than your\u00a0expected rate of usage\u00a0will usually     cause your Fair Share score to\u00a0increase. The more extreme the     underuse, the greater the Fair Share bonus.</li> <li>Using the cluster unevenly will cause your Fair Share score to     decrease.</li> </ul>","tags":[]},{"location":"Scientific_Computing/Running_Jobs_on_Maui_and_Mahuika/Fair_Share/#what-is-fair-share","title":"What is Fair Share?","text":"<p>Fair Share is a mechanism to set job priorities. It is based on a share of the cluster, that is, a fraction of the cluster's overall computing capacity.</p>","tags":[]},{"location":"Scientific_Computing/Running_Jobs_on_Maui_and_Mahuika/Fair_Share/#fair-share-on-mahuika-and-the-maui-xc-nodes","title":"Fair Share on Mahuika and the M\u0101ui XC nodes","text":"<p>On Mahuika and the M\u0101ui XC nodes, but not on the M\u0101ui ancillary nodes, we set a project's expected rate of use based on that project's percentage share of all then-current allocations awarded to that project's institution on that cluster. This percentage share is in turn derived from the sizes (in compute units or nodes) and durations (in days)\u00a0\u2014 and thus the expected rates of use of those same allocations.</p> <p>Therefore:</p> <ul> <li>If the size of your allocation increases, your project's share of     the cluster will increase. Conversely, if the size of your     allocation decreases, your project's share of the cluster will     decrease.</li> <li>If the size of another project's allocation increases, your     project's share of the cluster will decrease, since, even though     your allocation's size has remained the same, the total size of     other allocations has increased and thus your allocation's share has     decreased. Conversely, if the size of the other project's allocation     decreases, your project's share of the cluster will increase.</li> <li>If the cluster gets larger (e.g. we purchase and install more     computing capacity), your project's share of the cluster will not     change, but that share of the cluster will correspond to a higher     rate of core hour usage. This situation will only last until more     allocations are issued, or existing allocations are made larger, to     take advantage of the increased capacity. The opposite will occur if     the cluster shrinks, though cluster shrinkage is not expected to     occur.</li> </ul> <p>On Mahuika and the M\u0101ui XC nodes, Fair Share is not designed to ensure that all project teams get the same share of the cluster.</p>","tags":[]},{"location":"Scientific_Computing/Running_Jobs_on_Maui_and_Mahuika/Fair_Share/#fair-share-on-the-maui-ancillary-nodes","title":"Fair Share on the M\u0101ui ancillary nodes","text":"<p> The part of the M\u0101ui ancillary nodes that is managed by NeSI and scheduled using Slurm forms a very small resource, only four nodes of 40 CPU cores each. It is intended for pre- and post-processing work related to computational jobs carried out on the M\u0101ui XC nodes. Therefore, we do not make allocations of CPU core hours on these nodes. Instead, each project team that has a current allocation on the M\u0101ui XC nodes is entitled to an equal share of the time on these four M\u0101ui ancillary nodes. </p> <p> Because job priority on the M\u0101ui ancillary nodes is still heavily influenced by Fair Share, project teams that have recently been doing a lot of work on the M\u0101ui ancillary nodes will find their jobs there deprioritised, so that other project teams can access the resource. However, even heavy users of the M\u0101ui ancillary nodes can still access resources there if those CPU cores would otherwise be idle. </p>","tags":[]},{"location":"Scientific_Computing/Running_Jobs_on_Maui_and_Mahuika/Fair_Share/#how-does-fair-share-work","title":"How does Fair Share work?","text":"<p>The starting point for a Fair Share calculation is a comparison of the project's actual share of use to the expected share of use. This share of use is based on what all users of the cluster have actually used during the relevant period of time, not what the cluster was capable of delivering during that same period. Currently, each period is five minutes.</p> <p>Because five minutes is a very short time, Fair Share aggregates the ratio of actual share to expected share since records began on that cluster. But as the time gets further back from the present, each five-minute window has slightly less influence on fair share scores. Our current configuration has it that after two weeks (that is, 4,032 successive five-minute windows), the effect of the ratio for that five-minute slice is worth only half of what it was worth initially; after four weeks, it is worth a quarter; after six weeks, one eighth; and so on. The effect of this decay curve is that overuse or underuse in the recent past has a greater effect on your project's fair share score than the same extent of overuse or underuse long ago.</p> <p>One important implication of Fair Share is that allocations are implicitly aged: you cannot bank core hours by refraining from submitting work. If, for example, you expect to have a lot of computational work to carry out in September, you can't get a significant priority boost in September by refraining from carrying out computational work in March. In fact, you will get the best advantage from Fair Share by submitting work at close to a constant rate.</p> <p>If you expect that your project team will need widely varying rates of computer use during your allocation period and you can predict when your busy and quiet periods will be, please Contact our Support Team\u00a0to enquire about splitting your project's allocation up into parts. Please be aware that we cannot guarantee this option will be available for any given project, and that we are most likely to be able to accommodate such a request for projects that expect to use the cluster heavily on average, can predict when they will need their heaviest use with a high degree of confidence, and give us plenty of notice.</p> <p>For full details on Slurm's Fair share mechanism, please see this page (offsite).</p>","tags":[]},{"location":"Scientific_Computing/Running_Jobs_on_Maui_and_Mahuika/Fair_Share/#how-do-i-check-my-projects-fair-share-score","title":"How do I check my project's Fair Share score?","text":"<ul> <li>The command <code>nn_corehour_usage &lt;project_code&gt;</code>, on a Mahuika or M\u0101ui     login node, will show, along with other information, the current     fair share score and ranking of the specified project.</li> <li>The <code>sshare</code> command, on a Mahuika login node, will show the fair     share tree. A related command, <code>nn_sshare_sorted</code>, will show     projects in order from the highest fair share score to the lowest.</li> </ul> <p>In our current configuration, Fair Share scores are attached to projects, not to individual users.</p>","tags":[]},{"location":"Scientific_Computing/Running_Jobs_on_Maui_and_Mahuika/Fair_Share/#my-projects-fair-share-score-is-too-low-how-can-i-improve-it","title":"My project's Fair Share score is too low. How can I improve it?","text":"<p>If you have just carried out an unusually large spike of work, your fair share score will naturally be lowered for a while, and should come back to normal after a few days.</p> <p>If, on the other hand, you have more work to do than expected, please [contact us to apply for a larger allocation Contact our Support Team. Project teams may request a larger allocation on Mahuika or on the M\u0101ui XC cluster, though not on the M\u0101ui ancillary nodes.</p> <p>If you believe your project's fair share score has become corrupted, or your ability to get work done is affected by a low Fair Share entitlement for your institution on that cluster, please [get in touch with our support team Contact our Support Team.</p>","tags":[]},{"location":"Scientific_Computing/Running_Jobs_on_Maui_and_Mahuika/GPU_use_on_NeSI/","title":"GPU use on NeSI","text":"<p>Warning</p> <p>This page has been automatically migrated and may contain formatting errors.</p> <p>This page provides generic information about how to access NeSI's GPU cards.</p> <p>For application specific settings (e.g. OpenMP, Tensorflow on GPU, ...), please have a look at the dedicated pages listed at the end of this page.</p> <p>Prerequisite</p> <p>An overview of available GPU cards is available in the Available GPUs  on  NeSI  support page.  Details about GPU cards for each system and usage limits are in the  Mahuika Slurm  Partitions  and M\u0101ui_Ancil (CS500) Slurm  Partitions  support pages.  Details about pricing in terms of compute units can be found in the  What is an  allocation?  page.</p>","tags":["gpu"]},{"location":"Scientific_Computing/Running_Jobs_on_Maui_and_Mahuika/GPU_use_on_NeSI/#request-gpu-resources-using-slurm","title":"Request GPU resources using Slurm","text":"<p>To request a GPU for your Slurm job, add the following option at the beginning of your submission script:</p> <pre><code>#SBATCH --gpus-per-node=1\n</code></pre> <p>You can specify the type and number of GPU you need using the following syntax</p> <pre><code>#SBATCH --gpus-per-node=&lt;gpu_type&gt;:&lt;gpu_number&gt;\n</code></pre> <p>If not specified, the default GPU type is <code>P100</code>. For some types of GPU, you also need to specify a partition. Here is a summary of typical use cases:</p> <ul> <li> <p>1 P100 GPU on Mahuika</p> <pre><code>#SBATCH --gpus-per-node=P100:1\n</code></pre> </li> <li> <p>1 P100 GPU on M\u0101ui Ancillary Nodes</p> <pre><code>#SBATCH --partition=nesi_gpu\n#SBATCH --gpus-per-node=1\n</code></pre> </li> <li> <p>2 P100 GPUs per node on Mahuika</p> <pre><code>#SBATCH --gpus-per-node=P100:2\n</code></pre> <p>You cannot ask for more than 2 P100 GPU per node on Mahuika.</p> </li> <li> <p>1 A100 (40GB) GPU on Mahuika</p> <pre><code>#SBATCH --gpus-per-node=A100:1\n</code></pre> </li> <li> <p>2 A100 (40GB) GPUs on Mahuika</p> <pre><code>#SBATCH --gpus-per-node=A100:2\n</code></pre> <p>You cannot ask for more than 2 A100 (40GB) GPUs per node on Mahuika.</p> </li> <li> <p>1 A100-1g.5gb GPU on Mahuika</p> <pre><code>#SBATCH --gpus-per-node=A100-1g.5gb:1\n</code></pre> <p>This type of GPU is limited to 1 job per user and recommended for development and debugging.</p> </li> <li> <p>1 A100 (80GB) GPU on Mahuika</p> <pre><code>#SBATCH --partition=hgx\n#SBATCH --gpus-per-node=A100:1\n</code></pre> <p>These GPUs are on Milan nodes, check the dedicated support page for more information.</p> </li> <li> <p>4 A100 (80GB &amp; NVLink) GPU on Mahuika</p> <pre><code>#SBATCH --partition=hgx\n#SBATCH --gpus-per-node=A100:4\n</code></pre> <p>These GPUs are on Milan nodes, check the dedicated support page for more information.</p> <p>You cannot ask for more than 4 A100 (80GB) GPUs per node on Mahuika.</p> </li> <li> <p>1 A100 GPU on Mahuika, regardless of the type</p> <pre><code>#SBATCH --partition=gpu,hgx\n#SBATCH --gpus-per-node=A100:1\n</code></pre> <p>With this configuration, your job will spend less time in the queue, using whichever A100 GPU is available. It may land on a regular Mahuika node (A100 40GB GPU) or on a Milan node (A100 80GB GPU).</p> </li> </ul> <p>You can also use the <code>--gpus-per-node</code>option in Slurm interactive sessions, with the <code>srun</code> and <code>salloc</code> commands. For example:</p> <pre><code>srun --job-name \"InteractiveGPU\" --gpus-per-node 1 --cpus-per-task 8 --mem 2GB --time 00:30:00 --pty bash\n</code></pre> <p>will request and then start a bash session with access to a GPU, for a duration of 30 minutes.</p> <p>Prerequisite</p> <p>When you use the <code>--gpus-per-node</code>option, Slurm automatically sets the  <code>CUDA_VISIBLE_DEVICES</code> environment variable inside your job  environment to list the index/es of the allocated GPU card/s on each  node.  <pre><code>$ srun --job-name \"GPUTest\" --gpus-per-node=P100:2 --time 00:05:00 --pty bash\nsrun: job 20015016 queued and waiting for resources\nsrun: job 20015016 has been allocated resources\n$ echo $CUDA_VISIBLE_DEVICES\n0,1\n</code></pre></p>","tags":["gpu"]},{"location":"Scientific_Computing/Running_Jobs_on_Maui_and_Mahuika/GPU_use_on_NeSI/#load-cuda-and-cudnn-modules","title":"Load CUDA and cuDNN modules","text":"<p>To use an Nvidia GPU card with your application, you need to load the driver and the CUDA toolkit via the environment modules mechanism:</p> <pre><code>module load CUDA/11.0.2\n</code></pre> <p>You can list the available versions using:</p> <pre><code>module spider CUDA\n</code></pre> <p>Please contact us at support@nesi.org.nz if you need a version not available on the platform.</p> <p>Prerequisite</p> <p>On M\u0101ui Ancillary Nodes, use <code>module avail CUDA</code> to list available  versions.</p> <p>The CUDA module also provides access to additional command line tools:</p> <ul> <li> <ul> <li> <ul> <li>nvidia-smi             to directly monitor GPU resource utilisation,         -   nvcc             to compile CUDA programs,         -   cuda-gdb             to debug CUDA applications.</li> </ul> </li> </ul> </li> </ul> <p>In addition, the cuDNN (NVIDIA CUDA\u00ae Deep Neural Network library) library is accessible via its dedicated module:</p> <pre><code>module load cuDNN/8.0.2.39-CUDA-11.0.2\n</code></pre> <p>which will automatically load the related CUDA version. Available versions can be listed using:</p> <pre><code>module spider cuDNN\n</code></pre>","tags":["gpu"]},{"location":"Scientific_Computing/Running_Jobs_on_Maui_and_Mahuika/GPU_use_on_NeSI/#example-slurm-script","title":"Example Slurm script","text":"<p>The following Slurm script illustrates a minimal example to request a GPU card, load the CUDA toolkit and query some information about the GPU:</p> <pre><code>#!/bin/bash -e\n#SBATCH --job-name=GPUJob   # job name (shows up in the queue)\n#SBATCH --time=00-00:10:00  # Walltime (DD-HH:MM:SS)\n#SBATCH --gpus-per-node=1   # GPU resources required per node\n#SBATCH --cpus-per-task=2   # number of CPUs per task (1 by default)\n#SBATCH --mem=512MB         # amount of memory per node (1 by default)\n\n# load CUDA module\nmodule purge\nmodule load CUDA/11.0.2\n\n# display information about the available GPUs\nnvidia-smi\n\n# check the value of the CUDA_VISIBLE_DEVICES variable\necho \"CUDA_VISIBLE_DEVICES=${CUDA_VISIBLE_DEVICES}\"\n</code></pre> <p>Save this in a <code>test_gpu.sl</code> file and submit it using:</p> <pre><code>sbatch test_gpu.sl\n</code></pre> <p>The content of job output file would look like:</p> <pre><code>$ cat slurm-20016124.out\n\nThe following modules were not unloaded:\n   (Use \"module --force purge\" to unload all):\n\n  1) slurm   2) NeSI\nWed May 12 12:08:27 2021\n+-----------------------------------------------------------------------------+\n| NVIDIA-SMI 460.32.03    Driver Version: 460.32.03    CUDA Version: 11.2     |\n|-------------------------------+----------------------+----------------------+\n| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n|                               |                      |               MIG M. |\n|===============================+======================+======================|\n|   0  Tesla P100-PCIE...  On   | 00000000:05:00.0 Off |                    0 |\n| N/A   29C    P0    23W / 250W |      0MiB / 12198MiB |      0%      Default |\n|                               |                      |                  N/A |\n+-------------------------------+----------------------+----------------------+\n\n+-----------------------------------------------------------------------------+\n| Processes:                                                                  |\n|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n|        ID   ID                                                   Usage      |\n|=============================================================================|\n|  No running processes found                                                 |\n+-----------------------------------------------------------------------------+\nCUDA_VISIBLE_DEVICES=0\n</code></pre> <p>Prerequisite</p> <p>CUDA_VISIBLE_DEVICES=0 indicates that this job was allocated to CUDA  GPU index 0 on this node. It is not a count of allocated GPUs.</p>","tags":["gpu"]},{"location":"Scientific_Computing/Running_Jobs_on_Maui_and_Mahuika/GPU_use_on_NeSI/#nvidia-nsight-systems-and-compute-profilers","title":"NVIDIA Nsight Systems and Compute profilers","text":"<p>Nsight Systems is a system-wide analysis tool, particularly good for profiling CPU-GPU interactions. It is provided on Mahuika via the <code>Nsight-Systems</code> module:</p> <pre><code>$ module load Nsight-Systems/2020.5.1\nLoad `PyQt/5.12.1-gimkl-2020a-Python-3.8.2` module prior to running `nsys-ui`\n$ nsys --version\nNVIDIA Nsight Systems version 2020.5.1.85-5ee086b\n</code></pre> <p>This module gives you access to the nsys command line tool or the nsys-ui graphical interface.</p> <p>Nsight Compute is a profiler for CUDA kernels. It is accessible on Mahuika using the <code>Nsight-Compute</code> module:</p> <pre><code>$ module load Nsight-Compute/2020.3.0\nLoad `PyQt/5.12.1-gimkl-2020a-Python-3.8.2` module prior to running `nsys-ui`\n$ ncu --version\nNVIDIA (R) Nsight Compute Command Line Profiler\nCopyright (c) 2018-2020 NVIDIA Corporation\nVersion 2020.3.0.0 (build 29307467) (public-release)\n</code></pre> <p>Then you can use the ncu command line tool or the ncu-ui graphical interface.</p> <p>Prerequisite</p> <p>The <code>nsys-ui</code> and <code>ncu-ui</code> tools require access to a display server,  either via  X11 or a  Virtual  Desktop.  You also need to load the <code>PyQt</code> module beforehand:  <pre><code>module load PyQt/5.12.1-gimkl-2020a-Python-3.8.2\nmodule load Nsight-Systems/2020.5.1\nnsys-ui  # this will work only if you have a graphical session\n</code></pre></p>","tags":["gpu"]},{"location":"Scientific_Computing/Running_Jobs_on_Maui_and_Mahuika/GPU_use_on_NeSI/#application-and-toolbox-specific-support-pages","title":"Application and toolbox specific support pages","text":"<p>The following pages provide additional information for supported applications:</p> <ul> <li>ABAQUS</li> <li>GROMACS</li> <li>Lambda     Stack</li> <li>Matlab</li> <li>TensorFlow on     GPUs</li> </ul> <p>And programming toolkits:</p> <ul> <li>Offloading to GPU with     OpenMP</li> <li>Offloading to GPU with OpenACC using the Cray     compiler</li> <li>NVIDIA GPU     Containers</li> </ul>","tags":["gpu"]},{"location":"Scientific_Computing/Running_Jobs_on_Maui_and_Mahuika/Hyperthreading/","title":"Hyperthreading","text":"<p>Warning</p> <p>This page has been automatically migrated and may contain formatting errors.</p> <p>As CPU technology advanced engineers realised that adapting CPU architecture to include logical processors within the physical core (conventionally, a CPU) allows some computation to occur simultaneously. The name for this technology is simultaneous multithreading, and Intel's implementation of it is called Hyperthreading.</p> <p>CPUs capable of Hyperthreading consists of two logical processors per physical core. The logical processors can operate on data/instruction threads\u00a0simultaneously, meaning the physical core\u00a0can perform two operations concurrently. In other words, the difference between logical and physical cores is that logical cores are not full stand-alone CPUs, and share some hardware with nearby logical cores. Physical cores are made up of two logical cores.</p> <p>Hyperthreading is enabled by default on NeSI machines, meaning, by default, Slurm will allocate two threads to each physical core.\u00a0</p>","tags":[]},{"location":"Scientific_Computing/Running_Jobs_on_Maui_and_Mahuika/Hyperthreading/#hyperthreading-with-slurm","title":"Hyperthreading with slurm","text":"<p>When Slurm request a CPU, it is requesting logical cores, which, as mentioned above, there are two of per physical core.\u00a0If you use <code>--ntasks=n</code> to request CPUs, Slurm will start <code>n</code> MPI tasks which are each assigned to one physical core. Since Slurm \"sees\" logical cores, once your job starts you will have twice the number of CPUs as <code>ntasks</code>.</p> <p>If you set <code>--cpus-per-task=n</code>, Slurm will request <code>n</code> logical CPUs per task, i.e., will set <code>n</code> threads for the job. Your code must be capable of running Hyperthreaded (for example using OpenMP) if <code>--cpus-per-task &gt; 1</code>.</p> <p>Setting <code>--hint=nomultithread</code> with srun or sbatch \"causes Slurm to allocate only one thread from each core to this job\". This will allocate CPUs according to the following image:</p> Node name wbn009 Physical Core id 0 1 2 3 0 1 2 3 Logical CPU id 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 Number of Allocated CPUs 4 4 Allocated CPU ids 0 2 4 6 8 10 12 14 <p>Image adapted from Slurm's documentation page.</p>","tags":[]},{"location":"Scientific_Computing/Running_Jobs_on_Maui_and_Mahuika/Hyperthreading/#when-to-use-hyperthreading","title":"When to use Hyperthreading","text":"<p>Hyperthreading increases the efficiency of some jobs, but the fact that Slurm is counting in logical CPUs makes aspects of running non-Hyperthreaded jobs confusing, even when Hyperthreading is turned off in the job with <code>--hint=nomultithread</code>. To determine if the code you are running is capable of running Hyperthreaded, visit the manual pages for the software.</p> <p>Alternatively, it is possible to perform an ad-hoc test to determine if your code is capable of making use of Hyperthreading. First run a job that has requested 2 threads per physical core as described above. Then, use the <code>nn_seff</code> command to check the jobs CPU efficiency. If CPU efficiency is greater than 100%, then your code is making use of Hyperthreading, and gaining performance from it. If your job gives an error or stays at 100% efficiency, it is likely you can not run your code Hyperthreaded. 200% CPU efficiency would be the maximally efficient job, however, this is rarely observed and anything over 100% should be considered a bonus.</p>","tags":[]},{"location":"Scientific_Computing/Running_Jobs_on_Maui_and_Mahuika/Hyperthreading/#how-to-use-hyperthreading","title":"How to use Hyperthreading","text":"<ul> <li>Non-hyperthreaded jobs which use \u00a0<code>--mem-per-cpu</code> requests should     halve their memory requests as those are based on memory per logical     CPU, not per the number of threads or tasks. \u00a0For non-MPI jobs, or     for MPI jobs that request the same number of tasks on every node, we     recommend to specify\u00a0<code>--mem</code> (i.e. memory per node) instead. See     How to request memory     (RAM) for more     information.</li> <li>Non-MPI jobs which specify <code>--cpus-per-task</code> and use srun should     also set <code>--ntasks=1</code>, otherwise the program will be run twice in     parallel, halving the efficiency of the job.</li> </ul> <p>The precise rules about when Hyperthreading applies are as follows:</p> <p>Mahuika                  M\u0101ui</p> <p>Jobs                    Never share physical                              cores                    </p> <p>MPI tasks within the    Never share physical     Share physical cores by   same job                cores                    default. You can                                                    override this behaviour                                                    by using                                                    <code>--hint=nomultithread</code>                                                    in your job submission                                                    script.</p> <p>Threads within the same Share physical cores by   task                    default. You can                                  override this behaviour                           by using                <code>--hint=nomultithread</code>                            in your job submission                            script.                  </p>","tags":[]},{"location":"Scientific_Computing/Running_Jobs_on_Maui_and_Mahuika/Hyperthreading/#how-many-logical-cpus-will-my-job-use-or-be-charged-for","title":"How many logical CPUs will my job use or be charged for?","text":"<p>The possible job configurations and their results are shown in the following table. We have also included some recommendations to help you make the best choices, depending on the needs of your workflow.</p> Job configuration Mahuika M\u0101ui <ul> <li>Only one task</li> <li><code>--cpus-per-task</code> is not used</li> </ul> The job gets, and is charged for, two logical CPUs.\u00a0<code>--hint=nomultithread</code> is irrelevant. <p>The job gets one logical CPU, but is charged for 80. <code>--hint=nomultithread</code> is irrelevant.</p> <p>This configuration is extremely uneconomical on M\u0101ui. Consider using Mahuika or the M\u0101ui ancillary nodes instead.</p> <ul> <li>Only one task</li> <li><code>--cpus-per-task=</code>N</li> <li><code>--hint=nomultithread</code> is not used</li> </ul> <p>The job gets, and is charged for, N logical CPUs, rounded up to the nearest even number.</p> <p>Set N to an even number if possible.</p> <p>The job gets N logical CPUs, but is charged for 80.</p> <p>Set N to 80 if possible.</p> <ul> <li>Only one task</li> <li><code>--cpus-per-task=</code>N</li> <li><code>--hint=nomultithread</code> is used</li> </ul> The job gets, and is charged for, 2N logical CPUs. <p>The job gets 2N logical CPUs, but is charged for 80.</p> <p>Set N to 40 if possible.</p> <ul> <li>More than one task on one or more nodes</li> <li><code>--cpus-per-task</code> is not used</li> <li><code>--hint=nomultithread</code> is not used</li> </ul> <p>Each task gets two logical CPUs. The job is charged for two logical CPUs per task. <code>--hint=nomultithread</code> is irrelevant.</p> <p> </p> <p>Each task gets one logical CPU. The job is charged for 80 logical CPUs per allocated node.</p> <p>If possible, set the number of tasks per node to 80.</p> <ul> <li>More than one task on one or more nodes</li> <li><code>--cpus-per-task</code> is not used</li> <li><code>--hint=nomultithread</code> is used</li> </ul> <p>Each task gets two logical CPUs. The job is charged for 80 logical CPUs per allocated node.</p> <p>If possible, set the number of tasks per node to 40. </p> <ul> <li>More than one task on one or more nodes</li> <li><code>--cpus-per-task=</code>N</li> <li><code>--hint=nomultithread</code> is not used</li> </ul> <p>Each task gets N logical CPUs, rounded up to the nearest even number. The job is charged for that number of logical CPUs per task.</p> <p>Set N to an even number if possible.</p> <p>Each task gets N logical CPUs. The job is charged for 80 logical CPUs per allocated node.</p> <p>If possible, set N and the number of tasks per node such that N\u00a0\u00d7 (tasks per node) = 80.</p> <ul> <li>More than one task on one or more nodes</li> <li><code>--cpus-per-task=</code>N</li> <li><code>--hint=nomultithread</code> is used</li> </ul> Each task gets 2N logical CPUs. The job is charged for 2N logical CPUs per task. <p>Each task gets 2N logical CPUs. The job is charged for 80 logical CPUs per allocated node.</p> <p>If possible, set N and the number of tasks per node such that N\u00a0\u00d7 (tasks per node) = 40.</p>","tags":[]},{"location":"Scientific_Computing/Running_Jobs_on_Maui_and_Mahuika/Job_Checkpointing/","title":"Job Checkpointing","text":"<p>Warning</p> <p>This page has been automatically migrated and may contain formatting errors.</p> <p>Job/Application Checkpointing\u00a0is the snapshotting of a programs state, so that it can be restarted from that point in case of failure. This is especially important in long running jobs.</p> <p>How checkpointing can be implemented depends on the application/code being used, some will have inbuilt methods whereas others might require some scripting.</p>","tags":[]},{"location":"Scientific_Computing/Running_Jobs_on_Maui_and_Mahuika/Job_Checkpointing/#queueing","title":"Queueing","text":"<p>Checkpointing code has the added advantage that it allows you to split your work into smaller jobs, allowing them to move through the queue faster.\u00a0</p> <p>Below is an example of submitting the same job again, if previous has run successfully.</p> <pre><code># Slurm header '#SBATCH etc etc\n\nsbatch --dependency=afterok:${SLURM_JOB_ID} \"$0\" \n# \"$0\" is equal to the name of this script.\n\n#\u00a0Code\u00a0that\u00a0implements\u00a0checkpointing\n</code></pre> <p>This job will resubmit itself forever until stopped.</p> <p>Another example for a job requiring explicit step inputs.</p> <pre><code># Slurm header '#SBATCH etc etc\n\nn_steps=1000\nstarting_step=${1:-0} # Will be equal to first argument, or '0' if unset.\nending_step=$(( starting_step + n_steps )) \n\n# Submit next step with starting step equal to ending step of this job.\nsbatch --dependency=afterok:${SLURM_JOB_ID} \"$0\" ${ending_step}\n\nmy-program --nfirst ${starting_step} --nlast ${ending_step}\n</code></pre> <p>The use of <code>--dependency</code> has the advantage of adding the next job to the queue before starting, saving queue time in between jobs.</p>","tags":[]},{"location":"Scientific_Computing/Running_Jobs_on_Maui_and_Mahuika/Job_Checkpointing/#examples","title":"Examples","text":"","tags":[]},{"location":"Scientific_Computing/Running_Jobs_on_Maui_and_Mahuika/Job_Checkpointing/#matlab","title":"Matlab","text":"<pre><code>% If checkpoint file, load from there.\ncheckpoint='checkpoint_2020-03-09T0916.mat';\nif exist(checkpoint,'file')==2, load(checkpoint);startindex=i;else startindex=1;end\n\nfor i = startindex:100\n    % Long running process\n\n    % Save workspace at end of each loop.\n    save(['checkpoint_', datestr(now, 'yyyy-mm-ddTHHMM')])\nend\n</code></pre> <p>Prerequisite</p> <p>We strongly recommend implementing checkpointing on any job  running longer than 3 days!</p>","tags":[]},{"location":"Scientific_Computing/Running_Jobs_on_Maui_and_Mahuika/Job_prioritisation/","title":"Job prioritisation","text":"<p>Warning</p> <p>This page has been automatically migrated and may contain formatting errors.</p> <p>Each queued job has a priority score. \u00a0Jobs start when sufficient resources (CPUs, GPUs, memory, licenses) are available and not already reserved for jobs with a higher priority.</p> <p>To see the priorities of your currently pending jobs you can use the command <code>sprio -u $USER</code>.</p> <p>Priority scores are determined by a number of factors:</p>","tags":[]},{"location":"Scientific_Computing/Running_Jobs_on_Maui_and_Mahuika/Job_prioritisation/#1-quality-of-service","title":"1) Quality of Service","text":"<p>The \"debug\" Quality of Service can be gained by adding the <code>sbatch</code> command line option <code>--qos=debug</code>. This adds 5000 to the job priority so raises it above all non-debug jobs, but is limited to one small job per user at a time: no more than 15 minutes and no more than 2 nodes.</p>","tags":[]},{"location":"Scientific_Computing/Running_Jobs_on_Maui_and_Mahuika/Job_prioritisation/#2-fair-share","title":"2) Fair Share","text":"<p>Job priority decreases whenever the project uses more core-hours than expected, across all partitions. This Fair Share policy means that projects that have consumed many CPU core hours in the recent past compared to their expected rate of use (either by submitting and running many jobs, or by submitting and running large jobs) will have a lower priority, and projects with little recent activity compared to their expected rate of use will see their waiting jobs start sooner. \u00a0Fair Share contributes up to 1000 points to the job priority. To see the recent usage and current fair-share score of a project, you can use the command\u00a0nn_corehour_usage.</p>","tags":[]},{"location":"Scientific_Computing/Running_Jobs_on_Maui_and_Mahuika/Job_prioritisation/#3-job-age","title":"3) Job Age","text":"<p>Job priority slowly rises with time as a pending job gets older - 1 point per hour for up to 3 weeks.</p>","tags":[]},{"location":"Scientific_Computing/Running_Jobs_on_Maui_and_Mahuika/Job_prioritisation/#4-job-size-or-tres-trackable-resources","title":"4) Job Size or \"TRES\" (Trackable RESources)","text":"<p>This slightly favours jobs which request a larger count of CPUs (or memory or GPUs) as a means of countering their otherwise inherently longer wait times.</p>","tags":[]},{"location":"Scientific_Computing/Running_Jobs_on_Maui_and_Mahuika/Job_prioritisation/#5-project-allocation-class","title":"5) Project Allocation Class","text":"<p>This depends on which \"allocation class\" entitles your project to use NeSI.</p> Project class **Class Priority Score\u00a0** Proposal Development 10 Postgraduate 20 Collaborator 30 Merit 40 Commercial 40","tags":[]},{"location":"Scientific_Computing/Running_Jobs_on_Maui_and_Mahuika/Job_prioritisation/#6-nice-values","title":"6) Nice values","text":"<p>It is possible to give a job a \"nice\" value which is subtracted from its priority. You can do that with the <code>--nice</code> option of <code>sbatch</code> or the <code>scontrol update</code> command. \u00a0The command <code>scontrol top &lt;jobid&gt;</code> adjusts nice values to increase the priority of one of your jobs at the expense of any others you have in the same partition.</p>","tags":[]},{"location":"Scientific_Computing/Running_Jobs_on_Maui_and_Mahuika/Job_prioritisation/#7-holds","title":"7) Holds","text":"<p>Jobs with a priority of 0 are in a \"held\" state and will never start without further intervention. \u00a0You can hold jobs with the command <code>scontrol hold &lt;jobid&gt;</code> and release them with <code>scontrol release &lt;jobid&gt;</code>. \u00a0Jobs can also end up in this state when they get requeued after a node failure.\u00a0</p>","tags":[]},{"location":"Scientific_Computing/Running_Jobs_on_Maui_and_Mahuika/Job_prioritisation/#other-limits","title":"Other Limits","text":"<p>Cluster and partition-specific limits can sometimes prevent jobs from starting regardless of their priority score. \u00a0For details see the pages on\u00a0Mahuika\u00a0or\u00a0M\u0101ui.</p>","tags":[]},{"location":"Scientific_Computing/Running_Jobs_on_Maui_and_Mahuika/Job_prioritisation/#backfill","title":"Backfill","text":"<p>Backfill is a scheduling strategy that allows small, short jobs to run immediately if by doing so they will not delay the expected start time of any higher-priority jobs. Since the expected start time of pending jobs depends upon the expected completion time of running jobs it is important that you set reasonably accurate job time limits if\u00a0backfill is to work well.</p> <p>While the kinds of jobs that can be backfilled will also get a low job size score, it is our general experience that an ability to be backfilled is on the whole more useful when it comes to getting work done on the HPCs.</p> <p>More information about backfill can be found here.</p>","tags":[]},{"location":"Scientific_Computing/Running_Jobs_on_Maui_and_Mahuika/Mahuika_Slurm_Partitions/","title":"Mahuika Slurm Partitions","text":"<p>Warning</p> <p>This page has been automatically migrated and may contain formatting errors.</p>","tags":["mahuika","slurm"]},{"location":"Scientific_Computing/Running_Jobs_on_Maui_and_Mahuika/Mahuika_Slurm_Partitions/#definitions","title":"Definitions","text":"<p>CPU:\u00a0A logical core, also known as a hardware thread. Referred to as a \"CPU\" in the Slurm documentation. \u00a0Since Hyperthreading is enabled, there are two CPUs per physical core, and every task\u2014 and therefore every job \u2014 is allocated an even number of CPUs.</p> <p>Fairshare Weight:\u00a0CPU hours are multiplied by this factor to determine usage for the purpose of calculating a project's\u00a0fair-share score.</p> <p>Job:\u00a0A running batch script and any other processes which it might launch with srun.</p> <p>Node:\u00a0A single computer within the cluster with its own CPUs and RAM (memory), and sometimes also GPUs. A node is analogous to a workstation (desktop PC) or laptop.</p> <p>Task: An instance of a running computer program, consisting of one or more threads. All of a task's threads must run within the same node.</p> <p>Thread: A sequence of instructions executed by a CPU.</p> <p>Walltime:\u00a0Real world time, as opposed to CPU time (walltime x CPUs).</p>","tags":["mahuika","slurm"]},{"location":"Scientific_Computing/Running_Jobs_on_Maui_and_Mahuika/Mahuika_Slurm_Partitions/#general-limits","title":"General Limits","text":"<ul> <li>No individual job can request more than 20,000 CPU hours. This has     the consequence that a job can request more CPUs if it is shorter     (short-and-wide vs long-and-skinny).</li> <li>No individual job can request more than 576 CPUs (8 full nodes),     since larger MPI jobs are scheduled less efficiently and are     probably suitable for running on M\u0101ui.</li> <li>No user can have more than 1,000 jobs in the queue at a time.</li> </ul> <p>These limits are defaults and can be altered on a per-account basis if there is a good reason. For example we will increase the limit on queued jobs for those who need to submit large numbers of jobs, provided that they undertake to do so with job arrays.</p>","tags":["mahuika","slurm"]},{"location":"Scientific_Computing/Running_Jobs_on_Maui_and_Mahuika/Mahuika_Slurm_Partitions/#partitions","title":"Partitions","text":"<p>A partition can be specified via the appropriate\u00a0sbatch option, e.g.:</p> <pre><code>#SBATCH\u00a0--partition=milan\n</code></pre> <p>However on Mahuika there is generally no need to do so, since the default behaviour is that your job will be assigned to the most suitable partition(s) automatically, based on the resources it requests, including particularly its memory/CPU ratio and time limit.</p> <p>The milan partition is currently an exception - since it has a different operating system version it is currently configured to be opt-in only - your job will not land there it unless you request it.</p> <p>If you do specify a partition and your job is not a good fit for that partition then you may receive a warning, please do not ignore this. E.g.:</p> <pre><code>sbatch: \"bigmem\" is not the most appropriate partition for this job, which would otherwise default to \"large\". If you believe this is incorrect then please contact support@nesi.org.nz and quote the Job ID number.\n</code></pre> <p>Name</p> <p>Max Walltime</p> <p>Nodes</p> <p>CPUs/Node</p> <p>GPUs/Node </p> <p>Available Mem/CPU</p> <p>Available Mem/Node</p> <p>Description</p> <p>long</p> <p>3 weeks</p> <p>69</p> <p>72</p> <p> </p> <p>1500 MB</p> <p>105 GB</p> <p>For jobs that need to run for longer than 3 days.</p> <p>large</p> <p>3 days</p> <p>long + 157</p> <p>72</p> <p> </p> <p>1500 MB</p> <p>105 GB</p> <p>Default partition.</p> <p>milan</p> <p>7 days</p> <p>56 \u00a08</p> <p>256 256</p> <p> </p> <p>1850 MB</p> <p>3800 MB</p> <p>460 GB 960 GB</p> <p>Jobs using Milan Nodes</p> <p>bigmem /</p> <p>infill</p> <p>7 days</p> <p>6</p> <p>6</p> <p>72</p> <p>54</p> <p> </p> <p>6300 MB</p> <p>5500 MB</p> <p>460 GB</p> <p>300 GB</p> <p>Jobs requiring large amounts of memory.</p> <p>hugemem</p> <p>7 days</p> <p>4</p> <p>80 128 176</p> <p> </p> <p>18 GB 30 GB 35 GB</p> <p>1,500 GB 4,000 GB 6,000 GB</p> <p>Jobs requiring very large amounts of memory.</p> <p>gpu</p> <p>7 days</p> <p>1</p> <p>4</p> <p>2</p> <p>2</p> <p>1</p> <p>18, plus 54 shared with infill</p> <p>1 P100*</p> <p>2 P100*</p> <p>1 A100**</p> <p>2 A100**</p> <p>7 A100-1g.5gb***</p> <p>6300 MB</p> <p>160 GB, plus 300 GB shared with infill</p> <p>Nodes with GPUs. See below\u00a0for more info.</p> <p>hgx</p> <p>7 days</p> <p>4</p> <p>128</p> <p>4 A100****</p> <p>6300 MB</p> <p>460 GB</p> <p>Part of Milan Nodes. See below for more info.</p> <p>* NVIDIA Tesla P100 PCIe 12GB cards</p> <p>** NVIDIA Tesla A100 PCIe 40GB cards</p> <p>*** 1 NVIDIA Tesla A100 PCIe 40GB card divided into\u00a07 MIG GPU slices (5GB each).\u00a0</p> <p>**** NVIDIA Tesla A100 80GB, on a HGX baseboard with NVLink GPU-to-GPU interconnect between the 4 GPUs  </p>","tags":["mahuika","slurm"]},{"location":"Scientific_Computing/Running_Jobs_on_Maui_and_Mahuika/Mahuika_Slurm_Partitions/#quality-of-service","title":"Quality of Service","text":"<p>Orthogonal to the partitions, each job has a \"Quality of Service\", with the default QoS for a job being determined by the allocation class of its project. \u00a0There are other QoSs which you can select with the <code>--qos</code>option:</p>","tags":["mahuika","slurm"]},{"location":"Scientific_Computing/Running_Jobs_on_Maui_and_Mahuika/Mahuika_Slurm_Partitions/#debug","title":"Debug","text":"<p>Specifying <code>--qos=debug</code> will give the job very high priority, but is subject to strict limits: 15 minutes per job, and only 1 job at a time per user. Debug jobs may not span more than two nodes.</p>","tags":["mahuika","slurm"]},{"location":"Scientific_Computing/Running_Jobs_on_Maui_and_Mahuika/Mahuika_Slurm_Partitions/#interactive","title":"Interactive","text":"<p>Specifying <code>--qos=interactive</code> will give the job very high priority, but is subject to some limits: up to 4 jobs, 16 hours duration, 4 CPUs, 128 GB, and 1 GPU.</p>","tags":["mahuika","slurm"]},{"location":"Scientific_Computing/Running_Jobs_on_Maui_and_Mahuika/Mahuika_Slurm_Partitions/#requesting-gpus","title":"Requesting GPUs","text":"<p>The default GPU type is P100, of which you can request 1 or 2 per node:</p> <pre><code>#SBATCH --gpus-per-node=1     # or equivalently, P100:1\n</code></pre> <p>To request A100 GPUs, use instead:</p> <pre><code>#SBATCH --gpus-per-node=A100:1\n</code></pre> <p>See GPU use on NeSI for more details about Slurm and CUDA settings.</p>","tags":["mahuika","slurm"]},{"location":"Scientific_Computing/Running_Jobs_on_Maui_and_Mahuika/Mahuika_Slurm_Partitions/#limits-on-gpu-jobs","title":"Limits on GPU Jobs","text":"<ul> <li>There is a per-project limit of 6 GPUs being used at a time.</li> <li>There is also a per-project limit of 360 GPU-hours being allocated     to running jobs. This reduces the number of GPUs available for     longer jobs, so for example you can use 8 GPUs at a time if your     jobs run for a day, but only two GPUs if your jobs run for a week.     The intention is to guarantee that all users can get short debugging     jobs on to a GPU in a reasonably timely manner. \u00a0</li> <li>Each GPU job can use no more than 64 CPUs. \u00a0This is to ensure that     GPUs are not left idle just because their node has no remaining free     CPUs.</li> <li>There is a limit of one A100-1g.5gb GPU job per user.</li> </ul>","tags":["mahuika","slurm"]},{"location":"Scientific_Computing/Running_Jobs_on_Maui_and_Mahuika/Mahuika_Slurm_Partitions/#accessing-a100-gpus-in-the-hgx-partition","title":"Accessing A100 GPUs in the <code>hgx</code> partition","text":"<p>The A100 GPUs in the <code>hgx</code> partition are designated for workloads requiring large memory (up to 80GB) or multi-GPU computing (up to 4 GPUs connected via NVLink):</p> <ul> <li>Explicitly specify the partition to access them, with     <code>--partition=hgx</code>.</li> <li>Hosting nodes are Milan nodes. Check the dedicated support     page     for more information about the Milan nodes' differences from     Mahuika's Broadwell nodes.</li> </ul>","tags":["mahuika","slurm"]},{"location":"Scientific_Computing/Running_Jobs_on_Maui_and_Mahuika/Mahuika_Slurm_Partitions/#mahuika-infiniband-islands","title":"Mahuika Infiniband Islands","text":"<p>Mahuika is divided into\u00a0\u201cislands\u201d of 26 nodes (or 1,872 CPUs). Communication between two nodes on the same island is faster than between two nodes on different islands. MPI jobs placed entirely within one island will often perform better than those split among multiple islands.</p> <p>You can request that a job runs within a single InfiniBand island by adding:</p> <pre><code>#SBATCH --switches=1\n</code></pre> <p>Slurm will then run the job within one island provided that this does not delay starting the job by more than the maximum switch waiting time, currently configured to be 5 minutes. That waiting time limit can be reduced by adding <code>@&lt;time&gt;</code> after the number of switches e.g:</p> <pre><code>#SBATCH --switches=1@00:30:00\n</code></pre>","tags":["mahuika","slurm"]},{"location":"Scientific_Computing/Running_Jobs_on_Maui_and_Mahuika/Maui_Slurm_Partitions/","title":"M\u0101ui Slurm Partitions","text":"<p>Warning</p> <p>This page has been automatically migrated and may contain formatting errors.</p> <p>Prerequisite</p> <p>Partitions on these systems that may be used for NeSI workloads carry  the prefix nesi_.</p>","tags":["maui","slurm"]},{"location":"Scientific_Computing/Running_Jobs_on_Maui_and_Mahuika/Maui_Slurm_Partitions/#definitions","title":"Definitions","text":"<p>CPU - A logical core, also known as a hardware thread. Referred to as a \"CPU\" in the Slurm documentation. \u00a0Since Hyperthreading is enabled, there are two CPUs per physical core, and every task\u2014 and therefore every job \u2014 is allocated an even number of CPUs.</p> <p>Job:\u00a0A running batch script and any other processes which it might launch with srun.</p> <p>Node:\u00a0A single computer within the cluster with its own CPUs and RAM (memory), and sometimes also GPUs. A node is analogous to a workstation (desktop PC) or laptop.</p> <p>Walltime:\u00a0Real world time, as opposed to CPU time (walltime x CPUs).</p>","tags":["maui","slurm"]},{"location":"Scientific_Computing/Running_Jobs_on_Maui_and_Mahuika/Maui_Slurm_Partitions/#maui-xc50-slurm-partitions","title":"M\u0101ui (XC50) Slurm Partitions","text":"<p>Nodes are not shared between jobs on M\u0101ui, so the minimum charging unit is node-hours, where 1 node-hour is 40 core-hours, or 80 Slurm CPU-hours.</p> <p>There is only one partition available to NeSI jobs:</p> <p>\u00a0Name\u00a0</p> <p>Nodes</p> <p>Max Walltime</p> <p>Avail / Node</p> <p>Max / Account</p> <p>Description</p> <p>nesi_research</p> <p>316</p> <p>24 hours</p> <p>80 CPUs</p> <p>90 or 180 GB RAM</p> <p>240 nodes</p> <p>1200 node-hours running</p> <p>Standard partition for all NeSI jobs. </p>","tags":["maui","slurm"]},{"location":"Scientific_Computing/Running_Jobs_on_Maui_and_Mahuika/Maui_Slurm_Partitions/#limits","title":"Limits","text":"<p>As a consequence of the above limit on the node-hours reserved by your running jobs (GrpTRESRunMins in Slurm documentation, shown in <code>squeue</code> output when you hit it\u00a0as the reason \"AssocGrpCPURunMinutes\" ) you can occupy more nodes simultaneously if your jobs request a shorter time limit:</p> nodes hours node-hours limits reached 1 24 24 24 hours 50 24 1200 1200 node-hours, 24 hours 100 12 1200 1200 node-hours 240 5 1200 1200 node-hours, 240 nodes 240 1 240 240 nodes\u00a0 <p>Most of the time job priority\u00a0will be the most important influence on how long your jobs have to wait - the above limits are just backstops to ensure that M\u0101ui's resources are not all committed too far into the future, so that debug and other higher-priority jobs can start reasonably quickly.</p>","tags":["maui","slurm"]},{"location":"Scientific_Computing/Running_Jobs_on_Maui_and_Mahuika/Maui_Slurm_Partitions/#debug-qos","title":"Debug QoS","text":"<p>Each job has a \"QoS\", with the default QoS for a job being determined by the allocation class of its project. Specifying <code>--qos=debug</code> will override that and give the job very high priority, but is subject to strict limits: 15 minutes per job, and only 1 job at a time per user. Debug jobs are limited to 2 nodes.</p>","tags":["maui","slurm"]},{"location":"Scientific_Computing/Running_Jobs_on_Maui_and_Mahuika/Maui_Slurm_Partitions/#maui_ancil-cs500-slurm-partitions","title":"M\u0101ui_Ancil (CS500) Slurm Partitions","text":"<p>Name</p> <p>Nodes</p> <p>Max Walltime</p> <p>Avail / Node</p> <p>Max / Job</p> <p>Max / User</p> <p>Description</p> <p>nesi_prepost</p> <p>4</p> <p>24 hours</p> <p>80 CPUs</p> <p>720 GB RAM</p> <p>20 CPUs</p> <p>700 GB RAM</p> <p>80 CPUs</p> <p>700 GB RAM</p> <p>Pre and post processing tasks.</p> <p>nesi_gpu</p> <p>4 to 5</p> <p>72 hours</p> <p>4 CPUs</p> <p>12 GB RAM</p> <p>1 P100 GPU*</p> <p>4 CPUs</p> <p>12 GB RAM</p> <p>1 P100 GPU</p> <p>4 CPUs</p> <p>12 GB RAM</p> <p>1 P100 GPU</p> <p>GPU jobs and\u00a0visualisation.\u00a0</p> <p>nesi_igpu</p> <p>0 to 1</p> <p>2 hours</p> <p>4 CPUs</p> <p>12 GB RAM</p> <p>1 P100 GPU*</p> <p>4 CPUs</p> <p>12 GB RAM</p> <p>1 P100 GPU</p> <p>4 CPUs</p> <p>12 GB RAM</p> <p>1 P100 GPU</p> <p>Interactive GPU access 7am - 8pm.</p> <p>* NVIDIA Tesla P100 PCIe 12GB card</p>","tags":["maui","slurm"]},{"location":"Scientific_Computing/Running_Jobs_on_Maui_and_Mahuika/Maui_Slurm_Partitions/#requesting-gpus","title":"Requesting GPUs","text":"<p>Nodes in the\u00a0<code>nesi_gpu</code> partition have 1 P100 GPU card each. You can request it using:</p> <pre><code>#SBATCH --partition=nesi_gpu\n#SBATCH --gpus-per-node=1\n</code></pre> <p>Note that you need to specify the name of the partition. \u00a0You also need to specify a number of CPUs and amount of memory small enough to fit on these nodes.</p> <p>See GPU use on NeSI for more details about Slurm and CUDA settings.</p>","tags":["maui","slurm"]},{"location":"Scientific_Computing/Running_Jobs_on_Maui_and_Mahuika/Milan_Compute_Nodes/","title":"Milan Compute Nodes","text":"<p>Warning</p> <p>This page has been automatically migrated and may contain formatting errors.</p>","tags":[]},{"location":"Scientific_Computing/Running_Jobs_on_Maui_and_Mahuika/Milan_Compute_Nodes/#how-to-access","title":"How to access","text":"<p>To use Mahuika's Milan nodes, you will need to explicitly specify the <code>milan</code> partition in your <code>sbatch</code> command line. Jobs are submitted from the same Mahuika login node that you currently use, and share the same file system as other cluster nodes.\u00a0</p> <pre><code>sbatch -p milan ...\n</code></pre> <p>Alternatively, the same effect can be achieved by placing a pragma into the job description file:</p> <pre><code>#SBATCH --partition=milan\n</code></pre>","tags":[]},{"location":"Scientific_Computing/Running_Jobs_on_Maui_and_Mahuika/Milan_Compute_Nodes/#hardware","title":"Hardware","text":"<p>Each node has two AMD Milan CPUs, each with 8 \"chiplets\" of 8 cores and one level 3 cache, so each node has a total of 128 cores or 256 hyperthreaded CPUs. This represents a significant increase of the number CPUs per node compared to the Broadwell nodes (36 cores).\u00a0</p> <p>The memory available to Slurm jobs is 512GB per node, so approximately 2GB per CPU. There are 64 nodes available, 8 of which will have double the memory (1T).</p>","tags":[]},{"location":"Scientific_Computing/Running_Jobs_on_Maui_and_Mahuika/Milan_Compute_Nodes/#software","title":"Software","text":"","tags":[]},{"location":"Scientific_Computing/Running_Jobs_on_Maui_and_Mahuika/Milan_Compute_Nodes/#operating-system","title":"Operating System","text":"<p>The existing Mahuika compute nodes use Linux Centos 7.4 while the new ones use Rocky 8.5. \u00a0These are closely related Linux distributions. The move from 7 to 8 is more significant than the move from Centos to Rocky.</p> <p>Many system libraries have changed version numbers between versions 7 and 8, so\u00a0some software compiled on Centos 7 will not run as-is on Rocky 8.\u00a0This can result in the runtime error <code>error while loading shared libraries:... cannot open shared object file</code>,\u00a0 which can be fixed by providing a copy of the old system library. \u00a0</p> <p>We have repaired several of our existing environment modules that way. For programs which you have compiled yourself, we have installed a new environment module that provides many of the Centos 7 libraries:</p> <pre><code>module load LegacySystemLibs/7\n</code></pre> <p>Please  Contact our Support Team if that isn't sufficient to get your existing compiled code running on the new nodes.</p> <p>Of course you can also recompile code inside a job run in the Milan partition and so produce an executable linked against the new system libraries, but then that would be unlikely to work on the old nodes.</p> <p>In the longer term, all Mahuika nodes will be upgraded to Rocky 8.</p>","tags":[]},{"location":"Scientific_Computing/Running_Jobs_on_Maui_and_Mahuika/Milan_Compute_Nodes/#older-intel-and-cray-software","title":"Older Intel and Cray software","text":"<p>The directories <code>/cm</code> and <code>/opt/cray</code> contain software which was installed on Mahuika's Broadwell nodes when we purchased it rather than installed by the NeSI Application Support team. They are not present on the Milan nodes. As with the system libraries, you could take a copy of these libraries and carry on, but it is best to migrate away from using them if possible.</p> <p>This affects our pre-2020 toolchains such as intel/2018b, but we should have newer versions of such software already installed in most cases.</p>","tags":[]},{"location":"Scientific_Computing/Running_Jobs_on_Maui_and_Mahuika/Milan_Compute_Nodes/#intel-mkl-performance","title":"Intel MKL performance","text":"<p>In many ways, Intel's MKL is the best implementation of the BLAS and LAPACK libraries to which we have access, which is why we use it in our \"intel\" and \"gimkl\" toolchains. \u00a0Unfortunately, recent versions of MKL deliberately choose not to use the accelerated AVX instructions when not running on an Intel CPU. \u00a0</p> <p>In order to persuade MKL to use the same fast optimised kernels on the new AMD Milan CPUs, you can do:</p> <pre><code>module load AlwaysIntelMKL\n</code></pre> <p>We have set that as the default for our most recent toolchain gimkl/2022a.</p> <p>Two alternative implementations have also been installed: OpenBLAS and BLIS. If you try them then please let us know if they work better than MKL for your application. BLIS is expected to perform well as a BLAS alternative but not match MKL's LAPACK performance. \u00a0</p>","tags":[]},{"location":"Scientific_Computing/Running_Jobs_on_Maui_and_Mahuika/Milan_Compute_Nodes/#do-i-need-to-recompile-my-code","title":"Do I need to recompile my code?","text":"<p>Except for possible missing shared libraries (see above), you should not need to recompile your code. Please [let us know Contact our Support Team if you encounter any issues not listed above.</p>","tags":[]},{"location":"Scientific_Computing/Running_Jobs_on_Maui_and_Mahuika/Milan_Compute_Nodes/#aocc-compiler-suite","title":"AOCC compiler suite","text":"<p>AMD provides a compiler based on clang (C/C++) and flang (Fortran) which might perform better on their hardware. We have installed it but not integrated it into a high-level toolchain with MPI and BLAS. If you wish to try it:</p> <pre><code>module load AOCC\n</code></pre> <p>For more information on AOCC compiler suite please, visit AMD Optimizing C/C++ and Fortran Compilers (AOCC)</p>","tags":[]},{"location":"Scientific_Computing/Running_Jobs_on_Maui_and_Mahuika/Milan_Compute_Nodes/#network","title":"Network","text":"<p>Access to Mahuika's Milan nodes is currently only possible via the Slurm <code>sbatch</code> and <code>srun</code> commands. There is no ssh access, not even to the nodes where you have a job running. Programs that launch their remote tasks via ssh (eg: ORCA) are not expected to work. Other arbitrary connections to the new compute nodes such as might be used by debuggers, HTTP based progress monitoring, and non-MPI distributed programs such as Dask or PEST, will generally only work if you use the Infiniband address of the compute node, eg: wmc012.ib.hpcf.nesi.org.nz. This networking configuration is expected to be addressed in the future.</p>","tags":[]},{"location":"Scientific_Computing/Running_Jobs_on_Maui_and_Mahuika/Milan_Compute_Nodes/#any-questions","title":"Any questions?","text":"<p>Don't hesitate to contact us at support@nesi.org.nz. No question is too big or small. We are available for Zoom sessions or Weekly Online Office Hours if it's easier to discuss your question in a call rather than via email.</p>","tags":[]},{"location":"Scientific_Computing/Running_Jobs_on_Maui_and_Mahuika/NetCDF-HDF5_file_locking/","title":"NetCDF / HDF5 file locking","text":"<p>Warning</p> <p>This page has been automatically migrated and may contain formatting errors.</p> <p>NeSI provides libraries for creating and accessing files in the NetCDF and HDF5 formats on both Mahuika and M\u0101ui.</p> <p>On M\u0101ui, both libraries are part of the Cray Programming Environment and can be accessed using the modules 'cray-netcdf'/'cray-netcdf-hdf5parallel', and 'cray-hdf5'/'cray-hdf5-parallel'.</p> <p>Recent versions of NetCDF and HDF5 (HDF5 1.10.x and newer) use a file locking feature. This prevents data corruption in rare cases of single-writer/multiple-reader and multiple writer access patterns.</p> <p>On our current XC50 platform (M\u0101ui), file locking is not yet fully supported by Cray's DVS. (DVS allows accessing the parallel file system GPFS on the XC50 compute nodes).</p> <p>Accordingly, NetCDF-4 and HDF5 applications that write data from M\u0101ui compute nodes need to disable file locking with:</p> <pre><code>export HDF5_USE_FILE_LOCKING=FALSE\n</code></pre> <p>If file locking is enabled, HDF5/NetCDF4 applications may experience errors such as</p> <pre><code>ncdump: /path/to/file.nc: NetCDF: HDF error\n</code></pre> <p>or</p> <pre><code>Error in EM_FOPEN: NetCDF: HDF error - /path/to/file.nc \n</code></pre> <p>or</p> <pre><code>(-101) // Error at HDF5 layer\n</code></pre> <p>or</p> <pre><code>HDF5-DIAG: Error detected in HDF5 (1.10.2) thread 0:\n#000: ../../src/H5F.c line 445 in H5Fcreate(): unable to create file\nmajor: File accessibilty\nminor: Unable to open file\n#001: ../../src/H5Fint.c line 1519 in H5F_open(): unable to lock the file\nmajor: File accessibilty\nminor: Unable to open file\n#002: ../../src/H5FD.c line 1650 in H5FD_lock(): driver lock request failed\nmajor: Virtual File Layer\nminor: Can't update object\n#003: ../../src/H5FDsec2.c line 941 in H5FD_sec2_lock(): unable to lock file, errno = 524, error message = 'Unknown error 524'\nmajor: File accessibilty\nminor: Bad file ID accessed\n</code></pre> <p>Important: As file locking has to be disabled on the XC50, care should be taken to avoid concurrent reader/writer access in your application.</p> <p>For more information see:</p> <ul> <li>Design -File Locking under SWMR in     HDF5</li> <li>release notes, where mechanism for disabling file locking was     introduced</li> </ul>","tags":[]},{"location":"Scientific_Computing/Running_Jobs_on_Maui_and_Mahuika/SLURM-Best_Practice/","title":"SLURM: Best Practice","text":"<p>Warning</p> <p>This page has been automatically migrated and may contain formatting errors.</p>","tags":[]},{"location":"Scientific_Computing/Running_Jobs_on_Maui_and_Mahuika/SLURM-Best_Practice/#bash-header","title":"Bash Header","text":"<p>We recommend using <code>#!/bin/bash -e</code>\u00a0instead of plain <code>#!/bin/bash</code>, so that the failure of any command within the script will cause your job to stop immediately rather than attempting to continue on with an unexpected environment or erroneous intermediate data. \u00a0It also ensures that your failed jobs show a status of FAILED in sacct output.</p>","tags":[]},{"location":"Scientific_Computing/Running_Jobs_on_Maui_and_Mahuika/SLURM-Best_Practice/#resources","title":"Resources","text":"<p>Don't request more resources (CPUs, memory, GPUs) than you will need. In addition to using your core hours faster, resources intensive jobs will take longer to queue. Use the information provided at the completion of your job (eg: via the sacct command) to better define resource requirements.</p>","tags":[]},{"location":"Scientific_Computing/Running_Jobs_on_Maui_and_Mahuika/SLURM-Best_Practice/#wall-time","title":"Wall-time","text":"<p>Long jobs will spend more time in the queue, as there are more opportunities for the scheduler to find a time slot to run shorter jobs. So consider using job check-pointing or, where possible, more parallelism, to get job durations down to a few hours, or at worst, days.</p> <p>Leave some headroom for safety and run-to-run variability on the system but try to be as accurate as possible.</p> <p>If you have very many jobs of less than 5 minutes then they should probably be combined into larger jobs using a simple loop in the batch script so as to amortise the overheads of each job (starting, accounting etc).</p>","tags":[]},{"location":"Scientific_Computing/Running_Jobs_on_Maui_and_Mahuika/SLURM-Best_Practice/#memory-ram","title":"Memory (RAM)","text":"<p>If you request more memory (RAM) than you need for your job, it will wait longer in the queue and will be more expensive when it runs. On the other hand, if you don't request enough memory, the job may be killed for attempting to exceed its allocated memory limits.</p> <p>We recommend that you request a little more RAM, but not much more, than your program will need at peak memory usage.</p> <p>We also recommend using <code>--mem</code> instead of <code>--mem-per-cpu</code> in most cases. There are a few kinds of jobs for which <code>--mem-per-cpu</code> is more suitable. See our article on how to request memory for more information.</p>","tags":[]},{"location":"Scientific_Computing/Running_Jobs_on_Maui_and_Mahuika/SLURM-Best_Practice/#parallelism","title":"Parallelism","text":"<p>In general only MPI jobs should set ntasks greater than 1 or use srun. \u00a0If you don't know whether your program supports MPI, it probably doesn't.</p> <p>Only multithreaded jobs should set cpus-per-task. \u00a0If you don't know whether your program supports multithreading, try benchmarking with 2 CPUs and with 4 CPUs and see if there is a 2-fold difference in elapsed job time.</p> <p>Job arrays are an efficient mechanism of managing a collection of batch jobs with identical resource requirements. Most Slurm commands can manage job arrays either as individual elements (tasks) or as a single entity (e.g. delete an entire job array in a single command)</p>","tags":[]},{"location":"Scientific_Computing/Running_Jobs_on_Maui_and_Mahuika/SLURM-Best_Practice/#fairshare","title":"Fairshare","text":"<p>A low fairshare score will affect your jobs priority in the queue, learn more about how to effectively use your allocation here.</p>","tags":[]},{"location":"Scientific_Computing/Running_Jobs_on_Maui_and_Mahuika/SLURM-Best_Practice/#cross-machine-submission","title":"Cross machine submission","text":"<p>Jobs can be submitted from one machine to another by using the <code>--cluster</code> option. E.g. submitting a job from M\u0101ui_Ancil to M\u0101ui.</p> <p>By default the environment (modules and variables) will be inherited from the submitting shell into the job environment. But the environments vary between our different machines, including module names, location of slurm tools, etc., which could cause issues in this inheriting case. We suggest to use the environment variable <code>SBATCH_EXPORT=NONE</code> (do NOT us <code>--export=none</code> option) in the submitting shell. Therefore we suggest to submit a job, e.g. to M\u0101ui using:</p> <pre><code>SBATCH_EXPORT=NONE sbatch --cluster=maui job.sl\n</code></pre> <p>Please note: Above we only discussed the transition from your submitting environment to the job environment. The latter is the one your job script is running in. There is another environment created for your parallel application (when called srun). There we want to inherit from the job environment to have PATHs and setting available. Therefore, avoid setting <code>SBATCH_EXPORT=NONE</code> in your job script or in .bashrc or .profile for all cases. The slurm <code>--export=none</code> option would prevent inhering environments in both transitions. Another note: Alternatively you can set <code>SLURM_EXPORT_ENV=ALL</code> in your job script to enable the environment forwarding to the srun environment.</p>","tags":[]},{"location":"Scientific_Computing/Running_Jobs_on_Maui_and_Mahuika/Slurm_Interactive_Sessions/","title":"Slurm Interactive Sessions","text":"<p>Warning</p> <p>This page has been automatically migrated and may contain formatting errors.</p> <p>A SLURM interactive session reserves resources on compute nodes allowing you to use them interactively as you would the login node.</p> <p>There are two main commands that can be used to make a session, <code>srun</code> and <code>salloc</code>, both of which use most of the same options available to <code>sbatch</code> (see our Slurm Reference Sheet).\u00a0</p> <p>Prerequisite</p> <p>An interactive session will, once it starts, use the entire requested  block of CPU time and other resources unless earlier exited from, even  if unused. To avoid unnecessary charges to your project, don't forget  to exit an interactive session once finished.</p>","tags":[]},{"location":"Scientific_Computing/Running_Jobs_on_Maui_and_Mahuika/Slurm_Interactive_Sessions/#using-srun-pty-bash","title":"Using 'srun --pty bash'","text":"<p><code>srun</code> will add your resource request to the queue. When the allocation starts, a new bash session will start up on one of the granted nodes.</p> <p>For example;</p> <pre><code>srun --account nesi12345 --job-name \"InteractiveJob\" --cpus-per-task 8 --mem-per-cpu 1500 --time 24:00:00 --pty bash\n</code></pre> <p>You will receive a message.</p> <pre><code>srun: job 10256812 queued and waiting for resources\n</code></pre> <p>And when the job starts:</p> <pre><code>srun: job 10256812 has been allocated resources\n[wbn079 ~ SUCCESS ]$\n</code></pre> <p>Note the host name in the prompt has changed to the compute node <code>wbn079</code>.</p> <p>For a full description of <code>srun</code>\u00a0and its options, see here.</p>","tags":[]},{"location":"Scientific_Computing/Running_Jobs_on_Maui_and_Mahuika/Slurm_Interactive_Sessions/#using-salloc","title":"Using 'salloc'","text":"<p><code>salloc</code>\u00a0functions similarly <code>srun --pty bash</code>\u00a0in that it will add your resource request to the queue. However the allocation starts, a new bash session will start up on the login node. This is useful for running a GUI on the login node, but your processes on the compute nodes.</p> <p>For example:</p> <pre><code>salloc --account nesi12345 --job-name \"InteractiveJob\" --cpus-per-task 8 --mem-per-cpu 1500 --time 24:00:00\n</code></pre> <p>You will receive a message.</p> <pre><code>salloc: Pending job allocation 10256925\nsalloc: job 10256925 queued and waiting for resources\n</code></pre> <p>And when the job starts;</p> <pre><code>salloc: job 10256925 has been allocated resources\nsalloc: Granted job allocation 10256925 \n[mahuika01~ SUCCESS ]$\n</code></pre> <p>Note the that you are still on the login node\u00a0<code>mahuika01</code>, however you will now have permission to <code>ssh</code> to any node you have a session on .</p> <p>For a full description of <code>srun</code>\u00a0and its options, see here.</p>","tags":[]},{"location":"Scientific_Computing/Running_Jobs_on_Maui_and_Mahuika/Slurm_Interactive_Sessions/#requesting-a-postponed-start","title":"Requesting a postponed start","text":"<p><code>salloc</code> lets you specify that a job is not to start before a specified time, however the job may still be delayed if requested resources are not available. You can request a start time using the <code>--begin</code> flag.</p> <p>The <code>--begin</code> flag takes either absolute or relative times as values.</p> <p>Prerequisite</p> <p>If you specify absolute dates and/or times, Slurm will interpret those  according to your environment's current time zone. Ensure that you  know what time zone your environment is using, for example by running  <code>date</code> in the same terminal session.</p> <ul> <li><code>--begin=16:00</code> means start the job no earlier than 4 p.m. today.     (Seconds are optional, but the time must be given in 24-hour     format.)</li> <li><code>--begin=11/05/20</code> means start the job on (or after) 5     November 2020. Note that Slurm uses American date formats.     <code>--begin=2020-11-05</code> is another Slurm-acceptable way of saying the     same thing, and possibly easier for a New Zealander.</li> <li><code>--begin=2020-11-05T16:00:00</code> means start the job on (or after) 4     p.m. on 5 November 2020.</li> <li><code>--begin=now+1hour</code> means wait at least one hour before starting the     job.</li> <li><code>--begin=now+60</code> means wait at least one minute before starting the     job.</li> </ul> <p>If no <code>--begin</code> argument is given, the default behaviour is to start as soon as possible.</p>","tags":[]},{"location":"Scientific_Computing/Running_Jobs_on_Maui_and_Mahuika/Slurm_Interactive_Sessions/#while-you-wait","title":"While you wait","text":"<p>It's quite common to have to wait for some time before your interactive session starts, even if you specified, expressly or by implication, that the job is to start as soon as possible.</p> <p>While you're waiting, you will not have use of that shell prompt. Do not use <code>Ctrl</code>-<code>C</code> to get the prompt back, as doing so will cancel the job. If you need a shell prompt, detach your <code>tmux</code> or <code>screen</code> session, or switch to (or open) another terminal session to the same cluster's login node.</p> <p>In the same way, before logging out (for example, if you choose to shut down your workstation at the end of the working day), be sure to detach the <code>tmux</code> or <code>screen</code> session. In fact, we recommend detaching whenever you leave your workstation unattended for a while, in case your computer turns off or goes to sleep or its connection to the internet is disrupted while you're away.</p>","tags":[]},{"location":"Scientific_Computing/Running_Jobs_on_Maui_and_Mahuika/Slurm_Interactive_Sessions/#setting-up-a-detachable-terminal","title":"Setting up a detachable terminal","text":"<p>Prerequisite</p> <p>If you don't request your interactive session from within a detachable  terminal, any interruption to the controlling terminal, for example by  your computer going to sleep or losing its connection to the internet,  will permanently cancel that interactive session and remove it from  the queue, whether it has started or not.</p> <ol> <li>Log in to a Mahuika, M\u0101ui or M\u0101ui-ancil login node.</li> <li>Start up <code>tmux</code> or <code>screen</code>.</li> </ol>","tags":[]},{"location":"Scientific_Computing/Running_Jobs_on_Maui_and_Mahuika/Slurm_Interactive_Sessions/#modifying-an-existing-interactive-session","title":"Modifying an existing interactive session","text":"<p>Whether your interactive session is already running or is still waiting in the queue, you can make a range of changes to it using the <code>scontrol</code> command. Some changes are off limits for ordinary users, such as increasing the maximum permitted wall time, or unsafe, like decreasing the memory request. But many other changes are allowed.</p>","tags":[]},{"location":"Scientific_Computing/Running_Jobs_on_Maui_and_Mahuika/Slurm_Interactive_Sessions/#postponing-the-start-of-an-interactive-job","title":"Postponing the start of an interactive job","text":"<p>Suppose you submitted an interactive job just after lunch, and it's already 4 p.m. and you're leaving in an hour. You decide that even if the job starts now, you won't have time to do everything you need to do before the office shuts and you have to leave. Even worse, the job might start at 11 p.m. after you've gone to bed, and you'll get to work at 9:00 the next morning and find that it has wasted ten wall-hours of time.</p> <p>Slurm offers an easy solution: Identify the job, and use <code>scontrol</code> to postpone its start time.</p> <p>Prerequisite</p> <p>Job IDs are unique to each cluster but not across the whole of NeSI.  Therefore, <code>scontrol</code> must be run on a node belonging to the cluster  where the job is queued.</p> <p>The following command will delay the start of the job with numeric ID 12345678 until (at the earliest) 9:30 a.m. the next day:</p> <pre><code>scontrol update jobid=12345678 StartTime=tomorrowT09:30:00\n</code></pre> <p>This variation, if run on a Friday, will delay the start of the same job until (at the earliest) 9:30 a.m. on Monday:</p> <pre><code>scontrol update jobid=12345678 StartTime=now+3daysT09:30:00\n</code></pre> <p>Prerequisite</p> <p>Don't just set <code>StartTime=tomorrow</code> with no time specification unless  you like the idea of your interactive session starting at midnight or  in the wee small hours of the morning.</p>","tags":[]},{"location":"Scientific_Computing/Running_Jobs_on_Maui_and_Mahuika/Slurm_Interactive_Sessions/#bringing-forward-the-start-of-an-interactive-job","title":"Bringing forward the start of an interactive job","text":"<p>In the same way, you can use scontrol to set a job's start time to earlier than its current value. A likely application is to allow a job to start immediately even though it stood postponed to a later time:</p> <pre><code>scontrol update jobid=12345678 StartTime=now\n</code></pre>","tags":[]},{"location":"Scientific_Computing/Running_Jobs_on_Maui_and_Mahuika/Slurm_Interactive_Sessions/#other-changes-using-scontrol","title":"Other changes using <code>scontrol</code>","text":"<p>There are many other changes you can make by means of <code>scontrol</code>. For further information, please see the <code>scontrol</code> documentation (off site).</p>","tags":[]},{"location":"Scientific_Computing/Running_Jobs_on_Maui_and_Mahuika/Slurm_Interactive_Sessions/#modifying-multiple-interactive-sessions-at-once","title":"Modifying multiple interactive sessions at once","text":"<p>In the same way, if you have several interactive sessions waiting to start on the same cluster, you might want to postpone them all using a single command. To do so, you will first need to identify them, hence the earlier suggestion to something specific to interactive jobs in the job name.</p> <p>For example, if all your interactive job names start with the text \"IJ\", you could do this:</p> <pre><code># -u $(whoami) restricts the search to my jobs only.\n# The --states=PD option restricts the search to pending jobs only.\n#\n# Each &lt;tab&gt; string should be replaced with a literal tab character. If you\n# can't insert one by pressing the tab key on your keyboard, you should be\n# able to insert one by pressing Ctrl-V followed immediately by Ctrl-I.\n#\nsqueue -u $(whoami) --states=PD -o \"%A&lt;tab&gt;%j\" | grep \"&lt;tab&gt;IJ\"\n</code></pre> <p>The above command will return a list of your jobs whose names start with the text \"IJ\". In this respect, it's more flexible than the <code>-n</code> option to <code>squeue</code>, which requires the entire job name string in order to identify a match.</p> <p>In order to use <code>scontrol</code>, we need to throw away all of the line except for the job ID, so let's use <code>awk</code> to do this, and send the output to <code>scontrol</code> via <code>xargs</code>:</p> <pre><code>squeue -u $(whoami) --states=PD -o \"%A&lt;tab&gt;%j\" | grep \"&lt;tab&gt;IJ\" | \\\nawk '{print $1}' | \\\nxargs -I {} scontrol update jobid={} StartTime=tomorrowT09:30:00\n</code></pre> <p>If you want to do this automatically every working day and you have a consistent element that you use in the name of all your interactive jobs, you can set up cron jobs on M\u0101ui, Mahuika and/or M\u0101ui-ancil login nodes. This is left as an exercise for the reader, having regard to the following:</p> <ul> <li>Time zone: Even if your environment is set up to use a different     time zone (commonly New Zealand time, which adjusts for daylight     saving as needed), time schedules in the crontab itself are     interpreted in UTC. So if you want something to run at 4:30 p.m. New     Zealand time regardless of the time of year, the cron job will need     to run at 4:30 a.m. UTC (during winter) or 3:30 a.m. UTC (during     summer), and you will need to edit the crontab every six months or     so.</li> <li>Weekends: If you just have a single cron job that postpones     pending interactive jobs until the next day, interactive jobs     pending on a Friday afternoon will be postponed until Saturday     morning, which is probably not what you want. Either your cron job     detects the fact of a Friday and postpones jobs until Monday, or you     have two cron jobs, one that runs on Mondays to Thursdays, and a     different cron job running on Fridays.</li> </ul>","tags":[]},{"location":"Scientific_Computing/Running_Jobs_on_Maui_and_Mahuika/Slurm_Interactive_Sessions/#cancelling-an-interactive-session","title":"Cancelling an interactive session","text":"<p>You can cancel a pending interactive session by attaching the relevant session, putting the job in the foreground (if necessary) and pressing <code>Ctrl</code>-<code>C</code> on your keyboard.</p> <p>To cancel all your queued interactive sessions on a cluster in one fell swoop, a command like the following should do the trick:</p> <pre><code>squeue -u $(whoami) --states=PD -o \"%A&lt;tab&gt;%j\" | grep \"&lt;tab&gt;IJ\" | \\\nawk '{print $1}' | \\\nxargs -I {} scancel {}\n</code></pre> <p>If you frequently use interactive jobs, we recommend doing this before you go away on leave or fieldwork or other lengthy absence.</p>","tags":[]},{"location":"Scientific_Computing/Supported_Applications/","title":"Supported Applications","text":"<p>For more information on environment-modules see Finding Software.</p>"},{"location":"Scientific_Computing/Supported_Applications/ABAQUS/","title":"ABAQUS","text":"<p>Finite Element Analysis software for modeling, visualization and best-in-class implicit and explicit dynamics FEA.</p> <p>ABAQUS Homepage</p> <p>Warning</p> <p>ABAQUS is proprietary software. Make sure you meet the requirements for it's usage.</p> <p>Tip</p> <p>For a list of ABAQUS commands type:</p> <pre><code>abaqus help\n</code></pre>","tags":["mahuika","engineering","gpu","mpi","omp","fea"]},{"location":"Scientific_Computing/Supported_Applications/ABAQUS/#available-modules","title":"Available Modules","text":"Mahuika Maui <p> 6.14.2 2017 2018 2019 2020 2021 2022 </p> <pre><code>module load ABAQUS/2022</code></pre> <p> 2017 2018 2019 2020 2021 2022 </p> <pre><code>module load ABAQUS/2022</code></pre>","tags":["mahuika","engineering","gpu","mpi","omp","fea"]},{"location":"Scientific_Computing/Supported_Applications/ABAQUS/#licences","title":"Licences","text":"<p>The following network licence servers can be accessed from the NeSI cluster.</p> Institution Faculty Token University of Auckland Faculty of Engineering <p>Not Required</p> <p>Not Required</p> <p>Not Required</p> University of Waikato <p>Not Required</p> <p>If you do not have access, or want a server connected Contact our Support Team.</p> <p>You can force ABAQUS to use a specific licence type by setting the parameter <code>academic=TEACHING</code> or <code>academic=RESEARCH</code> in a relevant environment file.</p> <p>Tip</p> <p>Required ABAQUS licences can be determined by this simple and  intuitive formula <code>\u230a 5 x N<sup>0.422</sup> \u230b</code>\u00a0where <code>N</code> is number  of CPUs.</p> <p>Hyperthreading can provide significant speedup to your computations, however hyperthreaded CPUs will use twice the number of licence tokens. It may be worth adding\u00a0 <code>#SBATCH --hint nomultithread</code> to your slurm script if licence tokens are your main limiting factor.</p>","tags":["mahuika","engineering","gpu","mpi","omp","fea"]},{"location":"Scientific_Computing/Supported_Applications/ABAQUS/#solver-compatibility","title":"Solver Compatibility","text":"<p>Not all solvers are compatible with all types of parallelisation.</p> Element operations Iterative Direct Lanczos <code>mp_mode=threads</code> \u2716 \u2714 \u2714 \u2714 <code>mp_mode=mpi</code> \u2714 \u2714 \u2716 \u2716 <p>Warning</p> <p>If your input files were created using an older version of ABAQUS you  will need to update them using the command,  <pre><code>abaqus -upgrade -job new_job_name -odb old.odb\n</code></pre>  or  <pre><code>abaqus -upgrade -job new_job_name -inp old.inp\n</code></pre></p>","tags":["mahuika","engineering","gpu","mpi","omp","fea"]},{"location":"Scientific_Computing/Supported_Applications/ABAQUS/#examples","title":"Examples","text":"SerialShared MemoryUDFDistributed MemoryGPUs <p>For when only one CPU is required, generally as part of a job array</p> <pre><code>#!/bin/bash -e\n\n#SBATCH --job-name      ABAQUS-serial\n#SBATCH --time          00:05:00 # Walltime\n#SBATCH --cpus-per-task 1\n#SBATCH --mem           1500          # total mem\nmodule load ABAQUS/2022\nabaqus job=\"propeller_s4rs_c3d8r\" verbose=2 interactive\n</code></pre> <p><code>mp_mode=threads</code> Uses a nodes shared memory for communication. May have a small speedup compared to MPI when using a low number of CPUs, scales poorly. Needs significantly less memory than MPI. Hyperthreading may be enabled if using shared memory but it is not recommended.</p> <pre><code>#!/bin/bash -e\n\n#SBATCH --job-name      ABAQUS-Shared\n#SBATCH --time          00:05:00       # Walltime\n#SBATCH --cpus-per-task 4\n#SBATCH --mem           2G             # total mem\nmodule load ABAQUS/['6.14.2', '2017', '2018', '2019', '2020', '2021', '2022']\nabaqus job=\"propeller_s4rs_c3d8r verbose=2 interactive \\\n    cpus=${SLURM_CPUS_PER_TASK} mp_mode=threads \n</code></pre> <p>Shared memory run with user defined function (fortran or C). Function will be compiled at start of run. You may need to chance the function suffix if you usually compile on windows.</p> <pre><code>#!/bin/bash -e\n\n#SBATCH --job-name      ABAQUS-SharedUDF\n#SBATCH --time          00:05:00       # Walltime\n#SBATCH --cpus-per-task 4\n#SBATCH --mem           2G         # total mem\n\nmodule load imkl\nmodule  load ABAQUS/2022\nabaqus job=\"propeller_s4rs_c3d8r\" user=my_udf.f90 \\\n    verbose=2 interactive cpus=${SLURM_CPUS_PER_TASK} mp_mode=threads\n</code></pre> <p><code>mp_mode=mpi</code> Multiple processes\u00a0each with a single\u00a0thread. Not limited to one node. Model will be segmented into <code>-np</code>\u00a0pieces which should be equal to\u00a0<code>--ntasks</code> Each task could be running on a different node leading to increased communication overhead. Jobs can be limited to a single node by adding\u00a0<code>--nodes=1</code>\u00a0however this will increase your time in the queue as\u00a0contiguous cpu's are harder to schedule. This is the default method if <code>mp_mode</code> is left unspecified.</p> <pre><code>#!/bin/bash -e\n\n#SBATCH --job-name      ABAQUS-Distributed \n#SBATCH --time          00:05:00       # Walltime&lt;/span&gt;&lt;/span&gt;\n#SBATCH --ntasks        8\n#SBATCH --mem-per-cpu   1500          # Each CPU needs it&amp;#39;s own.\n#SBATCH --nodes         1\n\nmodule load ABAQUS/2022\nabaqus job \"propeller_s4rs_c3d8r\" verbose=2 interactive cpus=${SLURM_NTASKS} mp_mode=mpi\n</code></pre> <p>The GPU nodes are limited to 16 CPUs In order for the GPUs to be worthwhile, you should see a speedup equivalent to 56 CPU's per GPU used. GPU modes will generally have less memory/cpus.</p> <pre><code>#!/bin/bash -e\n\n#SBATCH --job-name      ABAQUS-gpu\n#SBATCH --time          00:05:00       # Walltime&lt;/span&gt;&lt;/span&gt;\n#SBATCH --cpus-per-task 4\n#SBATCH --mem           4G         # total mem&lt;/span&gt;&lt;/span&gt;\n#SBATCH --gpus-per-node\nmodule load ABAQUS/2022\nmodule load CUDA\nabaqus job=\"propeller_s4rs_c3d8r\" verbose=2 interactive \\\ncpus=${SLURM_CPUS_PER_TASK} gpus=${SLURM_GPUS_PER_NODE} mp_mode=threads\n</code></pre>","tags":["mahuika","engineering","gpu","mpi","omp","fea"]},{"location":"Scientific_Computing/Supported_Applications/ABAQUS/#user-defined-functions","title":"User Defined Functions","text":"<p>User defined functions (UDFs) can be included on the command line with the argument <code>user=&lt;filename&gt;</code> where <code>&lt;filename&gt;</code> is the C or fortran source code.</p> <p>Extra compiler options can be set in your local <code>abaqus_v6.env</code> file.</p> <p>The default compile commands are for <code>imkl</code>, other compilers can be loaded with <code>module load</code>, you may have to change the compile commands in your local <code>.env</code> file.</p>","tags":["mahuika","engineering","gpu","mpi","omp","fea"]},{"location":"Scientific_Computing/Supported_Applications/ABAQUS/#environment-file","title":"Environment file","text":"<p>The ABAQUS environment file\u00a0contains a number of parameters that define how the your job will run, some of these you may with to change.</p> <p>These parameters are read in the following order of preference,</p> <p><code>../ABAQUS/SMA/site/abaqus_v6.env</code> Set by NeSI and cannot be changed.</p> <p><code>~/abaqus_v6.env</code> (your home directory) If exists, will be used in all jobs submitted by you.</p> <p><code>&lt;working directory&gt;/abaqus_v6.env</code> If exists, will used in this job only.</p> <p>You may want to include this short snippet when making changes specific to a job.</p> <pre><code># Before starting abaqus\necho \"parameter=value\nparameter=value\nparameter=value\" &gt; \"abaqus_v6.env\"\n\n# After job is finished.\nrm \"abaqus_v6.env\"\n</code></pre> <p></p> <p>Note: Hyperthreading off, testing done on small mechanical FEA model. Results highly model dependant. Do your own tests.</p>","tags":["mahuika","engineering","gpu","mpi","omp","fea"]},{"location":"Scientific_Computing/Supported_Applications/ANSYS/","title":"ANSYS","text":"<p>ANSYS Homepage</p> <p>Warning</p> <p>ANSYS is proprietary software. Make sure you meet the requirements for it's usage.</p>","tags":["mahuika","application","engineering"]},{"location":"Scientific_Computing/Supported_Applications/ANSYS/#available-modules","title":"Available Modules","text":"Mahuika Maui Maui_ancil <p> 18.1 19.1 19.2 2019R3 2020R1 2020R2 2021R1 2021R2 2022R1 2022R2 2023R1 2023R2 </p> <pre><code>module load ANSYS/2023R2</code></pre> <p> 18.1 19.1 19.2 19.3 2020R1 2020R2 2021R1 2021R2 2022R1 2022R2 2023R1 2023R2 </p> <pre><code>module load ANSYS/2023R2</code></pre> <p> 19.2 </p> <pre><code>module load ANSYS/19.2</code></pre>","tags":["mahuika","application","engineering"]},{"location":"Scientific_Computing/Supported_Applications/ANSYS/#licences","title":"Licences","text":"<p>The following network licence servers can be accessed from the NeSI cluster.</p> Institution Faculty Token University of Auckland Faculty of Engineering <p>Not Required</p> Orbis Diagnostics <p>Not Required</p> Auckland University of Technology Faculty of Engineering <p>Not Required</p> <p>Not Required</p> University of Waikato <p>Not Required</p> <p>Not Required</p> University of Canterbury <p>Not Required</p> University of Otago <p>Not Required</p> <p>Not Required</p> Orbis Diagnostics <p>Not Required</p> <p>If you do not have access, or want a server connected Contact our Support Team.</p>","tags":["mahuika","application","engineering"]},{"location":"Scientific_Computing/Supported_Applications/ANSYS/#license-types","title":"License Types","text":"<p>The three main ANSYS licenses are;</p> <ul> <li> <p>ANSYS Teaching License (aa_t)</p> <p>This is the default license type, it can be used on up to 6 CPUs on models with less than 512k nodes</p> </li> <li> <p>ANSYS Research license (aa_r)</p> <p>No node restrictions. Can be used on up to 16 CPUs, for every additional CPU over 16 you must request additional 'aa_r_hpc' licenses.</p> </li> <li> <p>ANSYS HPC License (aa_r_hpc) One of these is required for each CPU over\u00a016 when using     a\u00a0research license.</p> </li> </ul>","tags":["mahuika","application","engineering"]},{"location":"Scientific_Computing/Supported_Applications/ANSYS/#license-order","title":"License Order","text":"<p>Whether to use a teaching or research license must be set manually. If your job is greater than the node limit, not switching to the research license before submitting a job will cause the job to fail.</p> <p>The license order can be changed in workbench under tools &gt; license preferences (provided you have X11 forwarding set up), or by running either of the following (ANSYS module must be loaded\u00a0first using <code>module load ANSYS</code>).</p> <pre><code>prefer_research_license\n</code></pre> <pre><code>prefer_teaching_license\n</code></pre> <p>Warning</p> <p>License preferences are individually tracked by each version of  ANSYS. Make sure you set preferences using the same version as in  your script.</p>","tags":["mahuika","application","engineering"]},{"location":"Scientific_Computing/Supported_Applications/ANSYS/#journal-files","title":"Journal files","text":"<p>Some ANSYS applications take a 'journal' text file as input. It is often useful to create this journal file in your SLURM script (tidiness, submitting jobs programmatically, etc). This can be done by using <code>cat</code> to make a file from a 'heredoc'.</p> <p>Below is an example of this from a fluent script.</p> <pre><code>#!/bin/bash -e\n\n#SBATCH --job-name      Fluent_Array\n#SBATCH --time          01:00:00          # Wall time\n#SBATCH --mem           512MB             # Memory per node\n#SBATCH --licenses      aa_r:1       # One license token per CPU, less 16\n#SBATCH --array         1-100 \n#SBATCH --hint          nomultithread     # No hyperthreading\n\nmodule load ANSYS/2023R2 \n\nJOURNAL_FILE=fluent_${SLURM_JOB_ID}.in\ncat  ${JOURNAL_FILE}\n/file/read-case-data testCase${SLURM_ARRAY_TASK_ID}.cas\n/solve/dual-time-iterate 10\n/file/write-case-data testOut${SLURM_ARRAY_TASK_ID}.cas\n/exit yes\nEOF\n\n# Use one of the -v options 2d, 2ddp, 3d, or 3ddp\nfluent -v3ddp -g -i ${JOURNAL_FILE}\nrm\u00a0${JOURNAL_FILE}\n</code></pre> <p><code>JOURNAL_FILE</code>\u00a0is a variable holding the name of a file, the next line <code>cat</code> creates the file then writes a block of text into it. The block of text written is everything between an arbitrary string (in this case <code>EOF</code>) and its next occurrence.</p> <p>In this case (assuming it is the first run of the array and the jobid=1234567), the file\u00a0 <code>fluent_1234567.in</code> will be created:</p> <pre><code>/file/read-case-data testCase1\n; This will read testCase1.cas and testCase1.dat\n; Inputs can be read separately with 'read-case' and 'read-data'\n\n/solve/dual-time-iterate 10\n; Solve 10 time steps\n\n/file/write-case-data testCase1 ok\n; Since our output name is the same as our input, we have to provide conformation to overwrite, 'ok' \n\nexit\u00a0yes\n;\u00a0Not\u00a0including\u00a0'exit\u00a0yes'\u00a0will\u00a0cause\u00a0fluent\u00a0to\u00a0exit\u00a0with\u00a0an\u00a0error.\u00a0(Everything will be fine, but SLURM will read it as FAILED).\n</code></pre> <p>then called as an input\u00a0<code>fluent -v3ddp -g -i fluent_1234567.in</code>, then deleted\u00a0<code>rm fluent_1234567.in</code></p> <p>This can be used with variable substitution to great effect as it allows the use of variables in what might otherwise be a fixed input.</p> <p>Tip</p> <p>Comments can be added to journal files using a <code>;</code>. For example:  <pre><code>; This is a comment\n</code></pre></p>","tags":["mahuika","application","engineering"]},{"location":"Scientific_Computing/Supported_Applications/ANSYS/#fluent","title":"Fluent","text":"<p>Some great documentation on journal files</p> <p><code>fluent -help</code> for a list of commands.</p> <p>Must have one of these flags.</p> <code>2d</code> 2D solver, single point precision. <code>3d</code> 3D solver, single point precision. <code>2ddp</code> 2D solver, double point precision. <code>3ddp</code> 3D solver, double point precision. Serial JobDistributed Memory Job <p>Single process with a single thread (2 threads if hyperthreading enabled).</p> <p>Usually submitted as part of an array, as in the case of parameter sweeps.</p> <pre><code>#!/bin/bash -e\n\n#SBATCH --job-name      Fluent-Serial\n#SBATCH --licenses      aa_r@uoa_foe:1    #One research license.\n#SBATCH --time          00:05:00          # Walltime\n#SBATCH --cpus-per-task 1                 # Double if hyperthreading enabled\n#SBATCH --mem           512MB             # total memory (per node)\n#SBATCH --hint          nomultithread     # Hyperthreading disabled\n\nmodule load ANSYS/2023R2\n\nJOURNAL_FILE=/share/test/ansys/fluent/wing.in\nfluent 3ddp -g -i ${JOURNAL_FILE}\n</code></pre> <p>Multiple processes each with a single\u00a0thread. Not limited to one node. Model will be segmented into <code>-t</code>\u00a0pieces which should be equal to\u00a0<code>--ntasks</code>.<code>Each task could be running on a different node leading to increased communication overhead. Jobs can be limited to a single node by adding</code>--nodes=1`\u00a0however this will increase your time in the queue as\u00a0contiguous cpu's are harder to schedule.</p> <pre><code>#!/bin/bash -e\n\n#SBATCH --job-name          Fluent-Dis\n#SBATCH --time              00:05:00          # Walltime\n#SBATCH --licenses          aa_r@uoa_foe:1,aa_r_hpc@uoa_foe:20\n##One research license, (ntasks-16) hpc licenses\n#SBATCH --nodes             1                 # Limit to n nodes (Optional)\n#SBATCH --ntasks            8                 # Number processes\n#SBATCH --cpus-per-task     1                 # Double if hyperthreading enabled\n#SBATCH --mem-per-cpu       1500              # Fine for small jobs; increase if needed\n#SBATCH --hint              nomultithread     # Hyperthreading disabled\n\nmodule load ANSYS/2023R2\nJOURNAL_FILE=/share/test/ansys/fluent/wing.in\nfluent 3ddp -g -t ${SLURM_NTASKS} -i ${JOURNAL_FILE}\n</code></pre>","tags":["mahuika","application","engineering"]},{"location":"Scientific_Computing/Supported_Applications/ANSYS/#interactive","title":"Interactive","text":"<p>While it will always be more time and resource efficient using a slurm script as shown above, there are occasions where the GUI is required. If you only require a few CPUs for a short while you may run the fluent on the login node, otherwise use of an slurm interactive session is recommended.</p> <p>For example.</p> <pre><code>salloc --job-name flUI --nodes 4 --ntasks-per-node 8 --mem-per-cpu 1500\u00a0--time\u00a004:00:00\n</code></pre> <p>Will return;</p> <pre><code>  salloc: Pending job allocation 10270935\n  salloc: job 10270935 queued and waiting for resources\n  salloc: job 10270935 has been allocated resources\n  salloc: Granted job allocation 10270935\n  salloc: Waiting for resource configuration\n  salloc: Nodes wbn[053-056] are ready for job\n</code></pre> <p>Tip</p> <p>Include all the commands you would usually use in your slurm header  here.</p> <p>Once you have your allocation, run the command</p> <pre><code>fluent\n</code></pre> <p>You will then be presented with the launcher, make any necessary changes then click launch.</p> <p>If everything has set up correctly you should see a printout of the hostnames with the resources requested. Note: 'host' should be mahuika0[1-2].</p> <pre><code>n24-31 wbn056 8/72 Linux-64 71521-71528 Intel(R) Xeon(R) E5-2695 v4\n n16-23 wbn055 8/72 Linux-64 52264-52271 Intel(R) Xeon(R) E5-2695 v4\n n8-15 wbn054 8/72 Linux-64 177090-177097 Intel(R) Xeon(R) E5-2695 v4\n n0-7 wbn053 8/72 Linux-64 48376-48384 Intel(R) Xeon(R) E5-2695 v4\n host mahuika01 Linux-64 185962 Intel(R) Xeon(R) E5-2695 v4\n</code></pre> <p>Warning</p> <p>Closing the fluent GUI will not end the SLURM interactive session. Use <code>exit</code> or <code>scancel</code><code>jobid</code> when finished, else you will continue to 'use' the requested CPUs.</p>","tags":["mahuika","application","engineering"]},{"location":"Scientific_Computing/Supported_Applications/ANSYS/#checkpointing","title":"Checkpointing","text":"<p>It is best practice when running long jobs to enable autosaves.</p> <pre><code>/file/autosave/data-frequency \n</code></pre> <p>Where `` is the number of iterations to run before creating a save.</p> <p>In order to save disk space you may also want to include the line</p>","tags":["mahuika","application","engineering"]},{"location":"Scientific_Computing/Supported_Applications/ANSYS/#interrupting","title":"Interrupting","text":"<p>Including the following code at the top of your journal file will allow you to interrupt the job.</p> <pre><code>(set! checkpoint/exit-filename \"./exit-fluent\")\n</code></pre> <p>Creating a file named <code>exit-fluent</code> in the run directory will cause the job to save the current state and exit (<code>touch exit-fluent</code>). This will also write a new journal file called <code>restart.inp</code> that restarts the simulation at that point.</p>","tags":["mahuika","application","engineering"]},{"location":"Scientific_Computing/Supported_Applications/ANSYS/#user-defined-functions","title":"User Defined Functions","text":"<p>When compiling code, make sure to <code>module load gimkl</code> in addition to the ANSYS module.</p>","tags":["mahuika","application","engineering"]},{"location":"Scientific_Computing/Supported_Applications/ANSYS/#case-definition","title":"Case Definition","text":"<p>When setting up the case file on your local machine, make sure you select 'Compiled UDF', and select the `.c` source file. You can also specify the name of the library, the default being 'libudf', if possible you should stick with the default name.</p> <p>Make sure all names follows unix naming conventions (no spaces or special characters) and are the same on the cluster as when you defined it.</p> <p>It will also save you time if the that the path to your UDF source is relative. The easiest way to do this is to have the source file in the same directory as your <code>.cas</code> file, then specify only the name as your UDF source.</p> <p>When calling a function, make sure you select the compiled NOT the interpreted version.</p> <p>`udf funcName` is funcName as being interpreted directly from your `.c` source file.</p> <p>`udf funcName::libudf` is funcName as compiled in library `libudf`</p>","tags":["mahuika","application","engineering"]},{"location":"Scientific_Computing/Supported_Applications/ANSYS/#compilation","title":"Compilation","text":"<p>When running in a new environment for the first time (local machine, Mahuika, M\u0101ui), the C code will have to first be compiled. The compiled code will be placed in a directory with the name of the library (by default this will be <code>libudf/</code>.</p> <p>If you copied the compiled library from a different environment, you will have to delete this directory first.</p> <p>If the compiled library with the name specified in the case file (e.g. <code>libudf/</code>) is not found, fluent will try to compile it from the specified source file.</p> <p>If for some reason the UDF does not compile automatically, you can manually build it with the following command in your fluent journal file (should go before loading model).</p> <pre><code>define/user-defined/compiled-functions compile \"\" yes \"\" \"\" \"\" \"\" \"\" \"\"\n</code></pre> <p>Note, the command must end with two <code>\"\"</code> to indicate there are no more files to add.</p> <p>As an example</p> <pre><code>define/user-defined/compiled-functions compile \"libudf\" yes \"myUDF.c\" \"\" \"\"\n</code></pre> <p>Will compile the code <code>myUDF.c</code> into a library named <code>libudf</code></p>","tags":["mahuika","application","engineering"]},{"location":"Scientific_Computing/Supported_Applications/ANSYS/#loading-file","title":"Loading File","text":"<pre><code>define/user-defined/compiled-functions load libudf\n</code></pre> <p>Will load the library <code>libudf</code> to be accessible by ANSYS.</p>","tags":["mahuika","application","engineering"]},{"location":"Scientific_Computing/Supported_Applications/ANSYS/#udf-errors","title":"UDF errors","text":"<pre><code>Error: chip-exec: function\n</code></pre> <p>might be using interpreted func</p> <p>solution specify as relative path, or unload compiled lib before saving .cas file.</p>","tags":["mahuika","application","engineering"]},{"location":"Scientific_Computing/Supported_Applications/ANSYS/#cfx","title":"CFX","text":"<p><code>cfx5solve -help</code> for a list of commands.</p> SerialDistributed Memory <p>Single process with a single thread (2 threads if hyperthreading enabled). Usually submitted as part of an array, as in the case of parameter sweeps.</p> <pre><code>#!/bin/bash -e\n#SBATCH --job-name      CFX-serial\n#SBATCH --licenses      aa_r@uoa_foe:1    #One research license\n#SBATCH --time          00:05:00          # Walltime\n#SBATCH --cpus-per-task 1                 # Double if hyperthreading enabled\n#SBATCH --mem           512MB             # total mem\n#SBATCH --hint          nomultithread     # Hyperthreading disabled\n\nmodule load ANSYS/2023R2\n\ninput=\"/share/test/ansys/cfx/pump.def\"\ncfx5solve -batch -def ${input}\n</code></pre> <p>Multiple processes\u00a0each with a single\u00a0thread. Not limited to one node. Model will be segmented into <code>-np</code>\u00a0pieces which should be equal to\u00a0<code>--ntasks</code>. Each task could be running on a different node leading to increased communication overhead. Jobs can be limited to a single node by adding\u00a0<code>--nodes=1</code>\u00a0however this may increase you time in the queue as\u00a0contiguous cpu's are harder to schedule.</p> <pre><code>#!/bin/bash -e\n#SBATCH --job-name          ANSYS-Dis\n#SBATCH --time              00:05:00          # Walltime\n#SBATCH --licenses          aa_r@uoa_foe:1,aa_r_hpc@uoa_foe:20\n##One research license, (ntasks-16) hpc licenses\n#SBATCH --nodes             1                 # Limit to n nodes (Optional)\n#SBATCH --ntasks            36                # Number processes\n#SBATCH --cpus-per-task     1                 # Double if hyperthreading enabled\n#SBATCH --mem-per-cpu       512MB             # Standard for large partition\n#SBATCH --hint              nomultithread     # Hyperthreading disabled\n\nmodule load ANSYS/2023R2\ninput=\"/share/test/ansys/mechanical/structural.dat\" \ncfx5solve -batch -def \"${input} -part ${SLURM_NTASKS}\n</code></pre> <p>Tip</p> <p>Initial values path specified in '.def' file can be overridden using the <code>-ini</code> flag.</p>","tags":["mahuika","application","engineering"]},{"location":"Scientific_Computing/Supported_Applications/ANSYS/#cfx-post","title":"CFX-Post","text":"<p>Even when running headless (without a GUI) CFX-Post requires connection to a graphical output. For some cases it may be suitable running CFX-Post on the login node and using your X-11 display, but for larger batch compute jobs you will need to make use of a dummy X-11 server.</p> <p>This is as simple as prepending your command with the X Virtual Frame Buffer command.</p> <pre><code>xvfb-run cfx5post input.cse\n</code></pre>","tags":["mahuika","application","engineering"]},{"location":"Scientific_Computing/Supported_Applications/ANSYS/#mechanical-apdl","title":"Mechanical APDL","text":"","tags":["mahuika","application","engineering"]},{"location":"Scientific_Computing/Supported_Applications/ANSYS/#examples","title":"Examples","text":"SerialShared MemoryDistributed Memory <p>Single process with a single *thread (2 threads if hyperthreading enabled). Usually submitted as part of an array, as in the case of parameter sweeps.</p> <pre><code>#!/bin/bash -e\n#SBATCH --job-name      ANSYS-serial\n#SBATCH --licenses aa_r@uoa_foe:1\n#SBATCH --time          00:05:00          # Walltime\n#SBATCH --mem           1500M             # total mem\n#SBATCH --hint          nomultithread     # Hyperthreading disabled\n\nmodule load ANSYS/2023R2\n\ninput=${ANSYS_ROOT}/ansys/data/verif/vm263.dat\nmapdl -b -i \"${input}\n</code></pre> <p>Single process multiple *threads. All threads must be on the same node, limiting scalability.</p> <p>Number of threads is set by <code>-np</code>\u00a0and should be equal to<code>--cpus-per-task</code>. Not recommended if using more than\u00a08 cores (16 CPUs if hyperthreading enabled).</p> <pre><code>#!/bin/bash -e\n#SBATCH --job-name      ANSYS-Shared\n#SBATCH --licenses aa_r@uoa_foe:1\n#SBATCH --time          00:05:00          # Walltime\n#SBATCH --cpus-per-task 8                 # Double if hyperthreading enabled\n#SBATCH --mem           12G               # 8 threads at 1500 MB per thread\n#SBATCH --hint          nomultithread     # Hyperthreading disabled\n\nmodule load ANSYS/2023R2\ninput=${ANSYS_ROOT}/ansys/data/verif/vm263.dat\nmapdl -b -np ${SLURM_CPUS_PER_TASK} -i ${input}\n</code></pre> <p>Multiple processes\u00a0each with a single\u00a0thread. Not limited to one node. Model will be segmented into <code>-np</code>\u00a0pieces which should be equal to\u00a0<code>--ntasks</code>. Each task could be running on a different node leading to increased communication overhead. Jobs can be limited to a single node by adding\u00a0\u00a0<code>--nodes=1</code>\u00a0however this will increase your time in the queue as\u00a0contiguous cpu's are harder to schedule. Distributed Memory Parallel is currently not supported on M\u0101ui.</p> <pre><code>#!/bin/bash -e\n\n#SBATCH --job-name          ANSYS-Dis\n#SBATCH --licenses aa_r@uoa_foe:1,aa_r_hpc@uoa_foe:4\n#SBATCH --time              00:05:00          # Walltime\n#SBATCH --nodes             1                 # (OPTIONAL) Limit to n nodes\n#SBATCH --ntasks            16                # Number processes\n#SBATCH --mem-per-cpu       1500\n#SBATCH --hint              nomultithread     # Hyperthreading disabled\n\nmodule load ANSYS/2023R2\ninput=${ANSYS_ROOT}/ansys/data/verif/vm263.dat\nmapdl -b -dis -np ${SLURM_NTASKS} -i \"${input}\"\n</code></pre> <p>Not all MAPDL solvers work using distributed memory.\u00a0</p> Sparse \u2714 PCG \u2714 ICCG \u2716 JCG \u2716 QMR \u2716 Block Lanczos eigensolver \u2716 PCG Lanczos eigensolver \u2714 Supernode eigensolver \u2716 Subspace eigensolver \u2714 Unsymmetric eigensolver \u2714 Damped eigensolver \u2714 QRDAMP eigensolver \u2716 Element formulation \u2714 Results calculation \u2714 Pre/Postprocessing \u2716","tags":["mahuika","application","engineering"]},{"location":"Scientific_Computing/Supported_Applications/ANSYS/#ls-dyna","title":"LS-DYNA","text":"","tags":["mahuika","application","engineering"]},{"location":"Scientific_Computing/Supported_Applications/ANSYS/#fluid-structure-example","title":"Fluid-Structure Example","text":"<pre><code>#!/bin/bash -e\n#SBATCH --job-name      LS-DYNA\n#SBATCH --account       nesi99999         # Project Account\n#SBATCH --time          01:00:00          # Walltime\n#SBATCH --ntasks        16                # Number of CPUs to use\n#SBATCH --mem-per-cpu   512MB             # Memory per cpu\n#SBATCH --hint          nomultithread     # No hyperthreading\n\nmodule load ANSYS/2023R2\ninput=3cars_shell2_150ms.k\nlsdyna -dis -np $SLURM_NTASKS i=\"$input\" memory=$(($SLURM_MEM_PER_CPU/8))M\n</code></pre>","tags":["mahuika","application","engineering"]},{"location":"Scientific_Computing/Supported_Applications/ANSYS/#fensap-ice","title":"FENSAP-ICE","text":"<p>FENSAP-ICE is a fully integrated ice-accretion and aerodynamics simulator.</p> <p>Currently FENSAP-ICE is only available on Mahuika and in ANSYS 19.2.</p> <p>The following FENSAP solvers are compatible with MPI</p> <ul> <li>FENSAP</li> <li>DROP3D</li> <li>ICE3D</li> <li>C3D</li> <li>OptiGrid</li> </ul>","tags":["mahuika","application","engineering"]},{"location":"Scientific_Computing/Supported_Applications/ANSYS/#case-setup","title":"Case setup","text":"","tags":["mahuika","application","engineering"]},{"location":"Scientific_Computing/Supported_Applications/ANSYS/#with-gui","title":"With GUI","text":"<p>If you have set up X-11 forwarding, you may launch the FENSAP ice using the command <code>fensapiceGUI</code>\u00a0from within your FENSAP project directory.</p> <ol> <li>Launch the run and select the desired number of (physical) CPUs.</li> <li>Open the 'configure' panel.     </li> <li> <p>Under 'Additional mpirun parameters' add     your inline SLURM options. You should include at least.</p> <pre><code>--job-name my_job\n--mem-per-cpu memory\n--time time\n--licenses \n--hint nomultithread\n</code></pre> <p>Note: All these parameters will be applied to each individual step. 4. Start the job. You can track progress under the 'log' tab. </p> </li> </ol> <p>You may close your session and the job will continue to run on the compute nodes. You will be able to view the running job at any time by opening the GUI within the project folder.</p> <p>Info</p> <p>Submitting your job through the use of the GUI has disadvantages and may not be suitable in all cases. -   Closing the session or losing connection will prevent the next     stage of the job starting (currently executing step will continue     to run).\u00a0 It is a good idea to launch the GUI inside a tmux/screen     session then send the process to background to avoid this. -   Each individual step will be launched with the same parameters     given in the GUI. -   By default 'restart' is set to disabled. If you wish to continue a     job from a given step/shot you must select so in the dropdown     menu.</p>","tags":["mahuika","application","engineering"]},{"location":"Scientific_Computing/Supported_Applications/ANSYS/#using-fensap2slurm","title":"Using fensap2slurm","text":"<p>Set up your model as you would normally, except rather than starting the run just click 'save'. You do not need to set number of CPUs or MPI configuration. Then in your terminal type <code>fensap2slurm path/to/project</code> or run <code>fensap2slurm</code> from inside the run directory.</p> <p>This will generate a template file for each stage of the job, edit these as you would a normal SLURM script and set the resources requirements.</p> <p>For your first shot, it is a good idea to set <code>CONTINUE=FALSE</code> for the last stage of the shot, that way you can set more accurate resource requirements for the remainder.</p> <p>The workflow can then by running <code>.solvercmd</code> e.g <code>bash .solvercmd</code>. Progress can be tracked through the GUI as usual.</p>","tags":["mahuika","application","engineering"]},{"location":"Scientific_Computing/Supported_Applications/ANSYS/#ansys-electromagnetic","title":"ANSYS-Electromagnetic","text":"<p>ANSYS-EM jobs can be submitted through a slurm script or by interactive session.</p>","tags":["mahuika","application","engineering"]},{"location":"Scientific_Computing/Supported_Applications/ANSYS/#rsm","title":"RSM","text":"<p>Unlike other ANSYS applications ANSYS-EM requires RSM (remote solver manager) running on all nodes. The command <code>startRSM</code> has been written to facilitate this and needs to be run after starting the slurm job but before running edt. Please contact NeSI support if the command is not working for you.</p>","tags":["mahuika","application","engineering"]},{"location":"Scientific_Computing/Supported_Applications/ANSYS/#example-slurm-script","title":"Example Slurm Script","text":"<pre><code>#!/bin/bash -e\n\n#SBATCH\u00a0--time\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0        04:00:00\n#SBATCH\u00a0--nodes\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0        2\n#SBATCH\u00a0--ntasks-per-node\u00a0\u00a0\u00a0\u00a0\u00a036\n#SBATCH --mem-per-cpu         1500\n\nmodule load ANSYS/2023R2\nINPUTNAME=\"Sim1.aedt\"\nstartRSM\n\nansysedt\u00a0-ng\u00a0-batchsolve\u00a0-distributed\u00a0-machinelistfile=\".machinefile\" -batchoptions \"HFSS/HPCLicenseType=Pool\" $INPUTNAME\n</code></pre> <p>All batch options can be listed using</p> <pre><code>ansysedt\u00a0-batchoptionhelp\n</code></pre> <p>(Note, this requires a working X-server)</p> <p>Info</p> <p>Each batch option must have it's own flag, e.g.  <pre><code>-batchoptions \"HFSS/HPCLicenseType=Pool\" -batchoptions \"Desktop/ProjectDirectory=$PWD\"\u00a0-batchoptions\u00a0\"HFSS/MPIVendor=Intel\"\n</code></pre></p>","tags":["mahuika","application","engineering"]},{"location":"Scientific_Computing/Supported_Applications/ANSYS/#em-interactive","title":"EM Interactive","text":"<p>First start an interactive slurm session.</p> <pre><code>salloc --job-name edt_interactive --nodes 2 --ntasks-per-node 36 --mem-per-cpu 1500\n</code></pre> <p>Then load your desired version of ANSYS</p> <pre><code>module load ANSYS/2023R2\n</code></pre> <p>Run the script to start startRSM, this will start ANSYS remote solver on your requested nodes, and set the environment variable <code>MACHINELIST</code>.</p> <pre><code>startRSM\n</code></pre> <p>Then launch ansys edt with the following flags</p> <pre><code>ansysedt\u00a0-machinelist\u00a0file=\".machinefile\" -batchoptions \"HFSS/HPCLicenseType=Pool HFSS/MPIVendor=Intel HFSS/UseLegacyElectronicsHPC=1\"\n</code></pre>","tags":["mahuika","application","engineering"]},{"location":"Scientific_Computing/Supported_Applications/ANSYS/#multiphysics","title":"Multiphysics","text":"","tags":["mahuika","application","engineering"]},{"location":"Scientific_Computing/Supported_Applications/ANSYS/#example-mapdl-fluent-interaction","title":"Example - MAPDL Fluent Interaction","text":"<pre><code>#!/bin/bash -e\n\n#SBATCH --job-name      ANSYS_FSI\n#SBATCH --account       nesi99999         # Project Account\n#SBATCH --time          01:00:00          # Walltime\n#SBATCH --ntasks        16                # Number of CPUs to use\n#SBATCH --mem-per-cpu   2GB               # Memory per CPU\n#SBATCH --hint          nomultithread     # No hyperthreading\n\nmodule load ANSYS/2023R2\n\nCOMP_CPUS=$((SLURM_NTASKS-1))\nMECHANICAL_CPUS=1\nFLUID_CPUS=$((COMP_CPUS-MECHANICAL_CPUS))\nexport SLURM_EXCLUSIVE=\"\" # don't share CPUs\necho \"CPUs: Coupler:1 Struct:$MECHANICAL_CPUS Fluid:$FLUID_CPUS\"\n\necho \"STARTING SYSTEM COUPLER\"\n\ncd Coupling\n\n# Run the system coupler in the background.\nsrun -N1 -n1 $WORKBENCH_CMD \\\n    ansys.services.systemcoupling.exe \\\n    -inputFile coupling.sci || scancel $SLURM_JOBID &amp;\ncd ..\nserverfile=\"$PWD/Coupling/scServer.scs\"\n\nwhile [[ ! -f \"$serverfile\" ]] ; do\n    sleep 1 # waiting for SC to start\ndone\nsleep 1\n\necho \"PARSING SYSTEM COUPLER CONFIG\"\n\n{\n    read hostport\n    port=${hostport%@*}\n    node=${hostport#*@}\n    read count\n    for solver in $(seq $count)\n    do\n        read solname\n        read soltype\n        case $soltype in \n            Fluid) fluentsolname=$solname;;\n            Structural) mechsolname=$solname;;\n        esac\n    done\n} &lt; \"$serverfile\"\n\necho \" Port number: $port\"\necho \" Node name: $node\"\necho \" Fluent name: $fluentsolname\"\necho \" Mechanical name: $mechsolname\"\n\necho \"STARTING ANSYS\"\n\ncd Structural\n\n# Run ANSYS in the background, alongside the system coupler and Fluent.\nmapdl -b -dis -mpi intel -np $MECHANICAL_CPUS \\\n    -scport $port -schost $node -scname \"$mechsolname\" \\\n    -i \"structural.dat\" &gt; struct.out || scancel $SLURM_JOBID &amp;\ncd ..\n\nsleep 2\necho \"STARTING FLUENT\"\n\ncd FluidFlow\n\n# Run Fluent in the background, alongside the system coupler and ANSYS.\nfluent 3ddp -g -t$FLUID_CPUS \\\n    -scport=$port -schost=$node -scname=\"$fluentsolname\" \\\n    -i \"fluidFlow.jou\" &gt; fluent.out || scancel $SLURM_JOBID &amp;\ncd ..\n\n# Before exiting, wait for all background tasks (the system coupler, ANSYS and\n# Fluent) to complete.\nwait\n</code></pre>","tags":["mahuika","application","engineering"]},{"location":"Scientific_Computing/Supported_Applications/ANSYS/#best-practices","title":"Best Practices","text":"","tags":["mahuika","application","engineering"]},{"location":"Scientific_Computing/Supported_Applications/ANSYS/#gpu-acceleration-support","title":"GPU acceleration support","text":"<p>GPUs can be slow for smaller jobs because it takes time to transfer data from the main memory to the GPU memory. We therefore suggest that you only use them for larger jobs, unless benchmarking reveals otherwise.</p>","tags":["mahuika","application","engineering"]},{"location":"Scientific_Computing/Supported_Applications/ANSYS/#interactive-use","title":"Interactive use","text":"<p>It is best to use journal files etc to automate ANSYS so that you can submit batch jobs, but when interactivity is really needed alongside more CPU power and/or memory than is reasonable to take from a login node (maybe postprocessing a large output file) then an alternative which may work is to run the GUI frontend on a login node while the MPI tasks it launches run on a compute node. This requires using salloc instead of sbatch, for example:</p> <pre><code>salloc -A nesi99999 -t 30 -n 16 -C avx --mem-per-cpu=512MB bash -c 'module load ANSYS; fluent -v3ddp -t$SLURM_NTASKS' \n</code></pre> <p>As with any job, you may have to wait a while before the resource is granted and you can begin, so you might want to use the --mail-type=BEGIN and --mail-user= options.</p>","tags":["mahuika","application","engineering"]},{"location":"Scientific_Computing/Supported_Applications/ANSYS/#hyperthreading","title":"Hyperthreading","text":"<p>Utilising hyperthreading (ie: removing the \"--hint=nomultithread\" sbatch directive and doubling the number of tasks) will give a small speedup on most jobs with less than 8 cores, but also doubles the number of <code>aa_r_hpc</code> license tokens required.</p>","tags":["mahuika","application","engineering"]},{"location":"Scientific_Computing/Supported_Applications/AlphaFold/","title":"AlphaFold","text":"<p>AlphaFold can predict protein structures with atomic accuracy even where no similar structure is known</p> <p>AlphaFold Homepage</p>","tags":[]},{"location":"Scientific_Computing/Supported_Applications/AlphaFold/#available-modules","title":"Available Modules","text":"Mahuika <p> 2.3.2 </p> <pre><code>module load AlphaFold/2.3.2</code></pre> <p>Prerequisite</p> <p>An extended version of AlphaFold2 on NeSI Mahuika cluster which  contains additional information such as visualisation of AlphaFold  outputs, etc can be found  here</p>","tags":[]},{"location":"Scientific_Computing/Supported_Applications/AlphaFold/#description","title":"Description","text":"<p>This package provides an implementation of the inference pipeline of AlphaFold v2.0. This is a completely new model that was entered in CASP14 and published in Nature. For simplicity, we refer to this model as AlphaFold throughout the rest of this document.</p> <p>Any publication that discloses findings arising from using this source code or the model parameters should\u00a0cite\u00a0the\u00a0AlphaFold paper. Please also refer to the\u00a0Supplementary Information\u00a0for a detailed description of the method.</p> <p>Home page is at https://github.com/deepmind/alphafold</p>","tags":[]},{"location":"Scientific_Computing/Supported_Applications/AlphaFold/#license-and-disclaimer","title":"License and Disclaimer","text":"<p>This is not an officially supported Google product.</p> <p>Copyright 2021 DeepMind Technologies Limited.</p>","tags":[]},{"location":"Scientific_Computing/Supported_Applications/AlphaFold/#alphafold-code-license","title":"AlphaFold Code License","text":"<p>Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with the License. You may obtain a copy of the License at\u00a0https://www.apache.org/licenses/LICENSE-2.0.</p> <p>Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License.</p>","tags":[]},{"location":"Scientific_Computing/Supported_Applications/AlphaFold/#model-parameters-license","title":"Model Parameters License","text":"<p>The AlphaFold parameters are made available for non-commercial use only, under the terms of the Creative Commons Attribution-NonCommercial 4.0 International (CC BY-NC 4.0) license. You can find details at:\u00a0https://creativecommons.org/licenses/by-nc/4.0/legalcode</p>","tags":[]},{"location":"Scientific_Computing/Supported_Applications/AlphaFold/#alphafold-databases","title":"AlphaFold Databases","text":"<p>AlphaFold databases are stored in <code>/opt/nesi/db/alphafold_db/</code>\u00a0 parent directory. In order to make the database calling more convenient, we have prepared modules for each version of the database. Running <code>module spider AlphaFold2DB</code> will list the available versions based on when they were downloaded (Year-Month)</p> <pre><code>$ module spider AlphaFold2DB\n\n--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\nAlphaFold2DB: AlphaFold2DB/2022-06\n--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\nDescription:\nAlphaFold2 databases\n\n Versions:\n         AlphaFold2DB/2022-06\n         AlphaFold2DB/2023-04\n</code></pre> <p>Loading a module will set the <code>$AF2DB</code> variable which is pointing to the\u00a0 selected version of the database. For an example.\u00a0</p> <pre><code>$ module load AlphaFold2DB/2023-04\n\n$ echo $AF2DB \n/opt/nesi/db/alphafold_db/2023-04\n</code></pre>","tags":[]},{"location":"Scientific_Computing/Supported_Applications/AlphaFold/#alphafold-module-232","title":"AlphaFold module ( &gt;= 2.3.2)","text":"<p>As of version 2.3.2 of AlphaFold, we recommend deploying AlphaFold via the module (previous versoions were done via a Singularity container )</p>","tags":[]},{"location":"Scientific_Computing/Supported_Applications/AlphaFold/#example-slurm-script-for-monomer","title":"Example Slurm script for monomer","text":"<p>Input fasta used in following example\u00a0 is 3RGK (https://www.rcsb.org/structure/3rgk).</p> <pre><code>#!/bin/bash -e\n\n#SBATCH --account       nesi12345\n#SBATCH --job-name      af-2.3.2-monomer\n#SBATCH --mem           24G\n#SBATCH --cpus-per-task 8\n#SBATCH --gpus-per-node P100:1\n#SBATCH --time          02:00:00\n#SBATCH --output        %j.out\n\nmodule purge\nmodule load AlphaFold2DB/2023-04\nmodule load AlphaFold/2.3.2\n\nINPUT=/nesi/project/nesi12345/alphafold/input_data\nOUTPUT=/nesi/project/nesi12345/alphafold/results\n\nrun_alphafold.py --use_gpu_relax \\\n--data_dir=$AF2DB \\\n--uniref90_database_path=$AF2DB/uniref90/uniref90.fasta \\\n--mgnify_database_path=$AF2DB/mgnify/mgy_clusters_2022_05.fa \\\n--bfd_database_path=$AF2DB/bfd/bfd_metaclust_clu_complete_id30_c90_final_seq.sorted_opt \\\n--uniref30_database_path=$AF2DB/uniref30/UniRef30_2021_03 \\\n--pdb70_database_path=$AF2DB/pdb70/pdb70 \\\n--template_mmcif_dir=$AF2DB/pdb_mmcif/mmcif_files \\\n--obsolete_pdbs_path=$AF2DB/pdb_mmcif/obsolete.dat \\\n--model_preset=monomer \\\n--max_template_date=2022-6-1 \\\n--db_preset=full_dbs \\\n--output_dir=$OUTPUT \\\n--fasta_paths=${INPUT}/rcsb_pdb_3GKI.fasta\n</code></pre>","tags":[]},{"location":"Scientific_Computing/Supported_Applications/AlphaFold/#example-slurm-script-for-multimer","title":"Example Slurm script for multimer","text":"<p>Input fasta used in following example</p> <pre><code>    T1083\nGAMGSEIEHIEEAIANAKTKADHERLVAHYEEEAKRLEKKSEEYQELAKVYKKITDVYPNIRSYMVLHYQNLTRRYKEAAEENRALAKLHHELAIVED\n    T1084\nMAAHKGAEHHHKAAEHHEQAAKHHHAAAEHHEKGEHEQAAHHADTAYAHHKHAEEHAAQAAKHDAEHHAPKPH\n</code></pre> <pre><code>#!/bin/bash -e\n\n#SBATCH --account       nesi12345\n#SBATCH --job-name      af-2.3.2-multimer\n#SBATCH --mem           30G\n#SBATCH --cpus-per-task 4\n#SBATCH --gpus-per-node P100:1\n#SBATCH --time          01:45:00\n#SBATCH --output        slurmout.%j.out\n\nmodule purge\nmodule load AlphaFold2DB/2023-04\nmodule load AlphaFold/2.3.2\n\nINPUT=/nesi/project/nesi12345/input_data\nOUTPUT=/nesi/project/nesi12345/alphafold/2.3_multimer\n\nrun_alphafold.py \\\n--use_gpu_relax \\\n--data_dir=$AF2DB \\\n--model_preset=multimer \\\n--uniprot_database_path=$AF2DB/uniprot/uniprot.fasta \\\n--uniref90_database_path=$AF2DB/uniref90/uniref90.fasta \\\n--mgnify_database_path=$AF2DB/mgnify/mgy_clusters_2022_05.fa \\\n--bfd_database_path=$AF2DB/bfd/bfd_metaclust_clu_complete_id30_c90_final_seq.sorted_opt \\\n--uniref30_database_path=$AF2DB/uniref30/UniRef30_2021_03 \\\n--pdb_seqres_database_path=$AF2DB/pdb_seqres/pdb_seqres.txt \\\n--template_mmcif_dir=$AF2DB/pdb_mmcif/mmcif_files \\\n--obsolete_pdbs_path=$AF2DB/pdb_mmcif/obsolete.dat \\\n--max_template_date=2022-6-1 \\\n--db_preset=full_dbs \\\n--output_dir=${OUTPUT} \\\n--fasta_paths=${INPUT}/test_multimer.fasta\n</code></pre>","tags":[]},{"location":"Scientific_Computing/Supported_Applications/AlphaFold/#alphafold-singularity-container-prior-to-v232","title":"AlphaFold Singularity container (prior to v2.3.2)","text":"<p>If you would like to use a version prior to 2.3.2, It can be done via the Singularity containers.</p> <p>We prepared a Singularity container image based on the official Dockerfile with some modifications. Image (.simg) and the corresponding definition file (.def) are stored in <code>/opt/nesi/containers/AlphaFold/</code></p>","tags":[]},{"location":"Scientific_Computing/Supported_Applications/AlphaFold/#example-slurm-scripts-for-singularity-container-based-af2-deployment","title":"Example Slurm scripts for Singularity container based AF2 deployment","text":"","tags":[]},{"location":"Scientific_Computing/Supported_Applications/AlphaFold/#monomer","title":"Monomer","text":"<pre><code>#!/bin/bash -e\n\n#SBATCH --account       nesi12345\n#SBATCH --job-name      alphafold2_monomer_example\n#SBATCH --mem           30G\n#SBATCH --cpus-per-task 6\n#SBATCH --gpus-per-node P100:1 \n#SBATCH --time          02:00:00\n#SBATCH --output        slurmout.%j.out\n\nmodule purge\nmodule load AlphaFold2DB/2022-06\nmodule load cuDNN/8.1.1.33-CUDA-11.2.0 Singularity/3.9.8\n\nINPUT=/path/to/input_data\nOUTPUT=/path/to/results\n\nexport SINGULARITY_BIND=\"$INPUT,$OUTPUT,$AF2DB\"\n\nsingularity exec --nv /opt/nesi/containers/AlphaFold/alphafold_2.2.0.simg python /app/alphafold/run_alphafold.py \\\n--use_gpu_relax \\\n--data_dir=$AF2DB \\\n--uniref90_database_path=$AF2DB/uniref90/uniref90.fasta \\\n--mgnify_database_path=$AF2DB/mgnify/mgy_clusters_2018_12.fa \\\n--bfd_database_path=$AF2DB/bfd/bfd_metaclust_clu_complete_id30_c90_final_seq.sorted_opt \\\n--uniclust30_database_path=$AF2DB/uniclust30/uniclust30_2018_08/uniclust30_2018_08 \\\n--pdb70_database_path=$AF2DB/pdb70/pdb70 \\\n--template_mmcif_dir=$AF2DB/pdb_mmcif/mmcif_files \\\n--obsolete_pdbs_path=$AF2DB/pdb_mmcif/obsolete.dat \\\n--model_preset=monomer \\\n--max_template_date=2022-1-1 \\\n--db_preset=full_dbs \\\n--output_dir=$OUTPUT \\\n--fasta_paths=$INPUT/rcsb_pdb_3GKI.fasta\n</code></pre>","tags":[]},{"location":"Scientific_Computing/Supported_Applications/AlphaFold/#multimer","title":"Multimer","text":"<pre><code>#!/bin/bash -e\n\n#SBATCH --account       nesi12345\n#SBATCH --job-name      alphafold2_monomer_example\n#SBATCH --mem           30G\n#SBATCH --cpus-per-task 6\n#SBATCH --gpus-per-node P100:1 \n#SBATCH --time          02:00:00\n#SBATCH --output        slurmout.%j.out\n\nmodule purge\nmodule load AlphaFold2DB/2022-06\nmodule load cuDNN/8.1.1.33-CUDA-11.2.0 Singularity/3.9.8\n\nINPUT=/path/to/input_data\nOUTPUT=/path/to/results\n\n\nexport SINGULARITY_BIND=\"$INPUT,$OUTPUT,$AF2DB\"\n\nsingularity exec --nv /opt/nesi/containers/AlphaFold/alphafold_2.2.0.simg python /app/alphafold/run_alphafold.py \\\n--use_gpu_relax \\\n--data_dir=$AF2DB \\\n--uniref90_database_path=$AF2DB/uniref90/uniref90.fasta \\\n--mgnify_database_path=$AF2DB/mgnify/mgy_clusters_2018_12.fa \\\n--bfd_database_path=$AF2DB/bfd/bfd_metaclust_clu_complete_id30_c90_final_seq.sorted_opt \\\n--uniclust30_database_path=$AF2DB/uniclust30/uniclust30_2018_08/uniclust30_2018_08 \\\n--pdb_seqres_database_path=$AF2DB/pdb_seqres/pdb_seqres.txt \\\n--template_mmcif_dir=$AF2DB/pdb_mmcif/mmcif_files \\\n--obsolete_pdbs_path=$AF2DB/pdb_mmcif/obsolete.dat \\\n--uniprot_database_path=$AF2DB/uniprot/uniprot.fasta \\\n--model_preset=multimer \\\n--max_template_date=2022-1-1 \\\n--db_preset=full_dbs \\\n--output_dir=$OUTPUT \\\n--fasta_paths=$INPUT/rcsb_pdb_3GKI.fasta\n</code></pre>","tags":[]},{"location":"Scientific_Computing/Supported_Applications/AlphaFold/#explanation-of-slurm-variables-and-singularity-flags","title":"Explanation of Slurm variables and Singularity flags","text":"<ol> <li>Values for\u00a0<code>--mem</code>\u00a0,\u00a0<code>--cpus-per-task</code>\u00a0and\u00a0<code>--time</code>\u00a0Slurm variables     are for\u00a03RGK.fasta. Adjust them accordingly</li> <li>We have tested this on both P100 and A100 GPUs where the runtimes     were identical. Therefore, the above example was set to former     via\u00a0<code>P100:1</code></li> <li>The\u00a0<code>--nv</code>\u00a0flag enables GPU support.</li> <li><code>--pwd /app/alphafold</code>\u00a0is to workaround this\u00a0existing     issue</li> </ol>","tags":[]},{"location":"Scientific_Computing/Supported_Applications/AlphaFold/#alphafold2-initial-release-this-version-does-not-support-multimer","title":"AlphaFold2 : Initial Release ( this version does not support <code>multimer</code>)","text":"<p>Input fasta used in following example and subsequent benchmarking is 3RGK (https://www.rcsb.org/structure/3rgk).</p>","tags":[]},{"location":"Scientific_Computing/Supported_Applications/AlphaFold/#troubleshooting","title":"Troubleshooting","text":"<ul> <li>If you are to encounter the message \"RuntimeError: Resource     exhausted: Out of memory\" , add the following variables to the     slurm script</li> </ul> <p>For module based runs</p> <pre><code>export TF_FORCE_UNIFIED_MEMORY=1\nexport XLA_PYTHON_CLIENT_MEM_FRACTION=4.0\n</code></pre> <p>For Singularity based runs</p> <pre><code>export SINGULARITYENV_TF_FORCE_UNIFIED_MEMORY=1 \nexport SINGULARITYENV_XLA_PYTHON_CLIENT_MEM_FRACTION=4.0\n</code></pre>","tags":[]},{"location":"Scientific_Computing/Supported_Applications/BLAST/","title":"BLAST","text":"<p>Basic Local Alignment Search Tool, or BLAST, is an algorithm</p> <p>BLAST Homepage</p>","tags":["mahuika","biology"]},{"location":"Scientific_Computing/Supported_Applications/BLAST/#available-modules","title":"Available Modules","text":"Mahuika <p> 2.3.0 2.6.0-gimkl-2017a 2.6.0-gimkl-2018b 2.9.0-gimkl-2018b 2.10.0-GCC-9.2.0 2.12.0-GCC-9.2.0 2.13.0-GCC-11.3.0 </p> <pre><code>module load BLAST/2.13.0-GCC-11.3.0</code></pre>","tags":["mahuika","biology"]},{"location":"Scientific_Computing/Supported_Applications/BLAST/#blast-databases","title":"BLAST Databases","text":"<p>We download the standard NCBI databases quarterly, and create a corresponding environment module named like <code>BLASTDB/&lt;yyyy-mm&gt;</code> which sets the BLASTDB environment variable accordingly. If you want to use one of these databases then you should find out what our most recent version is (<code>module avail BLASTDB</code>) and then load it in your batch script.</p> <pre><code>module load BLASTDB\nls $BLASTDB\n</code></pre> <p>Because we only keep a few recent versions of the databases, you may be required from time to time to change the BLASTDB module version if you use old job submission scripts as templates for new ones.</p>","tags":["mahuika","biology"]},{"location":"Scientific_Computing/Supported_Applications/BLAST/#example-scripts","title":"Example scripts","text":"<p>When given a large amount of query sequence to get through the BLAST search programs will take batches of it, running through the database with each batch and then starting over with the next batch. \u00a0This can cause the database to be repeatedly read from disk and so limit the speed of your search, and using multiple threads only makes it worse. So there are two reasonable ways to run BLAST programs on our system: single threaded for small jobs, or multithreaded with a local copy of the database for large jobs. \u00a0If in doubt try the simpler single-thread approach first and see if it takes too long.</p>","tags":["mahuika","biology"]},{"location":"Scientific_Computing/Supported_Applications/BLAST/#single-thread","title":"Single Thread","text":"<p>For jobs which need less than 24 CPU-hours, eg: those that use small databases (&lt; 10 GB) or small amounts of query sequence (&lt; 1 GB), or fast BLAST programs such as\u00a0blastn with its default (megablast) settings. \u00a0</p> <pre><code>#!/bin/bash -e\n\n#SBATCH --job-name      BLAST\n#SBATCH --time          00:30:00  # ~10 CPU minutes / MB blastn query vs nt\n#SBATCH --mem           30G\n#SBATCH --cpus-per-task 2\n\nmodule load BLAST/2.13.0-GCC-11.3.0\nmodule load BLASTDB/2023-01\n\n# This script takes one argument, the FASTA file of query sequences.\nQUERIES=$1\nFORMAT=\"6 qseqid qstart qend qseq sseqid sgi sacc sstart send staxids sscinames stitle length evalue bitscore\"\nBLASTOPTS=\"-evalue 0.05 -max_target_seqs 10\"\nBLASTAPP=blastn\nDB=nt\n#BLASTAPP=blastx\n#DB=nr\n\n$BLASTAPP $BLASTOPTS -db $DB -query $QUERIES -outfmt \"$FORMAT\" \\\n    -out $QUERIES.$DB.$BLASTAPP -num_threads $SLURM_CPUS_PER_TASK\n</code></pre>","tags":["mahuika","biology"]},{"location":"Scientific_Computing/Supported_Applications/BLAST/#multiple-threads-and-local-database-copy","title":"Multiple threads and local database copy","text":"<p>For jobs which need more than 24 CPU-hours, eg: those that use large databases (&gt; 10 GB) or large amounts of query sequence (&gt; 1 GB), or slow BLAST searches such as classic blastn (<code>blastn -task blastn</code>).</p> <p>This script copies the BLAST database into the per-job temporary directory $TMPDIR before starting the search. Since compute nodes do not have local disks, this database copy is in memory, and so must be allowed for in the memory requested by the job. \u00a0As of mid 2023 that is 283 GB for the nt database, 157 GB for refseq_protein.</p> <pre><code>#!/bin/bash -e\n\n#SBATCH --job-name      BLAST\n#SBATCH --time          02:30:00\n#SBATCH --mem           120G  # 30 GB plus the database\n#SBATCH --ntasks        1\n#SBATCH --cpus-per-task 36    # half a node\n\nmodule load BLAST/2.13.0-GCC-11.3.0\nmodule load BLASTDB/2023-01\n\n# This script takes one argument, the FASTA file of query sequences.\nQUERIES=$1\nFORMAT=\"6 qseqid qstart qend qseq sseqid sgi sacc sstart send staxids sscinames stitle length evalue bitscore\"\nBLASTOPTS=\"-task blastn\"\nBLASTAPP=blastn\nDB=nt\n#BLASTAPP=blastx\n#DB=nr\n\n# Keep the database in RAM\ncp $BLASTDB/{$DB,taxdb}.* $TMPDIR/ \nexport BLASTDB=$TMPDIR\n\n$BLASTAPP $BLASTOPTS -db $DB -query $QUERIES -outfmt \"$FORMAT\" \\\n    -out $QUERIES.$DB.$BLASTAPP -num_threads $SLURM_CPUS_PER_TASK\n</code></pre>","tags":["mahuika","biology"]},{"location":"Scientific_Computing/Supported_Applications/BRAKER/","title":"BRAKER","text":"<p>Pipeline for fully automated prediction of protein coding genes with GeneMark-ES/ET</p> <p>BRAKER Homepage</p>","tags":[]},{"location":"Scientific_Computing/Supported_Applications/BRAKER/#available-modules","title":"Available Modules","text":"Mahuika <p> 2.1.6-gimkl-2020a-Perl-5.30.1-Python-3.8.2 2.1.6-gimkl-2022a-Perl-5.34.1 3.0.2-gimkl-2022a-Perl-5.34.1 3.0.3-gimkl-2022a-Perl-5.34.1 </p> <pre><code>module load BRAKER/3.0.3-gimkl-2022a-Perl-5.34.1</code></pre>","tags":[]},{"location":"Scientific_Computing/Supported_Applications/BRAKER/#description","title":"Description","text":"<p>The rapidly growing number of sequenced genomes requires fully automated methods for accurate gene structure annotation. With this goal in mind, we have developed BRAKER1<sup>R1R0</sup>, a combination of GeneMark-ET\u00a0<sup>R2</sup>\u00a0and AUGUSTUS\u00a0<sup>R3,\u00a0R4</sup>, that uses genomic and RNA-Seq data to automatically generate full gene structure annotations in novel genome.</p> <p>However, the quality of RNA-Seq data that is available for annotating a novel genome is variable, and in some cases, RNA-Seq data is not available, at all.</p> <p>BRAKER2 is an extension of BRAKER1 which allows for\u00a0fully automated training\u00a0of the gene prediction tools GeneMark-EX\u00a0<sup>R14,\u00a0R15,\u00a0R17,\u00a0F1</sup>\u00a0and AUGUSTUS from RNA-Seq and/or protein homology information, and that integrates the extrinsic evidence from RNA-Seq and protein homology information into the\u00a0prediction.</p> <p>In contrast to other available methods that rely on protein homology information, BRAKER2 reaches high gene prediction accuracy even in the absence of the annotation of very closely related species and in the absence of RNA-Seq data.</p> <p>BRAKER3 is the latest pipeline in the BRAKER suite. It enables the usage of RNA-seq\u00a0and\u00a0protein data in a fully automated pipeline to train and predict highly reliable genes with GeneMark-ETP and AUGUSTUS. The result of the pipeline is the combined gene set of both gene prediction tools, which only contains genes with very high support from extrinsic evidence.</p> <p>Home page : https://github.com/Gaius-Augustus/BRAKER</p>","tags":[]},{"location":"Scientific_Computing/Supported_Applications/BRAKER/#license-and-disclaimer","title":"License and Disclaimer","text":"<p>All source code, i.e.\u00a0<code>scripts/*.pl</code>\u00a0or\u00a0<code>scripts/*.py</code>\u00a0are under the Artistic License (see\u00a0http://www.opensource.org/licenses/artistic-license.php).</p>","tags":[]},{"location":"Scientific_Computing/Supported_Applications/BRAKER/#prerequisites","title":"Prerequisites","text":"<p>Obtain GeneMark-ES/ET Academic License</p> <p>GeneMark-ES/ET which is one of the dependencies for BRAKER requires an individual academic license\u00a0 (this is free). This can be obtained as below  -   Download URL http://topaz.gatech.edu/genemark/license_download.cgi  -     -   Downloaded filename will be in the format of gm_key_64.gz.\u00a0**  -   Decompress this file with <code>gunzip gm_key_64.gz</code>\u00a0 and move it to      home directory as\u00a0 a hidden** file under the filename <code>.gm_key</code> .i.e. <code>~/.gm_key</code></p> <p>Copy AUGUSTUS config to a path with read/write permissions</p> <p>Make a copy of AUGUSTUS config from /opt/nesi/CS400_centos7_bdw/AUGUSTUS/3.4.0-gimkl-2022a/config to path with read/write permissions .i.e. project, nobackup,home\u00a0</p>","tags":[]},{"location":"Scientific_Computing/Supported_Applications/BRAKER/#example-slurm-scripts","title":"Example Slurm scripts","text":"<p>Following example uses the .fa files provided BRAKER developers</p> <pre><code>#!/bin/bash -e\n\n#SBATCH --account nesi12345\n#SBATCH --job-name braker-test\n#SBATCH --cpus-per-task 4\n#SBATCH --mem 1G\n#SBATCH --time 02:00:00\n#SBATCH --output slurmlogs/%x.%j.out\n#SBATCH --error slurmlogs/%x.%j.err\n\n\nmodule purge\nmodule load BRAKER/3.0.2-gimkl-2022a-Perl-5.34.1\n\n#export the path to augustus config copied above - prerequisites\nexport AUGUSTUS_CONFIG_PATH=/path/to/augustus/config\n\nsrun braker.pl --threads=${SLURM_CPUS_PER_TASK} --genome=genome.fa --prot_seq=proteins.fa\n</code></pre> <p>This will generate the output directory named braker in the current working directory with content similar to below\u00a0</p> <pre><code>augustus.hints.aa              braker.gtf   genemark_evidence.gff  prothint.gff\naugustus.hints.codingseq       braker.log   genemark_hintsfile.gff seed_proteins.faa\naugustus.hints.gtf             cmd.log      genome_header.map      species/\naugustus.hints_iter1.aa        errors/      hintsfile.gff          uniqueSeeds.gtf\naugustus.hints_iter1.codingseq evidence.gff hintsfile_iter1.gff    what-to-cite.txt\naugustus.hints_iter1.gff       GeneMark-EP/ prevHints.gff \naugustus.hints_iter1.gtf       GeneMark-ES/ proteins.fa \n</code></pre>","tags":[]},{"location":"Scientific_Computing/Supported_Applications/CESM/","title":"CESM","text":"<p>The Community Earth System Model (CESM) is a coupled climate model for simulating Earth\u2019s climate system. Composed of separate models simultaneously simulating the Earth\u2019s atmosphere, ocean, land, river run-off, land-ice, and sea-ice, plus one central coupler/moderator component, CESM allows researchers to conduct fundamental research into the Earth\u2019s past, present, and future climate states.</p>","tags":[]},{"location":"Scientific_Computing/Supported_Applications/CESM/#building-cesm2-on-maui","title":"Building CESM2 on M\u0101ui","text":"<p>Here we provide a guide for downloading, building and running a CESM test case yourself on M\u0101ui This guide is based on CESM 2.1.</p>","tags":[]},{"location":"Scientific_Computing/Supported_Applications/CESM/#prerequisites","title":"Prerequisites","text":"<p>Git Large File Storage seems to be required to download some of the CESM components. Download the Git-LFS archive from here, copy the git-lfs executable to the bin directory in your home, add that directory to PATH and run a git command to finish the Git-LFS installation. The following commands will achieve this:</p> <pre><code>mkdir git-lfs\ncd git-lfs\nwget https://github.com/git-lfs/git-lfs/releases/download/v2.12.0/git-lfs-linux-amd64-v2.12.0.tar.gz\ntar xf git-lfs-linux-amd64-v2.12.0.tar.gz\nmkdir -p ~/bin\ncp git-lfs ~/bin/\nexport PATH=$PATH:~/bin\necho export PATH=\\$PATH:\\$HOME/bin &gt;&gt; ~/.bashrc\ngit lfs install\n</code></pre>","tags":[]},{"location":"Scientific_Computing/Supported_Applications/CESM/#download-cesm","title":"Download CESM","text":"<p>These instructions are based on the upstream documentation. First switch to your project directory (or wherever else you would like the CESM source to live) and then run the commands to download CESM (replacing &lt;your_project_code&gt; with your project code):</p> <pre><code>cd /nesi/project/&lt;your_project_code&gt;\ngit clone -b release-cesm2.1.3 https://github.com/ESCOMP/CESM.git my_cesm_sandbox\ncd my_cesm_sandbox\n./manage_externals/checkout_externals\n</code></pre> <p>Make sure the above command is successful (see the upstream documentation linked above for how to check). You may need to rerun the command until it is successful, especially if it asks you to accept a certificate.</p>","tags":[]},{"location":"Scientific_Computing/Supported_Applications/CESM/#nesi-specific-cime-configuration","title":"NeSI specific CIME configuration","text":"<p>Clone the repo containing NeSI specific CIME configuration (CIME provides a case control system for configuring, building and executing Earth system models) and copy the config files to ~/.cime. In the following, replace &lt;your_project_code&gt; with your project code (this will overwrite any current configuration your have in ~/.cime):</p> <pre><code>cd /nesi/project/&lt;your_project_code&gt;\ngit clone https://github.com/nesi/nesi-cesm-config.git\ncd nesi-cesm-config\nmkdir -p ~/.cime\nsed 's/nesi99999/&lt;your_project_code&gt;/g' config_machines.xml &gt; ~/.cime/config_machines.xml\ncp config_batch.xml ~/.cime/config_batch.xml\n</code></pre> <p>Check that you are happy with the paths specified in ~/.cime/config_machines.xml, in particular, it is recommended that CESM users share the same input data locations as the input data can be large.</p>","tags":[]},{"location":"Scientific_Computing/Supported_Applications/CESM/#setting-up-and-running-a-test-case","title":"Setting up and running a test case","text":"<p>Here we will run the test described in the CESM quick start guide. The following are basic instructions to run create and run the case, see the above link for more information.</p> <p>First, create the case:</p> <pre><code>cd /nesi/project/&lt;your_project_code&gt;/my_cesm_sandbox/cime/scripts\n./create_newcase --case /nesi/nobackup/&lt;your_project_code&gt;/$USER/cesm/output/b.e20.B1850.f19_g17.test --compset B1850 --res f19_g17 --machine maui --compiler intel\ncd /nesi/nobackup/&lt;your_project_code&gt;/$USER/cesm/output/b.e20.B1850.f19_g17.test\n</code></pre> <p>The --machine M\u0101ui --compiler intel arguments to ./create_case tell CESM to use the NeSI specific configuration we added to your home directory in the previous step.</p> <p>Next, set up the case and preview the run:</p> <pre><code>./case.setup\n./preview_run\n</code></pre> <p>Check that everything looks correct in the preview and then build the case:</p> <pre><code>./case.build\n</code></pre> <p>Update any settings if necessary, for example here we turn off short term archiving:</p> <pre><code>./xmlchange DOUT_S=FALSE\n</code></pre> <p>Finally, run the job:</p> <pre><code>./case.submit\n</code></pre> <p>A job will be submitted to the Slurm queue, you can view the queue using squeue or squeue -u $USER for just your own jobs. Check the job succeeded as described on the upstream quick start guide.</p>","tags":[]},{"location":"Scientific_Computing/Supported_Applications/CESM/#performance-tuning-optimising-processor-layout","title":"Performance tuning - optimising processor layout","text":"<p>With CESM it can be important to tune the processor layout for best performance with respect to the different components (atmosphere, land, sea ice, etc.). Many different configurations are possible, although there are some hard coded constraints, and this is well documented in CIME (the case control system used by CESM):</p> <p>https://esmci.github.io/cime/versions/maint-5.6/html/users_guide/pes-threads.html</p> <p>The above link lists some of the common configurations, such as fully sequential or fully sequential except the ocean running concurrently.</p> <p>One approach to load balancing (i.e. optimising processor layout) is documented on the above page in the section \"One approach to load balancing\" here. It involves performing a number of short model runs to determine which components are most expensive and how the individual components scale. That information can then be used to determine an optimal load balance.</p>","tags":[]},{"location":"Scientific_Computing/Supported_Applications/COMSOL/","title":"COMSOL","text":"<p>COMSOL is a multiphysics solver that provides a unified workflow for electrical, mechanical, fluid, and chemical applications.</p> <p>COMSOL Homepage</p> <p>Warning</p> <p>COMSOL is proprietary software. Make sure you meet the requirements for it's usage.</p>","tags":["engineering","COMSOL","cae","multiphysics","cfd","fea"]},{"location":"Scientific_Computing/Supported_Applications/COMSOL/#available-modules","title":"Available Modules","text":"Mahuika <p> 5.3 5.4 5.5 5.6 6.0 6.1 </p> <pre><code>module load COMSOL/6.1</code></pre>","tags":["engineering","COMSOL","cae","multiphysics","cfd","fea"]},{"location":"Scientific_Computing/Supported_Applications/COMSOL/#licences","title":"Licences","text":"<p>The following network licence servers can be accessed from the NeSI cluster.</p> Institution Faculty Token University of Auckland Physics <p>Not Required</p> <p>Not Required</p> Auckland Bioengineering Institute Implantable devices group <p>Not Required</p> <p>Not Required</p> University of Auckland Faculty of Engineering <p>Not Required</p> <p>Not Required</p> University of Auckland Deptartment of Engineering Science <p>Not Required</p> <p>Not Required</p> University of Auckland Deptartment of Engineering Science <p>Not Required</p> <p>Not Required</p> University of Otago <p>Not Required</p> <p>Not Required</p> University of Canterbury <p>Not Required</p> <p>Not Required</p> <p>If you do not have access, or want a server connected Contact our Support Team.</p> <pre><code>comsol --help\n</code></pre> <p>Will display a list of COMSOL batch commands.</p> <p>Useful Links</p> <ul> <li>Running COMSOL in parallel on      clusters.</li> <li>Running parametric sweeps, batch sweeps, and cluster sweeps from      the command      line.</li> <li>COMSOL and      Multithreading.</li> </ul>","tags":["engineering","COMSOL","cae","multiphysics","cfd","fea"]},{"location":"Scientific_Computing/Supported_Applications/COMSOL/#batch-submission","title":"Batch Submission","text":"<p>When using COMSOL batch the following flags can be used to control distribution.</p> <code>-mpibootstrap slurm</code> \u00a0Instructs COMSOL to get it's settings from SLURM <code>-np &lt;cpus&gt;</code> Number of CPUs to use in each task. Equivalent to slurm input <code>--cpus-per-task</code> or environment variable <code>${SLURM_CPUS_PER_TASK}</code> <code>-nn &lt;tasks&gt;</code> Number of tasks total.\u00a0<code>--ntasks</code> or\u00a0<code>${SLURM_NTASKS}</code> <code>-nnhost &lt;tasks&gt;</code> Number of tasks per node. <code>--ntasks-per-node</code> <code>${SLURM_NTASKS_PER_NODE}</code> <code>-f &lt;path to hostlist&gt;</code> Host file. You wont't need to set this in most circumstances.","tags":["engineering","COMSOL","cae","multiphysics","cfd","fea"]},{"location":"Scientific_Computing/Supported_Applications/COMSOL/#example-scripts","title":"Example Scripts","text":"Serial Job <p>Single process with a single thread Usually submitted as part of an array, as in the case of parameter</p> <p>sweeps.</p> <pre><code>```\n#!/bin/bash -e\n\n#SBATCH --job-name      COMSOL-serial\n#SBATCH --licenses      comsol@uoa_foe\n#SBATCH --time          00:05:00          # Walltime\n#SBATCH --mem           1512               # total mem\n\nmodule load COMSOL/6.1\ncomsol batch -inputfile my_input.mph\n```\n</code></pre> Shared Memory Job <pre><code>#!/bin/bash -e\n#SBATCH --job-name      COMSOL-shared\n#SBATCH --licenses      comsol@uoa_foe\n#SBATCH --time          00:05:00        # Walltime\n#SBATCH --cpus-per-task 8\n#SBATCH --mem           4G              # total mem\nmodule load COMSOL/6.1\ncomsol batch -mpibootstrap slurm -inputfile my_input.mph \n</code></pre> <p>=== Distributed Memory Job</p> <pre><code>```\n#!/bin/bash -e\n\n#SBATCH --job-name      COMSOL-distributed\n#SBATCH --licenses      comsol@uoa_foe\n#SBATCH --time          00:05:00            # Walltime\n#SBATCH --ntasks        8         \n#SBATCH --mem-per-cpu   1500                # mem per cpu\n\nmodule load COMSOL/6.1\ncomsol batch -mpibootstrap slurm -inputfile my_input.mph\n```\n</code></pre> Hybrid JobLiveLink <pre><code>#!/bin/bash -e\n#SBATCH --job-name         COMSOL-hybrid\n#SBATCH --licenses         comsol@uoa_foe\n#SBATCH --time             00:05:00          # Walltime\n#SBATCH --ntasks           4                 \n#SBATCH --cpus-per-task    16\n#SBATCH --mem-per-cpu      1500B             # total mem\n\nmodule load COMSOL/6.1\ncomsol batch -mpibootstrap slurm -inputfile my_input.mph\n</code></pre> <pre><code>#!/bin/bash -e\n#SBATCH --job-name         COMSOL-livelink\n#SBATCH --licenses         comsol@uoa_foe\n#SBATCH --time             00:05:00\n#SBATCH --cpus-per-task    16\n#SBATCH --mem-per-cpu      1500\n\nmodule purge\n\nmodule load COMSOL/6.1\nmodule load MATLAB/2021b\n\ncomsol mphserver -silent &amp;\nmatlab -batch \"addpath('/opt/nesi/share/COMSOL/comsol154/multiphysics/mli/');mphstart;MyScript\"\n</code></pre> <p>Warning</p> <p>If no output file is set, using <code>--output</code> the input file will be  updated instead.</p>","tags":["engineering","COMSOL","cae","multiphysics","cfd","fea"]},{"location":"Scientific_Computing/Supported_Applications/COMSOL/#interactive-use","title":"Interactive Use","text":"<p>Providing you have set up X11, you can open the COMSOL GUI by running the command <code>comsol</code>.</p> <p>Large jobs should not be run on the login node.</p>","tags":["engineering","COMSOL","cae","multiphysics","cfd","fea"]},{"location":"Scientific_Computing/Supported_Applications/COMSOL/#livelink","title":"LiveLink","text":"<p>If you are using COMSOL LiveLink, you will need to load a MATLAB module (in addition to the COMSOL module), e.g.</p> <pre><code>module load MATLAB/2021b\n</code></pre> <p>Then</p> <pre><code>comsol matlab -mlroot &lt;path&gt;\n</code></pre> <p>Where <code>&lt;mlpath&gt;`` is the root directory of the MATLAB version you are using (</code>dirname $(dirname $(which matlab))`).</p>","tags":["engineering","COMSOL","cae","multiphysics","cfd","fea"]},{"location":"Scientific_Computing/Supported_Applications/COMSOL/#best-practice","title":"Best Practice","text":"<p>COMSOL is relatively smart with it's use of resources, if possible it is preferable to use <code>--cpus-per-task</code> over <code>--ntasks</code></p> <p>Memory requirements depend on job type, but will scale up with number of CPUs \u2248 linearly.</p> <p>Multithreading will benefit jobs using less than 8 CPUs, but is not recommended on larger jobs.</p> <p>Performance is highly depended on the model used. The above should only be used as a very rough guide. </p>","tags":["engineering","COMSOL","cae","multiphysics","cfd","fea"]},{"location":"Scientific_Computing/Supported_Applications/Clair3/","title":"Clair3","text":"<p>Syumphonizing pileup and full-alignment for high-performance long-read variant calling.</p> <p>Clair3 Homepage</p>","tags":[]},{"location":"Scientific_Computing/Supported_Applications/Clair3/#available-modules","title":"Available Modules","text":"Mahuika <p> 0.1.12-Miniconda3 1.0.0-Miniconda3 </p> <pre><code>module load Clair3/1.0.0-Miniconda3</code></pre>","tags":[]},{"location":"Scientific_Computing/Supported_Applications/Clair3/#description","title":"Description","text":"<p>Clair3 homepage</p> <p>Clair3 is a germline small variant caller for long-reads. Clair3 makes the best of two major method categories: pileup calling handles most variant candidates with speed, and full-alignment tackles complicated candidates to maximize precision and recall. Clair3 runs fast and has superior performance, especially at lower coverage. Clair3 is simple and modular for easy deployment and integration.</p> <p>Clair3 is the 3<sup>rd</sup> generation of Clair (the 2<sup>nd</sup>) and Clairvoyante (the 1<sup>st</sup>).</p> <p>A short pre-print describing Clair3's algorithms and results is at bioRxiv.</p>","tags":[]},{"location":"Scientific_Computing/Supported_Applications/Clair3/#license-and-disclaimer","title":"License and Disclaimer","text":"<p>Copyright 2021 The University of Hong Kong, Department of Computer Science</p> <p>Redistribution and use in source and binary forms, with or without modification, are permitted provided that the following conditions are met:</p> <ol> <li> <p>Re-distributions of source code must retain the above copyright     notice, this list of conditions and the following disclaimer.</p> </li> <li> <p>Re-distributions in binary form must reproduce the above copyright     notice, this list of conditions and the following disclaimer in the     documentation and/or other materials provided with the distribution.</p> </li> <li> <p>Neither the name of the copyright holder nor the names of its     contributors may be used to endorse or promote products derived from     this software without specific prior written permission.</p> </li> </ol>","tags":[]},{"location":"Scientific_Computing/Supported_Applications/Clair3/#example-slurm-script","title":"Example Slurm script","text":"<p>Warning</p> <p>Absolute path is needed for both <code>INPUT_DIR</code> and <code>OUTPUT_DIR</code> </p> <pre><code>#!/bin/bash -e\n\n#SBATCH --account       nesi12345\n#SBATCH --job-name      cliar3_job\n#SBATCH --mem           6G #12G is just a place holder. Adjust accordingly\n#SBATCH --cpus-per-task 4 #4 just a place holder. Adjust accordingly\n#SBATCH --time          01:00:00\n#SBATCH --output        slurmout.%j.out\n\n\n#Caution: Absolute path is needed for both INPUT_DIR and OUTPUT_DIR\n\nINPUT_DIR=/path/to/input/data         # e.g. /nesi/nobackup/nesi12345/input (absolute path needed)\nOUTPUT_DIR=/path/to/save/outputs      # /nesi/nobackup/nesi12345/output (absolute path needed)\nREF=/path/to/reference/genomes        # use the suggested Slurm variable which will read the value from `--cpus-per-task`\nMODEL_NAME=/model/name                # e.g. r941_prom_hac_g360+g422\n\n\nmodule purge\nmodule load Clair3/0.1.12-Miniconda3\n\nrun_clair3.sh \\\n--bam_fn=${INPUT_DIR} \\\n--ref_fn=${REF} \\\n--threads=$SLURM_CPUS_PER_TASK \\\n--platform=ont \\\n--model_path=${CONDA_PREFIX}/bin/models/${MODEL_NAME} \\\n--output=${OUTPUT_DIR} --enable_phasing\n</code></pre>","tags":[]},{"location":"Scientific_Computing/Supported_Applications/Cylc/","title":"Cylc","text":"","tags":[]},{"location":"Scientific_Computing/Supported_Applications/Cylc/#what-is-cylc","title":"What is Cylc","text":"<p>Cylc is a\u00a0general purpose workflow engine that can also orchestrate cycling systems\u00a0very efficiently. It is used in production weather, climate, and environmental forecasting on HPC, but is not specialised to those domains.</p> <p>Using a workflow engine may enable you to run large parametric or sensitivity studies while ensuring scalability, reproducibility and flexibility. If you have embarrassingly parallel jobs then Cylc might be a good solution for you. The workflow engine will allow for the concurrent execution of parallel jobs, depending on the task graph and available resources on the platform. One advantage of this approach over running a monolithic, parallel executable is that each task will require less resources that the complete problem, it is thus easier for each task to slip into the queue and start running.</p> <p>See the NeSI\u00a0 Snakemake page for another, possible choice.</p> <p>In this article, we show how you can create a simple workflow and run it on NeSI's platform. Consult the Cylc documentation for more elaborate examples, including some with a cycling (repeated) graph pattern. One of the strengths of Cylc is that simple workflows can be executed simply while allowing for very complex workflows, with thousands of tasks, which may be repeated ad infinitum.</p>","tags":[]},{"location":"Scientific_Computing/Supported_Applications/Cylc/#ssh-configuration","title":"SSH configuration","text":"<p>Cylc uses ssh\u00a0\u00a0to automatically start schedulers on configured \"run hosts\" and to submit jobs to remote platforms, so if you haven't already done so you need to allow ssh to connect to other hosts on the HPC cluster without prompting for a passphrase (all HPC nodes see the same filesystem, so this is easy):</p> <ul> <li>run\u00a0<code>ssh-keygen</code>\u00a0to generate a public/private key pair with\u00a0no     passphrase\u00a0(when it asks for a passphrase, just hit enter)</li> <li>add your own public key to your authorized keys     file:\u00a0<code>cat .ssh/id_rsa.pub &gt;&gt; .ssh/authorized_keys</code> </li> <li>check that your\u00a0keys, authorized_keys file, ssh     directory,\u00a0and\u00a0home directory\u00a0all have sufficiently secure     file permissions. If not,\u00a0<code>ssh</code>\u00a0will silently revert to requiring     password entry. See for     example\u00a0https://www.frankindev.com/2020/11/26/permissions-for-.ssh-folder-and-key-files/</li> <li>make sure your\u00a0home directory\u00a0has a maximum     of\u00a0750\u00a0permissions</li> </ul> <p>Now you should be able to run\u00a0<code>ssh mahuika02</code>(for example) without being asked for a passphrase.</p>","tags":[]},{"location":"Scientific_Computing/Supported_Applications/Cylc/#how-to-select-the-cylc-version","title":"How to select the Cylc version","text":"<p>Cylc has been installed on M\u0101ui and Mahuika, there is no need to load any module,</p> <pre><code>$ which cylc\n/opt/nesi/share/bin/cylc\n</code></pre> <p>(Access if via the NeSI module, which is loaded by default.)</p> <p>Be aware that the default version</p> <pre><code>$ cylc version\n7.9.1\n</code></pre> <p>is not the latest, and that configuration file and Cylc commands have changed significantly at version 8.</p> <p>New Cylc users should use version 8 or later,</p> <pre><code>$ cylc list-versions\n\n7.9.1 \n...\n8.0.1 \ncylc -&gt; 7.9.1\n</code></pre> <p>You can select your Cylc version by setting the environment variable CYLC_VERSION, for instance,</p> <pre><code>$ export CYLC_VERSION=8.0.1\n$ cylc version\n8.0.1\n</code></pre> <p>At the time of writing, the latest version is 8.0.1.</p>","tags":[]},{"location":"Scientific_Computing/Supported_Applications/Cylc/#a-simple-example-of-a-cylc-workflow","title":"A simple example of a Cylc workflow","text":"<p>To demonstrate Cylc, let's start with a workflow, which we call \"simple\",</p> <pre><code>$ mkdir -p ~/cylc-src/simple\n$ cd ~/cylc-src/simple\n</code></pre> <p>Create/edit the following flow.cylc file containing</p> <pre><code>[scheduling] # Define the tasks and when they should run\n  [[graph]]\n    R1 = \"\"\" # R1 means run this graph once\n      taskA &amp; taskB =&gt; taskC # Defines the task graph\n    \"\"\"\n[runtime] # Define what each task should run\n  [[root]] # Default settings inherited by all tasks\n    platform = mahuika-slurm # Run \"cylc conf\" to see platforms. \n    [[[directives]]] # Default SLURM options for the tasks below\n       --account = nesi99999 # CHANGE\n  [[taskA]]\n    script = echo \"running task A\"\n      [[[directives]]] # specific SLURM option for this task\n        --ntasks = 2\n  [[taskB]]\n    script = echo \"running task B\"\n  [[taskC]]\n    script = echo \"running task C\"\n</code></pre> <p>In the above example, we have three tasks (taskA, taskB and taskC), which run under SLURM (hence platform = mahuika-slurm). Type</p> <pre><code>cylc config --platform-names\n</code></pre> <p>to see a list of platforms. The SLURM settings for taskA are in the [[[directives]]] section.</p>","tags":[]},{"location":"Scientific_Computing/Supported_Applications/Cylc/#how-to-interact-with-cylc","title":"How to interact with Cylc","text":"<p>Cylc takes command lines. Type\u00a0</p> <pre><code>$ cylc help all\n</code></pre> <p>to see the available commands. Type\u00a0</p> <pre><code>$ cylc help install # or cylc install --help\n</code></pre> <p>to find out how to use a specific command (in this case \"install\").</p>","tags":[]},{"location":"Scientific_Computing/Supported_Applications/Cylc/#installing-a-workflow","title":"Installing a workflow","text":"<p>Prior to running a workflow, it must be installed to a run directory. Due to limited disk space in home directories on NeSI, Cylc has been configured to symlink the standard run directories to project directories, if $PROJECT is defined. Hence, you need to set</p> <pre><code>$ export PROJECT=nesi99999 # CHANGE\n</code></pre> <p>Then install the workflow with</p> <pre><code>cylc install simple\n</code></pre>","tags":[]},{"location":"Scientific_Computing/Supported_Applications/Cylc/#validating-the-workflow","title":"Validating the workflow","text":"<p>It's a good idea to check that there are no syntax errors in flow.cylc,</p> <pre><code>$ cylc validate simple\nValid for cylc-8.0.1\n</code></pre>","tags":[]},{"location":"Scientific_Computing/Supported_Applications/Cylc/#looking-at-the-workflow-graph","title":"Looking at the workflow graph","text":"<p>A useful command is</p> <pre><code>$ cylc graph simple\n</code></pre> <p>which will generate a png file, generally in the /tmp directory with a name like /tmp/tmpzq3bjktw.PNG. Take note of the name of the png file. To visualise the file you can type</p> <pre><code>$ display\u00a0 /tmp/tmpzq3bjktw.PNG # ADJUST the file name\n</code></pre> <p>Here, we see that our workflow \"simple\" has a \"taskC\", which waits for \"taskA\" and \"taskB\" to complete,</p> <p></p> <p>The \"1\" indicates that this workflow graph is executed only once.</p>","tags":[]},{"location":"Scientific_Computing/Supported_Applications/Cylc/#different-ways-to-interact-with-cylc","title":"Different ways to interact with Cylc","text":"<p>Every Cylc action can be executed via the command line. Alternatively, you can invoke each action through a terminal user interface (tui),\u00a0</p> <pre><code>$ cylc tui simple\n</code></pre> <p>Another alternative, is to use the graphical user interface</p> <pre><code>$ cylc gui\n</code></pre> <p>Read below on how access the web interface running on NeSI using your local browser.</p>","tags":[]},{"location":"Scientific_Computing/Supported_Applications/Cylc/#connecting-via-jupyter","title":"Connecting via Jupyter","text":"<p>If you're connecting through https://jupyter.nesi.org.nz you'll need to replace anything before the \":\" with https://jupyter.nesi.org.nz/user/USERNAME/proxy/ to get access to the web graphical user interface (where USERNAME is your NeSI user name). Hence the URL becomes https://jupyter.nesi.org.nz/user/USERNAME/proxy/8888/cylc?token=TOKEN</p>","tags":[]},{"location":"Scientific_Computing/Supported_Applications/Cylc/#connecting-via-ssh","title":"Connecting via SSH","text":"<p>First open ssh tunnelling, so that a given port on your local machine (e.g. your laptop) maps to the Cylc UI Server\u2019s port on the HPC. On your local machine, type</p> <pre><code>$ ssh -N -L PORT:localhost:PORT HOST\n</code></pre> <p>where PORT is a valid port number and HOST can be M\u0101ui or mahuika. See the NeSI page for the range of allowed ports (currently 1024-49151). Choose any number in this range but make sure your port number is fairly unique to avoid clashing with other users. Option -N is optional: it opens the connection without logging you into the shell.</p> <p>Then ssh to the host (e.g. mahuika)</p> <pre><code>$ ssh HOST\n</code></pre> <p>and add the following to $HOME/.cylc/uiserver/jupyter_config.py on the HOST.</p> <pre><code>c.ServerApp.open_browser=False\nc.ServerApp.port=PORT\n</code></pre> <p>where PORT and HOST match the values you selected when opening the ssh tunnel.</p> <p>You're now ready to fire up the web graphical interface</p> <pre><code>$ cylc gui\n</code></pre> <p>Just copy the URL that looks like</p> <pre><code>http://127.0.0.1:PORT/cylc?token=TOKEN\n</code></pre> <p>into your web browser. (Again substitute HOST and PORT with the values chosen above.)</p>","tags":[]},{"location":"Scientific_Computing/Supported_Applications/Cylc/#how-to-execute-a-workflow","title":"How to execute a workflow","text":"<p>To execute the workflow type</p> <pre><code>$ cylc play --no-detach simple\n</code></pre> <p>The \"--no-detach\" option makes scheduler run in the foreground so you can see its output in your terminal. Without this option it will \"daemonize\" so it can keep running even if you log out.</p> <p>Command</p> <pre><code>$ cylc scan\n</code></pre> <p>will list all running and installed workflows.</p>","tags":[]},{"location":"Scientific_Computing/Supported_Applications/Cylc/#checking-the-output","title":"Checking the output","text":"<pre><code>$ cylc cat-log simple//1/taskA  # note // between workflow and task ID\n</code></pre> <p>of the first cycle of taskA. The \"1\" refers to the task iteration, or cycle point. Our simple workflow only has one iteration (as dictated by the R1 graph above).</p>","tags":[]},{"location":"Scientific_Computing/Supported_Applications/Cylc/#how-to-clean-or-remove-a-workflow","title":"How to clean or remove a workflow","text":"<pre><code>$ cylc clean simple\n</code></pre> <p>will remove the file structure associated with workflow \"simple\".</p>","tags":[]},{"location":"Scientific_Computing/Supported_Applications/Cylc/#where-jobs-results-and-log-files-are-stored","title":"Where jobs, results and log files are stored","text":"<p>Cylc will create a directory under $HOME/cylc-run. On NeSI, the output of the runs will be stored in the project directory, with a symbolic link pointing from the user home directory to the project directory</p> <pre><code>$ ls -l $HOME/cylc-run/simple/run1\nlrwxrwxrwx 1 pletzera pletzera 54 Aug\u00a0 5 03:19 /home/pletzera/cylc-run/simple/run1 -&gt; /nesi/nobackup/nesi99999/pletzera/cylc-run/simple/run1\n</code></pre>","tags":[]},{"location":"Scientific_Computing/Supported_Applications/Cylc/#about-cylc","title":"About Cylc","text":"<p>More can be found about Cylc here, including what Cylc is and how you can leverage Cylc to submit parallel jobs.</p>","tags":[]},{"location":"Scientific_Computing/Supported_Applications/Delft3D/","title":"Delft3D","text":"<p>Integrated simulation of sediment transport and morphology, waves, water quality and ecology.</p> <p>Delft3D Homepage</p>","tags":["hydrodynamics"]},{"location":"Scientific_Computing/Supported_Applications/Delft3D/#available-modules","title":"Available Modules","text":"Mahuika <p> 66341-intel-2020a 141732-intel-2022a </p> <pre><code>module load Delft3D/141732-intel-2022a</code></pre>","tags":["hydrodynamics"]},{"location":"Scientific_Computing/Supported_Applications/Delft3D/#example-scripts","title":"Example scripts","text":"SerialShared MemoryDistributed Memory <p>For when only one CPU is required, generally as part of a job array.</p> <pre><code>#!/bin/bash -e\n\n#SBATCH --job-name      Delft3D\n#SBATCH --time          00:05:00       # Walltime\n#SBATCH --mem           512M           # Total Memory\n#SBATCH --hint          nomultithread  # Hyperthreading disabled\nmodule load Delft3D/141732-intel-2022a\nd_hydro test_input.xml\n</code></pre> <p>For domain based decompositions. Use\u00a0<code>--cpus-per-task</code>\u00a0to allocate resources. Each subdomain runs in a separate thread, inside one executable. Limited to one node.</p> <pre><code>#!/bin/bash -e\n\n#SBATCH --job-name      Delft3D\n#SBATCH --time          00:05:00       # Walltime\n#SBATCH --cpus-per-task 4\n#SBATCH --mem           2G             # Total Memory\n#SBATCH --hint         nomultithread  # Hyperthreading disabled\n\nmodule load Delft3D/141732-intel-2022a\n\nsrun\u00a0d_hyrdo\u00a0test_input.xml\n</code></pre> <p>Domain is split automatically in stripwise partitions. Can run across multiple nodes. Use\u00a0<code>--ntasks</code>\u00a0to allocate resources.</p> <p>Cannot be used in conjunction with: - DomainDecomposition - Fluid mud - Coup online - Drogues and moving observation points - Culverts - Power stations with inlet and outlet in different partitions - Non-hydrostatic solvers - Walking discharges - 2D skewed weirs - max(mmax,nmax)/npart \u2264 4 - Roller model - Mormerge - Mass balance polygons</p> <pre><code>#!/bin/bash -e\n\n#SBATCH --job-name      Delft3D_distributed\n#SBATCH --time          00:05:00       # Walltime\n#SBATCH --mem-per-cpu   1G             #SBATCH --hint          nomultithread  # Hyperthreading disabled\n\nmodule load Delft3D/141732-intel-2022a\nsrun\u00a0d_hyrdo\u00a0test_input.xml\n</code></pre> <p>Warning</p> <p>Trying to use more tasks than there are partitions in your model will cause failure.</p>","tags":["hydrodynamics"]},{"location":"Scientific_Computing/Supported_Applications/Dorado/","title":"Dorado","text":"<p>High-performance, easy-to-use, open source basecaller for Oxford Nanopore reads.</p> <p>Dorado Homepage</p>","tags":[]},{"location":"Scientific_Computing/Supported_Applications/Dorado/#available-modules","title":"Available Modules","text":"Mahuika <p> 0.2.1 0.2.4 0.3.0 0.3.1 0.3.2 0.3.4-rc2 0.3.4 0.4.0 0.4.1 0.4.2 0.4.3 </p> <pre><code>module load Dorado/0.4.3</code></pre>","tags":[]},{"location":"Scientific_Computing/Supported_Applications/Dorado/#description","title":"Description","text":"<p>Dorado is a high-performance, easy-to-use, open source basecaller for Oxford Nanopore reads.</p>","tags":[]},{"location":"Scientific_Computing/Supported_Applications/Dorado/#features","title":"Features","text":"<ul> <li>One executable with sensible defaults, automatic hardware detection     and configuration.</li> <li>Nvidia GPUs including multi-GPU with linear scaling.</li> <li>Modified basecalling (Remora models).</li> <li>Duplex basecalling.</li> <li>POD5\u00a0support for     highest basecalling performance.</li> <li>Based on libtorch, the C++ API for pytorch.</li> <li>Multiple custom optimisations in CUDA and Metal for maximising     inference performance.</li> </ul>","tags":[]},{"location":"Scientific_Computing/Supported_Applications/Dorado/#license-and-disclaimer","title":"License and Disclaimer","text":"<p>(c) 2022 Oxford Nanopore Technologies Ltd.</p> <p>Dorado is distributed under the terms of the Oxford Nanopore Technologies, Ltd. Public License, v. 1.0. If a copy of the License was not distributed with this file, You can obtain one at\u00a0http://nanoporetech.com</p> <p>.</p>","tags":[]},{"location":"Scientific_Computing/Supported_Applications/Dorado/#example-slurm-script","title":"Example Slurm script","text":"<ul> <li>The following Slurm script is a template to run Basecalling on the     NVIDIA A100 GPUs. We do not recommend running Dorado jobs on CPUs.</li> <li><code>--device 'cuda:all'</code>\u00a0will automatically pick up the GPU over CPU</li> <li>We are not providing the models as part of the module yet.\u00a0</li> </ul> <pre><code>#!/bin/bash -e\n\n#SBATCH --account        nesi12345\n#SBATCH --job-name       dorado\n#SBATCH --gpus-per-node  A100:1\n#SBATCH --mem            6G\n#SBATCH --cpus-per-task  4\n#SBATCH --time           00:10:00\n#SBATCH --output         slurmout.%j.out\n\nmodule purge\nmodule load Dorado/0.4.3\n\ndorado download --model dna_r10.4.1_e8.2_400bps_hac@v4.1.0\n\ndorado basecaller  --device 'cuda:all' dna_r10.4.1_e8.2_400bps_hac@v4.1.0 pod5s/ &gt; calls.bam\n</code></pre>","tags":[]},{"location":"Scientific_Computing/Supported_Applications/FDS/","title":"FDS","text":"<p>Fire Dynamics Simulator (FDS) is a large-eddy simulation (LES) code for low-speed flows,  with an emphasis on smoke and heat transport from fires.</p> <p>FDS Homepage</p>","tags":["mahuika","engineering"]},{"location":"Scientific_Computing/Supported_Applications/FDS/#available-modules","title":"Available Modules","text":"Mahuika Maui <p> 6.7.1-intel-2017a 6.7.1-intel-2018b 6.7.5-intel-2020a 6.7.5-1526-intel-2020a 6.7.7-intel-2020a 6.7.9-intel-2022a </p> <pre><code>module load FDS/6.7.9-intel-2022a</code></pre> <p> 6.7.1-CrayIntel-23.02-19 </p> <pre><code>module load FDS/6.7.1-CrayIntel-23.02-19</code></pre> <p>FDS (Fire Dynamics Simulator) was developed by the National Institute of Standards and Technology (NIST) for large-eddy simulation (LES) of low-speed flows, with an emphasis on smoke and heat transport from fires.</p> <p>General documentation can be found here.</p> <p>FDS can utilise both MPI and OpenMP</p>","tags":["mahuika","engineering"]},{"location":"Scientific_Computing/Supported_Applications/FDS/#example-script","title":"Example Script","text":"<pre><code>#!/bin/bash -e\n\n#SBATCH --time           02:00:00       #Walltime\n#SBATCH --ntasks         4              #One task per mesh, NO MORE\n#SBATCH --cpus-per-task  2              #More than 4 cpus/task not recommended.\n#SBATCH --output         %x.out     #Name output file according to job name\n#SBATCH --hint           nomultithread  #Hyperthreading decreases efficiency.\n\nmodule load FDS/6.7.1-intel-2017a\n\ninput=\"/nesi/project/nesi99999/path/to/input.fds\"\n\nsrun fds ${input}\n</code></pre>","tags":["mahuika","engineering"]},{"location":"Scientific_Computing/Supported_Applications/FDS/#recommendations","title":"Recommendations","text":"<ul> <li>FDS will run in Hybrid Parallel, but will be less efficient that     full MPI using the same number of CPUs.</li> <li>MPI if the preferable method of scaling, if you can partition your     mesh more you should do that before considering multi-threading     (OpenMP). e.g.\u00a0<code>ntasks=2, cpus-per-task=1</code>\u00a0is preferable     to\u00a0<code>ntasks=1, cpus-per-task=2</code></li> <li>Each mesh should have it's own task, assigning more tasks than there     are meshes will cause an error.</li> <li>Multi-threading efficiency drops off significantly after 4 physical     cores. <code>--cpus-per-task 4</code></li> <li>Hyper-threading is not recommended. Set\u00a0<code>--hint nomultithread</code></li> </ul>","tags":["mahuika","engineering"]},{"location":"Scientific_Computing/Supported_Applications/FDS/#scaling-with-mpi","title":"Scaling with MPI","text":"","tags":["mahuika","engineering"]},{"location":"Scientific_Computing/Supported_Applications/FDS/#scaling-with-omp","title":"Scaling with oMP","text":"","tags":["mahuika","engineering"]},{"location":"Scientific_Computing/Supported_Applications/GATK/","title":"GATK","text":"<p>The Genome Analysis Toolkit or GATK is a software package developed at the Broad Institute</p> <p>GATK Homepage</p>","tags":[]},{"location":"Scientific_Computing/Supported_Applications/GATK/#available-modules","title":"Available Modules","text":"Mahuika <p> 3.5-Java-1.8.0_144 3.5-Java-15.0.2 3.7 3.8-1 4.0.1.2-gimkl-2017a 4.0.11.0-gimkl-2018b 4.1.0.0-gimkl-2017a 4.1.4.1-gimkl-2018b 4.1.8.1-gimkl-2020a 4.2.5.0-gimkl-2020a 4.2.6.1-gimkl-2020a 4.3.0.0-gimkl-2022a 4.4.0.0-gimkl-2022a </p> <pre><code>module load GATK/4.4.0.0-gimkl-2022a</code></pre> <p>The Genome Analysis Toolkit (GATK), developed at the Broad Institute, provides a wide variety of tools focusing primarily on variant discovery and genotyping. It is regarded as the industry standard for identifying SNPS and indels in germline DNA and RNAseq data.</p> <p>General documentation for running GATK can be found at their website here.</p>","tags":[]},{"location":"Scientific_Computing/Supported_Applications/GATK/#running-gatk","title":"Running GATK","text":"<p>GATK uses requires the Java Runtime Environment. The appropriate version of Java is already included as part of the GATK module, you will not need to load a Java module separately.</p> <p>Note</p> <ul> <li><code>--time</code> and <code>--mem</code> defined in the following example are just place     holders.</li> <li>Please load the GATK version of your choice</li> </ul> <pre><code>#!/bin/bash -e\n#SBATCH --job-name=MarkDuplicates\n#SBATCH --output=%x_%j.out     # log file\n#SBATCH --error=%x_%j.err      # error log file\n#SBATCH --account=nesi12345    # your NeSI project code\n#SBATCH --time=2:00:00         # maximum run time hh:mm:ss\n#SBATCH --mem=30G              # maximum memory available to GATK\n\n# create temporary directory for Java so it does not fill up /tmp\nTMPDIR=/nesi/nobackup/&lt;project_ID&gt;/GATK_tmp/\nmkdir -p ${TMPDIR}\n\n# remove other modules that may be loaded\n# load specific GATK version\nmodule purge\nmodule load GATK/4.3.0.0-gimkl-2022a\n\n# tell Java to use ${TMPDIR} as the temporary directory\nexport _JAVA_OPTIONS=-Djava.io.tmpdir=${TMPDIR} \n\n# run GATK command\ngatk MarkDuplicates I=input.bam O=marked_duplicates.bam M=marked_dup_metrics.txt\n</code></pre>","tags":[]},{"location":"Scientific_Computing/Supported_Applications/GATK/#gatk-picard","title":"GATK-Picard","text":"<p>GATK versions 4.0 or higher all contains a copy of the Picard toolkit, you will not need to separately load the Picard module. To run GATK-picard commands, use:  </p> <pre><code>gatk &lt;picard function&gt; &lt;options&gt;\n</code></pre> <p>This is different what what is currently written on the GATK documentation, you do not need to call \"java -jar picard.jar &lt;Picard-function&gt;\". Simply replace the Java parts with \"gatk\" and the function of interest.</p> <p>Please also note that there are some inconsistencies between Picard and GATK flag naming conventions, so it is best to double check them.</p>","tags":[]},{"location":"Scientific_Computing/Supported_Applications/GATK/#common-issues","title":"Common Issues","text":"","tags":[]},{"location":"Scientific_Computing/Supported_Applications/GATK/#out-of-memory-or-insufficient-space-for-shared-memory-file","title":"Out of Memory or Insufficient Space for Shared Memory File","text":"<p>This is related to temporary files being created by Java in <code>/tmp</code>, and then running out of space. If you see the error message <code>IOException: No space left on device</code>, this is not necessarily referring to your nobackup or projects directory, but is likely to be Java applications pointing to the small temporary filesystem available in a compute node.</p> <p>To work around this, create another directory to use for temporrary files.</p> <pre><code># create a new temporary directory\nTMPDIR=\"/nesi/nobackup/&lt;project_directory&gt;/GATK_tmp/\"\nmkdir -p ${TMPDIR}\n\n# put this line in AFTER you load GATK but BEFORE running GATK\nexport _JAVA_OPTIONS=-Djava.io.tmpdir=${TMPDIR} \n</code></pre>","tags":[]},{"location":"Scientific_Computing/Supported_Applications/GATK/#file-is-not-a-supported-reference-file-type","title":"File is not a supported reference file type","text":"<p>The error message \"File is not a supported reference file type\" comes in one of the log files. It appears that sometimes GATK requires the file extension of \"fasta\" or \"fa\", for fasta files. Please make sure your file extensions correctly reflect the file type.</p>","tags":[]},{"location":"Scientific_Computing/Supported_Applications/GROMACS/","title":"GROMACS","text":"<p>GROMACS is a versatile package to perform molecular dynamics,  i.e. simulate the Newtonian equations of motion for systems with hundreds to millions of particles.</p> <p>GROMACS Homepage</p>","tags":[]},{"location":"Scientific_Computing/Supported_Applications/GROMACS/#available-modules","title":"Available Modules","text":"Mahuika Maui <p> 2019-intel-2018b-cuda-8.0.61-hybrid 2019.3-intel-2018b-cuda-10.0.130-hybrid 2020.4-gimkl-2020a-cuda-11.3.1-hybrid-PLUMED-2.6.2 2020.5-intel-2020a-cuda-11.0.2-hybrid 2020.6-gimkl-2020a-cuda-11.3.1-hybrid-PLUMED-2.8.0 2020.6-intel-2020a-cuda-11.3.1-hybrid 2021.5-gimkl-2022a-cuda-11.6.2-hybrid 2021.6-gimkl-2020a-cuda-11.6.2-hybrid-PLUMED-2.8.0 </p> <pre><code>module load GROMACS/2021.6-gimkl-2020a-cuda-11.6.2-hybrid-PLUMED-2.8.0</code></pre> <p> 2019-CrayIntel-23.02-19 2021.3-CrayGNU-23.02 2023-CrayGNU-23.02 </p> <pre><code>module load GROMACS/2023-CrayGNU-23.02</code></pre> <p>GROMACS (the GROningen MAchine for Chemical Simulations) is a versatile package to perform molecular dynamics, i.e. simulate the Newtonian equations of motion for systems with hundreds to millions of particles.</p> <p>It is primarily designed for biochemical molecules like proteins, lipids and nucleic acids that have a lot of complicated bonded interactions, but since GROMACS is extremely fast at calculating the nonbonded interactions (that usually dominate simulations) many groups are also using it for research on non-biological systems, e.g. polymers.</p> <p>GROMACS is available to anyone at no cost under the terms of the GNU Lesser General Public Licence. Gromacs is a joint effort, with contributions from developers around the world: users agree to acknowledge use of GROMACS in any reports or publications of results obtained with the Software.</p>","tags":[]},{"location":"Scientific_Computing/Supported_Applications/GROMACS/#job-submission","title":"Job submission","text":"<p>GROMACS performance depends on several factors, such as usage (or lack thereof) of GPUs, the number of MPI tasks and OpenMP threads, the load balancing algorithm, the ratio between the number of Particle-Particle (PP) ranks and Particle-Mesh-Ewald (PME) ranks, the type of simulation being performed, force field used and of course the simulated system. For a complete set of GROMACS options, please refer to GROMACS documentation.</p> <p>The following job script is just an example and asks for five MPI tasks, each of which consists of three OpenMP threads, for a total of 15 threads. Please try other <code>mdrun</code> flags in order to see if they make your simulation run faster. Examples of such flags are <code>-npme</code>, <code>-dlb</code>, <code>-ntomp</code>. If you use more MPI tasks per node you will have less memory per MPI task. If you use multiple MPI tasks per node, you need to set CRAY_CUDA_MPS=1 to enable the tasks to access the GPU device on each node at the same time.</p> <pre><code>#!/bin/bash -e\n\n#SBATCH --job-name      GROMACS_test # Name to appear in squeue\n#SBATCH --time          00:10:00     # Max walltime\n#SBATCH --mem-per-cpu   512MB        # Max memory per logical core\n#SBATCH --ntasks        5            # 5 MPI tasks\n#SBATCH --cpus-per-task 3            # 3 OpenMP threads per task\n\nmodule load GROMACS/5.1.4-intel-2017a\n\n# Prepare the binary input from precursor files \nsrun -n 1 gmx grompp -v -f minim.mdp -c protein.gro -p protein.top -o protein-EM-vacuum.tpr\n\n# Run the simulation\n# Note that the -deffnm option is an alternative to specifying several input files individually\n# Note also that the -ntomp option should be used when using hybrid parallelisation\nsrun gmx_mpi mdrun -ntomp ${SLURM_CPUS_PER_TASK} -v -deffnm protein-EM-vacuum -c input/protein.gr -cpt 30\n</code></pre> <p>Note: To prevent performance issues we moved the serial \"gmx\" to \"gmx_serial\". The present \"gmx\" prints a note and calls \"gmx_mpi mdrun\" (if called as \"gmx mdrun\") and \"gmx_serial\" in all other cases.</p> <p>Note: The hybrid version with CUDA can also run on pure CPU architectures. Thus you can use gmx_mpi from the GROMACS/???-cuda-???-hybrid module on Mahuika compute nodes as well as Mahuika GPU nodes.</p>","tags":[]},{"location":"Scientific_Computing/Supported_Applications/GROMACS/#checkpointing-and-restarting","title":"Checkpointing and restarting","text":"<p>In the examples given above, the <code>-cpt 30</code> option instructs Gromacs to write a full checkpoint file every 30 minutes. You can restart from a checkpoint file using the <code>-cpi</code> flag, thus: <code>-cpi state.cpt</code>.</p>","tags":[]},{"location":"Scientific_Computing/Supported_Applications/GROMACS/#warnings-regarding-cpu-affinity","title":"Warnings regarding CPU affinity","text":"<p>If you run GROMACS on a node that is simultaneously running other jobs (even other GROMACS jobs), you may see warnings like this in your output:      WARNING: In MPI process #0: Affinity setting failed. This can cause      performance degradation! If you think your setting are correct,      contact the GROMACS developers.</p> <p>One way to prevent these warnings, which is also useful for reducing the risk of inefficient CPU usage, is to request entire nodes. On the Mahuika cluster, this can be done using the following lines in your input, altered as appropriate:</p> MPI + SMPMPIOpenMP + MPI + SMPOpenMP + MPI <p>Using MPI parallelisation and hyperthreading, but no OpenMP parallelisation.</p> <pre><code>#SBATCH --nodes           4    # May vary\n#SBATCH --ntasks-per-node 72   # Must be 72\n                            # (the number of logical cores per node)\n#SBATCH --cpus-per-task   1    # Must be 1\n</code></pre> <p>Using MPI parallelisation with neither hyperthreading nor OpenMP parallelisation.</p> <pre><code>#SBATCH --nodes           4    # May vary\n#SBATCH --ntasks-per-node 36   # Must be 36\n                            # (the number of physical cores per node)\n#SBATCH --cpus-per-task   1    # Must be 1\n#SBATCH --hint=nomultithread\u00a0\u00a0\u00a0#\u00a0Don't\u00a0use\u00a0hyperthreading\n</code></pre> <p>Using hybrid parallelisation and hyperthreading:</p> <pre><code>#SBATCH --nodes           4    # May vary\n#SBATCH --ntasks-per-node 1    # Must be 1\n#SBATCH --cpus-per-task   72   # Must be 72\n                            # (the number of logical cores per node)\n</code></pre> <p>Using hybrid parallelisation but not hyperthreading:</p> <pre><code>#SBATCH --nodes           4    # May vary\n#SBATCH --ntasks-per-node 1    # Must be 1\n#SBATCH --cpus-per-task   36   # Must be 36\n                            # (the number of physical cores per node)\n#SBATCH --hint=nomultithread   # Don't use hyperthreading\n</code></pre> <p>If you opt to use hybrid parallelisation, it is also important to run <code>mdrun_mpi</code> with the <code>-ntomp &lt;number&gt;</code> option, where <code>&lt;number&gt;</code> should be the number of CPUs per task. You can make sure the value is correct by using\u00a0<code>-ntomp ${SLURM_CPUS_PER_TASK}</code>. Hybrid parallelisation can be more efficient than MPI-only parallelisation, as within the same node there is no need for inter-task communication.</p> <p>NOTE on using GROMACS on M\u0101ui:</p> <p>On the M\u0101ui cluster, normally there is no reason to specifically request a whole node, as all jobs are scheduled to run on one or more entire nodes.\u00a0 However, we have seen issues with slow performance and will recommend using the `--exclusive` flag when running GROMACS. It may also be advisable to request tasks or CPUs in multiples of 80, since that is the number of vCPUs per node.</p>","tags":[]},{"location":"Scientific_Computing/Supported_Applications/GROMACS/#nvidia-gpu-container","title":"NVIDIA GPU Container","text":"<p>NVIDIA has a GPU accelerated version of GROMACS in its NGC container registry (more details about NGC here). We have pulled a version of their container and stored it at this location (you can also pull your own version if you wish): /opt/nesi/containers/nvidia/gromacs-2020_2.sif. We have also provided an example submission script that calls the Singularity image here: /opt/nesi/containers/nvidia/gromacs-example.sl.</p>","tags":[]},{"location":"Scientific_Computing/Supported_Applications/GROMACS/#further-documentation","title":"Further Documentation","text":"<p>GROMACS Homepage</p> <p>GROMACS Manual</p>","tags":[]},{"location":"Scientific_Computing/Supported_Applications/Gaussian/","title":"Gaussian","text":"<p>Warning</p> <p>Gaussian is proprietary software. Make sure you meet the requirements for it's usage.</p>","tags":["mahuika","chemistry"]},{"location":"Scientific_Computing/Supported_Applications/Gaussian/#available-modules","title":"Available Modules","text":"Mahuika <p> 09-B.01 09-C.01 09-D.01 </p> <pre><code>module load Gaussian/09-D.01</code></pre>","tags":["mahuika","chemistry"]},{"location":"Scientific_Computing/Supported_Applications/Gaussian/#description","title":"Description","text":"<p>The Gaussian series of programs provides state-of-the-art capabilities for electronic structure modelling. It is used by chemists, chemical engineers, biochemists, physicists and other scientists worldwide. Starting from the fundamental laws of quantum mechanics, Gaussian predicts the energies, molecular structures, vibrational frequencies and molecular properties of molecules and reactions in a wide variety of chemical environments. Gaussian's models can be applied to both stable species and compounds which are difficult or impossible to observe experimentally (e.g., short-lived intermediates and transition structures).</p> <p>The Gaussian home page is at http://www.gaussian.com.</p>","tags":["mahuika","chemistry"]},{"location":"Scientific_Computing/Supported_Applications/Gaussian/#availablity","title":"Availablity","text":"<p>Gaussian is installed on the Mahuika cluster.</p>","tags":["mahuika","chemistry"]},{"location":"Scientific_Computing/Supported_Applications/Gaussian/#licensing-requirements","title":"Licensing requirements","text":"<p>Gaussian is made available to researchers under closed-source, commercial licence agreements with individuals, research groups or institutions. Whether you have access to Gaussian, which versions you have access to, and under what conditions, will vary depending on where you work or study.</p> <p>For the sake of compliance with Gaussian licence agreements, we maintain a special Gaussian UNIX group. Only members of this group may access and use Gaussian. You can ask to join the Gaussian group by emailing our support team at  Contact our Support Team.</p> <p>All University of Auckland staff and students are in the Gaussian group automatically. If you are not a staff member or student at the University of Auckland, we will add you to the Gaussian group if we are satisfied that you require access to Gaussian to carry out your research and that your institution's Gaussian licence agreement permits you to use Gaussian on a computer that is not owned by or housed at your institution. We may at any time remove you from the Gaussian group if we believe these conditions are no longer met.</p> <p>If you have any questions regarding your eligibility to access Gaussian or any particular version or installation of it, please contact [our support desk Contact our Support Team.</p>","tags":["mahuika","chemistry"]},{"location":"Scientific_Computing/Supported_Applications/Gaussian/#example-jobs","title":"Example jobs","text":"","tags":["mahuika","chemistry"]},{"location":"Scientific_Computing/Supported_Applications/Gaussian/#example-job-submission-script","title":"Example job submission script","text":"<p>The following job submission script is intended for use on Mahuika. Please note that it has a memory requirement built in: at least 2 GB for Gaussian itself, plus a further 2 GB as a buffer zone, for a minimum request of 4 GB (4,096 MB).</p> <pre><code>#!/bin/bash -e\n\n#SBATCH --job-name=H2O\n#SBATCH --account=nesi99999\n#SBATCH --time=00:01:00\n#SBATCH --ntasks=1\n#SBATCH --cpus-per-task=8\n#SBATCH --hint=nomultithread\n#SBATCH --mem=4096MB\n#SBATCH --output=%x-%j.out\n#SBATCH --error=%x-%j.err\n\necho \"============ JOB SUBMISSION SCRIPT ============\"\ncat $0\necho \"===============================================\"\necho \"\"\necho \"\"\n\nmodule load Gaussian/09-D.01\n\n# System name\nsystem=\"H2O\"\n\n# Get the current directory\nstart_dir=$(pwd)\ngjf_template=\"${system}.gjf.template\"\n\n# Prepare a job-specific nobackup directory and set GAUSS_SCRDIR accordingly\nif [[ -n \"${SLURM_ARRAY_TASK_COUNT}\" &amp;&amp; \"${SLURM_ARRAY_TASK_COUNT}\" -gt 1 ]]\nthen\n        job_code=\"${SLURM_ARRAY_JOB_ID}_${SLURM_ARRAY_TASK_ID}\"\nelse\n        job_code=\"${SLURM_JOB_ID}\"\nfi\nexport GAUSS_SCRDIR=\"/nesi/nobackup/${SLURM_JOB_ACCOUNT}/mahuika_job_${job_code}\"\n/usr/bin/mkdir -p \"${GAUSS_SCRDIR}\"\n\n# Calculate the number of CPUs to use within Gaussian\nif [[ -n \"${SLURM_CPUS_PER_TASK}\" ]]\nthen\n        gaussian_ncpus=\"${SLURM_CPUS_PER_TASK}\"\nelse\n        gaussian_ncpus=1\nfi\n\n# Calculate the amount of memory to use within Gaussian\n# That is, amount of memory requested of Slurm minus 2 GB\nif [[ -n \"${SLURM_MEM_PER_NODE}\" &amp;&amp; \"${SLURM_MEM_PER_NODE}\" -ge 4096 ]]\nthen\n        gaussian_memory=$((${SLURM_MEM_PER_NODE} - 2048))\nelse\n        /usr/bin/echo \"Error: Not enough RAM requested (${SLURM_MEM_PER_NODE}).\" &gt;&amp;2\n        /usr/bin/echo \"       Please set \\\"#SBATCH --mem\\\" to at least 4096 MB.\" &gt;&amp;2\n        exit 2\nfi\n\ngjf_working_copy=\"${GAUSS_SCRDIR}/${system}.gjf\"\ngaussian_checkpoint=\"${GAUSS_SCRDIR}/${system}.chk\"\n/usr/bin/sed -e \"s/&lt;&lt;NUMBER_OF_CORES&gt;&gt;/${gaussian_ncpus}/\" \"${gjf_template}\" | \\\n        /usr/bin/sed -e \"s/&lt;&lt;MEMORY&gt;&gt;/${gaussian_memory}/\" | \\\n        /usr/bin/sed -e \"s:&lt;&lt;CHECKPOINT_FILE&gt;&gt;:${gaussian_checkpoint}:\" &gt; \"${gjf_working_copy}\"\n\nsrun g09 &lt; \"${gjf_working_copy}\"\n</code></pre>","tags":["mahuika","chemistry"]},{"location":"Scientific_Computing/Supported_Applications/Gaussian/#example-template-input-file","title":"Example template input file","text":"<p>Any Gaussian input file must end with a blank line. We also recommend specifying a checkpoint file using the %Chk directive, as a saved checkpoint file facilitates recovery and restart if your Gaussian job fails or is killed by the scheduler. In this case, the value of the checkpoint file is a placeholder (as are the number of cores and the memory) and is replaced with a real value when the Slurm job starts.</p> <pre><code>$RunGauss$\n\n%NProcShared=&lt;&lt;NUMBER_OF_CORES&gt;&gt;\n%Mem=&lt;&lt;MEMORY&gt;&gt;MB\n%Chk=&lt;&lt;CHECKPOINT_FILE&gt;&gt;\n\n#P HF/STO-3G SP\n\nSingle-point energy calculation on water\n\n0 1\nH\nO 1 0.95\nH 2 0.95 1 109.0\n</code></pre>","tags":["mahuika","chemistry"]},{"location":"Scientific_Computing/Supported_Applications/Gaussian/#further-notes","title":"Further notes","text":"","tags":["mahuika","chemistry"]},{"location":"Scientific_Computing/Supported_Applications/Gaussian/#setting-the-memory-and-number-of-cores","title":"Setting the memory and number of cores","text":"<p>It is important to ensure that the memory and number of cores in the Gaussian input file itself are consistent with what you set in your job submission script.</p> <p>The key properties are <code>%NProcShared</code> and <code>%Mem</code>:</p> <ul> <li><code>%NProcShared</code> should be set to the number of CPU cores you intend     to use, matching the value of the <code>-c</code> or <code>--cpus-per-task</code>     directive in the Slurm job file.</li> <li><code>%Mem</code> should be set to the amount of memory you intend to use. It     should be about 2 GB (2,048 MB) less than the value of\u00a0<code>--mem</code>\u00a0in     the Slurm job submission script. Note that <code>--mem</code>\u00a0is interpreted as     being in MB rather than GB unless otherwise specified (i.e., with a     \"G\" on the end).</li> </ul> <p>If you use the example Slurm script and template gjf file provided above (with appropriate modifications for your chemical system and desired calculation), this should happen automatically.</p>","tags":["mahuika","chemistry"]},{"location":"Scientific_Computing/Supported_Applications/Gaussian/#saving-temporary-working-files-for-advanced-users","title":"Saving temporary working files (for advanced users)","text":"<p>If you want Gaussian's temporary files (<code>*.inp</code>, <code>*.d2e</code>, <code>*.int</code>, <code>*.rwf</code> and <code>*.scr</code>) to be written to a particular directory, you can achieve this by setting the <code>GAUSS_SCRDIR</code> environment variable in your job submission script, for instance:</p> <pre><code>export GAUSS_SCRDIR=/nesi/nobackup/nesi99999/mahuika_job_123456\n</code></pre> <p>This should happen automatically if you use an appropriately modified script based on the example job submission script given above.</p>","tags":["mahuika","chemistry"]},{"location":"Scientific_Computing/Supported_Applications/Java/","title":"Java","text":"<p>Java Platform, Standard Edition (Java SE) lets you develop and deploy  Java applications on desktops and servers.</p> <p>Java Homepage</p>","tags":["mahuika","general"]},{"location":"Scientific_Computing/Supported_Applications/Java/#available-modules","title":"Available Modules","text":"Mahuika Maui_ancil <p> 1.7.0_51 1.8.0_144 11.0.4 15.0.2 17 20.0.2 </p> <pre><code>module load Java/20.0.2</code></pre> <p> 1.8.0_144 </p> <pre><code>module load Java/1.8.0_144</code></pre>","tags":["mahuika","general"]},{"location":"Scientific_Computing/Supported_Applications/Java/#description","title":"Description","text":"<p>Java is a computer programming language that is concurrent, class-based, object-oriented, and specifically designed to have as few implementation dependencies as possible. It is intended to let application developers \"write once, run anywhere\" (WORA), meaning that code that runs on one platform does not need to be recompiled to run on another. Java applications are typically compiled to bytecode (class file) that can run on any Java virtual machine (JVM) regardless of computer architecture. The language derives much of its syntax from C and C++, but it has fewer low-level facilities.</p> <p>The Java home page is at http://www.java.com.</p>","tags":["mahuika","general"]},{"location":"Scientific_Computing/Supported_Applications/Java/#licensing-requirements","title":"Licensing requirements","text":"<p>All versions of Java on NeSI clusters have been made available by their respective owners at no cost under a limited, closed-source licence. The full licence terms and conditions for any given version of Java can be found by following the directions in <code>${JAVA_HOME}/LICENSE</code>.</p>","tags":["mahuika","general"]},{"location":"Scientific_Computing/Supported_Applications/Java/#example-scripts","title":"Example scripts","text":"","tags":["mahuika","general"]},{"location":"Scientific_Computing/Supported_Applications/Java/#example-script-for-mahuika","title":"Example script for Mahuika","text":"<pre><code>#!/bin/bash -e\n#SBATCH --job-name      MyMultithreadedJavaJob\n#SBATCH --time          1:00:00          # 1 hour walltime limit\n#SBATCH --cpus-per-task 8                # 8 CPU cores for 8 Java threads\n#SBATCH --mem           4096MB           # 4 GB of memory\n\nmodule load Java/1.8.0_144\n# The following line is needed in case your Java application\n# is called indirectly\nexport _JAVA_OPTIONS=-Djava.io.tmpdir=${TMPDIR}\njava -Xmx3g -Djava.io.tmpdir=${TMPDIR} -jar /path/to/foo.jar\n</code></pre>","tags":["mahuika","general"]},{"location":"Scientific_Computing/Supported_Applications/Java/#further-notes","title":"Further notes","text":"","tags":["mahuika","general"]},{"location":"Scientific_Computing/Supported_Applications/Java/#java-versions","title":"Java Versions","text":"<p>The default version of Java that is packaged with the operating system may not be appropriate for your work.\u00a0 To use a different version of Java us the `module` command to find and load for example:</p> <pre><code>$ module spider Java\n-----------------------------------------------------------------------\n-----------------------------------------------------------------------\nJava Platform, Standard Edition (Java SE) lets you develop and deploy \nJava applications on desktops and servers.\n\nVersions:\nJava/1.7.0_51\nJava/1.8.0_144\nJava/11.0.4\nJava/15.0.2\n\n$ module load Java/15.0.2\n</code></pre>","tags":["mahuika","general"]},{"location":"Scientific_Computing/Supported_Applications/Java/#memory-management-and-the-xmx-option","title":"Memory management and the -Xmx option","text":"<p>It is important to let the Java virtual machine know how much memory it is allowed to use. \u00a0 The main way this is done is via the <code>-Xmx</code> option,\u00a0which sets the maximum amount of heap space that it can use.</p> <p>As a first approximation, we recommend setting the <code>-Xmx</code> option to 75% of the requested memory. For example, if your job asks the scheduler for 32 GB of memory, you should provide the Java executable with <code>-Xmx24g</code>, which will cap its heap usage to 24 GB, leaving at least 6 GB for its stack and any other overheads.</p>","tags":["mahuika","general"]},{"location":"Scientific_Computing/Supported_Applications/Java/#temporary-files","title":"Temporary Files","text":"<p>Java programs which use temporary files can (and should) generally be persuaded to use $TMPDIR rather than just the default of <code>/tmp</code>by being given the option\u00a0<code>-Djava.io.tmpdir=$TMPDIR.</code>\u00a0 TMPDIR is automatically removed at the end of the job.</p> <ul> <li>If you run your Java program by using the <code>java</code> command, that is in     a form like     <code>java &lt;java_options&gt; java.program &lt;specific_program_options&gt;</code>, you     can specify the tmpdir as follows:     <code>java -Djava.io.tmpdir=$TMPDIR &lt;other_java_options&gt; java.program &lt;specific_program_options&gt;</code>.</li> <li>If your Java program is called indirectly, or is pre-wrapped, you     will need to put the following line in your job submission script     before calling the Java program:     <code>export _JAVA_OPTIONS=-Djava.io.tmpdir=${TMPDIR}</code>.</li> </ul>","tags":["mahuika","general"]},{"location":"Scientific_Computing/Supported_Applications/Julia/","title":"Julia","text":"<p>A high-level, high-performance dynamic language for technical computing.  This version was compiled from source with USE_INTEL_JITEVENTS=1 to enable profiling with VTune.</p> <p>Julia Homepage</p>","tags":[]},{"location":"Scientific_Computing/Supported_Applications/Julia/#available-modules","title":"Available Modules","text":"Mahuika Maui_ancil <p> 0.6.4 1.0.0 1.1.0 1.2.0-gimkl-2018b-VTune 1.4.1-GCC-9.2.0-VTune 1.5.1-GCC-9.2.0-VTune 1.6.0-GCC-9.2.0-VTune 1.6.2-GCC-9.2.0-VTune 1.7.1-GCC-9.2.0-VTune 1.7.2-GCC-11.3.0-VTune 1.8.0-GCC-11.3.0-VTune 1.8.5-GCC-11.3.0-VTune 1.9.1-GCC-11.3.0-VTune 1.9.4-GCC-11.3.0-VTune </p> <pre><code>module load Julia/1.9.4-GCC-11.3.0-VTune</code></pre> <p> 1.7.1-GCC-9.2.0-VTune </p> <pre><code>module load Julia/1.7.1-GCC-9.2.0-VTune</code></pre> <p>Julia is a flexible dynamic language, appropriate for scientific and numerical computing, with performance comparable to traditional statically-typed languages. The Julia home page is at\u00a0https://julialang.org/.</p>","tags":[]},{"location":"Scientific_Computing/Supported_Applications/Julia/#licensing-requirements","title":"Licensing requirements","text":"<p>The Julia language is (mostly) licensed under the MIT licence. For more details, including the full text of the licence and a list of exceptions, see\u00a0https://github.com/JuliaLang/julia/blob/master/LICENSE.md.</p>","tags":[]},{"location":"Scientific_Computing/Supported_Applications/Julia/#julia-packages","title":"Julia packages","text":"<p>Besides the core Julia language and interpreter, a great deal of functionality is provided by Julia packages contributed by the Julia developers and by third parties, or you can write your own packages. These packages are licensed separately from the main Julia software, so different terms and conditions may apply.</p>","tags":[]},{"location":"Scientific_Computing/Supported_Applications/Julia/#installing-julia-packages","title":"Installing Julia packages","text":"<p>Julia extensions, i.e. pieces of code that add functionality, are called modules, and for installation and management purposes modules are grouped into packages. Each package thus consists of one or more modules.</p> <p>NeSI provides a range of packages with our centrally managed Julia installations, however you may wish to install additional packages, either in your home directory or, more likely, in a project directory so your research team members can be sure of using the same version of relevant software.</p> <p>Julia provides its own package management system, which is itself a module, the Pkg module, that is included with the base Julia installation. You can use the Pkg module within a Julia script, or on the Julia command line. In this documentation, we will assume you are using the command line, but the commands are the same within a script.</p> <ol> <li> <p>Load the environment module (not the same as a Julia module)     corresponding to the version of Julia you want to use, e.g. Julia     1.1.0:</p> <pre><code>module load Julia/1.1.0\n</code></pre> </li> <li> <p>Launch the Julia executable:</p> <pre><code># Use Julia interactively\njulia\n# Alternatively, use a Julia script\njulia script.jl\n</code></pre> </li> <li> <p>If you have opened Julia interactively, you should now see a Julia     welcome message and prompt, like the following.</p> <pre><code>               _\n   _       _ _(_)_     |  Documentation: https://docs.julialang.org\n  (_)     | (_) (_)    |\n   _ _   _| |_  __ _   |  Type \"?\" for help, \"]?\" for Pkg help.\n  | | | | | | |/ _` |  |\n  | | |_| | | | (_| |  |  Version 1.1.0 (2019-01-21)\n _/ |\\__'_|_|_|\\__'_|  |  Official https://julialang.org/ release\n|__/                   |\n\njulia&gt;\n</code></pre> </li> <li> <p>Load the Julia package manager:</p> <pre><code>using Pkg\n</code></pre> </li> <li> <p>Most important variable for installing packages is called     <code>DEPOT_PATH</code>. The depot path is a series of directories that will be     searched, in order, for the package that you wish to install and its     dependencies. Clear the depot path.</p> <p>Warning</p> <p>It is possible for a package to be installed somewhere on <code>DEPOT_PATH</code>, but not compiled. If this happens, and the package is a dependency of what you're trying to install, Julia will try to compile it in situ. This is a bad thing most of the time, because you're unlikely to have write access to the install location, so the compilation will fail. Hence why clearing the depot path is important.</p> <pre><code>empty!(DEPOT_PATH)\n</code></pre> </li> <li> <p>Add your preferred Julia package directory to the newly empty depot     path.</p> <pre><code>push!(DEPOT_PATH, \"/nesi/project/nesi12345/julia\")\n</code></pre> <p>Tip</p> <p>While a conventional personal Julia package directory is <code>/home/joe.bloggs/.julia</code> or similar, there is no reason for the directory to be within any particular user's home directory, or for it to be a hidden directory with a name starting with a dot. For shared Julia package directories, a visible directory within a project directory will probably be more useful to you and your colleagues. In any case, for obvious reasons, you should choose a directory to which you have write access.</p> </li> <li> <p>Install the desired Julia package. In this case, we are showing the     machine-learning package Flux as an example.</p> <pre><code>Pkg.add(\"Flux\")\n</code></pre> <p>Julia should chug away for a while, downloading and compiling various packages into the chosen directory.</p> </li> </ol>","tags":[]},{"location":"Scientific_Computing/Supported_Applications/Julia/#making-julia-packages-available-at-runtime","title":"Making Julia packages available at runtime","text":"<p>For some reason, Julia uses the <code>DEPOT_PATH</code> variable only to control where newly obtained packages are to be installed. The directories where existing packages are searched for are stored in a different variable, <code>LOAD_PATH</code>.</p> <p>On NeSI, the default contents of <code>LOAD_PATH</code> are as follows:</p> <pre><code>LOAD_PATH\n5-element Array{String,1}:\n \"@\"\n \"@v#.#\"\n \"@stdlib\"\n \"/opt/nesi/mahuika/Julia/1.1.0/local/share/julia/environment/v1.1\"\n \".\"\n</code></pre> <p>The first three elements are special entries, while the fourth element is the set of centrally managed Julia packages, and the fifth is the current working directory. As you can see, custom depot directories are not present in <code>LOAD_PATH</code> by default.</p> <p>There are several ways to add a directory to <code>LOAD_PATH</code>, but almost certainly the easiest is to do the following in your environment:</p> <pre><code>$ export JULIA_LOAD_PATH=\"/nesi/project/nesi12345/julia:${JULIA_LOAD_PATH}\"\n</code></pre> <p>Tip</p> <p>By prepending the directory to <code>JULIA_LOAD_PATH</code> instead of appending  it, you ensure that your project's versions of Julia packages are used  by default, in preference to whatever might be managed centrally. This  is probably what you want to do. If you want to use the centrally  managed versions of Julia packages first and only use your project's  package if there isn't a centrally managed instance, you can append it  instead:</p> <pre><code>$ export JULIA_LOAD_PATH=${JULIA_LOAD_PATH}:/nesi/project/nesi12345/julia\"\n</code></pre> <p>Tip</p> <p>To revert to the default load path, just unset <code>JULIA_LOAD_PATH</code>:  <pre><code>$ unset JULIA_LOAD_PATH\n$ export JULIA_LOAD_PATH\n</code></pre></p>","tags":[]},{"location":"Scientific_Computing/Supported_Applications/Julia/#profiling-julia-code","title":"Profiling Julia code","text":"<p>In addition to the Julia Profile module (see the\u00a0official documentation), it is also possible to profile Julia code with external profilers. On Mahuika we have installed \"-VTune\" variants of Julia, which are built from source with support for profiling using Intel VTune. VTune is a nice tool for profiling parallel code (e.g. code making use of threading or MPI.jl).</p> <p>In order to collect profiling data with VTune you should:</p> <ul> <li> <p>load a \"-VTune\" variant of Julia, for example:</p> <pre><code>module load Julia/1.2.0-gimkl-2018b-VTune\n</code></pre> </li> <li> <p>load a VTune module:</p> <pre><code>module load VTune\n</code></pre> </li> <li> <p>enable Julia VTune profiling by setting an environment variable:</p> <pre><code>export ENABLE_JITPROFILING=1\n</code></pre> </li> <li> <p>prepend the usual command that you use to run your Julia program     with the desired VTune command, for example to run a hotspots     analysis:</p> <pre><code>srun amplxe-cl -collect hotspots -- julia your_program.jl\n</code></pre> </li> </ul> <p>VTune will create a result directory which contains the profiling information. This result can be loaded using the VTune GUI, assuming you have X11 forwarding enabled:</p> <pre><code>amplxe-gui --path-to-open &lt;vtune-result-directory&gt;\n</code></pre> <p>Additional information about VTune can be found in the User Guide.</p>","tags":[]},{"location":"Scientific_Computing/Supported_Applications/JupyterLab/","title":"JupyterLab","text":"<p>An extensible environment for interactive and reproducible computing, based on the Jupyter Notebook and Architecture.</p> <p>JupyterLab Homepage</p>","tags":[]},{"location":"Scientific_Computing/Supported_Applications/JupyterLab/#available-modules","title":"Available Modules","text":"Mahuika <p> 2.1.3-gimkl-2018b-Python-3.8.1 2.1.5-gimkl-2018b-Python-3.8.1 2.2.4-gimkl-2018b-Python-3.8.1 2021.5.0-gimkl-2020a-3.0.15 2021.8.2-gimkl-2020a-3.1.9 2021.9.0-gimkl-2020a-3.1.9 2022.2.0-gimkl-2020a-3.2.8 2022.5.0-gimkl-2020a-3.4.2 2022.6.0-gimkl-2020a-3.4.3 2022.7.0-gimkl-2020a-3.4.3 2022.8.0-gimkl-2020a-3.4.5 2023.1.0-gimkl-2022a-3.5.3 </p> <pre><code>module load JupyterLab/2023.1.0-gimkl-2022a-3.5.3</code></pre> <p>Warning</p> <p>This documentation contains our legacy instructions for running  JupyterLab by tunnelling through the lander node. If you are a Mahuika cluster user, we recommend using jupyter via\u00a0  jupyter.nesi.org.nz. Follow this link for more  information</p> <p>NeSI provides a service for working on Jupyter Notebooks. As a first step JupyterLab can be used on Mahuika nodes. JupyterLab is a single-user web-based Notebook server, running in the user space. JupyterLab servers should be started preferably on a compute node, especially for compute intensive or memory intensive workloads. For less demanding work the JupyterLab server can be started on a login or virtual lab node. After starting the server your local browser can be connected. Therefore port forwarding needs to be enabled properly. The procedure will be simplified in future, but now require the following steps, which are then described in more details:</p> <ul> <li>Launch JupyterLab<ul> <li>Connect to the NeSI system to establish SSH port     forwarding\u00a0<ul> <li>SSH Command Line     OR</li> <li>MobaXterm GUI</li> </ul> </li> <li>open another session to the NeSI system</li> <li>Launch the JupyterLab     server<ul> <li>on login nodes / virtual     labs OR</li> <li>on compute nodes</li> </ul> </li> <li>Launch JupyterLab in your local     browser</li> </ul> </li> <li>Kernels</li> <li>Packages</li> </ul>","tags":[]},{"location":"Scientific_Computing/Supported_Applications/JupyterLab/#launch-jupyterlab","title":"Launch JupyterLab","text":"<p>Since JupyterLab is a web based application, and at NeSI launched behind the firewall, a port needs to be forwarded to your local machine, where your browser should connected. This ports are numbers between 2000 and 65000, which needs to be unique on the present machine. The default port for JupyterLab is 8888, but only one user can use this at a time.</p> <p>To avoid the need for modifying the following procedure again and again, we suggest to (once) select a unique number (between 2000 and 65000). This number needs to be used while establishing the port forwarding and while launching JupyterLab. In the following we use the port number 15051 (please select another number).</p>","tags":[]},{"location":"Scientific_Computing/Supported_Applications/JupyterLab/#setup-ssh-port-forwarding","title":"Setup SSH port forwarding","text":"<p>Prerequisite</p> <ul> <li>In the following we assume you already configured      your<code>.ssh/config</code> to use two hop method as described in the      Standard Terminal      Setup.</li> </ul> <p>First, the port forwarding needs to be enabled between your local machine and the NeSI system. Therewith a local port will be connected to the remote port on the NeSI system. For simplicity, we kept both numbers the same (here 15051). This can be specified on the command line in the terminal or using the MobaXterm GUI.</p>","tags":[]},{"location":"Scientific_Computing/Supported_Applications/JupyterLab/#ssh-command-line","title":"SSH Command Line","text":"<p>The ssh command need to be called with following arguments, e.g. for Mahuika:</p> <pre><code>ssh -N -L 15051:localhost:15051 mahuika\n</code></pre> <p>Here -N means \"Do not execute a remote command\" and -L means \"Forward Local Port\".</p> <p>!!!  tip      -   For Maui_Ancil, e.g. w-mauivlab01 you may want to add the          following to your <code>.ssh/config</code> to avoid establishing the          additional hop manually.          <pre><code>Host maui_vlab\n   User &lt;username&gt;\n   Hostname w-mauivlab01.maui.niwa.co.nz\n   ProxyCommand ssh -W %h:%p maui\n   ForwardX11 yes\n   ForwardX11Trusted yes\n   ServerAliveInterval 300\n   ServerAliveCountMax 2\n</code></pre>          &lt;username&gt; needs to be changed. Hostnames can be adapted for          other nodes, e.g. <code>w-clim01</code></p>","tags":[]},{"location":"Scientific_Computing/Supported_Applications/JupyterLab/#mobaxterm-gui","title":"MobaXterm GUI","text":"<p>Tips</p> <ul> <li>MobaXterm has an internal terminal which acts like a linux      terminal and can be configured as described in the Standard      Terminal      Setup.      Therewith the SSH command      line approach above can      be used.</li> </ul> <p>MobaXterm has a GUI to setup and launch sessions with port forwarding, click 'Tools &gt; MobaSSH Thunnel (port forwarding)':</p> <ul> <li>specify the lander.nesi.org.nz as SSH server address (right, lower     box, first line)</li> <li>specify your user name (right, lower box, second line)</li> <li>specify the remote server address, e.g. login.mahuika.nesi.org.nz\u00a0     (right, upper box first line)</li> <li>specify the JupyterLab port number on the local side (left) and at     the remote server (right upper box, second line)</li> <li>Save</li> </ul> <p></p>","tags":[]},{"location":"Scientific_Computing/Supported_Applications/JupyterLab/#launch-the-jupyterlab-server","title":"Launch the JupyterLab server","text":"<p>After successfully establishing the port forwarding, we need open another terminal and login to the NeSI system in the usual way, e.g. opening a new terminal and start another ssh session:</p> <pre><code>ssh mahuika\n</code></pre> <p>On the Mahuika login node, load the environment module which provides JupyterLab:</p> <pre><code>module load JupyterLab\n</code></pre> <p>Or alternatively, and particularly if you are using a M\u0101ui ancillary node instead of Mahuika, you can use the Anaconda version of JupyterLab instead:</p> <pre><code>module load Anaconda3\nmodule load IRkernel  # optional\n</code></pre> <p>The JupyterLab server then can be started on the present node (login or virtual lab) or offloaded to a compute node. Please launch compute or memory intensive tasks on a compute node</p>","tags":[]},{"location":"Scientific_Computing/Supported_Applications/JupyterLab/#on-login-nodes-virtual-labs","title":"On login nodes / virtual labs","text":"<p>For very small (computational cheap and small memory) the JupyterLab can be started on the login or virtual lab using:</p> <pre><code>jupyter lab --port 15051 --no-browser\n</code></pre> <p>Where, <code>--port 15051</code> specifies the above selected port number and <code>--no-browser</code> option prevents JupyterLab from trying to open a browser on the compute/login node side. Jupyter will present output as described in the next section including the URL and a unique key, which needs to be copied in your local browser.</p>","tags":[]},{"location":"Scientific_Computing/Supported_Applications/JupyterLab/#on-compute-node","title":"On compute node","text":"<p>Especially notebooks with computational and memory intensive tasks should run on compute nodes. Therefore, a script is provided, taking care of port forwarding to the compute node and launching JupyterLab. A session with 60 min on 1 core can be launched using:</p> <pre><code>srun --ntasks 1 -t 60\u00a0 jupyter-compute 15051\u00a0 # please change port number\n</code></pre> <p>After general output, JupyterLab prints a URL with a unique key and the network port number where the web-server is listening, this should look similar to:</p> <pre><code>...\n[C 14:03:19.911 LabApp]\n  To access the notebook, open this file in a browser:\n      file:///scale_wlg_persistent/filesets/project/nesi99996/.local/share/jupyter/runtime/nbserver-503-open.html\n  Or copy and paste one of these URLs:\n      http://localhost:15051/?token=d122855ebf4d029f2bfabb0da03ae01263972d7d830d79c4\n</code></pre> <p>The last line will be needed in the browser later.</p> <p>Therewith the Notebook and its containing tasks are performed on a compute node. You can double check e.g. using</p> <pre><code>import os\nos.open('hostname').read()\n</code></pre> <p>More resources can be requested, e.g. by using:</p> <pre><code>srun --ntasks 1 -t 60 --cpus-per-task 5 --mem 512MB jupyter-compute 15051 \n</code></pre> <p>Where 5 cores are requested for threading and a total memory of 3GB. Please do not use <code>multiprocessing.cpu_count()</code> since this is returning the total amount of cores on the node. Furthermore, if you use libraries, which implement threading align the numbers of threads (often called jobs) to the selected number of cores (otherwise the performance will be affected).</p>","tags":[]},{"location":"Scientific_Computing/Supported_Applications/JupyterLab/#jupyterlab-in-your-local-browser","title":"JupyterLab in your local browser","text":"<p>Finally, you need to open your local web browser and copy and paste the URL specified by the JupyterLab server into the address bar. After initializing Jupyter Lab you should see a page similar to:</p> <p></p>","tags":[]},{"location":"Scientific_Computing/Supported_Applications/JupyterLab/#kernels","title":"Kernels","text":"<p>The following JupyterLab kernel are installed:</p> <ul> <li>Python3</li> <li>R\u00a0</li> <li>Spark</li> </ul>","tags":[]},{"location":"Scientific_Computing/Supported_Applications/JupyterLab/#r","title":"R","text":"<p>verify that the module IRkernel is loaded</p> <pre><code>module load IRkernel\n</code></pre>","tags":[]},{"location":"Scientific_Computing/Supported_Applications/JupyterLab/#spark","title":"Spark","text":"<p>pySpark and SparkR is supported in NeSI Jupyter notebooks. Therefore, the module Spark needs to be loaded before starting Jupyter. Please run Spark workflows on compute nodes.</p> <pre><code>module load Spark\n</code></pre>","tags":[]},{"location":"Scientific_Computing/Supported_Applications/JupyterLab/#packages","title":"Packages","text":"<p>There are a long list of default packages provided by the JupyterLab environment module (list all using <code>!pip list</code>) and R (list using <code>installed.packages(.Library)</code>, note the list is shortened).\u00a0</p> <p>Furthermore, you can install additional packages as described on the Python and R support page.</p>","tags":[]},{"location":"Scientific_Computing/Supported_Applications/Keras/","title":"Keras","text":"<p>Keras is a modular and extensible API for building neural networks in Python. Keras is included with\u00a0TensorFlow. Note that there are CPU and GPU versions\u00a0of TensorFlow, here we'll use TensorFlow 1.10 for GPUs, which is available as an environment module.</p> <p>Keras can be used to solve a wide set of problems using artificial neural networks, including pattern recognition. Ultimately, a neural network is just a black box that takes input values and computes output values. Internally, the output values are computed using artificial neurons, which are modelled after biological neurons. The connections between neurons have different \"weights\", which when submitted to different stimuli will output different signals. With sufficient training, we can teach a neural network to acquire the correct weights, i.e. adjust the weights until the desired output is produced.</p>","tags":[]},{"location":"Scientific_Computing/Supported_Applications/Keras/#counting-dots-in-images","title":"Counting dots in images","text":"<p>In this example, we will set up a neural network to count the number of dots embedded in an image.</p>","tags":[]},{"location":"Scientific_Computing/Supported_Applications/Keras/#generating-the-training-and-testing-datasets","title":"Generating the training and testing datasets","text":"<p>Start by generating images of dots.\u00a0 We'll generate 1000 images for our training set and 100 images to test our predictions. On Mahuika type the following commands to generate the training and testing data sets:</p> <pre><code>wget https://raw.githubusercontent.com/mkienzle/MachineLearning/master/Scripts/ProduceSyntheticData/DrawDots.R\nml R/3.6.1-gimkl-2018b\nRscript DrawDots.R -n 1000 -r 0 -R 5 -s 123 -o train -c train.csv -w 40\nRscript DrawDots.R -n 100 -r 0 -R 5 -s 234 -o test -c test.csv -w 40\n</code></pre> <p>The images are saved under directories train/ and test/, respectively. An example of image is test/img49.jpg.</p> <pre><code>display test/img49.jpg\n</code></pre> <p></p> <p>which shows five, partially overlapping dots. Note that along with the images, a comma separated values (csv) file (e.g. train/train.csv) containing the number of dots (0 to 5) for each image is also saved.</p>","tags":[]},{"location":"Scientific_Computing/Supported_Applications/Keras/#installing-image-processing-software","title":"Installing image processing software","text":"<p>The images need to be slightly manipulated. For instance we expect all the images to be black and white so we can collapse the red, green and blue channel into one. We'll need OpenCV to this task:</p> <pre><code>pip install opencv-python --user\n</code></pre>","tags":[]},{"location":"Scientific_Computing/Supported_Applications/Keras/#running-the-model","title":"Running the model","text":"<p>Our neural network</p> <pre><code>wget https://raw.githubusercontent.com/mkienzle/MachineLearning/master/Scripts/Conv2D/classify.py\n</code></pre> <p>is encoded in classify.py. It is made of three convolution layers, each followed by max pooling. The convolution and max pooling layers are often applied to extract features in images. Finally the image is flattened as a 1D array and a dense layer, which returns an estimate of the number of dots as a single floating point number, is added. The corresponding lines in classify.py look like (Python code):</p> <pre><code>clf = keras.Sequential()\nclf.add( keras.layers.Conv2D(32, kernel_size=(3,3), strides=(1,1),\n\u00a0\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 padding='same', data_format='channels_last', activation='relu') )\nclf.add( keras.layers.MaxPooling2D(pool_size=(2, 2)) )\nclf.add( keras.layers.Conv2D(128, kernel_size=(3,3), strides=(1,1),\n\u00a0\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 padding='same', data_format='channels_last', activation='relu') )\nclf.add( keras.layers.MaxPooling2D(pool_size=(2, 2)) )\nclf.add( keras.layers.Conv2D(256, kernel_size=(3,3), strides=(1,1),\n\u00a0\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 padding='same', data_format='channels_last', activation='relu') )\nclf.add( keras.layers.MaxPooling2D(pool_size=(2, 2)) )\nclf.add( keras.layers.Flatten() )\nclf.add( keras.layers.Dense(1) )\n</code></pre> <p>We're now ready to train and test our model:</p> <pre><code>#!/bin/bash -e\n#SBATCH --job-name keras-dots\n#SBATCH --partition gpu\n#SBATCH --gres gpu:1\n#SBATCH --ntasks 1\n#SBATCH --cpus-per-task 1\n#SBATCH --time 00:10:00\n#SBATCH --mem 512MB\nmodule load TensorFlow/1.10.1-gimkl-2017a-Python-3.6.3\npython classify.py --testDir=test --trainDir=train --save=someResults.png\n</code></pre> <p>Copy-paste the above and save in file classify.sl. Submit the Slurm script classify.sl</p> <pre><code>sbatch classify.sl\n</code></pre>","tags":[]},{"location":"Scientific_Computing/Supported_Applications/Keras/#looking-at-the-output","title":"Looking at the output","text":"<p>Upon completion of the run, expect to find file someResults.png in the same directory as classify.py. This file contains the predictions for the first 50 test images, which will vary for each training but the result will look like:</p> <p></p> <p>(The purple images have no dots.) With each image the number of dots is displayed as well as the value inferred by the model in parentheses. The inferred values are to be rounded to the nearest integer. Plot titles in red indicate failures. Among the 100 test images, the correct number of dots was found in 90 percent of the cases (the accuracy will change with each training due to the randomness of the process). The predicted number of dots should be off by no more than one unit in most cases.</p>","tags":[]},{"location":"Scientific_Computing/Supported_Applications/Lambda%20Stack/","title":"Lambda Stack","text":"","tags":[]},{"location":"Scientific_Computing/Supported_Applications/Lambda%20Stack/#introduction","title":"Introduction","text":"<p>Lambda Stack is an AI software stack from Lambda containing PyTorch, TensorFlow, CUDA, cuDNN and more. On NeSI you can run Lambda Stack via Singularity (based on the official Dockerfiles). We have provided some prebuilt Singularity images (under /opt/nesi/containers/lambda-stack/) or you can build your own (see the guide below). In the following sections, we will show you how to run Lambda Stack in a Slurm job or interactively via JupyterLab.</p> <p>You can list the available Lambda Stack version on NeSI by running:</p> <pre><code>$ ls /opt/nesi/containers/lambda-stack\nlambda-stack-focal-20201130.sif\nlambda-stack-focal-20201221.sif\nlambda-stack-focal-20210105.sif\nlambda-stack-focal-latest.sif\nREADME\n</code></pre> <p>In the filenames above, the dates correspond to the date the image was built and the file with -latest will correspond to the most recent version.</p>","tags":[]},{"location":"Scientific_Computing/Supported_Applications/Lambda%20Stack/#building-the-singularity-image-optional","title":"Building the Singularity image (optional)","text":"<p>This step is optional; if you choose to use the prebuilt Singularity images under <code>/opt/nesi/containers/lambda-stack/</code> you can skip this step.</p> <p>Note that Singularity images are immutable, so the versions of packages in the image are a snapshot of the available versions from when the image was built. If you need more recent versions of packages, you can't just update them within the image, instead you must build a new Singularity image with the required versions.</p> <p>Official Dockerfiles are provided for Lambda Stack but Docker can't be used on NeSI for security reasons, hence the need to create a Singularity image. Both Docker and Singularity require root access to build images (but Singularity does not require root to run them), so you will need to build the images somewhere you have admin rights (e.g. your laptop). These steps should work on Linux; if you run another operating system you could try installing an Ubuntu VM in VirtualBox.</p> <p>Make sure you have Docker and Singularity installed first and then follow the steps below.</p> <pre><code># clone the lambda stack Dockerfiles repo\ngit clone https://github.com/lambdal/lambda-stack-dockerfiles.git\ncd lambda-stack-dockerfiles\n\n# build the Docker image\nsudo docker build -t lambda-stack:20.04 -f Dockerfile.focal .\n\n# build the Singularity image from the Docker image\nsudo singularity build lambda-stack-focal-$(date +%Y%m%d).sif docker-daemon:lambda-stack:20.04\n</code></pre> <p>Note that the Docker build will require a lot of disk space during the build (~40GB) and the final image will be ~14GB. The Singularity image will be ~5GB and will also require a lot of space during the build. If you don't have enough space in /tmp for the Singularity build you could try running the following script (updating paths first) as root (e.g. using sudo):</p> <pre><code>#!/bin/bash\nexport SINGULARITY_TMPDIR=/path/to/somewhere/with/lots/of/space\nexport SINGULARITY_CACHEDIR=/path/to/somewhere/else/with/lots/of/space\nsingularity build lambda-stack-focal-$(date +%Y%m%d).sif docker-daemon:lambda-stack:20.04\n</code></pre>","tags":[]},{"location":"Scientific_Computing/Supported_Applications/Lambda%20Stack/#lambda-stack-via-slurm","title":"Lambda Stack via Slurm","text":"<p>The following Slurm script can be used as a template for running jobs using Lambda Stack.</p> <pre><code>#!/bin/bash\n#SBATCH --job-name=lambdastack\n#SBATCH --time=00:15:00     # required walltime\n#SBATCH --ntasks=1          # number of MPI tasks\n#SBATCH --cpus-per-task=1   # number of threads per MPI task\n#SBATCH --gpus-per-task=1   # optional, only if a GPU is required\n\n# path to the singularity image file (optionally replace with your own)\nSIF=/opt/nesi/containers/lambda-stack/lambda-stack-focal-latest.sif\n\n# load environment modules (these are always required)\nmodule purge\nmodule load Singularity\n\n# for convenience store the singularity command in an environment variable\n# feel free to add additional binds if you need them \nSINGULARITY=\"singularity exec --nv -B ${PWD} ${SIF}\"\n\n# run a command in the container\n${SINGULARITY} echo \"Hello World\"\n</code></pre>","tags":[]},{"location":"Scientific_Computing/Supported_Applications/Lambda%20Stack/#lambda-stack-via-jupyter","title":"Lambda Stack via Jupyter","text":"<p>The following steps will create a custom Lambda Stack kernel that can be accessed via NeSI's Jupyter service (based on the instructions here).</p> <p>First, we need to create a kernel definition and wrapper that will launch the Singularity image. Run the following commands on the Mahuika login node:</p> <pre><code># load the Singularity envioronment module\nmodule load Singularity\n\n# path to the singularity image file (optionally replace with your own)\nexport SIF=/opt/nesi/containers/lambda-stack/lambda-stack-focal-latest.sif\n\n# create a jupyter kernel using the Python within the Singularity image\nsingularity exec -B $HOME $SIF python -m ipykernel install --user \\\n        --name lambdastack --display-name=\"Lambda Stack Python 3\"\n</code></pre> <p>If successful this should report that a kernelspec has been installed. Change to the kernelspec directory:</p> <pre><code>cd $HOME/.local/share/jupyter/kernels/lambdastack\n</code></pre> <p>and create a wrapper script for launching the kernel, named wrapper.sh:</p> <pre><code>#!/usr/bin/env bash\n\n# path to the singularity image file (optionally replace with your own)\nSIF=/opt/nesi/containers/lambda-stack/lambda-stack-focal-latest.sif\n\n# load environment modules (these are always required)\nmodule purge\nmodule load Singularity\n\n# unfortunately $HOME is not the canonical path to your home directory,\n# we need to bind in canonical home path too so jupyter can find kernel\n# connection file\nhomefull=$(readlink -e $HOME)\n\n# for convenience store the singularity command in an environment variable\n# feel free to add additional binds if you need them \nSINGULARITY=\"singularity exec --nv -B ${HOME},${homefull},${PWD} ${SIF}\"\n\n# run a command in the container\necho ${SINGULARITY} python3 $@\n${SINGULARITY} python3 $@\n</code></pre> <p>Make the wrapper script executable:</p> <pre><code>chmod +x wrapper.sh\n</code></pre> <p>Next, edit the\u00a0kernel.json\u00a0to change the first element of the argv list to point to the wrapper script we just created. The file should look like this (change &lt;username&gt; to your NeSI username):</p> <pre><code>{\n\"argv\": [\n\"/home/&lt;username&gt;/.local/share/jupyter/kernels/lambdastack/wrapper.sh\",\n\"-m\",\n\"ipykernel_launcher\",\n\"-f\",\n\"{connection_file}\"\n],\n\"display_name\": \"Lambda Stack Python 3\",\n\"language\": \"python\"\n}\n</code></pre> <p>After refreshing the NeSI JupyterLab your Lambda Stack Python kernel should show up as \"Lambda Stack Python 3\".</p>","tags":[]},{"location":"Scientific_Computing/Supported_Applications/Lambda%20Stack/#example-running-transformers-benchmarks","title":"Example: running Transformers benchmarks","text":"<p>Here we give an example showing using Lambda Stack to run the Transformers library benchmarks. Transformers is a natural language processing library that uses either TensorFlow or PyTorch underneath. While both PyTorch and TensorFlow are included in the Lambda Stack distribution, the Transformers library is not, so the first thing we do is create a virtual environment and install transformers into it:</p> <pre><code># load the Singularity environment module\nmodule load Singularity\n\n# create a directory and change to it\nmkdir /nesi/project/&lt;project_code&gt;/transformers-benchmarks\ncd /nesi/project/&lt;project_code&gt;/transformers-benchmarks\n\n# path to the singularity image file (optionally replace with your own)\nexport SIF=/opt/nesi/containers/lambda-stack/lambda-stack-focal-latest.sif\n\n# launch a bash shell in the Singularity image\nsingularity exec -B $PWD $SIF bash\n</code></pre> <p>After executing the above command your prompt should have changed to Singularity&gt;, the following commands are all executed at this prompt (i.e. within the container):</p> <pre><code>virtualenv --system-site-packages transenv\nsource transenv/bin/activate\npip install transformers psutil py3nvml\n\n# exit the Singularity container bash prompt\nexit\n</code></pre> <p>Note we used `--system-site-packages`` so that we can use the Lambda Stack installed TensorFlow, PyTorch, etc., instead of installing them separately.</p> <p>Now clone the transformers git repo so we can run the benchmark script (these commands run outside the container):</p> <pre><code>git clone https://github.com/huggingface/transformers.git\n</code></pre> <p>Create the following script for running the benchmarks, named run-benchmark-torch.sh:</p> <pre><code>#!/bin/bash -e\n\n# load the virtual environment with transformers installed\nsource transenv/bin/activate\n\n# path to transformers benchmark script\nBENCH_SCRIPT=transformers/examples/pytorch/benchmarking/run_benchmark.py\n\n# run the benchmarks\npython ${BENCH_SCRIPT} --no_multi_process --training --no_memory \\\n                       --save_to_csv --env_print \\\n                       --models bert-base-cased bert-large-cased \\\n                                bert-large-uncased gpt2 \\\n                                gpt2-large gpt2-xl \\\n                       --batch_sizes 8 \\\n                       --sequence_lengths 8 32 128 512\n</code></pre> <p>Now create a Slurm script that will launch the job, names run-benchmark-torch.sl:</p> <pre><code>#!/bin/bash\n#SBATCH --job-name=lambdastack\n#SBATCH --time=00:30:00\n#SBATCH --ntasks=1\n#SBATCH --cpus-per-task=1\n#SBATCH --gpus-per-task=1\n#SBATCH --mem=12G\n\n# path to the singularity image file (optionally replace with your own)\nSIF=/opt/nesi/containers/lambda-stack/lambda-stack-focal-latest.sif\n\n# load environment modules (these are always required)\nmodule purge\nmodule load Singularity\n\n# for convenience store the singularity command in an environment variable\nSINGULARITY=\"singularity exec --nv -B ${PWD} ${SIF}\"\n\n# print PyTorch version and number of GPUs detected\n${SINGULARITY} python3 -c \"import torch; print('torch version', torch.__version__)\"\n${SINGULARITY} python3 -c \"import torch; print('num devices', torch.cuda.device_count())\"\n\n# run the benchmark script we created\n${SINGULARITY} bash ./run-benchmark-torch.sh\n</code></pre> <p>Submit this job to Slurm and then wait for the benchmarks to run:</p> <pre><code>sbatch run-benchmark-torch.sl\n</code></pre>","tags":[]},{"location":"Scientific_Computing/Supported_Applications/MAKER/","title":"MAKER","text":"<p>Genome annotation pipeline</p> <p>MAKER Homepage</p>","tags":[]},{"location":"Scientific_Computing/Supported_Applications/MAKER/#available-modules","title":"Available Modules","text":"Mahuika <p> 2.31.9-gimkl-2018b 2.31.9-gimkl-2020a </p> <pre><code>module load MAKER/2.31.9-gimkl-2020a</code></pre>","tags":[]},{"location":"Scientific_Computing/Supported_Applications/MAKER/#local-customisations","title":"Local Customisations","text":"<p>Since the MAKER control file maker_exe.ctl is just an annoyance in an environment module based system we have patched MAKER to make that optional. If it is absent then the defaults will be used directly.\u00a0</p>","tags":[]},{"location":"Scientific_Computing/Supported_Applications/MAKER/#parallelism","title":"Parallelism","text":"<p>MAKER can be used with MPI, though due to a complicated interaction between Infiniband libraries and MAKER's use of forking it can't be used across multiple nodes. So we\u00a0recommend running large MAKER jobs with up to 36 tasks on one node (ie: one full regular node), eg:</p> <pre><code>#SBATCH --nodes=1\n#SBATCH --ntasks-per-node=36\n#SBATCH --mem-per-cpu=1500\n\nmodule load MAKER/2.31.9-gimkl-2020a\nsrun maker -q\n</code></pre>","tags":[]},{"location":"Scientific_Computing/Supported_Applications/MAKER/#resources","title":"Resources","text":"<p>MAKER creates many files in its output, sometimes hundreds of thousands. \u00a0There is a risk that you exhaust your quota of inodes, so:</p> <ul> <li>Don't run too many MAKER jobs simultaneously.</li> <li>Delete unneeded output files promptly after MAKER finishes. \u00a0You can     use <code>nn_archive_files</code> or <code>tar</code> to archive them first.</li> </ul>","tags":[]},{"location":"Scientific_Computing/Supported_Applications/MATLAB/","title":"MATLAB","text":"<p>A high-level language and interactive environment for numerical computing.</p> <p>MATLAB Homepage</p> <p>Warning</p> <p>MATLAB is proprietary software. Make sure you meet the requirements for it's usage.</p>","tags":["engineering","ml"]},{"location":"Scientific_Computing/Supported_Applications/MATLAB/#available-modules","title":"Available Modules","text":"Mahuika Maui_ancil <p> 2017b 2018b 2019b 2020b 2021a 2021b 2022a 2022b 2023a 2023b </p> <pre><code>module load MATLAB/2023b</code></pre> <p> 2016b 2017a 2017b 2018b 2019b 2020a 2020b 2021a 2021b 2022a </p> <pre><code>module load MATLAB/2022a</code></pre>","tags":["engineering","ml"]},{"location":"Scientific_Computing/Supported_Applications/MATLAB/#licences","title":"Licences","text":"<p>The following network licence servers can be accessed from the NeSI cluster.</p> Institution Faculty Token University of Waikato <p>Not Required</p> National Institute of Water and Atmospheric Research <p>Not Required</p> Institute of Geological and Nuclear Sciences <p>Not Required</p> <p>Not Required</p> Massey University <p>Not Required</p> University of Auckland <p>Not Required</p> University of Otago <p>Not Required</p> Victoria University of Wellington <p>Not Required</p> Auckland University of Technology <p>Not Required</p> University of Canterbury <p>Not Required</p> Plant &amp; Food Research <p>Not Required</p> <p>If you do not have access, or want a server connected Contact our Support Team.</p> <p>No Licence?</p> <p>If you want to run MATLAB code on the cluster, but are not a member of an institution without access to floating licences, MATLAB code can still be run on the cluster using MCR.</p>","tags":["engineering","ml"]},{"location":"Scientific_Computing/Supported_Applications/MATLAB/#example-scripts","title":"Example scripts","text":"<p>Info</p> <p>When developing MATLAB code on your local machine, take measures to ensure it will be platform independent.\u00a0 Use relative paths when possible and not avoid using '\\s see here.</p> ScriptFunction <pre><code>#!/bin/bash -e\n#SBATCH --job-name   MATLAB_job     # Name to appear in squeue \n#SBATCH --time       01:00:00       # Max walltime \n#SBATCH --mem        512MB          # Max memory\n\nmodule load MATLAB/2023b\n\n# Run the MATLAB script MATLAB_job.m \nmatlab -nodisplay &lt; MATLAB_job.m \n</code></pre> <pre><code>#!/bin/bash -e\n\n#SBATCH --job-name       MATLAB_job    # Name to appear in squeue\n#SBATCH --time           06:00:00      # Max walltime\n#SBATCH --mem            2048MB        # Max memory\n#SBATCH --cpus-per-task  4             # 2 physical cores.\n#SBATCH --output         %x.log        # Location of output log\n\nmodule load MATLAB/2023b \n\nmatlab -batch \"addpath(genpath('.'));myFunction(5,20)\"\n# For versions older than 2019a, use '-nodisplay -r' instead of '-batch'\n</code></pre> <p>Command Line</p> <p>When using matlab on command line, all flag options use a single '<code>-</code>'  e.g. <code>-nodisplay</code>, this differs from the GNU convention of using <code>--</code>  for command line options of more than one character.</p> <p>Tip</p> <p>Using the prefix <code>!</code> will allow you to run bash commands from within  MATLAB. e.g. <code>!squeue -u $USER</code> will print your currently queued slurm  jobs.</p>","tags":["engineering","ml"]},{"location":"Scientific_Computing/Supported_Applications/MATLAB/#parallelism","title":"Parallelism","text":"<p>MATLAB does not support MPI therefore <code>#SBATCH --ntasks</code> should always be 1, but if given the necessary resources some MATLAB functions can make use of multiple threads (<code>--cpus-per-task</code>) or GPUs (<code>--gpus-per-node</code>).</p>","tags":["engineering","ml"]},{"location":"Scientific_Computing/Supported_Applications/MATLAB/#implicit-parallelism","title":"Implicit parallelism","text":"<p>Implicit parallelism requires no changes to be made in your code. By default MATLAB will utilise multi-threading for a wide range of operations, scalability will vary but generally you will not be able to utilise more than a 4-8 CPUs this way.</p>","tags":["engineering","ml"]},{"location":"Scientific_Computing/Supported_Applications/MATLAB/#explicit-parallelism","title":"Explicit parallelism","text":"<p>Tip</p> <p>If your code is explicitly parallel at a high level it is preferable to use  SLURM job arrays  as there is less computational overhead and the multiple smaller jobs  will queue faster and therefore improve your throughput.</p> <p>Explicit\u00a0parallelism is when you write your code specifically to make use of multiple CPU's. This can be done using MATLABs\u00a0parpool-based language constructs, MATLAB assigns each thread a 'worker' that can be assigned sections of code.</p> <p>MATLAB will make temporary files under your home directory (in <code>~/.matlab/local/&lt;cluster&gt;/jobs</code>) for communication with worker processes. To prevent simultaneous parallel MATLAB jobs from interfering with each other you should tell them to each use their own job-specific local directories:</p> <pre><code>pc = parcluster('local')\npc.JobStorageLocation = getenv('TMPDIR')\nparpool(pc, str2num(getenv('SLURM_CPUS_PER_TASK')))\n</code></pre> <p>Tip</p> <p>Parpool will throw a warning when started due to a difference in how  time zone is specified. To fix this, add the following line to your  SLURM script: <code>export TZ=\"Pacific/Auckland'</code></p> <p>The main ways to make use of parpool are;</p>","tags":["engineering","ml"]},{"location":"Scientific_Computing/Supported_Applications/MATLAB/#parfor","title":"parfor","text":"<p>Executes each iteration of a loop on a different worker. e.g.</p> <pre><code>parfor i=1:100\n\n   %Your operation here.\n\nend\n</code></pre> <p><code>parfor</code> operates similarly to a SLURM job array and must be embarrassingly parallel. Therefore all variables either need to be defined locally (used internally within one iteration of the loop), or static (not changing during execution of loop).</p> <p>More info here.</p>","tags":["engineering","ml"]},{"location":"Scientific_Computing/Supported_Applications/MATLAB/#parfeval","title":"parfeval","text":"<p><code>parfeval</code>\u00a0is used to assign a particular function to a thread, allowing it to be run asynchronously. e.g.</p> <pre><code>my_coroutine=parfeval(@my_async_function,2,in1,in2);\n\n% Do something that doesn't require outputs from 'my_async_function'\n\n[out1, out2]=fetchOutputs(my_coroutine); % If 'my_coroutine' has not finished execution will pause.\n\nfunction\u00a0[out1,out2]=my_async_function(in1,in2)\n\n%Your operation here.\n\nend\n</code></pre> <p><code>fetchOutputs</code>\u00a0is used to retrieve the values.</p> <p>More info here.</p> <p>Tip</p> <p>When killed (cancelled, timeout, etc), job steps utilising parpool may  show state <code>OUT_OF_MEMORY</code>, this is a quirk of how the steps are ended  and not necessarily cause to raise total memory requested.</p> <p>Determining which of these\u00a0categories your variables fall under is a good place to start when attempting to parallelise your code.</p>","tags":["engineering","ml"]},{"location":"Scientific_Computing/Supported_Applications/MATLAB/#using-gpus","title":"Using GPUs","text":"<p>As with standard parallelism, some MATLAB functions will work implicitly on GPUs while other require setup. More info on using GPUs with MATLAB here, and writing code for GPUs here.</p> <p>MATLAB uses NVIDIA CUDA toolkit. Depending on the version of MATLAB, a different version of CUDA is needed, see GPU Support by Release in MATLAB documentation. Use <code>module spider CUDA</code> to list all available CUDA modules and select the appropriate one. For example, for MATLAB R2021a, use <code>module load CUDA/11.0.2</code> before launching MATLAB.</p> <p>If you want to know more about how to access the different type of available GPUs on NeSI, check the GPU use on NeSI support page.</p> <p>Support for A100 GPUs</p> <p>To use MATLAB with a A100 or a A100-1g.5gb GPU, you need to use a  version of MATLAB supporting the Ampere architecture (see GPU  Support by  Release).  We recommend that you use R2021a or a more recent version.</p> <p>GPU cost</p> <p>A GPU device-hour costs more than a core-hour, depending on the type  of GPU. You can find a comparison table in our What is an  allocation?  support page.</p>","tags":["engineering","ml"]},{"location":"Scientific_Computing/Supported_Applications/MATLAB/#gpu-example","title":"GPU Example","text":"<pre><code>#!/bin/bash -e\n#SBATCH --job-name       MATLAB_GPU    # Name to appear in squeue\n#SBATCH --time           01:00:00      # Max walltime\n#SBATCH --mem            10G           # 10G per GPU\n#SBATCH --cpus-per-task  4             # 4 CPUs per GPU\n#SBATCH --output         %x.%j.log     # Location of output log\n#SBATCH --gpus-per-node  1             # Number of GPUs to use (max 2)\n\nmodule load MATLAB/2021a\nmodule load CUDA/11.0.2  # Drivers for using GPU\n\nmatlab -batch \"gpuDevice()\"\n</code></pre>","tags":["engineering","ml"]},{"location":"Scientific_Computing/Supported_Applications/MATLAB/#adding-support-packages","title":"Adding Support Packages","text":"<p>If you have X-11 set up you can install additional package through the GUI. You can also install manually if you already have the files by copying them into your Support Package root directory..</p> <p>Support packages are usually saved in your home directory, you can see the path using the MATLAB command <code>matlabshared.supportpkg.getSupportPackageRoot</code> if it is unset, you can specify it with <code>matlabshared.supportpkg.setSupportPackageRoot(\"&lt;path&gt;\")</code></p>","tags":["engineering","ml"]},{"location":"Scientific_Computing/Supported_Applications/MATLAB/#mexopencv","title":"mexopencv","text":"<p>mexopencv is mex wrapper MATLAB wrapper for the openCV library.</p> <p>Some of the internal MATLAB libraries clash with those used by OpenCV, to avoid problems cause by this</p> <ul> <li>Use a version of OpenCV built using GCC (as opposed to gimkl).</li> <li>Compile using the -DWITH_OPENCL=OFF flag.</li> </ul> <p>Please contact support if you have any issues.</p>","tags":["engineering","ml"]},{"location":"Scientific_Computing/Supported_Applications/MATLAB/#improving-performance-with-mexing","title":"Improving performance with mexing","text":"<p>Like other scripting languages, MATLAB code will generally run slower than compiled code since every MATLAB instruction needs to be parsed and interpreted. Instructions inside large MATLAB loops are often performance hotspots due to the interpreter's overhead, which consumes CPU time at every iteration.</p> <p>Fortunately MATLAB lets programmers extend their scripts with C/C++ or Fortran, which is referred to as mexing.</p> <p>more info about compiling software on NeSI here.</p>","tags":["engineering","ml"]},{"location":"Scientific_Computing/Supported_Applications/MATLAB/#writing-mex-functions","title":"Writing mex functions","text":"<p>This involves the following steps (using C++ as an example):</p> <ol> <li>Focus on a loop to extend, preferably a nested set of loops.</li> <li>Identify the input and output variables of the section of code to     extend.</li> <li>Write C++ code. The name of the C++ file should match the name of     the function to call from MATLAB, e.g.\u00a0<code>myFunction.cpp</code> for a     function named <code>myFunction</code>.</li> <li>Compile the extension using the MATLAB command\u00a0<code>mex myFunction.cpp</code></li> </ol> <p>At the minimum, the C++ extension should contain:</p> <pre><code>#include &lt;mex.h&gt;\n#include &lt;matrix.h&gt;\n\nvoid mexFunction(int nlhs, mxArray *plhs[],\n                 int nrhs, const mxArray *prhs[]) {\n    // implementation goes here\n}\n</code></pre> <p>Note that the above function should always be called\u00a0<code>mexFunction</code> and its signature be <code>int nlhs, mxArray *plhs[], int nrhs, const mxArray *prhs[]</code>. Here,\u00a0<code>nlhs</code> and <code>nrhs</code> refer to the number of output and input arguments respectively. Access each dummy argument with index <code>i = 0 ... nlhs-1</code> and <code>j = 0 ... nrhs-1</code> respectively. Regardless of the type of argument, whether it is a number, a matrix or an object, its type is\u00a0<code>mxArray</code>. Often you will need to cast the argument into a corresponding C++ type, e.g.</p> <pre><code>// cast as a double, note the asterisk in front of mxGetPr\ndouble x = (double) *mxGetPr(prhs[0]);\n</code></pre> <p>or</p> <pre><code>// cast as an array of doubles\ndouble* arr = (double*) mxGetPr(prhs[0]);\n</code></pre> <p>Use\u00a0<code>mxCreateDoubleMatrix</code> and <code>mxCreateDoubleScalar</code> to create a matrix and a number, respectively. For example:</p> <pre><code>// function returns [plhs[0], plhs[1]]\nplhs[0] = mxCreateDoubleMatrix(3, 2, mxREAL);  // 3 by 2 matrix\nplhs[1] = mxCreateDoubleScalar(2);  // number\n</code></pre> <p>All numbers are doubles. Use flat array indexing\u00a0<code>a[i + n*j - 1]</code> in C++ to access elements of a MATLAB matrix <code>a(i, j)</code> of size <code>n x m</code>.</p> <p>MATLAB will take care of destroying matrices and other object so you should feel free to create objects inside C++ code (required for functions that have return values).</p> <p>Some mex function source code examples can be found in the table here.</p>","tags":["engineering","ml"]},{"location":"Scientific_Computing/Supported_Applications/MATLAB/#compilation","title":"Compilation","text":"<p>MATLAB supports the following compilers.</p> C++ up to GCC 6.3.x C up to GCC 6.3.x FORTRAN up to GNU gfortran 6.3.x <p>The most up to date compilers supported by MATLAB can be loaded on NeSI using <code>module load gimkl/2017a</code></p> <p>If no\u00a0GCC module is loaded, the default system version of these compilers will be used.</p> <p>Further configuration can be done within MATLAB using the command <code>mex -setup</code></p> <p><code>mex &lt;file_name&gt;</code>\u00a0 will then compile the mex function.</p> <p>Default compiler flags can be overwritten with by setting the appropriate environment variables. The COMPFLAGS variable is ignored as it is Windows specific.</p> C++ <code>CXXFLAGS</code> C <code>CFLAGS</code> FORTRAN <code>FFLAGS</code> Linker <code>LDFLAGS</code> <p>For example, adding OpenMP flags for a fortran compile:</p> <p>Compiler Version</p> <p>Using an 'unsupported' compiler with versions of MATLAB 2020b onward  will result in an Error (previously was a 'Warning').</p>","tags":["engineering","ml"]},{"location":"Scientific_Computing/Supported_Applications/MATLAB/#known-bugs","title":"Known Bugs","text":"<p>When using versions of MATLAB more recent than 2021a you may notice the following warning.</p> <pre><code>ldd\n</code></pre> <p>This is due to our operating system being too old.</p> <p>With the exception of a few commands that directly make requests of the OS, your code should run fine.</p> <p>If you believe this bug is causing problems, running on our newer hardware will fix it.</p>","tags":["engineering","ml"]},{"location":"Scientific_Computing/Supported_Applications/Miniconda3/","title":"Miniconda3","text":"<p>A platform for Python-based data analytics</p> <p>Miniconda3 Homepage</p>","tags":[]},{"location":"Scientific_Computing/Supported_Applications/Miniconda3/#available-modules","title":"Available Modules","text":"Mahuika Maui Maui_ancil <p> 4.7.10 4.8.2 4.8.3 4.9.2 4.10.3 4.12.0 22.11.1-1 23.10.0-1 </p> <pre><code>module load Miniconda3/23.10.0-1</code></pre> <p> 22.11.1-1 </p> <pre><code>module load Miniconda3/22.11.1-1</code></pre> <p> 4.12.0 22.11.1-1 23.10.0-1 </p> <pre><code>module load Miniconda3/23.10.0-1</code></pre> <p>The <code>Miniconda3</code> environment module provides the Conda package and environment manager. Conda lets you install packages and their dependencies in dedicated environment, giving you more freedom to install software yourself at the expense of possibly less optimized packages and no curation by the NeSI team.</p> <p>Alternatives</p> <ul> <li>If you want a more reproducible and isolated environment, we      recommend using the Singularity      containers.</li> <li>If you only need access to Python and standard numerical libraries      (numpy, scipy, matplotlib, etc.), you can use the Python      environment      module.</li> </ul> <p>M\u0101ui Ancillary Nodes</p> <p>On M\u0101ui Ancillary Nodes, you can also use the <code>Anaconda3</code> module,  which provides a default environment pre-installed with a set of  numerical libraries (numpy, scipy, matplotlib, etc.).</p>","tags":[]},{"location":"Scientific_Computing/Supported_Applications/Miniconda3/#module-loading-and-conda-environments-isolation","title":"Module loading and conda environments isolation","text":"<p>When using the Miniconda3 module, we recommend using the following snippet to ensure that your conda environments can be activated and are isolated as possible from the rest of the system:</p> <pre><code>module purge &amp;&amp; module load Miniconda3\nsource $(conda info --base)/etc/profile.d/conda.sh\nexport PYTHONNOUSERSITE=1\n</code></pre> <p>Here are the explanations for each line of this snippet:</p> <ul> <li><code>module purge &amp;&amp; module load Miniconda3</code> ensures that no other     environment module can affect your conda environments. In     particular, the Python environment module change the <code>PYTHONPATH</code>     variable, breaking the isolation of the conda environments. If you     need other environment modules, make sure to load them after this     line.</li> <li><code>source $(conda info --base)/etc/profile.d/conda.sh</code> ensures that     you can use the <code>conda activate</code> command.</li> <li><code>export PYTHONNOUSERSITE=1</code> makes sure that local packages installed     in your home folder <code>~/.local/lib/pythonX.Y/site-packages/</code> (where     <code>X.Y</code> is the Python version, e.g. 3.8) by <code>pip install --user</code> are     excluded from your conda environments.</li> </ul> <p>Warning</p> <p>We strongly recommend against using <code>conda init</code>. It inserts a  snippet in your <code>~/.bashrc</code> file that will freeze the version of conda  used, bypassing the environment module system.</p> <p>M\u0101ui Ancillary Nodes</p> <p>On M\u0101ui Ancillary Nodes, you need to (re)load the <code>NeSI</code> module after  using <code>module purge</code>:  <pre><code>module purge &amp;&amp; module load NeSI Miniconda3\nsource $(conda info --base)/etc/profile.d/conda.sh\nexport PYTHONNOUSERSITE=1\n</code></pre></p>","tags":[]},{"location":"Scientific_Computing/Supported_Applications/Miniconda3/#prevent-conda-from-using-home-storage","title":"Prevent conda from using /home storage","text":"<p>Conda environments and the conda packages cache can take a lot of storage space. By default, Conda use /home storage, which is restricted to 20GB on NeSI. Here are some techniques to avoid running out of space when using Conda.</p> <p>First, we recommend that you move the cache folder used for downloaded packages on the <code>nobackup</code> folder of your project:</p> <pre><code>conda config --add pkgs_dirs /nesi/nobackup/&lt;project_code&gt;/$USER/conda_pkgs\n</code></pre> <p>where <code>&lt;project_code&gt;</code> should be replace with your project code. This setting is saved in your <code>~/.condarc</code> configuration file.</p> <p>Prerequisite</p> <p>Your package cache will be subject to the nobackup autodelete process  (details available in the Nobackup  autodelete  support page). The package cache folder is for temporary storage so it  is safe if files within the cache folder are removed.</p> <p>Next, we recommend using the <code>-p</code> or <code>--prefix</code> options when creating new conda environments, instead of <code>-n</code> or <code>--name</code> options. Using <code>-p</code> or <code>--prefix</code>, you can specify the conda environment folder location, ideally in your project folder. For example:</p> <pre><code>conda create --prefix /nesi/project/&lt;project_code&gt;/my_conda_env python=3.8\n</code></pre> <p>Then use the path of the conda environment to activate it:</p> <pre><code>conda activate /nesi/project/&lt;project_code&gt;/my_conda_env\n</code></pre> <p>Note that <code>-p</code> and <code>--prefix</code> options can also be used when creating an environment from an <code>environment.yml</code> file:</p> <pre><code>conda env create -f environment.yml -p /nesi/project/&lt;project_code&gt;/my_conda_env\n</code></pre> <p>Reduce prompt prefix</p> <p>By default, when activating a conda environment created with <code>-p</code> or  <code>--prefix</code>, the entire path of the environment is be added to the  prompt. To remove this long prefix in your shell prompt, use the  following configuration:  <pre><code>conda config --set env_prompt '({name})'\n</code></pre></p>","tags":[]},{"location":"Scientific_Computing/Supported_Applications/Miniconda3/#faster-solver-mamba-experimental-feature","title":"Faster solver <code>mamba</code> (experimental feature)","text":"<p>If you are using the module <code>Miniconda3/</code><code>22.11.1-1</code>, you can accelerate conda environments creation and package installation using the new <code>libmamba</code> solver. To use it, append the option <code>--solver=libmamba</code> to your command.</p> <p>For example, to create an environment from an <code>environment.yml</code> file, use:</p> <pre><code>conda env create --solver=libmamba -f environment.yml -p venv\n</code></pre> <p>or to install a package in an activate environment, use:</p> <pre><code>conda install --solver=libmamba CONDA_PACKAGE\n</code></pre> <p>where <code>CONDA_PACKAGE</code> is the package of interest.</p>","tags":[]},{"location":"Scientific_Computing/Supported_Applications/ORCA/","title":"ORCA","text":"<p>ORCA is a flexible, efficient and easy-to-use general purpose tool for quantum chemistry</p> <p>ORCA Homepage</p>","tags":["mahuika","chemistry"]},{"location":"Scientific_Computing/Supported_Applications/ORCA/#available-modules","title":"Available Modules","text":"Mahuika <p> 4.0.1-OpenMPI-2.0.2 4.2.1-OpenMPI-3.1.4 5.0.1-OpenMPI-4.1.1 5.0.3-OpenMPI-4.1.1 5.0.4-OpenMPI-4.1.5 </p> <pre><code>module load ORCA/5.0.4-OpenMPI-4.1.5</code></pre> <p>ORCA is a flexible, efficient and easy-to-use general purpose tool for quantum chemistry with specific emphasis on spectroscopic properties of open-shell molecules. It features a wide variety of standard quantum chemical methods ranging from semiempirical methods to DFT to single- and multireference correlated ab initio methods. It can also treat environmental and relativistic effects.</p> <p>The ORCA home page is at\u00a0https://orcaforum.kofo.mpg.de</p>","tags":["mahuika","chemistry"]},{"location":"Scientific_Computing/Supported_Applications/ORCA/#licensing-requirements","title":"Licensing requirements","text":"<p>ORCA is released as precompiled binaries at no cost, pursuant to a closed-source licence. \u00a0Users are advised that the terms of the ORCA licence allow its use in the course of academic research only, and that each research group is expected to register with the ORCA developers. If you have any questions regarding your eligibility to access ORCA or any particular version of it, please  Contact our Support Team.</p>","tags":["mahuika","chemistry"]},{"location":"Scientific_Computing/Supported_Applications/ORCA/#example-script","title":"Example script","text":"<pre><code>#!/bin/bash -e\n#SBATCH --job-name      ORCA_job\n#SBATCH --time          01:00:00\n#SBATCH --ntasks        16  # ORCA can be inefficient with ntasks &gt; 16\n#SBATCH --mem-per-cpu   1G\n\nmodule load ORCA/5.0.4-OpenMPI-4.1.5\n\n# ORCA under MPI requires that it be called via its full absolute path\norca_exe=$(which orca)\n\n# Don't use \"srun\" as ORCA does that itself when launching its MPI process.\n${orca_exe} MyInput.inp\n</code></pre>","tags":["mahuika","chemistry"]},{"location":"Scientific_Computing/Supported_Applications/ORCA/#further-notes","title":"Further notes","text":"","tags":["mahuika","chemistry"]},{"location":"Scientific_Computing/Supported_Applications/ORCA/#requesting-a-parallel-run","title":"Requesting a parallel run","text":"<p>ORCA requires a parallel run to be requested in its input as well as from the batch scheduler. To request a parallel run, you need to add a line to the input file like the following:</p> <pre><code>%pal nprocs &lt;np&gt; end\n</code></pre> <p>where <code>&lt;np&gt;</code> represents the total number of processors (cores) you have requested from the scheduler.</p>","tags":["mahuika","chemistry"]},{"location":"Scientific_Computing/Supported_Applications/ORCA/#checkpointing-and-restarting","title":"Checkpointing and restarting","text":"<p>ORCA provides for saving of checkpoint data, especially molecular orbital information, in a file with extension \".gbw\" (short for Geometry-Basis-Wavefunction). Given an input file name of \"foo.inp\" or some equivalent, the GBW file will be named \"foo.gbw\". The GBW file, like temporary and other output files, will be written in the same directory from which the ORCA executable is invoked.</p> <p>To restart from an existing GBW file, you should do the following:</p> <ol> <li>Ensure that the GBW file you want to start from is renamed so that     it does not have the same base name as your intended input file.     Otherwise, it will be overwritten and destroyed as soon as ORCA     starts running.</li> <li> <p>In your input file, specify the following lines, replacing     \"checkpoint.gbw\" with the name of the GBW file you intend to read     from:</p> <pre><code>! moread\n% moinp \"checkpoint.gbw\"\n</code></pre> </li> <li> <p>Run the calculation.</p> </li> </ol> <p>For more information about restarting from an older GBW file, including how to restart from GBW files produced using earlier versions of ORCA, please consult the ORCA manual.</p>","tags":["mahuika","chemistry"]},{"location":"Scientific_Computing/Supported_Applications/OpenFOAM/","title":"OpenFOAM","text":"<p>OpenFOAM is a free, open source CFD software package.</p> <p>OpenFOAM Homepage</p>","tags":["engineering","cfd","cae","fea"]},{"location":"Scientific_Computing/Supported_Applications/OpenFOAM/#available-modules","title":"Available Modules","text":"Mahuika <p> v1706-gimkl-2018b v1712-gimkl-2017a v1812-gimkl-2018b-libAcoustics v1812-gimkl-2018b v1906-gimkl-2018b v2106-gimkl-2020a v2212-gimkl-2022a-noParaview 2.4.0-gimkl-2018b-debug 2.4.0-gimkl-2018b 5.0-gimkl-2018b 6-gimkl-2018b 8-gimkl-2020a 10-gimkl-2022a </p> <pre><code>module load OpenFOAM/10-gimkl-2022a</code></pre> <p>OpenFOAM (Open Field Operation And Manipulation) is a open-source C++ toolbox maintained by the OpenFOAM foundation and ESI Group. Although primarily used for CFD (Computational Fluid Dynamics) OpenFOAM can be used in a wide range of fields from solid mechanics to chemistry.  </p> <p>The lack of licence limitations and native parallelisation makes OpenFOAM well suited for a HPC environment. OpenFOAM is an incredibly powerful tool, but does require a moderate degree of computer literacy to use effectively.</p> <p>OpenFOAM can be loaded using;</p> <pre><code>module load OpenFOAM\nsource $FOAM_BASH\n</code></pre>","tags":["engineering","cfd","cae","fea"]},{"location":"Scientific_Computing/Supported_Applications/OpenFOAM/#example-script","title":"Example Script","text":"<pre><code>#!/bin/bash -e\n\n#SBATCH --time              04:00:00\n#SBATCH --job-name          OF_16CORES\n#SBATCH --output            %x.output   #set output to job name\n#SBATCH --ntasks            16\n#SBATCH --mem-per-cpu               512MB\n\n#Working directory always needs to contain 'system', 'constant', and '0'\n\nmodule load OpenFOAM/10-gimkl-2022a\nsource ${FOAM_BASH}\n\ndecomposePar                   #Break domain into pieces for parallel execution.\nsrun simpleFoam -parallel       \nreconstructPar -latestTime     #Collect \n</code></pre>","tags":["engineering","cfd","cae","fea"]},{"location":"Scientific_Computing/Supported_Applications/OpenFOAM/#filesystem-limitations","title":"Filesystem Limitations","text":"<p>OpenFOAM generates a large number of files during run-time. In addition to the I/O load there is also the danger of using up available inodes.</p> <p>Filesystems in excess of their allocation will cause any job trying to write there to crash.</p> <p>There are a few ways to mitigate this</p> <ul> <li> <p>Use <code>/nesi/nobackup</code>     The nobackup directory has a significantly higher inode count and no     disk space limits.</p> </li> <li> <p>ControlDict Settings </p> </li> <li><code>WriteInterval</code>         Using a high write interval reduce number of output files and         I/O load.</li> <li> <p><code>deltaT</code>         Consider carefully an appropriate\u00a0time-step, use\u00a0adjustTimeStep         if suitable.</p> <ul> <li><code>purgeWrite</code>     Not applicable for many jobs, this keeps only the last n steps,     e.g. purgeWrite 5 will keep the last 5 time-steps, with the     directories being constantly\u00a0overwritten.</li> <li><code>runTimeModifiable</code>     When true, dictionaries will be re-read at the start of every     time step. Setting this to false will decrease I/O load.</li> <li><code>writeFormat</code>     Setting this to binary as opposed to ascii will decrease disk     use and I/O load.</li> </ul> </li> <li> <p>Monitor Filesystem     The command <code>nn_storage_quota</code> should be used to track filesystem     usage. There is a delay between making changes to a filesystem and     seeing it on <code>nn_storage_quota</code>.</p> <pre><code>Filesystem         Available      Used     Use%     Inodes     IUsed     IUse%\nhome_cwal219             20G    1.957G    9.79%      92160     21052    22.84%\nproject_nesi99999         2T      798G   38.96%     100000     66951    66.95%\nnobackup_nesi99999              6.833T            10000000    2691383   26.91%\n</code></pre> </li> <li> <p>Contact Support     If you are following the recommendations here yet are still     concerned about indoes, open a support ticket and we can raise the     limit for you.</p> </li> </ul>","tags":["engineering","cfd","cae","fea"]},{"location":"Scientific_Computing/Supported_Applications/OpenFOAM/#environment-variables","title":"Environment Variables","text":"<p>You may find it useful to use environment variables in your dictionaries e.g.</p> <pre><code>numberOfSubdomains ${SLURM_NTASKS};\n</code></pre> <p>Or create your variables\u00a0to be set in your Slurm script.</p> <pre><code>startFrom ${START_TIME};\n</code></pre> <p>This is essential when running parameter sweeps.</p> <p>You can also directly edit your dictionaries with <code>sed</code>,Concordia.ah</p> <pre><code>NSUBDOMAINS=10\nsed -i \"s/\\(numberOfSubdomains \\)[[:digit:]]*\\(;\\)/\\1 $NSUBDOMAINS\\2/g\" system/controlDict\n</code></pre>","tags":["engineering","cfd","cae","fea"]},{"location":"Scientific_Computing/Supported_Applications/OpenFOAM/#recommended-resources","title":"Recommended Resources","text":"<p>Generally, using 16 or less tasks will keep your job reasonably efficient. However this is highly dependant on the type of simulation and how the model was decomposed.</p>","tags":["engineering","cfd","cae","fea"]},{"location":"Scientific_Computing/Supported_Applications/OpenFOAM/#adding-custom-solvers","title":"Adding custom solvers","text":"<p>Rather than installing custom solvers centrally, we encourage users to install under their user or project.</p>","tags":["engineering","cfd","cae","fea"]},{"location":"Scientific_Computing/Supported_Applications/OpenFOAM/#downloading","title":"Downloading","text":"<p>Generally your custom solver will be stored in a git repo. Make sure you have the same version as the OpenFOAM you plan to use, this may require changing branch.</p> <p></p>","tags":["engineering","cfd","cae","fea"]},{"location":"Scientific_Computing/Supported_Applications/OpenFOAM/#if-releases-are-available","title":"If releases are available","text":"<p>Open the 'releases' tab, right click on the '.tar' or '.zip' of the version you want and select 'copy link address', then paste that link into your terminal after <code>wget</code>. For example:</p> <pre><code>wget https://github.com/vincentcasseau/hyStrath/archive/Concordia.tar.gz\n</code></pre> <p><code>wget</code> can also be used to fetch files from other sources.</p>","tags":["engineering","cfd","cae","fea"]},{"location":"Scientific_Computing/Supported_Applications/OpenFOAM/#if-repo-only","title":"If repo only","text":"<p>Use the command <code>git clone &lt;path to repo&gt;.git</code> For example:</p> <pre><code>git clone https://github.com/vincentcasseau/hyStrath.git\n</code></pre>","tags":["engineering","cfd","cae","fea"]},{"location":"Scientific_Computing/Supported_Applications/OpenFOAM/#decompress","title":"Decompress","text":"<p>If your source is a .zip file use the command <code>unzip &lt;filename&gt;</code> if it is a .tar.gz use the command <code>tar\u00a0-xvzf &lt;filename&gt;</code></p> <p>From here steps will vary, it is best to check the README of the package you are installing.</p>","tags":["engineering","cfd","cae","fea"]},{"location":"Scientific_Computing/Supported_Applications/OpenFOAM/#wmake","title":"wmake","text":"<p><code>wmake</code> is an OpenFOAM tool used to compile code, based on <code>make</code>.</p> <p>A more comprehensive description of the use of wmake can be found\u00a0here.</p> <p>A library/application named 'newApp' would have the structure.</p> <p>To build `newApp` one would run:</p> <pre><code>module load OpenFOAM\nsource $FOAM_BASH\nwmake\n</code></pre> <p>However, most applications will involve building multiple libraries and solvers, and will generally come with a shell script (<code>.sh</code>) that saves the user having to compile each item manually.</p> <p>Some apps will try to place the new libraries in <code>$FOAM_LIBBIN</code> and objects in <code>$FOAM_APPBIN</code>, this will cause the build to fail as you will not have permission to write there. Make sure the <code>Make/options</code>\u00a0file specifies variables\u00a0<code>$FOAM_USER_LIBBIN</code> or\u00a0<code>$FOAM_USER_APPBIN</code>\u00a0instead.</p>","tags":["engineering","cfd","cae","fea"]},{"location":"Scientific_Computing/Supported_Applications/OpenFOAM/#location","title":"Location","text":"<p>User compiled libraries are kept in\u00a0<code>$FOAM_USER_LIBBIN</code>, by default this is set to\u00a0<code>~/$USER/OpenFOAM/$USER-&lt;version&gt;/platforms/linux64GccDPInt32Opt/lib</code></p> <p>User compiled objects are kept in\u00a0<code>$FOAM_USER_APPBIN</code>, by default this is set to\u00a0<code>~/$USER/OpenFOAM/$USER-&lt;version&gt;/platforms/linux64GccDPInt32Opt/bin</code></p> <p>You will need to change these locations if you want the rest of your project members to have access.</p> <p>For example</p> <pre><code>module load OpenFOAM\n\nsource $FOAM_BASH\n\nexport FOAM_USER_LIBBIN=/nesi/project/nesi99999/custom_OF/lib\nexport FOAM_USER_APPBIN=/nesi/project/nesi99999/custom_OF/bin\n</code></pre> <p>These variables need to be set to the same chosen paths before compiling and before running the solvers.</p> <p>Warning</p> <p>Make sure to <code>export</code> your custom paths before\u00a0<code>source $FOAM_BASH</code>  else they will be reset to default.</p>","tags":["engineering","cfd","cae","fea"]},{"location":"Scientific_Computing/Supported_Applications/OpenSees/","title":"OpenSees","text":"<p>OpenSees is a software framework for developing applications to simulate the performance of structural and geotechnical systems subjected to earthquakes.</p> <p>OpenSees Homepage</p>","tags":["geo","earthquake"]},{"location":"Scientific_Computing/Supported_Applications/OpenSees/#available-modules","title":"Available Modules","text":"Mahuika <p> 3.0.0-gimkl-2017a 3.2.0-gimkl-2020a-METIS4 3.2.0-gimkl-2020a 20220411-gimkl-2020a </p> <pre><code>module load OpenSees/20220411-gimkl-2020a</code></pre> <p>There are three commands with which a OpenSees job can be launched.</p> <ul> <li><code>OpenSees</code>: For running a job in serial (single CPU).</li> <li><code>OpenSeesSP</code>: Intended for the single analysis of very large models.</li> <li><code>OpenSeesMP</code>: For advanced\u00a0parametric studies.</li> </ul> <p>More info can be found about running OpenSees in parallel here.</p> SerialJob <p>Single process with a single thread. Usually submitted as part of an array, as in the case of parameter sweeps.</p> <pre><code>```sl\n#!/bin/bash -e\n\n#SBATCH --job-name      OpenSees-Serial\n#SBATCH --time          00:05:00          # Walltime&lt;/span&gt;&lt;/span&gt;\n#SBATCH --cpus-per-task 1                 # Double if hyperthreading enabled.\n#SBATCH --mem           512MB             # total mem\n#SBATCH --hint          nomultithread     # Hyperthreading disabled\n\nmodule load OpenSees/20220411-gimkl-2020a\nOpenSees \"frame.tcl\"\n</code></pre>","tags":["geo","earthquake"]},{"location":"Scientific_Computing/Supported_Applications/OpenSees/#input-from-shell","title":"Input from Shell","text":"<p>Information can be passed from the bash shell to a Tcl script by use of environment variables.</p> <p>Set in Slurm script:</p> <pre><code>export MY_VARIABLE=\"Hello World!\"\n</code></pre> <p>Retrieved in Tcl script:</p> <pre><code>puts $::env(MY_VARIABLE)\n</code></pre>","tags":["geo","earthquake"]},{"location":"Scientific_Computing/Supported_Applications/ParaView/","title":"ParaView","text":"<p>ParaView is a scientific parallel visualizer.  This version supports CPU-only rendering without X context using the OSMesa library, it does not support GPU rendering, and it does not provide a GUI.  Use the GALLIUM_DRIVER environment variable to choose a software renderer, it is recommended to use  GALLIUM_DRIVER=swr  for best performance.  Ray tracing using the OSPRay library is also supported.</p> <p>ParaView Homepage</p>","tags":["visualisation"]},{"location":"Scientific_Computing/Supported_Applications/ParaView/#available-modules","title":"Available Modules","text":"Mahuika Maui_ancil <p> 5.3.0-gimkl-2017a 5.4.1-gimkl-2018b-Python-2.7.16 5.4.1-gimpi-2018b 5.6.0-gimkl-2018b-Python-3.7.3 5.6.0-gimpi-2018b 5.9.0-gimkl-2020a-Python-3.8.2 </p> <pre><code>module load ParaView/5.9.0-gimkl-2020a-Python-3.8.2</code></pre> <p> 5.5.2-gimpi-2018a-Server-EGL 5.5.2-gimpi-2018b-GUI-Mesa 5.5.2-gimpi-2018b-Server-OSMesa </p> <pre><code>module load ParaView/5.5.2-gimpi-2018b-Server-OSMesa</code></pre> <p>Warning</p> <p>The ParaView server loaded must be the same version as the client you  have installed locally.</p>","tags":["visualisation"]},{"location":"Scientific_Computing/Supported_Applications/ParaView/#setting-up-client-server-mode","title":"Setting up Client-Server Mode","text":"<p>If you want to use ParaView in client-server mode, use the following setup:</p> <ul> <li> <p>Load one of the ParaView Server modules listed above and launch the     server in your interactive visualisation session on the HPC using;</p> <pre><code>module load ParaView\n</code></pre> </li> <li> <p>To start the ParaView server run;  </p> <pre><code>pvserver\n</code></pre> </li> <li> <p>You should see;</p> <pre><code>Waiting for client...\nConnection URL: cs://mahuika02:11111\nAccepting connection(s): mahuika02:11111\n</code></pre> </li> <li> <p>Create an SSH tunnel for port \"11111\" from your local machine to the     cluster. e.g.</p> <pre><code>ssh mahuika -L 11111:mahuika02:11111\n</code></pre> <p>Make sure the host name and socket match those given by the server earlier!</p> </li> <li> <p>Launch the ParaView GUI on your local machine and go to \"File &gt;     Connect\" or click     the\u00a0\u00a0button.</p> </li> <li> <p>Click on \"Add Server\", choose server type \"Client / Server\", host     \"localhost\" (as we will be using the SSH tunnel), and port \"11111\",     then click on \"Configure\" .</p> </li> <li> <p></p> </li> <li> <p>Select the new server and click on \"Connect\"</p> </li> </ul>","tags":["visualisation"]},{"location":"Scientific_Computing/Supported_Applications/ParaView/#parallelisation","title":"Parallelisation","text":"<p>The CPU based versions of ParaView use the OpenSWR rasteriser as well as the OSPRay ray tracer for rendering graphics. These support parallel operation for better performance, but are configured to only use a single core by default. Run the following commands before launching ParaView GUI or ParaView Server if you want to use more cores (depending on the number of cores available in your session):</p> <pre><code>export KNOB_MAX_WORKER_THREADS=&lt;number of cores&gt;\nexport OSPRAY_THREADS=&lt;number of cores&gt;\n</code></pre> <p>ParaView Server also supports parallel execution using MPI.</p>","tags":["visualisation"]},{"location":"Scientific_Computing/Supported_Applications/Python/","title":"Python","text":"<p>Python is a programming language that lets you work more quickly and integrate your systems more effectively.</p> <p>Python Homepage</p>","tags":["ml","language"]},{"location":"Scientific_Computing/Supported_Applications/Python/#available-modules","title":"Available Modules","text":"Mahuika Maui Maui_ancil <p> 2.7.14-gimkl-2017a 2.7.16-gimkl-2018b 2.7.16-intel-2018b 2.7.18-gimkl-2020a 3.6.3-gimkl-2017a 3.7.3-gimkl-2018b 3.8.1-gimkl-2018b 3.8.2-gimkl-2020a 3.9.5-gimkl-2020a 3.9.9-gimkl-2020a 3.10.5-gimkl-2022a 3.11.3-gimkl-2022a 3.11.6-foss-2023a </p> <pre><code>module load Python/3.11.6-foss-2023a</code></pre> <p> 3.8.2-CrayGNU-23.02 3.8.2-CrayIntel-23.02-19 </p> <pre><code>module load Python/3.8.2-CrayIntel-23.02-19</code></pre> <p> 3.8.2 </p> <pre><code>module load Python/3.8.2</code></pre> <p>All versions of Python available on NeSI platforms are owned and licensed by the Python Software Foundation. Each version is released under a specific open-source licence. The licences are available on the Python documentation server.</p>","tags":["ml","language"]},{"location":"Scientific_Computing/Supported_Applications/Python/#system-python-vs-environment-modules","title":"System Python vs Environment Modules","text":"<p>Our operating systems include Python but not an up to date version, so we strongly recommend that you load one of our Python environment modules instead. \u00a0They include optimised builds of the most popular Python packages for computational work such as numpy, scipy, matplotlib, and many more.</p>","tags":["ml","language"]},{"location":"Scientific_Computing/Supported_Applications/Python/#nesi-customisations","title":"NeSI Customisations","text":"<p>Our most recent Python environment modules have:</p> <ul> <li> <p><code>multiprocessing.cpu_count()</code> patched to return only the number of     CPUs available to the process, which in a Slurm job can be fewer     than the number of CPUs on the node.</p> </li> <li> <p><code>PYTHONUSERBASE</code> set to a path which includes the toolchain, so that     incompatible builds of the same version of Python don't attempt to     share user-installed libraries.</p> </li> </ul>","tags":["ml","language"]},{"location":"Scientific_Computing/Supported_Applications/Python/#example-scripts","title":"Example scripts","text":"Serial JobDistributed Memory JobShared Memory Example <pre><code>#!/bin/bash -e\n\n#SBATCH --job-name    Python_Serial\n#SBATCH --time        01:00:00\n#SBATCH --mem         512MB\n\nmodule load Python/3.11.6-foss-2023a\n\npython MyPythonScript.py\n</code></pre> <pre><code>#!/bin/bash -e\n#SBATCH --job-name=PythonMPI\n#SBATCH --ntasks=2          # Number of MPI tasks\n#SBATCH --time=00:30:00\n#SBATCH --mem-per-cpu=512MB # Memory per logical CPU\n\nmodule load Python\nsrun python PythonMPI.py   # Executes ntasks copies of the script\n</code></pre> <pre><code>import numpy as np\nfrom mpi4py import MPI\n\ncomm = MPI.COMM_WORLD\nsize = comm.Get_size() # Total number of MPI tasks\nrank = comm.Get_rank() # Rank of this MPI task\n\n# Calculate the data (numbers 0-9) on the MPI ranks\nrank_data = np.arange(rank, 10, size)\n\n# perform some operation on the ranks data\nrank_data += 1\n\n# gather the data back to rank 0\ndata_gather = comm.gather(rank_data, root = 0)\n\n# on rank 0 sum the gathered data and print both the sum of, \n# and the unsummed data\nif rank == 0:\n    print('Gathered data:', data_gather)\n    print('Sum:', sum(data_gather))\n</code></pre> <p>The above Python script will create a list of numbers (0-9) split between the MPI tasks (ranks). Each task will then add one to the numbers it has, those numbers will then be gathered back to task 0, where the numbers will be summed and both the sum of, and the unsummed data is printed.</p> <pre><code>  #!/bin/bash -e\n  #SBATCH --job-name=PytonMultiprocessing\n  #SBATCH --cpus-per-task=2   # Number of logical CPUs\n  #SBATCH --time=00:10:00\n  #SBATCH --mem-per-cpu=512MB # Memory per logical CPU\n\n  module load Python\n  python PythonMultiprocessing.py\n</code></pre> <pre><code>import multiprocessing\n\ndef calc_square(numbers, result1):\n     for idx, n in enumerate(numbers):\n        result1[idx] = n*n\n\ndef calc_cube(numbers, result2):\n    for idx, n in enumerate(numbers):\n        result2[idx] = n*n*n\n\nif __name__ == \"__main__\":\n    numbers = [2,3,4]\n    # Sets up the shared memory variables, allowing the variables to be\n    # accessed globally across processes\n    result1 = multiprocessing.Array('i',3)\n    result2 = multiprocessing.Array('i',3)\n    # set up the processes\n    p1 = multiprocessing.Process(target=calc_square, args=(numbers,result1,))\n    p2 = multiprocessing.Process(target=calc_cube, args=(numbers,result2,))\n\n    # start the processes\n    p1.start()\n    p2.start()\n\n    # end the processes\n    p1.join()\n    p2.join()\n\n    print(result1[:])\n    print(result2[:])\n</code></pre> <p>The above Python script will calculated the square and cube of an array of numbers using multiprocessing and print the results from outside of those processes, safely circumventing Python's global interpreter lock.</p> <p>For more in depth examples of and descriptions of Multiprocessing in Python you may find [this Multithreading/Multiprocessing Youtube</p> <p>tutorial series](https://www.youtube.com/watch?v=PJ4t2U15ACo&amp;list=PLeo1K3hjS3uub3PRhdoCTY8BxMKSW7RjN&amp;index=1) helpful</p> Job Arrays <p>Job arrays can be handled using the Slurm environment variable <code>SLURM_ARRAY_TASK_ID</code> as array index. This index can be called directly from within the script or using a command line argument. In the following both options are presented:</p> <p>The job scripts calling both examples:</p> <pre><code>#!/bin/bash -e\n\n#SBATCH -J test\n#SBATCH --time=00:01:00\n#SBATCH --ntasks=1\n#SBATCH --cpus-per-task=1\n#SBATCH --array=1-2 # Array jobs\n\nmodule load Anaconda3\n\necho \"SLURM_ARRAY_TASK_ID.$SLURM_ARRAY_TASK_ID of $SLURM_ARRAY_TASK_COUNT\"\n\n#env variable in python\npython hello_world.py\n\n#as command line argument\npython hello_world_args.py -ID $SLURM_ARRAY_TASK_ID\n</code></pre> <p>the version getting the env variable in the python script <code>hello_world.py</code></p> <pre><code>#!/usr/bin/env python3\n\nimport os\nmy_id = os.environ['SLURM_ARRAY_TASK_ID']\nprint(\"hello world with ID {}\".format(my_id))\n</code></pre> <p>the version getting the env variable as argument in the python script <code>hello_world_args.py</code></p> <pre><code>#!/usr/bin/env python3\n\"\"\"\nModule for handling inpu arguments\n\"\"\"\n\nimport argparse\n\n# get tests from file\nclass LoadFromFile(argparse.Action):\n    \"\"\"\n    class for reading arguments from file\n    \"\"\"\n    def __call__(self, parser, namespace, values, option_string=None):\n        with values as F:\n            vals = F.read().split()\n        setattr(namespace, self.dest, vals)\n\ndef get_args():\n    \"\"\"\n    Definition of the input arguments\n    \"\"\"\n    parser = argparse.ArgumentParser(description='Hello World')\n    parser.add_argument('-ID', type=int, action='store', dest='my_id',\n                        help='Slurm ID')\n    return parser.parse_args()\n\n\n    ARGS = get_args()\n    print(\"hello world from ID {}\".format(ARGS.my_id))\n</code></pre>","tags":["ml","language"]},{"location":"Scientific_Computing/Supported_Applications/Python/#python-packages","title":"Python Packages","text":"<p>Programmers around the world have written and released many packages for Python, which are not included with the core Python distribution and must be installed separately. Each Python environment module comes with its own particular suite of packages, and the system Python has its own installed packages.</p> <p>The provided packages can be listed using</p> <pre><code>module load Python/3.10.5-gimkl-2022a\npython -c \"help('modules')\"\n</code></pre>","tags":["ml","language"]},{"location":"Scientific_Computing/Supported_Applications/Python/#installing-packages-in-your-home","title":"Installing packages in your $HOME","text":"<p>This is the simplest way to install additional packages, but you might fill your <code>$HOME</code> quota and cannot share installations with collaborators.</p> <pre><code>module load Python/3.10.5-gimkl-2022a\npip install --user prodXY\n</code></pre> <p>If you are working on multiple projects, this method will cause issues as your projects may require different versions of packages which are not compatible.</p> <p>We strongly recommend using separate Python virtual environments to isolate dependencies between projects, avoid filling your home space and being able to share installation with collaborators</p>","tags":["ml","language"]},{"location":"Scientific_Computing/Supported_Applications/Python/#installing-packages-in-a-python-virtual-environment","title":"Installing packages in a Python virtual environment","text":"<p>A Python virtual environment is lightweight system to create an environment which contains specific packages for a project, without interfering with the global Python installation. Each virtual environment is a different directory.</p> <p>To create a Python virtual environment, use the <code>venv</code> module as follows</p> <pre><code>module load Python/3.10.5-gimkl-2022a\npython3 -m venv /nesi/project/PROJECT_ID/my_venv\n</code></pre> <p>where <code>PROJECT_ID</code> is your NeSI project ID.</p> <p>Note that you need to activate the virtual environment before using it (to run a script or install packages)</p> <pre><code>source /nesi/project/PROJECT_ID/my_venv/bin/activate\n</code></pre> <p>Then you can install any pip-installable package in the virtual environment using</p> <pre><code>pip install prodXY\n</code></pre> <p>Then a Slurm job submission script running your Python script would look like</p> <pre><code>#!/bin/bash -e\n#SBATCH --job-name    MyPythonJob\n#SBATCH --time        01:00:00\n#SBATCH --mem         512MB\n\nmodule purge\nmodule load Python/3.10.5-gimkl-2022a\nsource /nesi/project/PROJECT_ID/my_venv/bin/activate\npython MyPythonScript.py\n</code></pre>","tags":["ml","language"]},{"location":"Scientific_Computing/Supported_Applications/Python/#python-virtual-environment-isolation","title":"Python virtual environment isolation","text":"<p>By default, Python virtual environments are fully isolated from the system installation. It means that you will not be able to access packages already prepared by NeSI in the corresponding Python environment module.</p> <p>To avoid this, use the option <code>--system-site-packages</code> when creating the virtual environment</p> <pre><code>module load Python/3.10.5-gimkl-2022a\npython3 -m venv --system-site-packages /nesi/project/PROJECT_ID/my_venv\n</code></pre> <p>A downside of this is that now your virtual environment also finds packages from your <code>$HOME</code> folder. To avoid this behavirour, make sure to use <code>export PYTHONNOUSERSITE=1</code> before calling pip or running a Python script. For example, in a Slurm job submission script</p> <pre><code>#!/bin/bash -e\n#SBATCH --job-name    MyPythonJob\n#SBATCH --time        01:00:00\n#SBATCH --mem         512MB\n\nmodule purge\nmodule load Python/3.10.5-gimkl-2022a\nsource /nesi/project/PROJECT_ID/my_venv/bin/activate\nexport PYTHONNOUSERSITE=1\npython MyPythonScript.py\n</code></pre>","tags":["ml","language"]},{"location":"Scientific_Computing/Supported_Applications/Python/#further-notes","title":"Further notes","text":"","tags":["ml","language"]},{"location":"Scientific_Computing/Supported_Applications/Python/#ipython","title":"iPython","text":"<p>iPython (interactive Python) is an enhanced tool for accessing a Python command line. It is available in many NeSI Python modules.</p>","tags":["ml","language"]},{"location":"Scientific_Computing/Supported_Applications/Python/#starting-ipython","title":"Starting iPython","text":"<p>To open an iPython console, simply run the <code>ipython</code> command:</p> <pre><code>[jblo123@build-wm ~]$ module load Python/3.6.3-gimkl-2017a\n[jblo123@build-wm ~]$ ipython\n</code></pre>","tags":["ml","language"]},{"location":"Scientific_Computing/Supported_Applications/Python/#listing-available-functions","title":"Listing available functions","text":"<p>You can use iPython to list the functions available that start with a given string. Please note that if the string denotes a module (i.e., it has a full stop somewhere in it), that module (or the function you want from it) must first be imported, using either an \"import X\" statement or a \"from X import Y\" statement.</p> <pre><code>import os\nos.&lt;TAB&gt;   # List all functions in the os module\nos.O_&lt;TAB&gt; # List functions starting with \"O_\" from the os module\nlen&lt;TAB&gt;   # List functions starting with \"len\"\n</code></pre> <p>Here,</p> <pre><code>o&lt;TAB&gt;\n</code></pre> <p>or even</p> <pre><code>os&lt;TAB&gt;\n</code></pre> <p>and expect to see the methods and values provided by the os module - you have to put the full stop after the \"os\" if you want to do that.</p>","tags":["ml","language"]},{"location":"Scientific_Computing/Supported_Applications/Python/#getting-information-about-an-object","title":"Getting information about an object","text":"<p>In iPython, you can query any object by typing the object name followed by a question mark (?), then hitting Enter. For instance:</p> <pre><code>In [1]: x = 5\nIn [2]: x?\nType:        int\nString form: 5\nDocstring:\nint(x=0) -&gt; int or long\nint(x, base=10) -&gt; int or long\n\nConvert a number or string to an integer, or return 0 if no arguments\nare given.  If x is floating point, the conversion truncates towards zero.\nIf x is outside the integer range, the function returns a long instead.\n\nIf x is not a number or if base is given, then x must be a string or\nUnicode object representing an integer literal in the given base.  The\nliteral can be preceded by '+' or '-' and be surrounded by whitespace.\nThe base defaults to 10.  Valid bases are 0 and 2-36.  Base 0 means to\ninterpret the base from the string as an integer literal.\n    &gt;&gt; int('0b100', base=0)\n4\n</code></pre> <p>You can also do this on functions (<code>len?</code>), methods (<code>os.mkdir?</code>) and modules (<code>os.path?</code>). If you try to do it on something that isn't defined yet, Python will tell you that the object in question couldn't be found.</p>","tags":["ml","language"]},{"location":"Scientific_Computing/Supported_Applications/Python/#quitting-ipython","title":"Quitting iPython","text":"<p>Just enter the <code>quit</code> command at the iPython prompt.</p>","tags":["ml","language"]},{"location":"Scientific_Computing/Supported_Applications/R/","title":"R","text":"","tags":["mahuika","R"]},{"location":"Scientific_Computing/Supported_Applications/R/#description","title":"Description","text":"<p>R is a language and environment for statistical computing and graphics. It is a GNU project which is similar to the S language and environment, itself developed at Bell Laboratories (formerly AT&amp;T, now Lucent Technologies) by John Chambers and colleagues. R can be considered as a different implementation of S. There are some important differences, but much code written for S runs unaltered under R.</p> <p>R provides a wide variety of statistical (linear and nonlinear modelling, classical statistical tests, time-series analysis, classification, clustering, and so forth) and graphical techniques, and is highly extensible. The S language is often the vehicle of choice for research in statistical methodology, and R provides an Open Source route to participation in that activity.</p>","tags":["mahuika","R"]},{"location":"Scientific_Computing/Supported_Applications/R/#licence","title":"Licence","text":"<p>R is made available at no cost under the terms of version 2 of the GNU General Public Licence.</p>","tags":["mahuika","R"]},{"location":"Scientific_Computing/Supported_Applications/R/#nesi-customisations","title":"NeSI Customisations","text":"<ul> <li>We patch the snow package so that there is no need to use RMPISNOW   when using it over MPI.</li> <li>Our most recent R environment modules set R_LIBS_USER to a path   which includes the compiler toolchain, so for   example\u00a0~/R/gimkl-2022a/4.2 rather than the usual default   of\u00a0~/R/x86_64-pc-linux-gnu-library/4.2.</li> </ul>","tags":["mahuika","R"]},{"location":"Scientific_Computing/Supported_Applications/R/#available-modules","title":"Available Modules","text":"","tags":["mahuika","R"]},{"location":"Scientific_Computing/Supported_Applications/R/#r-base","title":"R Base","text":"Mahuika Maui_ancil <p> 3.4.2-gimkl-2017a 3.5.0-gimkl-2017a 3.5.1-gimkl-2017a 3.5.3-gimkl-2018b 3.6.1-gimkl-2018b 3.6.2-gimkl-2020a 4.0.1-gimkl-2020a-TCLTK 4.0.1-gimkl-2020a 4.1.0-gimkl-2020a 4.2.1-gimkl-2022a-TCLTK 4.2.1-gimkl-2022a 4.3.1-gimkl-2022a 4.3.2-foss-2023a </p> <pre><code>module load R/4.3.2-foss-2023a</code></pre> <p> 3.5.1-gimkl-2018b 3.6.1-gimkl-2018b </p> <pre><code>module load R/3.6.1-gimkl-2018b</code></pre> <p>We also have some environment modules which extend the base R ones with extra packages:</p>","tags":["mahuika","R"]},{"location":"Scientific_Computing/Supported_Applications/R/#r-geo","title":"R-Geo","text":"<p>Includes rgeos, rgdal and other geometric and geospatial packages based on the libraries GEOS, GDAL, PROJ and UDUNITS.</p> Mahuika <p> 3.6.1-gimkl-2018b 3.6.2-gimkl-2020a 4.0.1-gimkl-2020a 4.1.0-gimkl-2020a 4.2.1-gimkl-2022a 4.3.1-gimkl-2022a 4.3.2-foss-2023a </p> <pre><code>module load R-Geo/4.3.2-foss-2023a</code></pre>","tags":["mahuika","R"]},{"location":"Scientific_Computing/Supported_Applications/R/#r-bundle-bioconductor","title":"R-bundle-Bioconductor","text":"<p>Includes many of the BioConductor suite of packages.</p> Mahuika <p> 3.11-gimkl-2020a-R-4.0.1 3.13-gimkl-2020a-R-4.1.0 3.15-gimkl-2022a-R-4.2.1 3.17-gimkl-2022a-R-4.3.1 </p> <pre><code>module load R-bundle-Bioconductor/3.17-gimkl-2022a-R-4.3.1</code></pre>","tags":["mahuika","R"]},{"location":"Scientific_Computing/Supported_Applications/R/#example-r-scripts","title":"Example R scripts","text":"SerialArrayParallel with 'doParallel'Parallel with 'doMPI'Parallel with 'snow' <pre><code>png(filename=\"plot.png\")  # This line redirects plots from screen to plot.png file.\n\n# Define the cars vector with 5 values\ncars &lt;- c(1, 3, 6, 4, 9)\n\n# Graph the cars vector with all defaults\nplot(cars)\n</code></pre> <pre><code>jobid &lt;- as.numeric(Sys.getenv(\"SLURM_ARRAY_TASK_ID\"))\njobid\n</code></pre> <p>The following example sums 50 normally distributed random value vectors of sizes 1 million to 1000050. Set the number of workers in your submission script with <code>--cpus-per-task=</code>... Note that all workers run on the same node. Hence, the number of workers is limited to the number of cores (physical if --hint=nomultithread or logical if using <code>--hint=multithread</code>).</p> <pre><code>library(doParallel)\nregisterDoParallel(strtoi(Sys.getenv(\"SLURM_CPUS_PER_TASK\")))\n\n# 50 calculations, store the result in 'x'\nx &lt;- foreach(z = 1000000:1000050, .combine = 'c') %dopar% {\n  sum(rnorm(z))\n}\n\nprint(x)\n</code></pre> <p>This example is similar to the above except that workers can run across multiple nodes. Note that we don't need to specify the number of workers when starting the cluster -- it will be derived by the mpiexec command, which slurm will invoke. You will need to load the gimkl module to expose the MPI library.</p> <pre><code>library(doMPI, quiet=TRUE)\ncl &lt;- startMPIcluster()\nregisterDoMPI(cl)\n\n# 50 calculations, store the result in 'x'\nx &lt;- foreach(z = 1000000:1000050, .combine = 'c') %dopar% {\n  sum(rnorm(z))\n}\n\ncloseCluster(cl)\nprint(x)\nmpi.quit()\n</code></pre> <pre><code>library(snow)\n# If there are multiple tasks only one reaches here, others become slaves.\n\n# Select MPI-based or fork-based parallelism depending on ntasks\nif(strtoi(Sys.getenv(\"SLURM_NTASKS\")) &gt; 1) {\n    cl &lt;- makeMPIcluster()\n} else {\n    cl &lt;- makeSOCKcluster(max(strtoi(Sys.getenv('SLURM_CPUS_PER_TASK')), 1))\n}\n\n# 50 calculations to be done:\nx &lt;- clusterApply(cl, 1000000:1000050, function(z) sum(rnorm(z)))\n\nstopCluster(cl)\n</code></pre>","tags":["mahuika","R"]},{"location":"Scientific_Computing/Supported_Applications/R/#example-slurm-scripts","title":"Example Slurm Scripts","text":"SerialArrayParallel with MPI <pre><code>#!/bin/bash -e\n\n#SBATCH --job-name    MySerialRJob\n#SBATCH --time        01:00:00\n#SBATCH --mem         512MB\n#SBATCH --output      MySerialRJob.%j.out # Include the job ID in the names of\n#SBATCH --error       MySerialRJob.%j.err # the output and error files\n\nmodule load 4.2.1-gimkl-2022a\n\n# Help R to flush errors and show overall job progress by printing\n# \"executing\" and \"finished\" statements.\necho \"Executing R ...\"\nsrun Rscript MySerialRJob.R\necho \"R finished.\"\n</code></pre> <pre><code>#!/bin/bash -e\n\n#SBATCH --job-name    MyArrayRJob\n#SBATCH --time        01:00:00\n#SBATCH --array       1-10\n#SBATCH --mem         512MB\n#SBATCH --output      MyArrayRJob.%j.out # Include the job ID in the names of\n#SBATCH --error       MyArrayRJob.%j.err # the output and error files\n\nmodule load R/4.2.1-gimkl-2022a\n\n# Help R to flush errors and show overall job progress by printing\n# \"executing\" and \"finished\" statements.\necho \"Executing R ...\"\nsrun Rscript MyArrayRJob.R\necho \"R finished.\"\n</code></pre> <pre><code>#!/bin/bash -e\n\n#SBATCH --job-name      MyMPIRJob\n#SBATCH --time          01:00:00\n#SBATCH --ntasks        12\n#SBATCH --cpus-per-task 1\n#SBATCH --mem-per-cpu   512MB\n#SBATCH --output        MyMPIRJob.%j.out # Include the job ID in the names of\n#SBATCH --error         MyMPIRJob.%j.err # the output and error files\n\nmodule load R/4.2.1-gimkl-2022a\n# need MPI\nmodule load gimkl/2022a\n\n# Help R to flush errors and show overall job progress by printing\n# \"executing\" and \"finished\" statements.\necho \"Executing R ...\"\n# Our R has a patched copy of the snow library so that there is no need to use\n# RMPISNOW.\nsrun Rscript doMPI\necho \"R finished.\"\n</code></pre>","tags":["mahuika","R"]},{"location":"Scientific_Computing/Supported_Applications/R/#generating-images-and-plots","title":"Generating images and plots","text":"<p>Normally when plotting or generating other sorts of images, R expects a graphical user interface to be available so it can render and display the image on the fly. However, it is possible to instruct R to export the image directly to a file instead of displaying it on the screen, using code like the following:</p> <pre><code>png(filename=\"plot.png\")\n</code></pre> <p>This statement instructs R to export all future graphical output to a PNG file named <code>plot.png</code>, until a different device driver is selected.</p> <p>For more information about graphical device drivers, please see the R documentation.</p>","tags":["mahuika","R"]},{"location":"Scientific_Computing/Supported_Applications/R/#dealing-with-packages","title":"Dealing with packages","text":"<p>Much R functionality is not supplied with the base installation, but is instead added by means of packages written by the R developers or by third parties. \u00a0We include a large number of such R packages in our R environment modules.</p>","tags":["mahuika","R"]},{"location":"Scientific_Computing/Supported_Applications/R/#getting-a-list-of-installed-packages","title":"Getting a list of installed packages","text":"<p>It is best to view the list of available R packages interactively. To do so, call up the package library:</p> <pre><code>module R/4.2.1-gimkl-2022a\nR\n</code></pre> <pre><code>library()\n</code></pre> <p>or just use the module command:</p> <pre><code>module show R/4.2.1-gimkl-2022a\n</code></pre> <p>Please note that different installations of R, even on the same NeSI cluster, may contain different collections of packages. Furthermore, if you have your own packages in a directory that R can automatically detect, these will also be shown in a separate section.</p>","tags":["mahuika","R"]},{"location":"Scientific_Computing/Supported_Applications/R/#getting-a-list-of-available-libraries","title":"Getting a list of available libraries","text":"<p>You can print a list of the library directories in which R will look for packages by running the following command in an R session:</p> <pre><code>.libPaths()\n</code></pre> <p>For R/4.2.1 the command\u00a0<code>.libPaths()</code>\u00a0will return the following:</p> <pre><code>.libPaths()\n</code></pre> <pre><code>[1] \"/home/YOUR_USER_NAME/R/gimkl-2022a/4.2\"                            \n[2] \"/opt/nesi/CS400_centos7_bdw/R/4.2.1-gimkl-2022a/lib64/R/library\"\n</code></pre> <p>When using the <code>library()</code> function R will first look to your Home/Personal library for the package and then to the Systems Library provided by NeSI. This can be used in conjuction with <code>installed.packages()</code> to see what is available in a specific library. eg:</p> <pre><code>installed.packages(\"/home/YOUR_USER_NAME/R/gimkl-2022a/4.2\")\n...\nggplot2 NA NA NA \"no\" \"4.2.1\"\nggrepel NA NA NA \"yes\" \"4.2.1\"\netc...\n</code></pre>","tags":["mahuika","R"]},{"location":"Scientific_Computing/Supported_Applications/R/#specifying-custom-library-directories","title":"Specifying custom library directories","text":"<p>You can add your own custom library directories by putting a list of extra directories in the <code>.Renviron</code> file in your home directory. This list should look like the following:</p> <pre><code>export R_LIBS=/home/jblo123/R/foo:/home/jblo123/R/bar\n</code></pre> <p>Note that, of the contents of the <code>R_LIBS</code> variable, only those directories that actually exist will show up in the output of <code>.libPaths()</code>.</p> <p>Alternatively, you can specify in your R script:</p> <pre><code>dir.create(\"/nesi/project/&lt;projectID&gt;/Rpackages\", showWarnings = FALSE, recursive = TRUE)\n.libPaths(new=\"/nesi/project/&lt;projectID&gt;/Rpackages\")\n</code></pre>","tags":["mahuika","R"]},{"location":"Scientific_Computing/Supported_Applications/R/#downloading-and-installing-a-new-package","title":"Downloading and installing a new package","text":"<p>To install a package into R, use the install.packages command.</p> <p>For example, to install the sampling package:</p> <pre><code>module load R/4.2.1-gimkl-2022a\nR\n</code></pre> <pre><code>install.packages(\"sampling\")\n</code></pre> <p>You will most likely be asked if you want to use a personal library and, if you have not previously done so, whether you wish to create a new personal library. Answer \"y\" to both questions.</p> <p>Enter the number for one of the Australian sites from the list of download mirrors that will appear, as the lone New Zealand mirror site is more often out of date.</p> <p>R will then download, compile and install the new package for you.</p> <p>You can confirm the package has been installed by using the <code>library()</code> command:</p> <pre><code>library(\"foo\")\n</code></pre> <p>If the package has been correctly installed, you will get no response. On the other hand, if the package is missing or was not installed correctly, an error message will typically be returned:</p> <pre><code>library(\"foo\")\n</code></pre> <pre><code>Error in library(\"foo\") : there is no package called \u2018foo\u2019\n</code></pre>","tags":["mahuika","R"]},{"location":"Scientific_Computing/Supported_Applications/R/#compiling-a-c-library-for-use-with-r","title":"Compiling a C library for use with R","text":"<p>You can compile custom C libraries for use with R using the R shared library compiler:</p> <pre><code>module load R/4.2.1-gimkl-2022a\nR CMD SHLIB mylib.c\n</code></pre> <p>This will create the shared object mylib.so. You can then reference the library in your R script:</p> <pre><code>R\n</code></pre> <pre><code>dyn.load(\"~/R/lib64/mylib.so\")\n</code></pre>","tags":["mahuika","R"]},{"location":"Scientific_Computing/Supported_Applications/R/#quitting-an-interactive-r-session","title":"Quitting an interactive R session","text":"<p>At the R command prompt, when you want to quit R, type the following:</p> <pre><code>quit()\n</code></pre> <p>You will be asked \"Save workspace image? [y/n/c]\". Type n.</p>","tags":["mahuika","R"]},{"location":"Scientific_Computing/Supported_Applications/R/#troubleshooting","title":"Troubleshooting","text":"","tags":["mahuika","R"]},{"location":"Scientific_Computing/Supported_Applications/R/#missing-devtools","title":"Missing devtools","text":"<p>Package installation will occasionally fail due to missing system libraries (eg HarfBuzz, FriBidi or devtools), this is resolved by loading the devtools module prior to the version of R you require.</p> <pre><code>module load devtools\nmodule load R/4.2.1-gimkl-2022a\n</code></pre>","tags":["mahuika","R"]},{"location":"Scientific_Computing/Supported_Applications/R/#cant-install-sf-rgdal-etc","title":"Can't install sf, rgdal etc","text":"<p>Use the R-Geo module</p> <pre><code>module load R-Geo/4.2.1-gimkl-2022a\n</code></pre>","tags":["mahuika","R"]},{"location":"Scientific_Computing/Supported_Applications/R/#clusterparallel-environment-variable-not-accessed","title":"Cluster/Parallel environment variable not accessed","text":"<p>Depending on the working environment, registering of a cluster and accessing the <code>SLURM_CPUS_PER_TASK</code> environment variable may not return an integer, in particular the function <code>strtoi</code> (string to integer) doesn't work correctly. Instead use <code>as.numeric</code></p> <p>Options:</p> <ul> <li><code>strtoi(Sys.getenv(\"SLURM_CPUS_PER_TASK\"))</code></li> <li><code>as.numeric(Sys.getenv(\"SLURM_CPUS_PER_TASK\"))</code></li> </ul>","tags":["mahuika","R"]},{"location":"Scientific_Computing/Supported_Applications/RAxML/","title":"RAxML","text":"<p>RAxML search algorithm for maximum likelihood based inference of phylogenetic trees.</p> <p>RAxML Homepage</p>","tags":["mahuika","biology"]},{"location":"Scientific_Computing/Supported_Applications/RAxML/#available-modules","title":"Available Modules","text":"Mahuika <p> 8.2.10-gimkl-2017a 8.2.12-gimkl-2020a 8.2.12-gimkl-2022a </p> <pre><code>module load RAxML/8.2.12-gimkl-2022a</code></pre>","tags":["mahuika","biology"]},{"location":"Scientific_Computing/Supported_Applications/RAxML/#description","title":"Description","text":"<p>RAxML search algorithm for maximum likelihood based inference of phylogenetic trees. The RAxML home page is at https://github.com/stamatak/standard-RAxML.</p>","tags":["mahuika","biology"]},{"location":"Scientific_Computing/Supported_Applications/RAxML/#licensing-requirements","title":"Licensing requirements","text":"<p>RAxML is licensed under the terms of the GNU General Public License (\"the GPL\"), version 2 or (at your option) any later version. A copy of version 3 of the GPL as included with the RAxML software is available here.</p>","tags":["mahuika","biology"]},{"location":"Scientific_Computing/Supported_Applications/RAxML/#example-scripts","title":"Example scripts","text":"","tags":["mahuika","biology"]},{"location":"Scientific_Computing/Supported_Applications/RAxML/#example-script-for-the-mahuika-cluster","title":"Example script for the Mahuika cluster","text":"<pre><code>#!/bin/bash -e\n\n#SBATCH --job-name      RAxML_job\n#SBATCH --time          01:00:00\n#SBATCH --ntasks        1\n#SBATCH --cpus-per-task 4\n#SBATCH --mem           2G\n\nmodule load RAxML/8.2.12-gimkl-2020a\n\nsrun raxmlHPC-PTHREADS-AVX -T $SLURM_CPUS_PER_TASK -m GTRCAT -s aln.fasta -n tree.out\n</code></pre>","tags":["mahuika","biology"]},{"location":"Scientific_Computing/Supported_Applications/RAxML/#documentation","title":"Documentation","text":"<p><code>raxmlHPC-AVX -help</code> and the RAxML manual.</p>","tags":["mahuika","biology"]},{"location":"Scientific_Computing/Supported_Applications/RAxML/#parallel-versions","title":"Parallel Versions","text":"<p>Each of our RAxML environment modules contains multiple RAxML executables:</p> <ul> <li><code>raxmlHPC-AVX</code></li> <li><code>raxmlHPC-SSE3</code></li> <li><code>raxmlHPC-PTHREADS-AVX</code></li> <li><code>raxmlHPC-PTHREADS-SSE3</code></li> <li><code>raxmlHPC-MPI-AVX</code></li> <li><code>raxmlHPC-MPI-SSE3</code></li> <li><code>raxmlHPC-HYBRID-AVX</code></li> <li><code>raxmlHPC-HYBRID-SSE3</code></li> </ul> <p>The combinations of Slurm settings and RAxML types which make sense are:</p> <ul> <li><code>raxmlHPC-AVX</code>\u00a0or\u00a0<code>raxmlHPC-SSE3</code> with one task on only one CPU.</li> <li><code>raxmlHPC-PTHREADS-AVX</code> or <code>raxmlHPC-PTHREADS-SSE3</code> with one task     running on multiple CPUs.</li> <li><code>raxmlHPC-MPI-AVX</code>\u00a0or <code>raxmlHPC-MPI-SSE3</code> with multiple tasks, each     running on one CPU.</li> <li><code>raxmlHPC-HYBRID-AVX</code>\u00a0or <code>raxmlHPC-HYBRID-SSE3</code> with multiple tasks,     each of which runs on multiple CPUs.</li> </ul> <p>MPI and HYBRID are only useful for bootstrapped trees.</p> <p>For the multi-threaded cases (PTHREADS and HYBRID) you should tell RAxML how many threads to use with the RAxML option <code>-T $SLURM_CPUS_PER_TASK</code>.</p> <p>The \"AVX\" executables use the AVX SIMD instructions, while the \"SSE3\" executables use the older and slower Intel SIMD (Single Instruction Multiple Data) instructions, which can be anywhere from 10% to 30% slower. There should be no need to use an SSE3 executable, unless you find that an AVX executable doesn't work for any reason.</p>","tags":["mahuika","biology"]},{"location":"Scientific_Computing/Supported_Applications/Relion/","title":"Relion","text":"<p>RELION (for REgularised LIkelihood OptimisatioN, pronounce rely-on)</p> <p>Relion Homepage</p>","tags":["no_toc","no_lic","no_desc","no_ver"]},{"location":"Scientific_Computing/Supported_Applications/Relion/#available-modules","title":"Available Modules","text":"Mahuika <p> 2.0.3-gimkl-2017a 2.1-gimkl-2017a 3.0beta-gimkl-2017a-GPU 3.0beta-gimkl-2017a 4.0.1-gimkl-2020a </p> <pre><code>module load Relion/4.0.1-gimkl-2020a</code></pre> <p>Getting started with Relion is most easily done via its X11 GUI, which is launched with the command \"relion\". \u00a0</p> <pre><code>module load Relion\nrelion\n</code></pre> <p>After the desired options have been selected press the \"Check command\" button to see the command it constructs in your terminal window. That command line can then be pasted into a Slurm batch script.</p> <p>If you leave \"Number of MPI procs\" at 1 then the Relion GUI will produce a command like</p> <pre><code>which relion_run_ctffind ...\n</code></pre> <p>That will work but can be simplified to</p> <pre><code>relion_run_ctffind ...\n</code></pre> <p>If MPI is used then Relion will correctly recommend the MPI version of the tool, eg:</p> <pre><code>which relion_run_ctffind_mpi ...\n</code></pre> <p>but MPI programs need to be launched via srun, so that should be:</p> <pre><code>srun relion_run_ctffind_mpi ...\n</code></pre> <p>We have made some effort to integrate the Relion GUI directly with Slurm so that it can submit Slurm jobs directly, however this might not entirely work yet.</p> <p>Some of the Relion tools benefit tremendously from using a GPU.</p> <p>For licensing reasons we ask that you install the GPU accelerated\u00a0MotionCorr2 yourself if you find Relion's own CPU-only version of the same algorithm insufficient.</p>","tags":["no_toc","no_lic","no_desc","no_ver"]},{"location":"Scientific_Computing/Supported_Applications/Singularity/","title":"Singularity","text":"<p>Singularity Homepage</p>","tags":["containers","singularity","docker"]},{"location":"Scientific_Computing/Supported_Applications/Singularity/#available-modules","title":"Available Modules","text":"Mahuika Maui_ancil <p> 3.5.2 3.6.1 3.6.4 3.7.1 3.8.0 3.8.3 3.8.5 3.9.4 3.9.8 3.10.0 3.10.3 3.11.0 3.11.3 </p> <pre><code>module load Singularity/3.11.3</code></pre> <p> 3.5.2 3.6.4 3.7.1 3.8.0 3.9.4 </p> <pre><code>module load Singularity/3.9.4</code></pre> <p>Singularity is a science-focused application containerisation solution that is specifically tailored for integration with HPC systems and suitable for deployment across a broad range of science infrastructure environments. Singularity enables a high level of portability for research applications across various Linux distributions (and derivatives), from laptops to supercomputers.</p> <p>Containerisation allows users to package a complete runtime environment into a single container image (typically a single file), including system libraries of various Linux flavours, custom user software, configuration files, and most other dependencies. The container image file can be easily copied and run on any Linux-based computing platform, enabling simple portability and supporting reproducibility of scientific results.</p> <p>Unlike a virtual machine, a running container instance\u00a0shares the host operating system's kernel, relying heavily on Linux namespaces (kernel partitioning and isolation features for previously global Linux system resources). Resources and data outside of the container can be mapped into the container to achieve integration, for example, Singularity makes it simple to expose GPUs to the container and to access input/output files &amp; directories mounted on the host (such as those on shared filesystems).</p> <p>Contrary to other containerisation tools such as Docker, Singularity removes the need for elevated privileges (\"root access\", e.g., via the \"sudo\" command) at container runtime. This feature is essential for enabling containers to run on shared platforms like an HPC, where users cannot be allowed to elevate privileges for security reasons. Singularity container images are also read-only by default at runtime, to help reproducibility of results, and they integrate easily with scheduling systems like Slurm and with MPI parallelisation.</p> <p>Containerisation technologies, both the fundamental underlying Linux kernel features, the various runtime support tools, and associated cloud-services (such as container libraries, remote builders, and image signing services), are a broad and fast moving landscape. The information here is provided as an overview and may not necessarily be completely up-to-date with the latest available features, however we will endeavour to ensure it accurately reflects NeSI's currently supported Singularity version.</p>","tags":["containers","singularity","docker"]},{"location":"Scientific_Computing/Supported_Applications/Singularity/#building-a-new-container","title":"Building a new container","text":"<p>For more general information on building containers please see the Singularity Documentation.\u00a0</p> <p>As building a container requires root privileges in general, this cannot be done directly on any NeSI nodes. You will need to copy a Singularity Image Format (SIF) to the cluster from on a local Linux machine or the cloud. Alternatively you can make use of a remote build service (currently only the syslabs builder is available).</p> <p>However, it is possible to build some containers directly on NeSI, using the Milan compute nodes and Apptainer. Specific instructions are provided in a dedicated support page Build an Apptainer container on a Milan compute node. Please note this may fail to build some containers and encourage you to contact us at support@nesi.org.nz if you encounter an issue.</p>","tags":["containers","singularity","docker"]},{"location":"Scientific_Computing/Supported_Applications/Singularity/#remote-build-service","title":"Remote Build Service","text":"<p>Running the command <code>singularity remote login</code> will provide you with a link to syslabs.io, once you have logged in you will be prompted to create a key. Copying the string from your newly created key into your terminal will authorise remote builds from your current host.</p> <p>Specify you want to use the remote builder by adding the <code>--remote</code> flag to the build command.</p> <pre><code>singularity build --remote myContainer.sif myContainer.def\n</code></pre>","tags":["containers","singularity","docker"]},{"location":"Scientific_Computing/Supported_Applications/Singularity/#build-environment-variables","title":"Build Environment\u00a0Variables","text":"<p>The environment variables <code>SINGULARITY_TMPDIR</code> and <code>SINGULARITY_CACHEDIR</code> environment can be used to overwrite the default location of these directories. By default both of these values are set to <code>/tmp</code> which has limited space, large builds may exceed this limitation causing the builder to crash.</p> <p>You may wish to change these values to somewhere in your project or nobackup directory.</p> <pre><code>export SINGULARITY_TMPDIR=/nesi/nobackup/nesi99999/.s_tmpdir\nsetfacl -b \"$SINGULARITY_TMPDIR\"  # avoid Singularity issues due to ACLs set on this folder\n</code></pre> <pre><code>export SINGULARITY_CACHEDIR=/nesi/nobackup/nesi99999/.s_cachedir\n</code></pre> <p>Please Sir, may I have a build node?</p>","tags":["containers","singularity","docker"]},{"location":"Scientific_Computing/Supported_Applications/Singularity/#moving-a-container-to-nesi","title":"Moving a container to NeSI","text":"<p>A container in Singularity's SIF format can be easily moved to the HPC filesystem by:</p> <ul> <li>Copying the image file from your local computer with basic file     transfer tools - please refer to our documentation on\u00a0Moving files     to/from the     cluster     and\u00a0Data Transfer using     Globus     (if you have a very large container) for details</li> <li>Downloading the container from an online repository</li> </ul> <p>To download a container, use commands such as</p> <pre><code>module load Singularity\nsingularity pull library://sylabsed/linux/alpine\n</code></pre> <p>Please refer to the Singularity documentation for further details.</p>","tags":["containers","singularity","docker"]},{"location":"Scientific_Computing/Supported_Applications/Singularity/#using-a-docker-container","title":"Using a Docker container","text":"<p>Singularity can transparently use Docker containers, without the need to be root or to have Docker installed.</p> <p>To download and convert a Docker container as a Singularity image, use the <code>pull</code> command with a <code>docker://</code> prefix. The following example downloads the latest version of the Ubuntu docker container and save it in the <code>ubuntu.sif</code> Singularity image file:</p> <pre><code>singularity pull ubuntu.sif docker://ubuntu\n</code></pre> <p>Access to private containers that needs registration is also supported, as detailed in the Singularity documentation.</p> <p>If you are building your own containers, you can also use Docker containers as basis for a Singularity image, by specifying it in the definition file as follows:</p> <pre><code>Bootstrap: docker\nFrom: ubuntu:latest\n\n%post\n    # intallation instructions go here\n</code></pre>","tags":["containers","singularity","docker"]},{"location":"Scientific_Computing/Supported_Applications/Singularity/#running-a-container-on-mahuika-or-maui-ancil","title":"Running a container on Mahuika or M\u0101ui Ancil","text":"<p>Singularity is not currently available on the M\u0101ui XC50 supercomputer.</p> <p>Singularity containers can easily be run on Mahuika or M\u0101ui Ancil once they are uploaded to a NeSI filesystem. Load the Singularity module first by running the command</p> <pre><code>module load Singularity\n</code></pre> <p>You can now execute a command inside the container using</p> <pre><code>singularity exec my_container.sif &lt;command&gt;\n</code></pre> <p>If your container has a \"%runscript\" section, you can execute it using</p> <pre><code>singularity run my_container.sif\n</code></pre> <p>To have a look at the contents of your container, you can \"shell\" into it using</p> <pre><code>singularity shell my_container.sif\n</code></pre> <p>Note the prompt is now prefixed with \"Singularity\",</p> <pre><code>Singularity&gt;\n</code></pre> <p>Exit the container by running the command</p> <pre><code>Singularity&gt; exit\n</code></pre> <p>which will bring you back to the host system.</p>","tags":["containers","singularity","docker"]},{"location":"Scientific_Computing/Supported_Applications/Singularity/#accessing-directories-outside-the-container","title":"Accessing directories outside the container","text":"<p>Singularity containers are immutable by default to support reproducibility of science results. Singularity will automatically bind your home directory if possible, giving you access to all files in your home directory tree.</p> <p>If the work directory from which you spin up the container is outside your home directory (e.g., in the \"nobackup\" or \"project\" file spaces) and you need to access its contents, you will need to bind this directory using, e.g., the command</p> <pre><code>singularity run --bind $PWD my_container.sif\n</code></pre> <p>Note that older releases of Singularity bind the work directory automatically.</p> <p>You can easily bind extra directories and optionally change their locations to a new path inside the container using, e.g.,</p> <pre><code>singularity run --bind \"/nesi/project/&lt;your project ID&gt;/inputdata:/var/inputdata,\\\n/nesi/nobackup/&lt;your project ID&gt;/outputdata:/var/outputdata\" my_container.sif\n</code></pre> <p>Directories <code>inputdata</code> and <code>outputdata</code> can now be accessed inside your container under <code>/var/inputdata</code> and <code>/var/outputdata</code>. Alternatively, you can set environment variable <code>SINGULARITY_BIND</code> before running your container,</p> <pre><code>export SINGULARITY_BIND=\"/nesi/project/&lt;your project ID&gt;/inputdata:/var/inputdata,\\\n/nesi/nobackup/&lt;your project ID&gt;/outputdata:/var/outputdata\"\n</code></pre>","tags":["containers","singularity","docker"]},{"location":"Scientific_Computing/Supported_Applications/Singularity/#accessing-a-gpu","title":"Accessing a GPU","text":"<p>If your Slurm job has requested access to an NVIDIA GPU (see GPU use on NeSI to learn how to request a GPU), a singularity container can transparently access it using the <code>--nv</code> flag:</p> <pre><code>singularity run --nv my_container.sif\n</code></pre> <p>Note</p> <p>Make sure that your container contains the CUDA toolkit and additional  libraries needed by your application (e.g. cuDNN). The <code>--nv</code> option  only ensures that the basic CUDA libraries from the host are bound  into the container and that the GPU device is accessible in the  container.</p>","tags":["containers","singularity","docker"]},{"location":"Scientific_Computing/Supported_Applications/Singularity/#network-isolation","title":"Network isolation","text":"<p>Singularity bridges the host network into the container by default. If you want to isolate the network, add flags <code>--net --network=none</code> when you run the container, e.g.,</p> <pre><code>singularity run --net --network=none my_container.sif\n</code></pre>","tags":["containers","singularity","docker"]},{"location":"Scientific_Computing/Supported_Applications/Singularity/#slurm-example","title":"Slurm example","text":"<p>It is easy to run Singularity containers inside Slurm jobs. Here is an example setup to run a container that uses 4 CPUs:</p> <pre><code>#!/bin/bash -e\n#SBATCH --job-name=singularity\n#SBATCH --time=01:00:00\n#SBATCH --mem=1024MB\n#SBATCH --ntasks=1\n#SBATCH --cpus-per-task=4\n\nmodule load Singularity\n\n# Bind directories and append SLURM job ID to output directory\nexport SINGULARITY_BIND=\"/nesi/project/&lt;your project ID&gt;/inputdata:/var/inputdata,\\\n/nesi/nobackup/&lt;your project ID&gt;/outputdata_${SLURM_JOB_ID:-0}:/var/outputdata\"\n\n# Run container %runscript\nsrun singularity run my_container.sif\n</code></pre> <p>Note that the output directory \"outputdata\" in the HPC file system is automatically suffixed with the Slurm job ID in the above example, but it is always available under the same path \"/var/outputdata\" from within the container. This makes it easy to run multiple containers in separate Slurm jobs. Please refer to our SLURM: Reference Sheet\u00a0for further details on using Slurm.</p>","tags":["containers","singularity","docker"]},{"location":"Scientific_Computing/Supported_Applications/Singularity/#tips-tricks","title":"Tips &amp; Tricks","text":"<ul> <li>Make sure that your container runs before uploading it - you will     not be able to rebuild it from a new definition file directly on the     HPC</li> <li>Try to configure all software to run in user space without requiring     privilege escalation via \"sudo\" or other privileged capabilities     such as reserved network ports - although Singularity supports some     of these features inside a container on some systems, they may not     always be available on the HPC or other platforms, therefore relying     on features such as Linux user namespaces could limit the     portability of your container</li> <li>If your container runs an MPI application, make sure that the MPI     distribution that is installed inside the container is compatible     with Intel MPI</li> <li>Write output data and log files to the HPC file system using a     directory that is bound into the container - this helps     reproducibility of results by keeping the container image immutable,     it makes sure that you have all logs available for debugging if a     job crashes, and it avoids inflating the container image file</li> </ul>","tags":["containers","singularity","docker"]},{"location":"Scientific_Computing/Supported_Applications/Software%20Installation%20Request/","title":"Software Installation Request","text":"<p>To request that we install a scientific application (either a new application, or a new version of an already installed application), please Contact our Support Team. In your message, please provide the following information:</p> <ul> <li>What is the name and version number of the software you would like     to be installed? If you wish to use a copy from a version control     repository, what tag or release do you need? Please be aware that we     usually require a stable release version of a piece of software     before we will install it for all users.</li> <li>Do you have a preference about which platform (Mahuika or M\u0101ui) we     install it on?</li> <li>Why would you like us to install this software package?</li> <li>What is the web site or home web page of the package? If you don't     know this information or the package doesn't have a web site, who is     the author or lead developer? In some cases, there exist two or more     packages with the same or very similar names. If we know the web     site we can be sure that we are installing the same package that you     are requesting.</li> <li>How is the package installed? For example, compiled from source,     precompiled binary, or installed as a Python, Perl, R, etc. library?</li> <li>What dependencies, if any, does the package require? Please be aware     that the exact dependency list may depend on the particular use     cases you have in mind (like the ability to read and write a     specific file format).</li> <li>Have you (or another member of your project team) tried to install     it yourself on a NeSI system? If so, were you successful?</li> <li>If you or your institution doesn't own the copyright in the     software, under what licence are you permitted to use it? Does that     licence allow you to install and run it on a NeSI system? (Hint:     Most free, open-source software licences will allow you to do this.)</li> <li>Who else do you know of who wants to use that software on a NeSI     system? Please provide their names, institutional affiliations, and     NeSI project codes (if you know them).</li> <li>What tests do you have that will allow us to verify that the     software is performing correctly and at an acceptable speed?</li> </ul> <p>Our team will review your request and will make a decision as to whether we will install the application and make it generally available.</p>","tags":[]},{"location":"Scientific_Computing/Supported_Applications/Supernova/","title":"Supernova","text":"<p>Supernova is a software package for de novo assembly from Chromium Linked-Reads</p> <p>Supernova Homepage</p>","tags":["mahuika","biology"]},{"location":"Scientific_Computing/Supported_Applications/Supernova/#available-modules","title":"Available Modules","text":"Mahuika <p> 2.1.1 </p> <pre><code>module load Supernova/2.1.1</code></pre>","tags":["mahuika","biology"]},{"location":"Scientific_Computing/Supported_Applications/Supernova/#description","title":"Description","text":"<p>Supernova is a software package for de novo assembly from Chromium Linked-Reads that are made from a single whole-genome library from an individual DNA source. Supernova creates diploid assemblies, thus separately representing maternal and paternal chromosomes over long distances.</p> <p>The Supernova software package includes two processing pipelines and one for post-processing:</p> <ul> <li><code>supernova mkfastq</code> wraps Illumina's bcl2fastq to correctly     demultiplex Chromium-prepared sequencing samples and to convert     barcode and read data to FASTQ files.</li> <li><code>supernova run</code> takes FASTQ files containing barcoded reads from     <code>supernova mkfastq</code> and builds a graph-based assembly. The approach     is to first build an assembly using read kmers (K = 48), then     resolve this assembly using read pairs (to K = 200), then use     barcodes to effectively resolve this assembly to K \u2248 100,000. The     final step pulls apart homologous chromosomes into phase blocks,     which are often several megabases in length.</li> <li><code>supernova mkoutput</code> takes Supernova's graph-based assemblies     and produces several styles of FASTA suitable for downstream     processing and analysis.</li> </ul> <p>Download latest release from 10xGenomics.</p> <p>https://support.10xgenomics.com/de-novo-assembly/software/downloads/latest</p>","tags":["mahuika","biology"]},{"location":"Scientific_Computing/Supported_Applications/Supernova/#availability","title":"Availability","text":"<p><code>Supernova/2.1.1</code> is installed as a module and can be loaded via <code>module load Supernova</code></p>","tags":["mahuika","biology"]},{"location":"Scientific_Computing/Supported_Applications/Supernova/#license","title":"License","text":"<p>The developer grants a Limited License to all users. If you intend to use Supernova on NeSI operated infrastructure please read the developers own licensing agreement.</p> <p>https://support.10xgenomics.com/de-novo-assembly/software/downloads/latest</p>","tags":["mahuika","biology"]},{"location":"Scientific_Computing/Supported_Applications/Supernova/#example-script","title":"Example script","text":"<pre><code>#SBATCH -J mySupernovajob\n#SBATCH --partition=hugemem\n#SBATCH --ntasks=1\n#SBATCH --mem=460G\n#SBATCH --cpus-per-task=16\n#SBATCH --time=168:00:00\n#SBATCH --hint=nomultithread\n\nmodule load Supernova/2.1.1\n\nsupernova run --id=.....................\n</code></pre>","tags":["mahuika","biology"]},{"location":"Scientific_Computing/Supported_Applications/Supernova/#getting-supernova-to-run-successfully","title":"Getting Supernova to run successfully","text":"<p>We suggest users initially read the developers notes, at https://support.10xgenomics.com/de-novo-assembly/guidance/doc/achieving-success-with-de-novo-assembly</p> <p>Further to that we also suggest,</p> <ul> <li>check --maxreads, to be passed to supernova, is correctly set.     Recommended     reading..https://bioinformatics.uconn.edu/genome-size-estimation-tutorial/http://qb.cshl.edu/genomescope/</li> <li>When passing <code>--localmem</code> to supernova, ensure this number is less     than the total memory passed to Slurm.\u00a0</li> <li>Pass <code>${SLURM_CPUS_PER_TASK}</code> to supernova with the <code>--localcores</code>     argument.</li> </ul>","tags":["mahuika","biology"]},{"location":"Scientific_Computing/Supported_Applications/Supernova/#tracking-job-progress-via-browser","title":"Tracking job progress via browser","text":"<p>Find the beginning of the <code>_log</code> file, located in the directory where the call to supernova was run, or the path specified in the Slurm batch file via <code>--output</code>.</p> <pre><code>    head -n 30 &lt;job_name&gt;.out\n\nsupernova run (2.1.1)\n\nCopyright (c) 2018 10x Genomics, Inc. All rights reserved.\n-------------------------------------------------------------------------------\n\nMartian Runtime - '2.1.1-v2.3.3'\nServing UI at http://wbh001:37982?auth=Bx2ccMZmJxaIfRNBOZ_XO_mQd1njNGL3rZry_eNI1yU\n\nRunning preflight checks (please wait)...\n</code></pre> <p>Find the line..</p> <pre><code>Serving UI at http://wbh001:37982?auth=Bx2ccMZmJxaIfRNBOZ_XO_mQd1njNGL3rZry_eNI1yU\u00a0\n</code></pre> <p>The link assumes the form..</p> <p>http:// &lt;node&gt;: &lt;port&gt;?&lt;auth&gt;</p> <ul> <li>&lt;node&gt; Taken from above code snippet is wbh001</li> <li>&lt;port&gt; Taken from above code snippet is 37982</li> <li>&lt;auth&gt; Taken from above code snippet is     Bx2ccMZmJxaIfRNBOZ_XO_mQd1njNGL3rZry_eNI1yU\u00a0</li> </ul> <p>In a new local terminal window open an ssh tunnel to the node. This takes the following general form</p> <p><code>ssh -L &lt;d&gt;:&lt;node&gt;:&lt;port&gt; -N &lt;server&gt;</code></p> <ul> <li>&lt;d&gt; An integer</li> <li>&lt;server&gt; see:      https://support.nesi.org.nz/hc/en-gb/articles/360000625535-Recommended-Terminal-Setup</li> </ul> <p>When details are added to the general form from the specifics in the snippet above, the following could be run..</p> <pre><code>ssh -L 9999:wbh001:39782 -N mahuika\n</code></pre> <p>Next, open your preferred web browser and construct the following link..</p> <pre><code>http://localhost:&lt;d&gt;?&lt;auth&gt;\n</code></pre> <p>take &lt;d&gt; and &lt;auth&gt; from the code snippet above..</p> <pre><code>http://localhost:9999/?auth=Bx2ccMZmJxaIfRNBOZ_XO_mQd1njNGL3rZry_eNI1yU\n</code></pre> <p></p>","tags":["mahuika","biology"]},{"location":"Scientific_Computing/Supported_Applications/Supernova/#things-to-watch-out-for","title":"Things to watch out for","text":"<ul> <li>Supernova will create checkpoints after completing stages in the     pipeline. In order to run from a previously created checkpoint you     will first need to delete the _lock file located in the main output     directory (the directory named by <code>ID=${SLURM_JOB_NAME}</code> where the     <code>_log</code> file is also located) and passed to supernova in the     <code>--id=${ID}</code> argument in the sample Slurm script above. Avoid     changing any other settings in both the call to Slurm and supernova.</li> </ul>","tags":["mahuika","biology"]},{"location":"Scientific_Computing/Supported_Applications/Synda/","title":"Synda","text":"","tags":[]},{"location":"Scientific_Computing/Supported_Applications/Synda/#what-is-synda","title":"What is synda?","text":"<p>Synda\u00a0is a command line tool to search and download files from the Earth System Grid Federation (ESGF) archive. Synda is a useful tool if you need to download Climate Model Intercomparison Project Phase 6 (CMIP6) data in particular. Synda supports different download protocols (e.g. HTTPS) and can download files in parallel.</p> <p>We'll describe the steps to install and use synda on Mahuika or Maui_ancil.</p>","tags":[]},{"location":"Scientific_Computing/Supported_Applications/Synda/#how-to-install-synda","title":"How to install synda","text":"<p>1. Load Anaconda3</p> <pre><code>module load Anaconda3\n</code></pre> <p>2. Create a conda environment and activate the environment</p> <pre><code>conda create --prefix=&lt;SYNDA_ENV&gt;\n</code></pre> <p>where &lt;SYNDA_ENV&gt; is a directory of your choice. Activate your new environment</p> <pre><code>conda activate &lt;SYNDA_ENV&gt;\n</code></pre> <p>3. Install synda (currently version 3.10)</p> <pre><code>conda install -c IPSL synda\n</code></pre> <p>4. Configure synda</p> <p>Set the ST_HOME environment variable and populate <code>$ST_HOME</code>, for instance</p> <pre><code>export ST_HOME=/nesi/nobackup/&lt;YOUR PROJECT&gt;/synda_home\n</code></pre> <p>Note: you may want to add the above \"export ST_HOME=&lt;...&gt;\" somewhere near the bottom of your ~/.bashrc file.</p> <p>To search and download climate model data you will in addition need to create an account on one or more of the ESGF nodes, e.g.\u00a0https://esgf-node.llnl.gov/projects/esgf-llnl/. This will require you to provide a user name (USER_NAME) and a password - you will receive an openID in return. Copy your openID string as you will need later.</p> <p>To configure and store your ESGF credentials, type</p> <pre><code>synda -h\nsynda check-env\n</code></pre> <p>Paste in the openID you received when creating your ESGF account. If you created an account on esgf-node.llnl.gov then your openID will be:</p> <pre><code>openID url: https://esgf-node.llnl.gov/esgf-idp/openid/USER_NAME\n</code></pre> <p>(USER_NAME is the user name you chose when creating the account). You can change your default configuration by editing $ST_HOME/conf/sdt.conf. I have</p> <pre><code>indexes = esgf-node.llnl.gov\ndefault_index = esgf-node.llnl.gov\n</code></pre>","tags":[]},{"location":"Scientific_Computing/Supported_Applications/Synda/#example","title":"Example","text":"<p>Find all the datasets for given ocean surface temperature (\"tos\"), variant label (\"r1i1p1f1\") produced by institution \"NOAA-GFDL\" for historical data and table Id \"Omon\":</p> <pre><code>synda search project=CMIP6 realm=ocean variable=tos variant_label=r1i1p1f1 institution_id=NOAA-GFDL table_id=Omon experiment_id=historical\n</code></pre> <p>This will return</p> <pre><code>new\u00a0 CMIP6.CMIP.NOAA-GFDL.GFDL-CM4.historical.r1i1p1f1.Omon.tos.gn.v20180701\nnew\u00a0 CMIP6.CMIP.NOAA-GFDL.GFDL-CM4.historical.r1i1p1f1.Omon.tos.gr.v20180701\nnew\u00a0 CMIP6.CMIP.NOAA-GFDL.GFDL-ESM4.historical.r1i1p1f1.Omon.tos.gn.v20190726\nnew\u00a0 CMIP6.CMIP.NOAA-GFDL.GFDL-ESM4.historical.r1i1p1f1.Omon.tos.gr.v20190726\n</code></pre> <p>Choose one of the datasets. To find out how big the dataset is, type:\u00a0</p> <pre><code>synda stat\u00a0CMIP6.CMIP.NOAA-GFDL.GFDL-ESM4.historical.r1i1p1f1.Omon.tos.gr.v20190726\n</code></pre> <p>which returns</p> <pre><code>Total files count: 9\nNew files count: 9\nTotal size: 262.8 MB\nNew files size: 262.8 MB\n</code></pre> <p>To download the dataset, type</p> <pre><code>synda get CMIP6.CMIP.NOAA-GFDL.GFDL-CM4.historical.r1i1p1f1.Omon.tos.gn.v2018070\n</code></pre> <p>The datasets (NetCDF files) will be downloaded locally in your directory.</p>","tags":[]},{"location":"Scientific_Computing/Supported_Applications/TensorFlow_on_CPUs/","title":"TensorFlow on CPUs","text":"<p>An open-source software library for Machine Intelligence</p> <p>TensorFlow Homepage</p>","tags":[]},{"location":"Scientific_Computing/Supported_Applications/TensorFlow_on_CPUs/#available-modules","title":"Available Modules","text":"Mahuika <p> 2.0.1-gimkl-2018b-Python-3.8.1 2.2.0-gimkl-2018b-Python-3.8.1 2.2.2-gimkl-2018b-Python-3.8.1 2.2.3-gimkl-2018b-Python-3.8.1 2.3.1-gimkl-2020a-Python-3.8.2 2.4.1-gimkl-2020a-Python-3.8.2 2.8.2-gimkl-2022a-Python-3.10.5 2.13.0-gimkl-2022a-Python-3.11.3 </p> <pre><code>module load TensorFlow/2.13.0-gimkl-2022a-Python-3.11.3</code></pre> <p>TensorFlow is a popular software library for machine learning applications, see our TensorFlow article for further information. It is often used with GPUs, as runtimes of the computationally demanding training and inference steps are often shorter compared to multicore CPUs. However, running TensorFlow on CPUs can nonetheless be attractive for projects where:</p> <ul> <li>Runtime is dominated by IO, so that computational performance of     GPUs does not provide much advantage with respect to overall runtime     and core-hour charges</li> <li>The workflow can benefit from parallel execution on many nodes with     large aggregated IO bandwidth (e.g., running an inference task on a     very large dataset, or training a large ensemble of models)</li> </ul> <p>Tests with a machine learning application based on the Inception v3 network for image classification\u00a0 using a Nvidia P100 GPU and 18 Intel Broadwell cores on Mahuika (1/2 node) resulted in the following GPU-vs-CPU speedups (based on full task runtimes including IO):</p> <ul> <li>4x for training</li> <li>2.6x for inference</li> </ul> <p>Keep in mind that these numbers will depend strongly on the application - they are only intended as an example.</p>","tags":[]},{"location":"Scientific_Computing/Supported_Applications/TensorFlow_on_CPUs/#choosing-the-right-python-package","title":"Choosing the right Python package","text":"<p>It is very important to choose the right TensorFlow package for optimal performance. Intel provide optimised TensorFlow packages with Intel oneDNN (previously called MKL-DNN) support for the conda package manager. It is not recommended to build your own package, unless you need a specific feature - if you do need to build TensorFlow yourself, make sure that you include oneDNN.</p> <p>All TensorFlow modules on Mahuika are GPU-optimised releases. To install a CPU-optimised TensorFlow release on Mahuika, run</p> <pre><code>module load Miniconda3\nconda create -p /nesi/project/&lt;project ID&gt;/conda_envs/tf_cpu tensorflow-mkl\nsource activate /nesi/project/&lt;project ID&gt;/conda_envs/tf_cpu\n</code></pre> <p>To install TensorFlow on M\u0101ui Ancil, run</p> <pre><code>module load Anaconda3\nconda create -p /nesi/project/&lt;project ID&gt;/conda_envs/tf_cpu tensorflow-mkl\nsource activate /nesi/project/&lt;project ID&gt;/conda_envs/tf_cpu\n</code></pre> <p>Conda will create a new environment in your project directory with an optimised CPU version of TensorFlow. You can choose a specific version as well using the syntax \"tensorflow-mkl==x.y.z\".</p> <p>When the installation is complete, import TensorFlow in Python as usual,</p> <pre><code>python -c \"import tensorflow\"\n</code></pre> <p>Important: It is safe to ignore warning messages of the kind \"The TensorFlow library was not compiled to use [...] instructions [...]\" at runtime - Intel oneMKL will automatically use optimal processor capabilities.</p>","tags":[]},{"location":"Scientific_Computing/Supported_Applications/TensorFlow_on_CPUs/#setting-up-slurm-on-mahuika","title":"Setting up Slurm on Mahuika","text":"<p>Runtime environment setup has a significant influence on performance. The following Slurm script should work well as a starting point for a TensorFlow job on a single Mahuika node:</p> <pre><code>#!/bin/bash -e\n\n#SBATCH --job-name=tensorflow\n#SBATCH --account=&lt;your NeSI account ID&gt;\n#SBATCH --time=&lt;overall runtime estimate&gt;\n#SBATCH --mem=&lt;overall memory consumption&gt;\n#SBATCH --partition=long\n#SBATCH --nodes=1\n#SBATCH --ntasks=1\n#SBATCH --cpus-per-task=&lt;number of threads&gt;\n#SBATCH --hint=nomultithread                    # No hyperthreading\n\n# Allow threads to transition quickly\nexport KMP_BLOCKTIME=0\n# Bind threads to cores\nexport KMP_AFFINITY=granularity=fine,compact,0,0\n\nmodule load Miniconda3\nsource activate /nesi/project/&lt;project ID&gt;/conda_envs/tf_cpu\nsrun python my_tensorflow_program.py\n</code></pre> <p>If you are unsure about setting up the memory and runtime parameters, have a look at our article Ascertaining job dimensions. Please also read the section on operator parallelisation below before you choose a number of CPUs.</p> <p>Environment variables \"KMP_BLOCKTIME\" and \"KMP_AFFINITY\" configure threading behaviour of the Intel oneDNN library. While these settings should work well for a lot of applications, it is worth trying out different setups (e.g., longer blocktimes) and compare runtimes. Please see our article on Thread Placement and Thread Affinity as well as this Intel article for further information and tips for improving peformance on CPUs.</p>","tags":[]},{"location":"Scientific_Computing/Supported_Applications/TensorFlow_on_CPUs/#setting-up-operator-parallelisation-in-tensorflow-1x","title":"Setting up operator parallelisation in TensorFlow 1.x","text":"<p>TensorFlow has the ability to execute a given operator using multiple threads (\"intra-operator parallelisation\"), as well as different operators in parallel (\"inter-operator parallelisation\"). Although TensorFlow will try and guess values for these parameters, it can be worth setting them up explicitly to maximise performance. Note that these instructions are only valid for TensorFlow 1.x.</p> <p>Insert the following code at the beginning of your program:</p> <pre><code>import os\n\n# Get number of threads from Slurm\nnumThreads = int(os.getenv('SLURM_CPUS_PER_TASK',1))\n\n# Set number of threads for inter-operator parallelism,\n# start with a single thread\nnumInterOpThreads = 1\n\n# The total number of threads must be an integer multiple\n# of numInterOpThreads to make sure that all cores are used\nassert numThreads % numInterOpThreads == 0\n\n# Compute the number of intra-operator threads; the number\n# of OpenMP threads for low-level libraries must be set to\n# the same value for optimal performance\nnumIntraOpThreads = numThreads // numInterOpThreads\nos.environ['OMP_NUM_THREADS'] = str(numIntraOpThreads)\n\n# Import TensorFlow after setting OMP_NUM_THREADS to make sure\n# that low-level libraries are initialised correctly\nimport tensorflow as tf\n\n# Configure TensorFlow\nconfig = tf.ConfigProto()\nconfig.inter_op_parallelism_threads = numInterOpThreads\nconfig.intra_op_parallelism_threads = numIntraOpThreads\ntf.Session(config=config)\n</code></pre> <p>It depends on your application how beneficial each operator parallelisation strategy is, so it is worth testing different configurations.</p>","tags":[]},{"location":"Scientific_Computing/Supported_Applications/TensorFlow_on_GPUs/","title":"TensorFlow on GPUs","text":"<p>An open-source software library for Machine Intelligence</p> <p>TensorFlow Homepage</p>","tags":["gpu"]},{"location":"Scientific_Computing/Supported_Applications/TensorFlow_on_GPUs/#available-modules","title":"Available Modules","text":"Mahuika <p> 2.0.1-gimkl-2018b-Python-3.8.1 2.2.0-gimkl-2018b-Python-3.8.1 2.2.2-gimkl-2018b-Python-3.8.1 2.2.3-gimkl-2018b-Python-3.8.1 2.3.1-gimkl-2020a-Python-3.8.2 2.4.1-gimkl-2020a-Python-3.8.2 2.8.2-gimkl-2022a-Python-3.10.5 2.13.0-gimkl-2022a-Python-3.11.3 </p> <pre><code>module load TensorFlow/2.13.0-gimkl-2022a-Python-3.11.3</code></pre> <p>TensorFlow is an open source library for machine learning. TensorFlow can train and run deep neural networks. It can also serve as a backend for other techniques requiring automatic differentiation and GPU acceleration.</p> <p>TensorFlow is callable from Python with the numerically intensive parts of the algorithms implemented in C++ for efficiency. This page focus on running TensorFlow with GPU support.</p> <p>See also</p> <ul> <li>To request GPU resources using <code>--gpus-per-node</code> option of Slurm,      see the GPU use on      NeSI      documentation page.</li> <li>To run TensorFlow on CPUs instead, have a look at our article      TensorFlow on      CPUs      for tips on how to configure TensorFlow and Slurm for optimal      performance.</li> </ul>","tags":["gpu"]},{"location":"Scientific_Computing/Supported_Applications/TensorFlow_on_GPUs/#use-nesi-modules","title":"Use NeSI modules","text":"<p>TensorFlow is available on Mahuika as an environment module</p> <pre><code>module load TensorFlow/2.4.1-gimkl-2020a-Python-3.8.2\n</code></pre> <p>Note this will automatically load the right versions of CUDA and cuDNN modules needed to run TensorFlow on GPUs.</p> <p>You can list the available versions of the module using:</p> <pre><code>module spider TensorFlow\n</code></pre> <p>To install additional Python packages for your project, you can either:</p> <ol> <li>install packages in your home folder,</li> <li>install packages in a dedicated Python virtual environment for your     project.</li> </ol> <p>The first option is easy but will consume space in your home folder and can create conflicts if you have multiple projects with different versions requirements. To install packages this way, you need to use<code>pip install --user</code>. For example, to install the SciKeras package:</p> <pre><code>pip install --user scikeras\n</code></pre> <p>The second option provides a better separation between projects. Additionally, it saves space in your home folder if you create your virtual environments in the project or nobackup folder. The following example illustrates how to create a virtual environment, activate it and install the SciKeras package in it with <code>pip</code>:</p> <pre><code>$ export PYTHONNOUSERSITE=1\n$ python3 -m venv --system-site-packages my_venv\n$ source my_venv/bin/activate\n(my_venv) $ pip install scikeras\n</code></pre> <p>where <code>my_venv</code> is the path of the virtual environment folder.</p> <p>The <code>--system-site-packages</code> option allows the virtual environment to access the TensorFlow package provided by the environment module previously loaded:</p> <pre><code>$ module load TensorFlow/2.4.1-gimkl-2020a-Python-3.8.2\n$ source my_venv/bin/activate\n(my_venv) $ python -c \"import tensorflow as tf; print(tf.__version__)\"\n[...]\n2.4.1\n</code></pre> <p>Don't forget to activate the virtual environment before calling your Python scripts in a Slurm submission script, using:</p> <pre><code>source &lt;path_to_virtual_environment&gt;/bin/activate\n</code></pre> <p>Virtual environment isolation</p> <p>Use <code>export PYTHONNOUSERSITE=1</code> to ensure that your virtual  environment is isolated from packages installed in your home folder  <code>~/.local/lib/python3.8/site-packages/</code>.</p>","tags":["gpu"]},{"location":"Scientific_Computing/Supported_Applications/TensorFlow_on_GPUs/#conda-environments","title":"Conda environments","text":"<p>As an alternative, you can also create conda environments to install a specific version of Python, TensorFlow and any additional packages required for your project. On Mahuika, use the Miniconda3 module:</p> <pre><code>export PYTHONNOUSERSITE=1\nmodule load Miniconda3/4.9.2\nconda create -p my_conda_venv python=3.8\n</code></pre> <p>Note that here we use the <code>-p</code> option to create the conda environment in a local <code>my_conda_venv</code> folder. Use a subfolder in your project or nobackup folder to save space in your home folder.</p> <p>Then activate the conda environment and install TensorFlow using <code>conda install</code> or <code>pip install</code>, depending on your preferences:</p> <pre><code>source $(conda info --base)/etc/profile.d/conda.sh  # if you didn't use \"conda init\" to set your .bashrc\nconda activate ./my_conda_venv\npip install tensorflow==2.5.0\n</code></pre> <p>To use TensorFlow on GPUs, you also need to load cuDNN/CUDA modules with the proper versions. See the official documentation about tested configurations for compatibilities. For example, Tensorflow 2.5.0 requires you to load the <code>cuDNN/8.1.1.33-CUDA-11.2.0</code> module:</p> <pre><code>module load cuDNN/8.1.1.33-CUDA-11.2.0  # for Tensorflow 2.5\n</code></pre> <p>You can list the available versions of cuDNN (and associated CUDA module) using:</p> <pre><code>module spider cuDNN\n</code></pre> <p>Please contact us at support@nesi.org.nz if you need a version not available on the platform.</p> <p>M\u0101ui Ancillary Nodes</p> <ul> <li>Load the Anaconda3 module instead of Miniconda3 to manipulate      conda environments: <pre><code>module load Anaconda3/2020.02-GCC-7.1.0\n</code></pre></li> <li>Use <code>module avail</code> to list available versions of modules, e.g.      <pre><code>module avail cuDNN\n</code></pre></li> </ul> <p>Additionnally, depending your version of TensorFlow, you may need to take into consideration the following:</p> <ul> <li>install the <code>tensorflow-gpu</code> Python package if your are using     TensorFlow 1,</li> <li>make sure to use a supported version of Python when creating the     conda environment (e.g. TensorFlow 1.14.0 requires Python 3.3 to     3.7),</li> <li>use <code>conda install</code> (not <code>pip install</code>) if your version of     TensorFlow relies on GCC 4.8 (TensorFlow &lt; 1.15).</li> </ul> <p>Tip</p> <p>Make sure to use <code>module purge</code> before loading Miniconda3, to ensure  that no other Python module is loaded and could interfere with your  conda environment.</p> <pre><code>module purge\nmodule load Miniconda3/4.9.2\nexport PYTHONNOUSERSITE=1\nsource $(conda info --base)/etc/profile.d/conda.sh  # if you didn't use \"conda init\" to set your .bashrc\nconda ...  # any conda commands (create, activate, install...)\n</code></pre>","tags":["gpu"]},{"location":"Scientific_Computing/Supported_Applications/TensorFlow_on_GPUs/#singularity-containers","title":"Singularity containers","text":"<p>You can use containers to run your application on the NeSI platform. We provide support for Singularity containers, that can be run by users without requiring additional privileges. Note that Docker containers can be converted into Singularity containers.</p> <p>For TensorFlow, we recommend using the official container provided by NVIDIA. More information about using Singularity with GPU enabled containers is available on the NVIDIA GPU Containers support page.</p>","tags":["gpu"]},{"location":"Scientific_Computing/Supported_Applications/TensorFlow_on_GPUs/#specific-versions-for-a100","title":"Specific versions for A100","text":"<p>Here are the recommended options to run TensorFlow on the A100 GPUs:</p> <ul> <li>If you use TensorFlow 1, use the TF1 container provided by     NVIDIA,     which comes with a version of TensorFlow 1.15 compiled specifically     to support the A100 GPUs (Ampere architecture). Other official     Python packages won't support the A100, triggering various crashes     and slowdowns.</li> <li>If you use TensorFlow 2, any version from 2.4 and above will provide     support for the A100 GPUs.</li> </ul>","tags":["gpu"]},{"location":"Scientific_Computing/Supported_Applications/TensorFlow_on_GPUs/#example-slurm-script","title":"Example Slurm script","text":"<p>In the following example, we will use the make_image_classifier provided by TensorFlow Hub to illustrate a training workflow. The example task consists in retraining the last layers of an already trained deep neural network in order to make it classify pictures of flowers. This type of task is known as \"transfer learning\".</p> <ol> <li> <p>Create a virtual environment to install the     <code>tensorflow-hub[make_image_classifier]</code> package:</p> <pre><code>module purge  # start from a clean environment\nmodule load TensorFlow/2.4.1-gimkl-2020a-Python-3.8.2\nexport PYTHONNOUSERSITE=1\npython3 -m venv --system-site-packages tf_hub_venv\nsource tf_hub_venv/bin/activate\npip install tensorflow-hub[make_image_classifier]~=0.12\n</code></pre> </li> <li> <p>Download and uncompress the example dataset containing labelled     photos of flowers (daisies, dandelions, roses, sunflowers and     tulips):</p> <pre><code>wget http://download.tensorflow.org/example_images/flower_photos.tgz -O - | tar -xz\n</code></pre> </li> <li> <p>Copy the following code in a job submission script named     <code>flowers.sl</code>:</p> <pre><code>#!/bin/bash -e\n#SBATCH --job-name=flowers-example\n#SBATCH --gpus-per-node=1\n#SBATCH --cpus-per-task=2\n#SBATCH --time 00:10:00\n#SBATCH --mem 4G\n\n# load TensorFlow module and activate the virtual environment\nmodule purge\nmodule load TensorFlow/2.4.1-gimkl-2020a-Python-3.8.2\nexport PYTHONNOUSERSITE=1\nsource tf_hub_venv/bin/activate\n\n# select a model to train, here MobileNetV2\nMODEL_URL=\"https://tfhub.dev/google/tf2-preview/mobilenet_v2/feature_vector/4\"\n\n# run the training script\nmake_image_classifier \\\n  --image_dir flower_photos \\\n  --tfhub_module \"$MODEL_URL\" \\\n  --image_size 224 \\\n  --saved_model_dir \"model-${SLURM_JOBID}\"\n</code></pre> </li> <li> <p>Submit the job:</p> <pre><code>sbatch flowers.sl\n</code></pre> </li> </ol> <p>Once the job has finished, the trained model will be saved in a <code>results-JOBID</code> folder, where <code>JOBID</code> is the Slurm job ID number.</p> <p>All messages printed by TensorFlow during the training, including training and validation accuracies, are captured in the Slurm output file, named <code>slurm-JOBID.out</code> by default.</p> <p>Tip</p> <p>While your job is running, you can monitor the progress of model  training using <code>tail -f</code> on the Slurm output file:  <pre><code>tail -f slurm-JOBID.out  # replace JOBID with an actual number\n</code></pre>  Press CTRL+C to get the bash prompt back.</p>","tags":["gpu"]},{"location":"Scientific_Computing/Supported_Applications/Trinity/","title":"Trinity","text":"<p>Trinity represents a novel method for the efficient and robust de novo reconstruction</p> <p>Trinity Homepage</p>","tags":[]},{"location":"Scientific_Computing/Supported_Applications/Trinity/#available-modules","title":"Available Modules","text":"Mahuika <p> 2.8.4-gimkl-2017a 2.8.4-gimkl-2018b 2.8.5-gimkl-2018b 2.8.6-gimkl-2020a 2.11.0-gimkl-2020a 2.13.2-gimkl-2020a 2.14.0-gimkl-2022a </p> <pre><code>module load Trinity/2.14.0-gimkl-2022a</code></pre> <p>Trinity, developed at the\u00a0Broad Institute\u00a0and the\u00a0Hebrew University of Jerusalem, performs de\u00a0novo reconstruction of transcriptomes from RNA-seq data. It combines three independent software modules: Inchworm, Chrysalis, and Butterfly, applied sequentially to process large volumes of RNA-seq reads. Trinity partitions the sequence data into many individual de Bruijn graphs, each representing the transcriptional complexity at a given gene or locus, and then processes each graph independently to extract full-length splicing isoforms and to tease apart transcripts derived from paralogous genes.</p> <p>General documentation for running Trinity can be found on their GitHub page here.</p>","tags":[]},{"location":"Scientific_Computing/Supported_Applications/Trinity/#running-trinity-on-nesi","title":"Running Trinity on NeSI","text":"<p>The recommended approach for running Trinity on NeSI is to split the run into two separate job submissions. The first submission will run Trinity Phase 1 (read clustering) and the second submission will run Trinity Phase 2 (assembling read clusters). We have observed faster run times and reduced core hour usage when applying this approach to benchmark data, compared to running both phases in one multithreaded job (see the Benchmarks section below).</p>","tags":[]},{"location":"Scientific_Computing/Supported_Applications/Trinity/#file-system-considerations","title":"File system considerations","text":"<p>You should run Trinity within your nobackup project directory, which has no limit on disk space usage but does have a\u00a0file count quota. Trinity creates a large number of files, particularly in the \"read_partitions\" directory, thus it is important that you Contact our Support Team\u00a0before running Trinity on NeSI, as we may need to increase your default file count quota.</p>","tags":[]},{"location":"Scientific_Computing/Supported_Applications/Trinity/#quality-control","title":"Quality Control","text":"<p>We must stress the importance of read QC prior to running the assembly otherwise it is likely to fail or take a very long time to complete.</p>","tags":[]},{"location":"Scientific_Computing/Supported_Applications/Trinity/#running-trinity-phase-1","title":"Running Trinity Phase 1","text":"<p>Trinity Phase 1 can be broken into three main components:</p> <ul> <li>Initial in silico normalisation step and kmer counting</li> <li>Inchworm</li> <li>Chrysalis</li> </ul> <p>So far we have found no reason to run each component individually since they have been observed to require similar resources. This phase typically has high memory requirements and supports multithreading in places.</p> <p>The following Slurm script is a template for running Trinity Phase 1</p> <p>Note\u00a0 :</p> <ul> <li><code>--cpus-per-task</code> and <code>--mem</code> defined in the following example are   just place holders.\u00a0</li> <li>Use a subset of your sample, run a test first to find the   suitable/required amount of CPUs and memory for your dataset</li> </ul> <pre><code>#!/bin/bash -e\n#SBATCH --job-name=trinity-phase1\n#SBATCH --account=nesi12345   # your NeSI project code\n#SBATCH --time=30:00:00       # maximum run time\n#SBATCH --ntasks=1            # always 1\n#SBATCH --cpus-per-task=16    # number of threads to use for Trinity\n#SBATCH --mem=220G            # maximum memory available to Trinity\n#SBATCH --hint=nomultithread  # disable hyper-threading\n\n# load a Trinity module\nmodule load Trinity/2.14.0-gimkl-2022a\n\n# run trinity, stop before phase 2\nsrun Trinity --no_distributed_trinity_exec \\\n  --CPU ${SLURM_CPUS_PER_TASK} --max_memory 200G \\\n  [your_other_trinity_options]\n</code></pre> <p>The extra Trinity arguments are:</p> <ul> <li><code>--no_distributed_trinity_exec</code> tells Trinity to stop before running   Phase 2</li> <li><code>--CPU ${SLURM_CPUS_PER_TASK}</code> tells Trinity to use the number of   CPUs specified by the sbatch option <code>--cpus-per-task</code> (i.e. you only   need to update it in one place if you change it)</li> <li><code>--max_memory</code> should be the same (or maybe slightly lower, so you   have a small buffer) than the value specified with the sbatch option   <code>--mem</code></li> <li><code>[your_other_trinity_options]</code> should be replaced with the other   trinity options you would usually use, e.g. <code>--seqType fq</code>, etc.</li> </ul>","tags":[]},{"location":"Scientific_Computing/Supported_Applications/Trinity/#running-trinity-phase-2","title":"Running Trinity Phase 2","text":"<p>Upstream documentation for running Trinity Phase 2 in parallel can be found here.</p> <p>Trinity Phase 2 performs all the mini-assemblies in parallel. This phase consists of a large number\u00a0(e.g. tens or hundreds of thousands) of commands that can be executed in parallel, each having independent inputs and outputs (embarrassingly parallel).</p> <p>By default, Phase 2 will be run in the same way as Phase 1, i.e. using multiple threads on a single compute node to work through the list of Phase 2 commands that need to be run. However, these commands are very I/O intensive and can easily saturate the I/O bandwidth of a single node. By utilising Trinity's \"grid mode\" we can distribute these commands across many compute nodes, accessing a higher total I/O bandwidth than is possible from a single node, which appears to improve performance considerably. We have installed HPC GridRunner (recommended in the Trinity documentation) to achieve this.</p> <p>HPC GridRunner runs a master process, submitted as its own Slurm job, that works by splitting the Phase 2 commands into batches of a certain size (specified by the user) and submits these batches of commands to the Slurm queue as separate jobs (referred to here as sub-jobs). The user can configure how many sub-jobs are allowed to be in the Slurm queue at any given time (so as not to overload the queue). Thus when a sub-job finishes, HPC GridRunner will submit another, maintaining the requested number of sub-jobs in the queue at any given time, until all commands have been run.</p> <p>An example configuration script for HPC GridRunner follows. This script was tested and worked with our benchmark case but some parameters may need to be adjusted, such as memory and time requirements. Note the Trinity documentation suggested each command will need a maximum of 1 GB memory, however we observed some commands spiking above 4 GB. This could vary depending on your inputs.</p> <pre><code>[GRID]\n# grid type:\ngridtype=SLURM\n\n# template for a grid submission\n# make sure:\n#     --partition is chosen appropriately for the resource requirements \n#       (here we choose either large or bigmem, whichever is available first)\n#     --ntasks and --cpus-per-task should always be 1\n#     --mem may need to be adjusted\n#     --time may need to adjusted\n#       (must be enough time for a batch of commands to finish)\n#     --account should be your NeSI project code\n#     add other sbatch options as required\ncmd=sbatch --partition=large,bigmem --mem=5G --ntasks=1 --cpus-per-task=1 --time=01:00:00 --account=nesi12345\n\n# note -e error.file -o out.file are set internally, so dont set them in the above cmd.\n\n#############################################################################\n# settings below configure the Trinity job submission system, not tied to the grid itself.\n#############################################################################\n\n# number of grid submissions to be maintained at steady state by the Trinity submission system\nmax_nodes=100\n\n# number of commands that are batched into a single grid submission job.\ncmds_per_node=100\n</code></pre> <p>The important details are:</p> <ul> <li><code>cmds_per_node</code> is the size of each batch of commands, i.e. here   each Slurm sub-job runs 100 commands and then exits</li> <li><code>max_nodes</code> is the number of sub-jobs that can be in the queue at   any given time (each sub-job is single threaded, i.e. it uses just   one core)</li> <li>name this file <code>SLURM.conf</code> in the directory you will submit the job   from</li> <li>memory usage may be low enough that the sub-jobs can be run on   either the large or bigmem partitions, which should improve   throughput compared to bigmem alone</li> </ul> <p>A template Slurm submission script for Trinity Phase 2 is shown below:</p> <pre><code>#!/bin/bash -e\n#SBATCH --job-name=trinity-phase2grid\n#SBATCH --account=nesi12345  # your NeSI project code\n#SBATCH --time=30:00:00      # enough time for all sub-jobs to complete\n#SBATCH --ntasks=1           # always 1 - this is the master process\n#SBATCH --cpus-per-task=1    # always 1\n#SBATCH --mem=20G            # memory requirements for master process\n#SBATCH --partition=bigmem   # submit to an appropriate partition\n#SBATCH --hint=nomultithread\n\n# load Trinity and HPC GridRunner\nmodule load Trinity/2.14.0-gimkl-2022a\nmodule load HpcGridRunner/20210803\n\n# run Trinity - this will be the master HPC GridRunner process that handles\n#   submitting sub-jobs (batches of commands) to the Slurm queue\nsrun Trinity --CPU ${SLURM_CPUS_PER_TASK} --max_memory 20G \\\n  --grid_exec \"hpc_cmds_GridRunner.pl --grid_conf ${SLURM_SUBMIT_DIR}/SLURM.conf -c\" \\\n  [your_other_trinity_options]\n</code></pre> <ul> <li>This assumes that you named the HPC GridRunner configuration script   SLURM.conf and placed it in the same directory that you submit this   job from</li> <li>The options <code>--CPU</code> and <code>--max_memory</code> aren't used by Trinity in   \"grid mode\" but are still required to be set (i.e. it shouldn't   matter what you set them to)</li> </ul>","tags":[]},{"location":"Scientific_Computing/Supported_Applications/Trinity/#benchmarks","title":"Benchmarks","text":"<p>Here we provide details of a number of Trinity assemblies that have been carried out on NeSI, in order to give a rough idea of how Trinity can perform on NeSI and an indication of its resource requirements.</p> <p>Timings mentioned here should be taken as indicative only and, even if assembling the same sample again, would be expected to vary significantly depending on various factors, such as the load on the system and project fair share scores and priorities.</p>","tags":[]},{"location":"Scientific_Computing/Supported_Applications/Trinity/#test-sample","title":"Test sample","text":"<p>We ran a small test job of 8 million paired reads. Although much smaller than usual this allowed us to quickly see the effect of making changes to the workflow and ran quickly enough that we could compare the default way of running Trinity to splitting the Trinity run into two phases and using \"grid mode\".</p> <p>Phase 1 took less than 1 hour to complete, using 8 cores and 10 GB memory. For Phase 2 we requested 8 GB memory for the master process and 4 GB memory and 1 hour wall time for the sub-jobs. There were\u00a0195,741 mini-assemblies to run in Phase 2.</p> <p>The table below summarises the timings for Phase 2, comparing the default, single node way to run Phase 2, to using Trinity's \"grid mode\".</p> Type of run Number of cores / grid specification Run time (hrs:mins:secs) Approximate core hour cost Single node (default) 16 cores 24:09:36 387 Grid max_nodes=20; cmds_per_node=500 07:59:58 168 Grid max_nodes=40; cmds_per_node=500 04:10:45 171 Grid max_nodes=60; cmds_per_node=500 02:36:58 160 Grid max_nodes=80; cmds_per_node=500 02:14:46 182 <p>This shows that performance is much better with Trinity's \"grid mode\". Not only are the run times significantly lower but the total number of core hours used is also much lower.</p>","tags":[]},{"location":"Scientific_Computing/Supported_Applications/Trinity/#marine-sediment-sample-1","title":"Marine sediment sample 1","text":"<p>This benchmark concerns the assembly of a marine sediment sample, containing two distinct microbial populations, from two distinct geographical locations, of approximately 286 million paired reads. The assembly was performed using the two-phase Trinity workflow discussed above, using those submission scripts as templates.</p> <p>Phase 1 ran on 18 threads with 220 GB memory on the bigmem partition and took approximately 15 hours to complete.</p> <p>For Phase 2, the master process ran on a single core with 20 GB memory on the bigmem partition. HPC GridRunner was configured with both <code>cmds_per_node</code> and <code>max_nodes</code> set to 100, with the sub-jobs running on either large or bigmem partitions and requesting 5 GB memory and 1 hour wall time each. The number of commands (mini-assemblies) that needed to be run during this phase was 2,020,460. Phase 2 took approximately 19 hours to complete (elapsed time) and cost around 1,800 core hours.</p>","tags":[]},{"location":"Scientific_Computing/Supported_Applications/Trinity/#marine-sediment-sample-2","title":"Marine sediment sample 2","text":"<p>This sediment sample, containing 303 million reads, was all from the same location and was taken from a treatment experiment. The assembly was performed using the two-phase Trinity workflow discussed above, using those submission scripts as templates.</p> <p>There were\u00a04,136,295 mini-assemblies to run in Phase 2. The master process requested 30 GB memory on the bigmem partition and HPC GridRunner was configured with both <code>cmds_per_node</code> and <code>max_nodes</code> set to 100. The sub-jobs ran on either the large or bigmem partitions and required 1 hour wall time and 5 GB memory each. Phase 2 took approximately 32 hours to complete (elapsed time) and cost around 3,100 core hours.</p>","tags":[]},{"location":"Scientific_Computing/Supported_Applications/TurboVNC/","title":"TurboVNC","text":"<p>TurboVNC is a derivative of VNC (Virtual Network Computing) that is tuned to provide</p> <p>TurboVNC Homepage</p>","tags":[]},{"location":"Scientific_Computing/Supported_Applications/TurboVNC/#available-modules","title":"Available Modules","text":"Mahuika <p> 2.2.3-GCC-7.4.0 2.2.4-GCC-9.2.0 </p> <pre><code>module load TurboVNC/2.2.4-GCC-9.2.0</code></pre>","tags":[]},{"location":"Scientific_Computing/Supported_Applications/TurboVNC/#setup","title":"Setup","text":"","tags":[]},{"location":"Scientific_Computing/Supported_Applications/TurboVNC/#0-ubuntu-only","title":"0. Ubuntu only","text":"<p>You will also need java runtime</p> <pre><code>sudo apt install -y openjdk-11-jre\u00a0\n</code></pre>","tags":[]},{"location":"Scientific_Computing/Supported_Applications/TurboVNC/#1-download","title":"1. Download","text":"<p>Download TurboVNC here.</p> <p>https://turbovnc.org/</p> <p>On Ubuntu, you can install the vnc-java package, e.g.:</p> <pre><code>sudo apt install vnc-java\u00a0\n</code></pre> <p>Do not use gvncviewer, as it doesn't allow to connect to a VNC server available over a nonstandard port.</p>","tags":[]},{"location":"Scientific_Computing/Supported_Applications/TurboVNC/#usage","title":"Usage","text":"","tags":[]},{"location":"Scientific_Computing/Supported_Applications/TurboVNC/#setting-a-turbovnc-password","title":"Setting a TurboVNC password","text":"<p>You can (and should) configure a TurboVNC password using the following commands:</p> <pre><code>module load TurboVNC\nvncpasswd\n</code></pre> <p>The password should be between six and eight characters long. A password shorter than six characters will be rejected; if the password is longer than eight characters, only the first eight characters will be considered. See\u00a0<code>man vncpasswd</code> for more details.</p> <p>The\u00a0<code>vncpasswd</code>\u00a0utility can also be used to set one-time use passwords, as well as view-only passwords that you can set up to give people access to your VNC session without allowing them to do things in your name.</p>","tags":[]},{"location":"Scientific_Computing/Supported_Applications/TurboVNC/#setting-up-the-server-on-the-cluster","title":"Setting up the server on the cluster","text":"<p>Mahuika and M\u0101ui each have two login nodes. Therefore, if you logged in to <code>login.mahuika.nesi.org.nz</code> (<code>mahuika</code>) or <code>login.maui.nesi.org.nz</code> (<code>maui</code>), note whether you are working on <code>mahuika01</code>, <code>mahuika02</code>, <code>maui01</code> or <code>maui02</code>.</p> <p>Having done that, run the following commands:</p> <pre><code>module load TurboVNC\nvncserver\n</code></pre> <p>Warning</p> <p>Do not use <code>-securitytypes none</code> as an argument to <code>vncserver</code>! If you  do so, anyone who has a cluster login and knows how to find a VNC  server in the list of processes can connect to your VNC server and  impersonate you. You are responsible for anything done on the  cluster under your user account.</p> <p>You will receive a message</p> <pre><code>Desktop 'TurboVNC: mahuika01: (&lt;username&gt;)' started on display mahuika01:1\n</code></pre> <p>Note the display number (highlighted red). Also work out the server port number, which is 5900 + the display number. That is, if the display number is 1, the port will be 5901; if the display number is 2, the port will be 5902; and so on.</p>","tags":[]},{"location":"Scientific_Computing/Supported_Applications/TurboVNC/#connecting-to-the-turbovnc-server-from-your-workstation","title":"Connecting to the TurboVNC server from your workstation","text":"","tags":[]},{"location":"Scientific_Computing/Supported_Applications/TurboVNC/#within-the-niwa-network-or-niwa-vpn","title":"Within the NIWA network (or NIWA VPN)","text":"<ol> <li> <p>Open the TurboVNC viewer:</p> <pre><code>vncviewer\n</code></pre> </li> <li> <p>Within the TurboVNC viewer, connect to the host and display number,     e.g. to\u00a0<code>mahuika01.mahuika.nesi.org.nz:1</code>. Alternatively, use the     host and port number:\u00a0<code>mahuika01.mahuika.nesi.org.nz::5901</code>\u00a0(note     the two colons between hostname and port number).</p> </li> </ol>","tags":[]},{"location":"Scientific_Computing/Supported_Applications/TurboVNC/#outside-the-niwa-network-and-not-on-the-niwa-vpn","title":"Outside the NIWA network, and not on the NIWA VPN","text":"<ol> <li> <p>Open an SSH tunnel through the lander node to the SSH port (22) on     the desired login node:</p> <pre><code># This command sets up local SSH port forwarding.\n# The command is of the form:\n# ssh -L &lt;local_port&gt;:&lt;destination_host&gt;:&lt;destination_port&gt; &lt;gateway_host&gt;\n#\n# We can't use aliases (mahuika, maui) or load-balancing hostnames\n# (login.mahuika.nesi.org.nz, login.maui.nesi.org.nz) because those run the risk\n# of connecting to the wrong host, e.g. connecting to mahuika02 when the TurboVNC\n# server is running on mahuika01.\n#\n# Also, the hostname of destination_host is as seen from gateway_host,\n# not from your local workstation.\n#\n# The choice of local port is yours, but you may find the following convention\n# useful:\n#\n# TurboVNC server on mahuika01 =&gt; local_port = destination_port + 10,000\n# TurboVNC server on mahuika02 =&gt; local_port = destination_port + 20,000\n# TurboVNC server on maui01 =&gt; local_port = destination_port + 30,000\n# TurboVNC server on maui02 =&gt; local_port = destination_port + 40,000\n#\n# Following this convention, for a connection via the SSH server on mahuika01,\n# such that the destination port is 22:\n#\n# local_port = 22 + 10000 = 10022\n#\nssh -L 10022:mahuika01.mahuika.nesi.org.nz:22 -N lander\n</code></pre> </li> <li> <p>In a new terminal open an SSH tunnel from the already open tunnel to     the desired TurboVNC port:</p> <pre><code># This command sets up local SSH port forwarding.\n# The command is of the form:\n# ssh -L &lt;local_port&gt;:&lt;destination_host&gt;:&lt;destination_port&gt; &lt;gateway_host&gt;\n#\n# The hostname of destination_host is as seen from gateway_host,\n# not from your local workstation. But with the above tunnel set up,\n# anything done on (for example) port 10022 on localhost is seen as if\n# it were done directly on mahuika01.\n#\n# The choice of local port is yours, but you may find the following convention\n# useful:\n#\n# TurboVNC server on mahuika01 =&gt; local_port = destination_port + 10,000\n# TurboVNC server on mahuika02 =&gt; local_port = destination_port + 20,000\n# TurboVNC server on maui01 =&gt; local_port = destination_port + 30,000\n# TurboVNC server on maui02 =&gt; local_port = destination_port + 40,000\n#\n# Following this convention, for a connection to a TurboVNC running on\n# display 1 on mahuika01, such that the destination port is 5901:\n#\n# local_port = 5901 + 10000 = 15901\n#\n# The rationale for not using local ports 5901, 5902 etc., is that we\n# want you to be able to run a VNC server on your own machine if you\n# wish. Using the same (local) port as a TurboVNC server would want\n# to use will potentially cause problems.\n#\n# Because the traffic is sent to port 10022 on localhost, which is\n# forwarded to port 22 on mahuika01, the first \"localhost\" (between\n# 15901 and 5901) is localhost as seen from mahuika01, i.e. it is\n# mahuika01. The second localhost is your local workstation. But\n# you have to use your NeSI Linux username, not your local Linux\n# username, to authenticate. Clear as mud?\nssh -L 15901:localhost:5901 -N -p 10022 -l my_nesi_linux_username localhost\n</code></pre> <p>As an alternative to steps 1 and 2, if using MobaXTerm in Windows, set up and then start port forwarding connections to look like this: </p> <ul> <li>The tunnel through the lander node must be started before the     tunnel through localhost can be started.</li> <li>The destination server for the tunnel through the lander node     must be the NeSI login node where your TurboVNC server is     running.</li> <li>The destination port for the second tunnel must be the port     corresponding to your display number:\u00a0<code>5901</code>\u00a0for display     1,\u00a0<code>5902</code>\u00a0for display 2, and so forth.</li> </ul> </li> <li> <p>Open the VNC viewer:</p> <ul> <li>From the Ubuntu command line:     <code>vncviewer localhost::&lt;local_port&gt;</code>\u00a0(e.g.     <code>vncviewer localhost::15901</code>)</li> <li>On Windows: Select TurboVNC Viewer from the Start menu (or use     an equivalent option), and enter <code>localhost::&lt;local_port&gt;</code> (e.g.     <code>vncviewer localhost::15901</code>) at the dialog</li> </ul> </li> <li> <p>If prompted for a password, click the button to enter an empty     password</p> </li> </ol>","tags":[]},{"location":"Scientific_Computing/Supported_Applications/TurboVNC/#putting-your-turbovnc-client-in-fullscreen-mode","title":"Putting your TurboVNC client in fullscreen mode","text":"<pre><code>Ctrl + Alt + Shift + F\n</code></pre>","tags":[]},{"location":"Scientific_Computing/Supported_Applications/TurboVNC/#stopping-the-client","title":"Stopping the client","text":"<p>Make sure you have closed your VNC Viewer session before stopping either SSH tunnel.</p> <p>Make sure the second SSH tunnel (the one to \"localhost\") is stopped before you close the first tunnel.</p>","tags":[]},{"location":"Scientific_Computing/Supported_Applications/TurboVNC/#stopping-the-server","title":"Stopping the server","text":"<ol> <li> <p>Go to your tmux session on the server, or (alternatively) go to or     open some other session on that server. If you use a different     session, you will have to load the TurboVNC module if it's not     already loaded.</p> </li> <li> <p>Remind yourself of your TurboVNC display number.</p> </li> <li> <p>Run the following command:</p> <pre><code># Example: vncserver -kill :1\nvncserver -kill :&lt;display_number&gt;\n</code></pre> </li> </ol>","tags":[]},{"location":"Scientific_Computing/Supported_Applications/TurboVNC/#finding-open-turbovnc-servers","title":"Finding open TurboVNC servers","text":"<p>If you've opened several TurboVNC servers, you may have lost track of some of them. The following command will help you find your TurboVNC server processes running on the current computer:</p> <pre><code>ps -f -u $(whoami) | grep Xvnc\n</code></pre> <p>This command will return results that look like the following:</p> <pre><code>my_username   55402      1  0 Feb04 ?        00:00:00 /opt/nesi/mahuika/TurboVNC/2.2.3-GCC-7.4.0/bin/Xvnc :2 -desktop TurboVNC: mahuika01:2 (my_username) -auth /home/my_username/.Xauthority -geometry 1240x900 -depth 24 -rfbwait 120000 -securitytypes none -rfbport 5902 -fp catalogue:/etc/X11/fontpath.d -deferupdate 1 -dridir /usr/lib64/dri -registrydir /usr/lib64/xorg\nmy_username  143972  77594  0 21:34 pts/18   00:00:00 grep --binary-files=without-match --color=auto Xvnc\n</code></pre> <p>The colon-number combination between the executable path (i.e. <code>/opt/nesi/mahuika/TurboVNC/2.2.3-GCC-7.4.0/bin/Xvnc</code> in the above example) and <code>-desktop</code> shows the display number. In this case the display number is 2.</p> <p>If you have several Xvnc processes open on that host, you can kill those you don't want to keep by means of the <code>kill</code> command, or alternatively by using the <code>vncserver</code> command described above.</p>","tags":[]},{"location":"Scientific_Computing/Supported_Applications/VASP/","title":"VASP","text":"<p>The Vienna Ab initio Simulation Package (VASP) is a computer program for atomic scale materials modelling, e.g. electronic structure calculations and quantum-mechanical molecular dynamics, from first principles.</p> <p>VASP Homepage</p> <p>Warning</p> <p>VASP is proprietary software. Make sure you meet the requirements for it's usage.</p>","tags":["mahuika","chemistry"]},{"location":"Scientific_Computing/Supported_Applications/VASP/#available-modules","title":"Available Modules","text":"Mahuika Maui <p> 4.6-gimkl-2017a 4.6-gimkl-2020a 5.3.5-intel-2017a-VTST-BEEF 5.3.5-intel-2017a 5.4.4-gimkl-2022a 5.4.4-intel-2017a 5.4.4-intel-2018b-VTST-sol 5.4.4-intel-2018b 5.4.4-intel-2020a 6.2.1-intel-2022a-NOOMP 6.2.1-NVHPC-22.3-GCC-11.3.0-CUDA-11.6.2 6.3.2-intel-2022a 6.3.2-NVHPC-22.3-GCC-11.3.0-CUDA-11.6.2 6.4.1-intel-2022a </p> <pre><code>module load VASP/6.4.1-intel-2022a</code></pre> <p> 5.4.4-CrayIntel-23.02-19-VTST-sol 5.4.4-CrayIntel-23.02-19 6.3.2-CrayIntel-23.02-19 </p> <pre><code>module load VASP/6.3.2-CrayIntel-23.02-19</code></pre>","tags":["mahuika","chemistry"]},{"location":"Scientific_Computing/Supported_Applications/VASP/#description","title":"Description","text":"<p>The Vienna Ab initio Simulation Package (VASP) is a programme for atomic scale materials modelling.</p> <p>VASP computes an approximate solution to the many-body Schr\u00f6dinger equation of a chemical system by either Density Functional Theory (and the KS equations), or Hartree-Fock.</p> <p>Periodic boundary conditions make VASP particularly useful for studying materials with bulk-like properties. VASP uses a basis set made up of plane-waves as opposed to atom centred basis sets, and describes core atomic charges with pseudopotentials.</p> <p>VASP can (among many other things) perform</p> <ul> <li>structural relaxation and calculation of forces between nuclei</li> <li>molecular dynamics</li> <li>magnetic moments</li> <li>partial\u00a0charges</li> <li>optical properties</li> </ul> <p>For more information, please visit the VASP home page at http://www.vasp.at.</p>","tags":["mahuika","chemistry"]},{"location":"Scientific_Computing/Supported_Applications/VASP/#availability","title":"Availability","text":"<p>VASP is currently available on the Mahuika and M\u0101ui clusters.</p>","tags":["mahuika","chemistry"]},{"location":"Scientific_Computing/Supported_Applications/VASP/#licensing-requirements","title":"Licensing requirements","text":"<p>VASP is made available to researchers under commercial licence agreements with individuals, research groups or institutions. Whether you have access to VASP, which versions you have access to, and under what conditions, will vary depending on where you work or study. You will only be permitted to access and use any given version of VASP on any NeSI cluster if you have a valid licence to use that version of VASP with your research group, employer, or institution, and if the terms of your licence permit cluster use.</p> <p>If you do have a valid license, please Contact our Support Team to gain access to the VASP executables. You may be asked to provide proof of your license if you are not from a known group or if the license is new.</p>","tags":["mahuika","chemistry"]},{"location":"Scientific_Computing/Supported_Applications/VASP/#best-practices-for-vasp-calculations","title":"Best practices for VASP calculations","text":"","tags":["mahuika","chemistry"]},{"location":"Scientific_Computing/Supported_Applications/VASP/#example-script","title":"Example script","text":"<pre><code>#!/bin/bash -e\n\n#SBATCH --job-name        MyVASPJob\n#SBATCH --time            01:00:00\n#SBATCH --nodes           1\n#SBATCH --ntasks          16               # start 16 MPI tasks or 16\n#SBATCH --mem             20G\n#SBATCH --hint            nomultithread\n\nmodule purge\nmodule load VASP/6.4.1-intel-2022a\n\n# Use the -K switch so that, if any of the VASP processes exits with an\n# error, the entire job will stop instead of continuing to waste core\n# hours on a defunct run.\nsrun -K vasp_std\n</code></pre>","tags":["mahuika","chemistry"]},{"location":"Scientific_Computing/Supported_Applications/VASP/#avoid-hyperthreading","title":"Avoid hyperthreading","text":"<p>We and several researchers have found that VASP doesn't behave well with hyperthreading, and will run at a third to a half of its expected speed. To disable hyperthreading, please use either</p> <pre><code>#SBATCH --hint nomultithread\n</code></pre> <p>or, equivalently,</p> <pre><code>#SBATCH --hint=nomultithread\n</code></pre> <p>as shown in our example script above.</p>","tags":["mahuika","chemistry"]},{"location":"Scientific_Computing/Supported_Applications/VASP/#how-many-cores-should-i-request","title":"How many cores should I request?","text":"<p>Unsurprisingly, the number of cores used in a VASP calculation has significant influence on how long the calculation takes to finish. The more cores a problem can parallelise over, the more it can do at once. However, this parallelisation carries with it some communication costs. Too many cores for too small a problem can decrease efficiency and speed (not to mention waste resources), as the cost of communicating tasks/threads becomes greater than the cost of the calculation itself.</p> <p>To determine an appropriate number of cores to request for a VASP calculation, it helps to know how VASP distributes its work. VASP parallelises its workload by allocating each KS orbital (enumerated by <code>NBANDS</code>) to the available cores in a round-robin fashion. In other words, each band is allocated to a single MPI task sequentially until all bands are assigned (more details on MPI are provided later). Each core must be allocated at least one orbital, and ideally will have \u201ca small integer\u201d (2-8) orbitals to work on. As a rule of thumb, requesting 1/8th the number of <code>NBANDS</code> is a reasonable starting point which hopefully achieves this 2 to 8 bands-per-core suggestion.</p> <p>Requesting cores is best done with <code>--ntasks</code> in your Slurm script. To determine the value of <code>NBANDS</code> (and other parameters such as the number of k-points), you can perform a \u201cdry run\u201d of your calculation by setting <code>ALGO=None</code> in the <code>INCAR</code>. This will set up the calculation, but does not go into solving ionic or electronic loops. The calculation will finish quickly, and you can then pull the value of <code>NBANDS</code> from the <code>OUTCAR</code>.</p>","tags":["mahuika","chemistry"]},{"location":"Scientific_Computing/Supported_Applications/VASP/#fill-a-node-before-working-across-nodes","title":"Fill a node before working across nodes","text":"<p>You may find that packing your entire job onto a single node will make it run faster. Distributing a job across multiple nodes may also put the job at greater risk of node failure. You can request a single node with <code>--nodes=1</code>for jobs of up to <code>--ntasks=128</code>, which is enough for most VASP calculations. If you would like help making a large job more efficient, please [contact our support team Contact our Support Team.</p>","tags":["mahuika","chemistry"]},{"location":"Scientific_Computing/Supported_Applications/VASP/#vasp-runs-faster-on-milan-nodes","title":"VASP runs faster on Milan nodes","text":"<p>Milan compute nodes are not only our most powerful compute nodes, but often have shorter queues! These nodes are still opt-in at the moment, meaning you need to specify <code>--partition=milan</code> in your Slurm script, which we strongly encourage everyone to do!</p>","tags":["mahuika","chemistry"]},{"location":"Scientific_Computing/Supported_Applications/VASP/#mpi-and-openmp-with-vasp","title":"MPI and OpenMP with VASP","text":"<p>VASP6 can now parallelise work using the message passing interface (MPI) and OpenMP at the same time. Where MPI parallelises on a per-orbital basis, OpenMP can create multiple threads of work nested within an MPI task, and so is a lower level, intra-node, form of communication. In other words, OpenMP parallelises/distributes the work of a single orbital. If you have large functions (for example, many plane waves and a small number of bands), you may wish to experiment with assigning multiple threads to each MPI task. This can be done by setting <code>--cpus-per-task=2</code> - which will start 2 OpenMP threads for everyone 1 MPI task.</p> <p>VASP may be further parallelised by treating k-points in parallel (controlled with <code>KPAR</code>), and parallelisation the FFTs (controlled with <code>NPAR</code> for VASP5 only). To optimise your VASP job parallelisation in these ways, see the following links:</p> <p>Basic parallisation</p> <p>Optimising the parallelisation</p>","tags":["mahuika","chemistry"]},{"location":"Scientific_Computing/Supported_Applications/VASP/#our-vasp5-modules-do-not-support-openmp","title":"Our VASP5 modules do not support OpenMP","text":"<p>Our VASP5 modules do not support OpenMP and therefore cannot use OpenMP threading. VASP5 can perform an analogous form of parrellisation using the <code>NPAR</code> tag in the <code>INCAR</code>, however. For more information on how to use <code>NPAR</code> see VASP\u2019s documentation page.</p>","tags":["mahuika","chemistry"]},{"location":"Scientific_Computing/Supported_Applications/VASP/#visualisation-with-atomic-simulation-environments-gui","title":"Visualisation with Atomic Simulation Environment's GUI","text":"<p>It is often helpful, or necessary, to visualise your <code>POSCAR</code>, <code>CONTCAR</code>, or other files that contain structural information. One easy way to do this is with the Atomic Simulation Environment's (ASE) GUI. ASE is a Python library that can script a wide variety of VASP tasks. In particular ASE's GUI can help visualise structures, set-up supercells, move atoms, generate <code>POSCAR</code>s, and much more. To use the GUI simply add our latest Python version to your environment with <code>module load Python</code>. Then, call the GUI with <code>ase-gui</code> (for a new structure), or <code>ase-gui &lt;structure_file&gt;</code> to open an existing structure. For more information on how to use ASE see their main page here.</p>","tags":["mahuika","chemistry"]},{"location":"Scientific_Computing/Supported_Applications/VASP/#which-vasp-environment-module-should-i-use","title":"Which VASP environment module should I use?","text":"<p>In general, unless you require otherwise for the sake of consistency with earlier work or you rely on a removed feature, we recommend the most recent version for which you have a license.</p> <p>We have previously used version suffixes such as \"-BEEF\" and \"-VTST\" to indicate the presence of various VASP extensions, but are now moving away from that as the number of such extensions has grown and we have not found any disadvantage in always including them.</p>","tags":["mahuika","chemistry"]},{"location":"Scientific_Computing/Supported_Applications/VASP/#vasp5","title":"VASP5","text":"<p>If you do not have a license to use VASP 6 then use either VASP/5.4.4-CrayIntel-23.02-19-VTST-sol on M\u0101ui, or VASP/5.4.4-intel-2020a on Mahuika. Despite the lack of a version suffix, this build \u00a0includes the BEEF, VTST and VASP-Sol extensions, and also CUDA versions of the VASP executables.</p> <p>Our testing suggests that OpenMP is not working in our VASP 5 builds, and so it doesn't pay to set <code>--cpus-per-task</code> greater than one.</p>","tags":["mahuika","chemistry"]},{"location":"Scientific_Computing/Supported_Applications/VASP/#vasp-6","title":"VASP 6","text":"<p>For most purposes we recommend VASP/6.3.2-intel-2022a on Mahuika, which includes the BEEF, VTST, VASP-Sol, DFT-D4, LIBXC, and HDF5 extensions, and has functioning OpenMP support.</p> <p>VASP6 has reimplemented its GPU support in such a way that it now makes sense to build the GPU version separately, so to try the latest GPU version of VASP, we have VASP/6.3.2-NVHPC-22.3-GCC-11.3.0-CUDA-11.6.2.</p> <p>On M\u0101ui our only VASP 6 environment module is VASP/6.3.2-CrayIntel-23.02-19.</p>","tags":["mahuika","chemistry"]},{"location":"Scientific_Computing/Supported_Applications/VASP/#extensions","title":"Extensions","text":"<p>There are a number of 3rd party extensions to VASP which we include. None of them affect VASP unless specified in your <code>INCAR</code> file.</p>","tags":["mahuika","chemistry"]},{"location":"Scientific_Computing/Supported_Applications/VASP/#vtst","title":"VTST","text":"<p>The VASP Transition State Tools, a third-party package for finding transition states and computing rate constants.</p>","tags":["mahuika","chemistry"]},{"location":"Scientific_Computing/Supported_Applications/VASP/#beef","title":"BEEF","text":"<p>Our recent non-CUDA VASP executables all include BEEF (Bayesian Error Estimation Functionals).</p>","tags":["mahuika","chemistry"]},{"location":"Scientific_Computing/Supported_Applications/VASP/#vasp-sol","title":"VASP-Sol","text":"<p>VASPsol is an implicit solvation model that describes the effect of electrostatics, cavitation, aa dispersion on the interaction between a solute and solvent into the plane-wave DFT</p>","tags":["mahuika","chemistry"]},{"location":"Scientific_Computing/Supported_Applications/VASP/#dft-d4","title":"DFT-D4","text":"<p>Building on the very popular D3 model, DFT-D4 is the newest update to the dispersion-corrected Dnsity Functional Theory (DFT-D4 Van der Waals functional. This method is available for VASP6 only and is applied by setting IVDW=13 in the INCAR. If dispersion forces are important in your calculation, DFT-D4 offers a good way of describing these.</p>","tags":["mahuika","chemistry"]},{"location":"Scientific_Computing/Supported_Applications/VASP/#libxc","title":"LIBXC","text":"<p>LIBXC is a library which contains over 400 functionals\u00a0from all rungs on Jacob's ladder. Each functional is assigned an integer number ID which can be found on this website.</p> <p>To use one of the LIBXC functionals, the following must be set in your <code>INCAR</code></p> <p>GGA=LIBXC</p> <p>LIBXC1 = [string] or [integer]</p> <p>and sometimes,</p> <p>LIBXC2\u00a0= [string] or [integer]</p> <p>As per the VASP documentation, \"LIBXC2can be used only if the functional specified withLIBXC1 corresponds to only exchange and not to exchange and correlation.\" For more information on correct usage of LIBXC please seeVASP's documentation on this.</p>","tags":["mahuika","chemistry"]},{"location":"Scientific_Computing/Supported_Applications/VASP/#which-vasp-executable-should-i-use","title":"Which VASP executable should I use?","text":"<p>VASP is unusual among scientific software packages in that some of its execution options are controlled neither by the nature of the input data, nor by command line flags, but by the executable itself. We offer a range of VASP executables, each built with a different set of compile-time options so that the resulting binary is optimised for a particular sort of problem.</p> <p>The different VASP executables are as follows:</p> Name Description <code>vasp_ncl</code> The most demanding VASP executable, suitable for non-collinear calculations (i.e., with spin-orbit coupling) <code>vasp_std</code> A VASP executable with intermediate memory demands, suitable for collinear calculations without spin-orbit coupling <code>vasp_gam</code> A VASP executable with low memory demands, suitable for gamma-point calculations <code>vasp_gpu</code> Like <code>vasp_std</code>, but with GPU support included. Only in VASP 5, because in VASP 6 the GPU build is a separate environment module. <code>vasp_gpu_ncl</code> Like <code>vasp_ncl</code>, but with GPU support included. Only in VASP 5, because in VASP 6 the GPU build is a separate environment module.","tags":["mahuika","chemistry"]},{"location":"Scientific_Computing/Supported_Applications/VASP/#openacc-gpu-version-of-vasp6","title":"OpenACC GPU version of VASP6","text":"<p>There is now an official OpenACC GPU version of VASP, replacing the older CUDA GPU version. See the official VASP documentation about the OpenACC version of VASP:</p> <p>https://www.vasp.at/wiki/index.php/OpenACC_GPU_port_of_VASP</p> <p>The *-NVHPC-* versions of the VASP modules on mahuika have been built with OpenACC support.</p> <p>VASP can run really well on GPUs, although how much you will benefit from GPUs largely depends on the specific simulation/calculation that you are running (simulation type, parameters, number of atoms, etc). Therefore, it could be useful to run some smaller benchmarks (e.g. reduced number of time steps) with different GPU and CPU-only configurations in your Slurm scripts, before moving on to run larger production simulations. When considering which configuration to use for production you should take into account performance and compute unit cost.</p> <p>General information about using GPUs on NeSI can be found here and details about the available GPUs on NeSI here.</p> <p>Here are some additional notes specific to running VASP on GPUs on NeSI:</p> <ul> <li>The command that you use to run VASP does not change - unlike     the previous CUDA version, which had a <code>vasp_gpu</code> executable,     with the OpenACC version the usual VASP executables (<code>vasp_std</code>,     <code>vasp_gam</code>, <code>vasp_ncl</code>) are all built with OpenACC GPU support     in the *-NVHPC-* modules, so just use those as usual</li> <li> <p>Always select one MPI process (Slurm task) per GPU, for example:</p> <ul> <li> <p>Running on 1 P100 GPU  </p> <pre><code>``` sl\n# snippet of Slurm script\n#SBATCH --nodes=1\n#SBATCH --ntasks-per-node=1  # 1 task per node as we set 1 GPU per node below\n#SBATCH --cpus-per-task=1\n#SBATCH --gpus-per-node=P100:1\n# end snippet\n```\n</code></pre> </li> <li> <p>Running on 4 HGX A100 GPUs on a single node  </p> <pre><code>``` sl\n# snippet of Slurm script\n#SBATCH --nodes=1\n#SBATCH --ntasks-per-node=4  # 4 tasks per node as we set 4 GPUs per node below\n#SBATCH --cpus-per-task=1\n#SBATCH --gpus-per-node=A100:4\n#SBATCH --partition=hgx  # required to get the HGX A100s instead of PCI A100s\n# end snippet\n```\n</code></pre> <ul> <li>Multiple threads per MPI process (--cpus-per-task) might be     beneficial for performance but you should start by setting this     to 1 to get a baseline</li> <li>VASP will scale better across multiple GPUs when they are all on     the same node compared to across multiple nodes</li> <li>if you see memory errors like     <code>call to cuMemAlloc returned error 2: Out of memory</code> you     probably ran out of GPU memory. You could try requesting more     GPUs (so the total amount of available memory is higher) and/or     moving to GPUs with more memory (note: GPU memory is distinct     from the usual memory you have to request for your job via     <code>#SBATCH --mem</code> or similar; when you are allocated a GPU you get     access to all the GPU memory on that device)<ul> <li>P100 GPUs have 12 GB GPU memory and you can have a maximum of 2 per node</li> <li>PCI A100 GPUs have 40 GB GPU memory and you can have a maximum of 2 per node</li> <li>HGX A100 GPUs have 80 GB GPU memory and you can have a maximum of 4 per node</li> </ul> </li> <li>the HGX GPUs have a faster interconnect between the GPUs within a single node; if using multiple GPUs you may get better performance with the HGX A100s than with the PCI A100s</li> <li>A100 GPUs have more compute power than P100s so will perform better if your simulation can take advantage of the extra power</li> </ul> </li> </ul> </li> </ul>","tags":["mahuika","chemistry"]},{"location":"Scientific_Computing/Supported_Applications/VTune/","title":"VTune","text":"<p>Intel VTune Amplifier XE is the premier performance profiler for C, C++, C#, Fortran,  Assembly and Java.</p> <p>VTune Homepage</p>","tags":[]},{"location":"Scientific_Computing/Supported_Applications/VTune/#available-modules","title":"Available Modules","text":"Mahuika Maui_ancil <p> 2019_update4 2019_update8 2023.1.0 </p> <pre><code>module load VTune/2023.1.0</code></pre> <p> 2019_update8 </p> <pre><code>module load VTune/2019_update8</code></pre>","tags":[]},{"location":"Scientific_Computing/Supported_Applications/VTune/#what-is-vtune","title":"What is VTune?","text":"<p>VTune is a tool that allows you to quickly identify where most of the execution time of a program is spent. This is known as profiling. It is good practice to profile a code before attempting to modify the code to improve its performance. VTune collects key profiling data and presents them in an intuitive way.\u00a0 Another tool that provides similar information is\u00a0ARM MAP.</p>","tags":[]},{"location":"Scientific_Computing/Supported_Applications/VTune/#how-to-use-vtune","title":"How to use VTune","text":"<p>We'll show how to profile a C++ code with VTune - feel free to choose your own code instead. Start with\u00a0</p> <pre><code>git clone https://github.com/pletzer/fidibench\n</code></pre> <p>and build the code using the \"gimkl\" tool chain</p> <pre><code>cd fidibench\nmkdir build\ncd build\nmodule load gimkl CMake\ncmake ..\nmake\n</code></pre> <p>This will compile a number of executables. Note that VTune does not require one to apply a special compiler switch to profile. You can profile an existing executable if you like. We choose \"upwindCxx\" as the executable to profile. It is under upwind/cxx, so</p> <pre><code>cd upwind/cxx\n</code></pre> <p>Run the executable with</p> <pre><code>module load VTune\nsrun --ntasks=1 --cpus-per-task=2 --hint=nomultithread amplxe-cl -collect hotspots -result-dir vtune-res ./upwindCxx -numCells 256 -numSteps 10\n</code></pre> <p>Executable\u00a0\"upwindCxx\" takes arguments \"-numCells 256\" (the number of cells in each dimension) and\u00a0 \"-numSteps 10\" (the number of time steps).\u00a0Note the command \"amplxe-cl -collect hotspots -result-dir\" which was inserted before the executable. The output may look like</p> <p>Top Hotspots</p> <p>Function\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 Module\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 CPU Time</p> <p>------------------------------------------\u00a0 --------------\u00a0 --------</p> <p>Upwind&lt;(unsigned long)3&gt;::advect._omp_fn.1\u00a0 upwindCxx\u00a0 \u00a0 \u00a0 \u00a0 25.979s</p> <p>_int_free \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 libc.so.6 \u00a0 \u00a0 \u00a0 \u00a0 9.170s</p> <p>operator new\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 libstdc++.so.6\u00a0 \u00a0 6.521s</p> <p>free\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 libmpi.so.12\u00a0 \u00a0 \u00a0 0.300s</p> <p>indicating that the vast majority of time is spent in the \"advect\" method (26s), with significant amounts of time spent allocating (6.5s) and deallocating (9.2s) memory.</p>","tags":[]},{"location":"Scientific_Computing/Supported_Applications/VTune/#drilling-further-into-the-code","title":"Drilling further into the code","text":"<p>Often this is enough to give you a feel for where the code can be improved. To explore further you can fire up</p> <pre><code>amplxe-gui &amp;\n</code></pre> <p>Go to the bottom and select \"Open Result...\", choose the directory where the profiling results are saved and click on the .amplxe file. The summary will look similar to the above table. However, you can now dive into selected functions to get more information. Below we see that 16.5 out of 26 seconds were spent starting the two OpenMP threads.</p>","tags":[]},{"location":"Scientific_Computing/Supported_Applications/VirSorter/","title":"VirSorter","text":"<p>VirSorter: mining viral signal from microbial genomic data.</p> <p>VirSorter Homepage</p>","tags":[]},{"location":"Scientific_Computing/Supported_Applications/VirSorter/#available-modules","title":"Available Modules","text":"Mahuika <p> 1.0.6-gimkl-2020a-Perl-5.30.1 2.1-gimkl-2020a-Python-3.8.2 2.2.3-gimkl-2020a-Python-3.8.2 </p> <pre><code>module load VirSorter/2.2.3-gimkl-2020a-Python-3.8.2</code></pre> <p>We have customised VirSorter slightly for the cluster environment:</p> <ul> <li>The number of jobs must be specified</li> <li>We default to <code>--skip-deps-install</code>and <code>--use-conda-off</code></li> </ul> <p>We don't provide the VirSorter databases, so you will have to run <code>virsorter setup</code>\u00a0first</p> <p>If you are doing many runs, it is good practice to use the <code>--rm-tmpdir</code>\u00a0option to\u00a0delete all the temporary files which VirSorter makes, and so reduce the risk of reaching your file quota.</p> <p>VirSorter defaults its LOCAL_SCRATCH configuration setting to \"/tmp\", but we would prefer that Slurm jobs instead use the automatically cleaned up per-job directory $TMPDIR. This is actually a Snakemake rule configuration, so can be set at the end of the virsorter command line.</p> <p>So an example which uses the SLURM_CPUS_PER_TASK and TMPDIR values provided to Slurm jobs would be:</p> <pre><code>module load VirSorter/2.1-gimkl-2020a-Python-3.8.2\nvirsorter run \\\n    --seqfile test.fasta \\\n    --jobs ${SLURM_CPUS_PER_TASK:-2} \\\n    --rm-tmpdir \\\n    all \\\n    --config LOCAL_SCRATCH=${TMPDIR:-/tmp}\n</code></pre>","tags":[]},{"location":"Scientific_Computing/Supported_Applications/WRF/","title":"WRF","text":"<p>The Weather Research and Forecasting (WRF) Model is a next-generation mesoscale numerical weather prediction system designed for both atmospheric research and operational forecasting applications. It features two dynamical cores, a data assimilation system, and a software architecture supporting parallel computation and system extensibility. The model serves a wide range of meteorological applications across scales from tens of meters to thousands of kilometres.</p> <p>Download WRF:</p> <pre><code>cd /nesi/project/&lt;your_project_code&gt;\nwget https://github.com/wrf-model/WRF/archive/v4.1.1.tar.gz\ntar xf v4.1.1.tar.gz\n</code></pre> <p>This guide is based on WRF 4.1.1 and WPS 4.2 (it was also tested with Polar WRF 4.1.1 but for that, you would need to apply the Polar WRF and WPS patches before compiling).</p> <p>WRF needs NetCDF, NetCDF-Fortran, PnetCDF and their dependencies to be installed. On M\u0101ui, these are available as modules. On Mahuika, we recommend to download these packages and build them by hand (instructions are provided below).</p>","tags":[]},{"location":"Scientific_Computing/Supported_Applications/WRF/#wrf-on-mahuika","title":"WRF on Mahuika","text":"","tags":[]},{"location":"Scientific_Computing/Supported_Applications/WRF/#environment-on-mahuika","title":"Environment on Mahuika","text":"<p>We'll use the Intel compiler and Intel MPI library.</p> <pre><code>module purge\nmodule load HDF5/1.12.2-iimpi-2022a\n</code></pre> <p>Although NeSI has NetCDF modules installed, WRF wants the C and Fortran NetCDF libraries, include files and modules all installed under the same root directory. Hence we build those by hand.</p>","tags":[]},{"location":"Scientific_Computing/Supported_Applications/WRF/#building-wrf-dependencies-on-mahuika","title":"Building WRF dependencies on Mahuika","text":"<p>Copy-paste the commands below to build WRF on Mahuika:</p> <pre><code># create build directory for WRF's dependencies\nexport WRF_DEPS_DIR=$PWD/wrf-deps\nmkdir $WRF_DEPS_DIR\ncd $WRF_DEPS_DIR\n\n# required for building and running\nexport LD_LIBRARY_PATH=$WRF_DEPS_DIR/lib:$LD_LIBRARY_PATH\n\n# to access the nc-config anf nf-config commands\nexport PATH=$WRF_DEPS_DIR/bin:$PATH\n\n# set the compilers\nexport CC=icc\nexport CXX=icc\nexport FC=ifort\nexport F77=ifort\nexport MPICC=mpiicc\nexport MPICXX=mpiicc\nexport MPIF77=mpiifort\nexport MPIF90=mpiifort\n\n# now install the dependencies\n\n# netcdf-c\nwget https://downloads.unidata.ucar.edu/netcdf-c/4.9.2/netcdf-c-4.9.2.tar.gz\ntar xf netcdf-c-4.9.2.tar.gz\ncd netcdf-c-4.9.2/\n./configure --prefix=$WRF_DEPS_DIR\nmake &amp; make install\ncd ..\n\n# netcdf-fortran\nwget https://downloads.unidata.ucar.edu/netcdf-fortran/4.6.1/netcdf-fortran-4.6.1.tar.gz\ntar xf netcdf-fortran-4.6.1.tar.gz\ncd netcdf-fortran-4.6.1\nCPPFLAGS=\"$(../bin/nc-config --cflags)\" LDFLAGS=\"$(../bin/nc-config --libs)\" \\\n./configure --prefix=$WRF_DEPS_DIR\nmake &amp; make install\ncd ..\n\n# pnetcdf\nwget https://parallel-netcdf.github.io/Release/pnetcdf-1.12.3.tar.gz\ntar xf pnetcdf-1.12.3.tar.gz\ncd pnetcdf-1.12.3\n./configure --prefix=$WRF_DEPS_DIR --with-netcdf4=$WRF_DEPS_DIR\nmake &amp; make install\ncd ..\n\n# back to the top directory\ncd ..\n</code></pre> <p>Then proceed to configure WRF by setting</p> <pre><code>export NETCDF=$WRF_DEPS_DIR\nexport HDF5=$WRF_DEPS_DIR\nexport PNETCDF=$WRF_DEPS_DIR\n</code></pre> <pre><code>cd WRF-4.1.1\n./configure\n[select 15]\n</code></pre> <p>and build the code with</p> <pre><code>./compile em_real &gt;&amp; log.compile\n</code></pre> <p>This may take several hours to compile. Check the log file to ensure that the compilation was successful.</p>","tags":[]},{"location":"Scientific_Computing/Supported_Applications/WRF/#running-wrf-on-mahuika","title":"Running WRF on Mahuika","text":"<p>An example Slurm script for running WRF on Mahuika extension, which can be submitted with sbatch name_of_script.sl:</p> <pre><code>#!/bin/bash -e\n#SBATCH --job-name=wrf\n#SBATCH --time=01:00:00\n#SBATCH --ntasks=240\n#SBATCH --hint=nomultithread\n#SBATCH --partition=milan\n\nmodule purge\nmodule load HDF5/1.12.2-iimpi-2022a\nmodule list\nexport LD_LIBRARY_PATH=$WRF_DEPS_DIR/lib:$LD_LIBRARY_PATH\n\n# run real\nsrun --output=real.log ./real.exe\n\n# run wrf\nsrun --output=wrf.log ./wrf.exe\n</code></pre>","tags":[]},{"location":"Scientific_Computing/Supported_Applications/WRF/#wrf-on-maui","title":"WRF on M\u0101ui","text":"","tags":[]},{"location":"Scientific_Computing/Supported_Applications/WRF/#building-wrf","title":"Building WRF","text":"<p>Load environment modules and set up the build environment (here we will build with the Intel compiler):</p> <pre><code># load PrgEnv-intel (this assumes you currently have PrgEnv-cray loaded)\nmodule switch PrgEnv-cray PrgEnv-intel\n\n# load dependencies\nmodule load cray-netcdf cray-hdf5 cray-parallel-netcdf\n\n# the following two lines are optional and may improve performance\nmodule switch craype-x86-skylake craype-broadwell\nmodule load craype-hugepages2M\n\nexport NETCDF=$NETCDF_DIR\nexport HDF5=$HDF5_DIR\nexport PNETCDF=$PNETCDF_DIR\n</code></pre> <p>Apply patches for Polar WRF if required and then configure WRF:</p> <pre><code>./configure\n# choose option 50 - INTEL (ftn/icc) Cray XC (dmpar)\n# choose an appropriate nesting option or leave it at the default\n</code></pre> <p>The configure script takes some options, for example, to configure for a debug build (no optimisations and debug symbols enabled) you could run ./configure -d instead.</p> <p>The configure script writes out a configure.wrf file which can also be edited manually if desired. For example, if you want an optimised build with debug symbols included, you could add -g to the FCDEBUG variable.</p> <p>Next compile WRF:</p> <pre><code>./compile em_real &gt;&amp; log.compile\n</code></pre> <p>It is important to check the output in log.compile as the command will not exit with an error if it fails. Check the end of the log file for a message saying compilation was successful. You could also check the timestamps of the executables in the main subdirectory (main/wrf.exe, main/real.exe, etc).</p>","tags":[]},{"location":"Scientific_Computing/Supported_Applications/WRF/#running-wrf","title":"Running WRF","text":"<p>An example Slurm script for running WRF on M\u0101ui, which can be submitted with sbatch name_of_script.sl:</p> <pre><code>#!/bin/bash -e\n#SBATCH --job-name=wrf\n#SBATCH --time=01:00:00\n#SBATCH --ntasks=240\n#SBATCH --hint=nomultithread\n\n# important, required on Maui\nexport HDF5_USE_FILE_LOCKING=FALSE\n\n# optional, may improve performance\nmodule load craype-hugepages2M\n\n# run real\nsrun --output=real.log ./real.exe\n\n# run wrf\nsrun --output=wrf.log ./wrf.exe\n</code></pre>","tags":[]},{"location":"Scientific_Computing/Supported_Applications/WRF/#parallel-netcdf","title":"Parallel netCDF","text":"<p>For some model configurations enabling parallel IO can be beneficial. By default WRF uses serial netCDF IO, which can be verified by looking in the namelist.input file for the io_form_* variables (these will mostly be in the time_control section of the namelist). If these variables are set to \"2\" then they will use serial netCDF. Setting them to 11 will use parallel netCDF instead. It is also recommended (possibly required) to set nocolons = .true. in the time_control section of the namelist when using parallel IO.</p>","tags":[]},{"location":"Scientific_Computing/Supported_Applications/WRF/#building-wps","title":"Building WPS","text":"<p>Download WPS:</p> <pre><code>wget https://github.com/wrf-model/WPS/archive/v4.2.tar.gz\ntar xf v4.2.tar.gz\ncd WPS-4.2\n</code></pre> <p>Load environment modules and set up the build environment (here we will build with the Intel compiler). We will build WPS using dynamic linking so that we can link against the system JasPer shared library.</p> <pre><code># load PrgEnv-intel (this assumes you currently have PrgEnv-cray loaded)\nmodule switch PrgEnv-cray PrgEnv-intel\n\n# load dependencies\nmodule load cray-netcdf cray-hdf5 cray-parallel-netcdf\n\n# the following two lines are optional and may improve performance\nmodule switch craype-x86-skylake craype-broadwell\nmodule load craype-hugepages2M\n\nexport NETCDF=$NETCDF_DIR\nexport HDF5=$HDF5_DIR\nexport PNETCDF=$PARALLEL_NETCDF_DIR\nexport JASPERLIB=/usr/lib/libjasper.so.1\nexport JASPERINC=/usr/include/jasper/jasper.h\nexport WRF_DIR=../WRF-4.1.1\nexport CRAYPE_LINK_TYPE=dynamic\n</code></pre> <p>Configure WPS:</p> <pre><code>./configure\n# choose option 39 - Cray XC Intel parallel build\n</code></pre> <p>This creates the configure.wps file, which can be edited in a similar way to configure.wrf above, if desired.</p> <p>Now compile WPS:</p> <pre><code>./compile &gt;&amp; log.compile\n</code></pre> <p>Check that the executables were successfully created: geogrid.exe, ungrib.exe and metgrid.exe.</p>","tags":[]},{"location":"Scientific_Computing/Supported_Applications/WRF/#running-wps","title":"Running WPS","text":"<p>Both geogrid and metgrid are parallel applications and can be run on M\u0101ui compute nodes. However, ungrib is serial and should not be run on a compute node unless it is very quick to finish. Alternatively you could run ungrib on an interactive/login node if it will not take up many resources, or you could compile WRF and WPS on a M\u0101ui Ancillary node and run it there.</p> <p>Note that WPS does a lot of file IO and therefore probably won't scale up to as many processes as WRF.</p> <p>This example shows running geogrid in a Slurm job:</p> <pre><code>#!/bin/bash -e\n#SBATCH --job-name=wps\n#SBATCH --time=01:00:00\n#SBATCH --ntasks=40\n#SBATCH --hint=nomultithread\n\n# need to specify modules as we linked dynamically\nmodule unload PrgEnv-cray PrgEnv-intel PrgEnv-gnu\nmodule load PrgEnv-intel cray-netcdf cray-hdf5 cray-parallel-netcdf\n\n# the following two lines are optional and may improve performance\nmodule switch craype-x86-skylake craype-broadwell\nmodule load craype-hugepages2M\n\n# important, required on Maui\nexport HDF5_USE_FILE_LOCKING=FALSE\n\n# run geogrid\nsrun --output=geogrid.log ./geogrid.exe\n</code></pre>","tags":[]},{"location":"Scientific_Computing/Supported_Applications/ipyrad/","title":"ipyrad","text":"<p>ipyrad is an interactive toolkit for assembly and analysis of restriction-site associated genomic</p> <p>ipyrad Homepage</p>","tags":[]},{"location":"Scientific_Computing/Supported_Applications/ipyrad/#available-modules","title":"Available Modules","text":"Mahuika <p> 0.9.61-gimkl-2020a-Python-3.8.2 0.9.81-Miniconda3 0.9.85-gimkl-2022a-Python-3.10.5 </p> <pre><code>module load ipyrad/0.9.85-gimkl-2022a-Python-3.10.5</code></pre>","tags":[]},{"location":"Scientific_Computing/Supported_Applications/ipyrad/#description","title":"Description","text":"<p>ipyrad, an interactive assembly and analysis toolkit for restriction-site associated DNA (RAD-seq) and related data types. Please explore the documentation to find out more about the features of ipyrad.\\</p> <p>Home page is at https://ipyrad.readthedocs.io/en/latest/index.html</p>","tags":[]},{"location":"Scientific_Computing/Supported_Applications/ipyrad/#cite-the-manuscript","title":"Cite the Manuscript","text":"<p>Eaton DAR &amp; Overcast I. \"ipyrad: Interactive assembly and analysis of RADseq datasets.\" Bioinformatics (2020).</p>","tags":[]},{"location":"Scientific_Computing/Supported_Applications/ipyrad/#license","title":"License","text":"<p>GPLv3</p>","tags":[]},{"location":"Scientific_Computing/Supported_Applications/ipyrad/#getting-started","title":"Getting Started","text":"<p>Following example uses\u00a0 rad_example which can be downloaded as per instructions on\u00a0 https://ipyrad.readthedocs.io/en/latest/tutorial_advanced_cli.html </p> <pre><code>curl -LkO https://eaton-lab.org/data/ipsimdata.tar.gz\ntar -xvzf ipsimdata.tar.gz\n</code></pre> <p>Start by creating a new Assembly\u00a0 <code>data1</code>\u00a0 , and then we\u2019ll edit the params file to tell it how to find the input data files for this data set.</p> <pre><code>module purge\nmodule load ipyrad/0.9.85-gimkl-2022a-Python-3.10.5\nipyrad -n data1\n\nNew file 'params-data1.txt' created in ........\n</code></pre> <p><code>params-data1.txt</code> will be created on current working directory. Review and edit the paths in parameter file to match the destinations of input data, barcode paths,etc.</p>","tags":[]},{"location":"Scientific_Computing/Supported_Applications/ipyrad/#slurm-script-for-using-multiple-cpus-a-single-compute-node","title":"Slurm Script for Using Multiple CPUs a Single Compute Node","text":"<pre><code>#!/bin/bash\n\n#SBATCH --account       nesi12345\n#SBATCH --job-name      ipyrad\n#SBATCH --cpus-per-task 12\n#SBATCH --time          00:05:00\n#SBATCH --mem           10G\n#SBATCH --output        ipyrad_output_%j.txt\n\n## assembly name\nassembly_name=\"data1\"\n\n## load environment and module\nmodule purge\nmodule load ipyrad/0.9.85-gimkl-2022a-Python-3.10.5\n\n## create, prepare and change to a job specific dir\njobdir=\"ipyrad_${SLURM_JOB_ID}\"\nparams=\"params-${assembly_name}.txt\"\n\nmkdir $jobdir\nsed \"s#$(pwd) #$(pwd)/$jobdir#\" $params &gt; $jobdir/$params\ncd $jobdir\n\n\n## call ipyrad on your params file and perform 7 steps from the workflow\nsrun ipyrad -p $params -s 12 --force \n</code></pre>","tags":[]},{"location":"Scientific_Computing/Supported_Applications/ont-guppy-gpu/","title":"ont-guppy-gpu","text":"<p>Data processing toolkit that contains the Oxford Nanopore Technologies' basecalling algorithms,</p> <p>ont-guppy-gpu Homepage</p>","tags":[]},{"location":"Scientific_Computing/Supported_Applications/ont-guppy-gpu/#available-modules","title":"Available Modules","text":"Mahuika <p> 3.4.4 3.6.0 4.2.2 4.5.3 5.0.7 5.0.16 6.0.1 6.1.2 6.2.1 6.4.2 6.4.6 6.5.7 </p> <pre><code>module load ont-guppy-gpu/6.5.7</code></pre>","tags":[]},{"location":"Scientific_Computing/Supported_Applications/ont-guppy-gpu/#description","title":"Description","text":"<p>Guppy,\u00a0 is a data processing toolkit that contains Oxford Nanopore\u2019s (https://nanoporetech.com/) basecalling algorithms, and several bioinformatic post-processing features, such as barcoding/demultiplexing, adapter trimming, and alignment. The Guppy toolkit also performs modified basecalling (5mC, 6mA and CpG) from the raw signal data, producing an additional FAST5 file of modified base probabilities.</p>","tags":[]},{"location":"Scientific_Computing/Supported_Applications/ont-guppy-gpu/#license-and-disclaimer","title":"License and Disclaimer","text":"<p>\u201cBase Caller Software\u201d shall mean Oxford\u2019s proprietary software, including all functional specifications associated therewith made available to the Oxford Group\u2019s customers on the Oxford Group\u2019s websites, as amended from time to time (the \u201cBase Caller Documentation\u201d), designed to convert certain Instrument Data to Biological Data, as may be made available to Customers by Oxford, whether free of charge or for a fee.</p> <p>Guppy is available to ONT customers via their community website https://community.nanoporetech.com/</p>","tags":[]},{"location":"Scientific_Computing/Supported_Applications/ont-guppy-gpu/#example-slurm-script","title":"Example Slurm script","text":"<ul> <li>Following Slurm script is a template to run Basecalling on NVIDIA     P100 GPUs.( We do not recommend running Guppy jobs on CPUs )</li> <li><code>--device auto</code> will automatically pick up the GPU over CPU</li> <li>Also,\u00a0 NeSI Mahuika cluster can provide A100 GPUs\u00a0 which can be 5-6     times faster than P100 GPUs for Guppy Basecalling with\u00a0 version. 5     and above. This can be requested with     <code>#SBATCH --gpus-per-node A100:1</code> variable</li> <li>Config files are stored in     /opt/nesi/CS400_centos7_bdw/ont-guppy-gpu/(version)/data/\u00a0***     with read permissions to all researchers (replace (version)***     with the version of the module)</li> </ul> <pre><code>#!/bin/bash -e\n\n#SBATCH --account        nesi12345\n#SBATCH --job-name       ont-guppy-gpu_job\n#SBATCH --gpus-per-node  A100:1\n#SBATCH --mem            6G\n#SBATCH --cpus-per-task  4\n#SBATCH --time           10:00:00\n#SBATCH --output         slurmout.%j.out\n\nmodule purge\nmodule load ont-guppy-gpu/6.4.2\n\nguppy_basecaller -i /path/to/input/data/directory -s /path/to/save/fastq/files \\\n--config /opt/nesi/CS400_centos7_bdw/ont-guppy-gpu/6.4.2/data/nameof.cfg \\\n--device auto --recursive --records_per_fastq 4000 \\\n--calib_detect --calib_reference lambda_3.6kb.fasta --detect_mid_strand_adapter\n</code></pre>","tags":[]},{"location":"Scientific_Computing/Supported_Applications/snpEff/","title":"snpEff","text":"<p>SnpEff is a variant annotation and effect prediction tool.</p> <p>snpEff Homepage</p>","tags":[]},{"location":"Scientific_Computing/Supported_Applications/snpEff/#available-modules","title":"Available Modules","text":"Mahuika <p> 4.2 4.3t 5.0-Java-11.0.4 </p> <pre><code>module load snpEff/5.0-Java-11.0.4</code></pre> <p>snpEff is a genetic variant annotation, and functional effect prediction tool.</p>","tags":[]},{"location":"Scientific_Computing/Supported_Applications/snpEff/#configuration-file","title":"Configuration File","text":"<p>snpEff requires a one-off configuration of the <code>.config</code> file. The following instructions are a one-off set up of the configuration file required for snpEff.</p> <ol> <li> <p>Load the latest version of the <code>snpEff</code> module.</p> </li> <li> <p>Make a copy of the snpEff config file, replacing    &lt;project_id&gt;, with your project ID.</p> <pre><code>cp $EBROOTSNPEFF/snpEff.config /nesi/project/&lt;project_id&gt;/my_snpEff.config\n</code></pre> </li> <li> <p>Open the<code>my_snpEff.config</code> file, and edit line 17 from the top    to point to a preferred path within your project directory or home    directory, e.g., edit line 17 <code>data.dir = ./data/</code> to something    like:<code>data.dir =/nesi/project/&lt;project_id&gt;</code>    Please note that you must have read and write permissions to this    directory.</p> </li> <li> <p>Run <code>snpEff.jar</code> using the <code>-c</code> flag to point to your new config    file, e.g., <code>-c path/to/snpEff/my_snpEff.config</code> For example:</p> <pre><code>java -jar $EBROOTSNPEFF/snpEff.jar -c /nesi/project/&lt;project_id&gt;/my_snpEff.config\n</code></pre> </li> </ol>","tags":[]},{"location":"Scientific_Computing/Supported_Applications/snpEff/#example-script","title":"Example Script","text":"<p>You will need to set up your configuration file before you run snpEff.</p> <pre><code>#!/bin/bash -e\n\n#SBATCH --account        nesi12345\n#SBATCH --job-name       my_snp_EFF_job\n#SBATCH --time           20:00\n#SBATCH --memory         4G\n#SBATCH --output         %x_%j.out   # Name output file according to job name with Job ID\n\n# purge all other modules that have already been loaded\nmodule purge\n\n# load specific snpEff version\nmodule load snpEff/5.0-Java-11.0.4\n\n# bring up help menu\njava -jar $EBROOTSNPEFF/snpEff.jar -h\n\n# run snpEff\njava -jar $EBROOTSNPEFF/snpEff.jar -c /nesi/project/&lt;project_id&gt;/my_snpEff.config &lt;other flags&gt;\n</code></pre>","tags":[]},{"location":"Scientific_Computing/Terminal_Setup/Git_Bash_Windows/","title":"Git Bash (Windows)","text":"<p>Warning</p> <p>This page has been automatically migrated and may contain formatting errors.</p> <p>Prerequisite</p> <ul> <li>Have a NeSI      account.</li> <li>Be a member of an active      project.</li> </ul>","tags":[]},{"location":"Scientific_Computing/Terminal_Setup/Git_Bash_Windows/#first-time-setup","title":"First time setup","text":"<p>Git Bash can be downloaded as part of Git here.</p> <p>The login process can be simplified with a few configurations.</p> <ol> <li> <p>Open Git Bash and run\u00a0<code>nano ~/.ssh/config</code>\u00a0to open your ssh config     file and add\u00a0the following\u00a0(replacing\u00a0<code>&lt;username&gt;</code>\u00a0with your     username):</p> <pre><code>Host mahuika\n   User &lt;username&gt;\n   Hostname login.mahuika.nesi.org.nz\n   ProxyCommand ssh -W %h:%p lander\n   ForwardX11 yes\n   ForwardX11Trusted yes\n   ServerAliveInterval 300\n   ServerAliveCountMax 2\n\nHost maui\n   User &lt;username&gt;\n   Hostname login.maui.nesi.org.nz\n   ProxyCommand ssh -W %h:%p lander\n   ForwardX11 yes\n   ForwardX11Trusted yes\n   ServerAliveInterval 300\n   ServerAliveCountMax 2\n\nHost lander\n   User &lt;username&gt;\n   HostName lander.nesi.org.nz\n   ForwardX11 yes\n   ForwardX11Trusted yes\n   ServerAliveInterval 300\n   ServerAliveCountMax 2\n\nHost *\n   ControlMaster auto\n   ControlPersist 1\n</code></pre> <p>Close and save with ctrl x, y, Enter</p> </li> <li> <p>Ensure the permissions are correct by     running\u00a0<code>chmod 600 ~/.ssh/config</code>.</p> </li> </ol>","tags":[]},{"location":"Scientific_Computing/Terminal_Setup/Git_Bash_Windows/#usage","title":"Usage","text":"<p>Assuming you have followed the setup above you will be able to connect to the clusters directly using;</p> <pre><code>ssh\u00a0mahuika\n</code></pre> <p>or</p> <pre><code>ssh\u00a0maui\n</code></pre> <p>As multiplexing is not configured you will have to enter in your login credentials every time you open a new terminal or try to move a file.</p> <pre><code>scp &lt;path/filename&gt; mahuika:~/\n</code></pre> <p>(For more info visit\u00a0data transfer).</p>","tags":[]},{"location":"Scientific_Computing/Terminal_Setup/MobaXterm_Setup_Windows/","title":"MobaXterm Setup (Windows)","text":"<p>Warning</p> <p>This page has been automatically migrated and may contain formatting errors.</p> <p>Prerequisite</p> <ul> <li>Have an active account and      project.</li> <li>Set up your Linux      Password.</li> <li>Set up Second Factor      Authentication.</li> <li>Windows operating system.</li> </ul> <p>Setting up MobaXterm as shown below will allow you to connect to the Cluster with less keyboard inputs as well as allow use of the file transfer GUI.</p> <ol> <li>Download MobaXTerm     here<ul> <li>Use the Portable Edition if you don't have administrator rights     on your machine. This is the recommended way for NIWA     researchers.</li> <li>Otherwise, choose freely the Portable or Installer Edition.</li> </ul> </li> <li>To set up a session, Click 'Session' in the top left corner:</li> <li>In \"SSH\",<ul> <li>Set the remote host to\u00a0<code>login.mahuika.nesi.org.nz</code> for Mahuika     users or <code>login.maui.nesi.org.nz</code> for M\u0101ui users.</li> <li>Enable the \"Specify username\" option and put your Username in     the corresponding box.</li> </ul> </li> <li>In the \"Advanced SSH settings\"<ul> <li>Set SSH-browser type to 'SCP (enhanced speed)'.</li> <li>Optionally, tick the 'Follow SSH path' button.</li> </ul> </li> </ol> <ol> <li>In the \u201cNetwork settings\u201d tab:<ul> <li>Select \"SSH gateway (jump host)\" to open a popup window</li> <li>In this window enter <code>lander.nesi.org.nz</code> in the \u201cGateway host\u201d     field, as well as your NeSI username in the Username field for     the gateway SSH server then select OK to close the window.</li> </ul> </li> </ol> <p></p> <p></p> <ol> <li>Click 'OK' on the open window, usually this will start a new session     immediately. See usage below.</li> </ol> <p>Prerequisite</p> <p>There is a bug which causes some users to be repeatedly prompted  <code>&lt;username&gt;@lander.nesi.org.nz's password:</code>  This can be resolved by clicking \"OK\" each time you are prompted then  logging in as normal once you are prompted for your <code>First Factor:</code> or  <code>Password:</code>.  See Login  Troubleshooting for more  details</p>","tags":[]},{"location":"Scientific_Computing/Terminal_Setup/MobaXterm_Setup_Windows/#usage","title":"Usage","text":"<p>You will see your saved session in the left hand panel under 'Sessions'. Double click to start.</p> <p></p> <p>You will be prompted by dialogue box.</p> <pre><code>Login Password (First Factor):\n</code></pre> <p>Enter your password.</p> <pre><code>Authenticator Code (Second Factor):\n</code></pre> <p>Enter your second factor six digit number (no space).</p> <p>Then Mahuika users may be prompted again:</p> <pre><code>Login Password:\n</code></pre> <p>Enter your password again</p> <p>M\u0101ui users will instead be prompted with:</p> <pre><code>Password:\n</code></pre> <p>M\u0101ui users must enter their password combined with their second factor. For example, if your password is \"Password\" and your current second factor is \"123456\" then you must enter \"Password123456\".</p> <p>Prerequisite</p> <p>If you choose to save your password, the process will be the same  minus the prompts for First Factor.</p>","tags":[]},{"location":"Scientific_Computing/Terminal_Setup/MobaXterm_Setup_Windows/#credential-manager","title":"Credential Manager","text":"<p>If you are using the built in credential manager you will have to make sure to delete old entries when changing your password.</p>","tags":[]},{"location":"Scientific_Computing/Terminal_Setup/MobaXterm_Setup_Windows/#troubleshooting","title":"Troubleshooting","text":"","tags":[]},{"location":"Scientific_Computing/Terminal_Setup/MobaXterm_Setup_Windows/#after-changing-password","title":"After Changing Password","text":"<p>If you are experiencing login issues after resetting your password, it is likely due to an issue with MobaXterm saved sessions and Password management system for saved session.</p> <p>Two steps to try:</p> <ul> <li>Remove any previously saved sessions either related to <code>lander</code> OR     <code>mahuika</code> from sessions panel on the left</li> <li>Access MobaXterm password management system as below and remove     saved credentials<ul> <li>Go to Settings-&gt;Configuration and go to the     General tab and click on MobaXterm password management</li> <li>You will see the saved sessions for <code>lander</code> (and perhaps     <code>mahuika</code> as well). I recommend removing all of it and restart     MobaXterm before the next login attempt</li> </ul> </li> </ul> <p>Then setup a new session according to the support doc instructions as before. !!! prerequisite What Next?      -   Moving files to/from a          cluster.</p>","tags":[]},{"location":"Scientific_Computing/Terminal_Setup/Standard_Terminal_Setup/","title":"Standard Terminal Setup","text":"<p>Warning</p> <p>This page has been automatically migrated and may contain formatting errors.</p> <p>Prerequisite</p> <ul> <li>Have an active account and      project.</li> <li>Set up your Linux      Password.</li> <li>Set up Second Factor      Authentication.</li> <li>Using standard Linux/Mac terminal or Windows Subsystem for      Linux      with\u00a0Ubuntu      terminal.</li> </ul>","tags":["ssh","howto"]},{"location":"Scientific_Computing/Terminal_Setup/Standard_Terminal_Setup/#first-time-setup","title":"First time setup","text":"<p>The login process can be simplified significantly with a few easy configurations.</p> <ol> <li> <p>In a new local terminal run;\u00a0<code>mkdir -p ~/.ssh/sockets</code>\u00a0this will     create a hidden file in your home directory to store socket     configurations.</p> </li> <li> <p>Open your ssh config file with\u00a0\u00a0<code>nano ~/.ssh/config</code> and add\u00a0the     following\u00a0(replacing\u00a0<code>username</code>\u00a0with your username):</p> <pre><code>Host mahuika\n   User username\n   Hostname login.mahuika.nesi.org.nz\n   ProxyCommand ssh -W %h:%p lander\n   ForwardX11 yes\n   ForwardX11Trusted yes\n   ServerAliveInterval 300\n   ServerAliveCountMax 2\n\nHost maui\n   User username\n   Hostname login.maui.nesi.org.nz\n   ProxyCommand ssh -W %h:%p lander\n   ForwardX11 yes\n   ForwardX11Trusted yes\n   ServerAliveInterval 300\n   ServerAliveCountMax 2\n\nHost lander\n   User username\n   HostName lander.nesi.org.nz\n   ForwardX11 yes\n   ForwardX11Trusted yes\n   ServerAliveInterval 300\n   ServerAliveCountMax 2\n\nHost *\n    ControlMaster auto\n    ControlPath ~/.ssh/sockets/ssh_mux_%h_%p_%r\n    ControlPersist 1\n</code></pre> <p>Close and save with ctrl x, y, Enter</p> </li> <li> <p>Ensure the permissions are correct by     running\u00a0<code>chmod 600 ~/.ssh/config</code>.</p> </li> </ol>","tags":["ssh","howto"]},{"location":"Scientific_Computing/Terminal_Setup/Standard_Terminal_Setup/#usage","title":"Usage","text":"<p>Assuming you have followed the setup above you will be able to connect to the clusters directly using;</p> <pre><code>ssh\u00a0mahuika\n</code></pre> <p>or</p> <pre><code>ssh\u00a0maui\n</code></pre> <p>Subsequent local terminals opened will be able to scp files without having to re-enter authentication e.g.</p> <pre><code>scp &lt;path/filename&gt; mahuika:~/\n</code></pre> <p>(For more info visit\u00a0data transfer). !!! prerequisite What Next?      -   Moving files to/from a          cluster.      -   Setting up a          X-Server\u00a0(optional).</p>","tags":["ssh","howto"]},{"location":"Scientific_Computing/Terminal_Setup/Ubuntu_LTS_terminal_Windows/","title":"Ubuntu LTS terminal (Windows)","text":"<p>Warning</p> <p>This page has been automatically migrated and may contain formatting errors.</p> <p>Prerequisite</p> <ul> <li>Be a member of an active      project.</li> <li>Windows with WSL      enabled.</li> </ul> <p>Currently the native Windows command prompt (even with WSL enabled) does not support certain features, until this is fixed we recommend using the Ubuntu LTS Terminal.</p> <ol> <li> <p>Open the Microsoft store, search for 'Ubuntu', find and install the     latest version of the Ubuntu LTS it should look something like\u00a0     'Ubuntu 20.04 LTS' , though you may find a later version.  </p> <p> </p> </li> <li> <p>Close the\u00a0\u201cAdd your Microsoft account.. dialogue box as you do not     need an account for the installation.You may have to click \u201cInstall\u201d     for a second time (If the above dialogue box reappears,\u00a0close as     before and download/install will begin.  </p> <p> </p> </li> <li> <p>Launch \u201cUbuntu 18.04 LTS\u201d from start menu and wait for the first     time installation to complete.</p> </li> <li> <p>As you are running Ubuntu on Windows for the first time, it will     require to be configured. Once the installation was complete, you     will be prompted to \u201cEnter new UNIX username\u201d and press     &lt;Enter&gt;. This username can be anything you want.  </p> <p> </p> </li> <li> <p>Now, type in a new password for the username you picked and press     &lt;Enter&gt;. (Again this password is anything you want). Then     retype the password to confirm and press &lt;Enter&gt;  </p> <p></p> </li> <li> <p>To create a symbolic link to your Windows filesystems in your home     directory run the following command replacing c with the name of     your Windows filesystems found in /mnt/.\u00a0</p> <p><pre><code>ln -s /mnt/c/Users/YourWindowsUsername/ WinFS\n</code></pre> !!! prerequisite What Next?  -   Set up your SSH config      file.</p> </li> </ol>","tags":[]},{"location":"Scientific_Computing/Terminal_Setup/WinSCP-PuTTY_Setup_Windows/","title":"WinSCP/PuTTY Setup (Windows)","text":"<p>Warning</p> <p>This page has been automatically migrated and may contain formatting errors.</p> <p>Prerequisite</p> <ul> <li>Have an active account and      project.</li> <li>Set up your NeSI account      password.</li> <li>Set up Second Factor      Authentication.</li> <li>Be using the Windows operating system.</li> </ul> <p>WinSCP is an SCP client for windows implementing the SSH protocol from PuTTY.</p> <p>WinSCP can be downloaded\u00a0here.</p> <p>Upon startup:</p> <p></p> <p>1.\u00a0Add a New Site and set:</p> <ul> <li>Enter in Host Name:\u00a0login.mahuika.nesi.org.nz\u00a0or     login.maui.nesi.org.nz</li> <li>Enter your NeSI account username into User name: (Password     optional)</li> </ul> <p>Prerequisite</p> <p>For \"file protocol\" (the topmost drop-down menu), either SCP or SFTP  is acceptable. If you are trying to move many small files or have a  slow or flaky Internet connection, you may find that SFTP performs  better than SCP. Feel free to try both and see which works best for  you.</p> <p></p> <p>5. Open Advanced Settings.</p> <p></p> <p>6. Navigate to Connection &gt; Tunnel\u00a0and set:</p> <ul> <li>Enable \"Connect through SSH tunnel\".</li> <li>Under \"Host name:\" enter lander.nesi.org.nz</li> <li>Under \"User name:\" enter your username.</li> <li>Optionally, enter your password in the \"Password:\" box.</li> </ul> <p>10. OK &gt; Save</p>","tags":[]},{"location":"Scientific_Computing/Terminal_Setup/WinSCP-PuTTY_Setup_Windows/#setup-for-putty-terminal","title":"Setup for PuTTY Terminal","text":"<p>The default WinSCP terminal lacks much functionality. We highly recommend you use the PuTTY terminal\u00a0instead.</p> <p>1. Download PuTTY here\u00a0and install.</p> <p>2.In WinSCP open 'Tools &gt; Preferences'</p> <p></p> <p>3. Under Integration &gt; Applications enable Remember session password and pass it to PuTTY</p> <p></p>","tags":[]},{"location":"Scientific_Computing/Terminal_Setup/WinSCP-PuTTY_Setup_Windows/#setup-for-xming-optional","title":"Setup for Xming (Optional)","text":"<p>Xming is an X server for Windows allowing graphical interface with the HPC and can be downloaded here.</p> <p>1. Install Xming following the prompts. (Make sure 'Normal PuTTY Link SSH Client' is selected).</p> <p>2. Under Integration &gt; Applications\u00a0and add -X after PuTTY/Terminal client path.</p> <p></p> <p>3. Restart your session.</p> <p>Prerequisite</p> <p>In order for X11 forwarding to work you must have an Xming server  running before connecting to the HPC.</p>","tags":[]},{"location":"Scientific_Computing/Terminal_Setup/WinSCP-PuTTY_Setup_Windows/#usage","title":"Usage","text":"<p>Files can be dragged, dropped and modified in the WinSCP GUI just like in any windows file system.</p> <p></p> <p>\u00a0Will open a PuTTY terminal. Assuming you followed the steps setting up PuTTY, this should automatically enter in your details.</p> <p>\u00a0Will open the default WinSCP terminal.\u00a0While the functionality is identical to any other terminal the interface is slightly abstracted, with a separate window for input and command history drop-down.</p> <p>\u00a0Type here to change directory. The GUI doesn't follow your current terminal directory like MobaXterm so must be changed manually.\u00a0(Recommend making this larger as the default is very hard to type in).</p> <p> Bookmark current directory.</p>","tags":[]},{"location":"Scientific_Computing/Terminal_Setup/WinSCP-PuTTY_Setup_Windows/#troubleshooting","title":"Troubleshooting","text":"","tags":[]},{"location":"Scientific_Computing/Terminal_Setup/WinSCP-PuTTY_Setup_Windows/#repeated-authentication-prompts","title":"Repeated Authentication Prompts","text":"<p>By default, WinSCP will create multiple tunnels for file transfers. Occasionally this can lead to an excessive number of prompts. Limiting number of tunnels will reduce the number of times you are prompted.\u00a0</p> <p>1. Open settings</p> <p></p> <p>2. Under 'Transfer' -&gt; 'Background', set the 'Maximal number of transfers at the same time' to '1' and untick 'Use multiple connections for a single transfer'.</p> <p> </p> <p>Prerequisite</p> <p>As WinSCP uses multiple tunnels for file transfer you will be required  to authenticate again on your first file operation of the session. The  second prompt for your second-factor token can be skipped, just as  with login authentication.</p> <p>!!! prerequisite What Next?      -   Moving files to/from a          cluster.      -   Setting up          an\u00a0X-Server          (optional).</p>","tags":[]},{"location":"Scientific_Computing/Terminal_Setup/Windows_Subsystem_for_Linux_WSL/","title":"Windows Subsystem for Linux (WSL)","text":"<p>Warning</p> <p>This page has been automatically migrated and may contain formatting errors.</p> <p>Prerequisite</p> <ul> <li>Windows 10.</li> </ul> <p>Windows subsystem for Linux is a feature that allows\u00a0you to utilise some linux commands and command line tools.</p> <p>WSL is enabled by default on later versions of Windows 10.</p> <p>Prerequisite</p> <p>You can test whether WSL is installed by opening 'Windows PowerShell'  and typing <code>wsl</code>.</p>","tags":[]},{"location":"Scientific_Computing/Terminal_Setup/Windows_Subsystem_for_Linux_WSL/#enabling-wsl","title":"Enabling WSL","text":"<ol> <li>Open 'Turn Windows features on or off' </li> <li>Scroll down and tick the 'Windows Subsystem for Linux' option.      Click OK</li> <li>Wait for the installation to finish then restart your computer. !!! prerequisite What Next?<ul> <li>Set up your SSH config      file.</li> </ul> </li> </ol>","tags":[]},{"location":"Scientific_Computing/Terminal_Setup/X11_on_NeSI/","title":"X11 on NeSI","text":"<p>Warning</p> <p>This page has been automatically migrated and may contain formatting errors.</p> <p>Prerequisite</p> <ul> <li>Have working      terminal      set up.</li> </ul> <p>X-11 is a protocol for rendering graphical user interfaces (GUIs) that can be sent along an SSH tunnel. If you plan on using a GUI on a NeSI cluster you will need to have an X-Server and X-Forwarding set up.</p>","tags":[]},{"location":"Scientific_Computing/Terminal_Setup/X11_on_NeSI/#x-servers","title":"X-Servers","text":"<p>You must have a X-server running on your local machine in order for a GUI to be rendered.</p> <p>Download links for X-servers can be found below.</p> MacOS Xquartz Linux Xorg Windows Xming <p>Make sure you have launched the server and it is running in the background, look for this\u00a0\u00a0symbol in your taskbar\u00a0</p> <p>Prerequisite</p> <p>MobaXterm has a build in X server, no setup required. By default the  server is started alongside MobaXterm. You can check it's status in  the top left hand corner  (=on,\u00a0=off).\u00a0</p>","tags":[]},{"location":"Scientific_Computing/Terminal_Setup/X11_on_NeSI/#x-forwarding","title":"X-Forwarding","text":"<p>Finally your ssh tunnel must be set up to 'forward' along X-11 connections.\u00a0</p>","tags":[]},{"location":"Scientific_Computing/Terminal_Setup/X11_on_NeSI/#openssh-terminal","title":"OpenSSH (terminal)","text":"<p>Make sure the <code>-Y</code> or <code>-X</code> flag is included</p> <pre><code>ssh -Y user@lander.nesi.org.nz\n</code></pre> <pre><code>ssh -Y login.nesi.org.nz\n</code></pre>","tags":[]},{"location":"Scientific_Computing/Terminal_Setup/X11_on_NeSI/#mobaxterm","title":"MobaXterm","text":"<p>Under 'session settings' for your connection make sure the X-11 forwarding box is checked.</p> <p></p> <p>If the  button in the top right corner of your window is coloured, the X-server should be running.</p>","tags":[]},{"location":"Scientific_Computing/Terminal_Setup/X11_on_NeSI/#x-forwarding-with-tmux","title":"X-Forwarding with tmux","text":"<p>In order to connect X11 into a tmux session you make the following change to your config file.</p> <pre><code>tmux show -g | sed 's/DISPLAY //' &gt; ~/.tmux.conf\n</code></pre>","tags":[]},{"location":"Scientific_Computing/Terminal_Setup/X11_on_NeSI/#interactive-slurm-jobs","title":"Interactive Slurm jobs","text":"<p>In order to make use of X11 in an interactive Slurm job:</p>","tags":[]},{"location":"Scientific_Computing/Terminal_Setup/X11_on_NeSI/#srun","title":"srun","text":"<p>Add the flag <code>--x11</code></p> <pre><code>srun --ntasks 36 --mem-per-cpu 1500 --time 01:00:00 --x11 --pty bash\n</code></pre>","tags":[]},{"location":"Scientific_Computing/Terminal_Setup/X11_on_NeSI/#salloc","title":"salloc","text":"<p>add the flag -Y when sshing to the node.</p> <pre><code>ssh -Y wbn001\n</code></pre>","tags":[]},{"location":"Scientific_Computing/Terminal_Setup/X11_on_NeSI/#xvfb","title":"XVFB","text":"<p>If your application requires X11 in order to run, but does not need to be interactive you can use X11 Virtual Frame Buffer. This may be required to in order to run visual applications on the compute nodes. Prepending any command with <code>xfvb-run</code> will provide a dummy X11 server for the application to render to. e.g.</p> <pre><code>xvfb-run xterm\n</code></pre>","tags":[]},{"location":"Scientific_Computing/The_NeSI_High_Performance_Computers/Available_GPUs_on_NeSI/","title":"Available GPUs on NeSI","text":"<p>NeSI has a range of Graphical Processing Units (GPUs) to accelerate compute-intensive research and support more analysis at scale. Depending on the type of GPU, you can access them in different ways, such as via batch scheduler (Slurm), interactively (using Jupyter on NeSI), or Virtual Machines (VMs).</p> <p>The table below outlines the different types of GPUs, who can access them and how, and whether they are currently available or on the future roadmap.</p> <p>If you have any questions about GPUs on NeSI or the status of anything listed in the table,  Contact our Support Team.</p> GPGPU Purpose Location Access mode Who can access Status 9 NVIDIA Tesla P100 PCIe 12GB cards (1 node with 1 GPU, 4 nodes with 2 GPUs) Mahuika Slurm and Jupyter NeSI users Currently available 7 NVIDIA A100 PCIe 40GB cards (4 nodes with 1 GPU, 2 nodes with 2 GPUs) Machine Learning (ML) applications Mahuika Slurm NeSI users Currently available 7 A100-1g.5gb instances (1 NVIDIA A100 PCIe 40GB card divided into 7 MIG GPU slices with 5GB memory each) Development and debugging Mahuika Slurm and Jupyter NeSI users Currently available 5 NVIDIA Tesla P100 PCIe 12GB (5 nodes with 1 GPU) Post-processing M\u0101ui Ancil Slurm NeSI users Currently available 4 NVIDIA HGX A100 (4 GPUs per board with 80GB memory each, 16 A100 GPUs in total) Large-scale Machine Learning (ML) applications Mahuika Slurm NeSI users Available as part of the Milan Compute Nodes 4 NVIDIA A40 with 48GB memory each (2 nodes with 2 GPUs, but capacity for 6 additional GPUs already in place) Teaching / training Flexible HPC Jupyter, VM, or bare metal tenancy possible (flexible) Open to conversations with groups who could benefit from these In development."},{"location":"Scientific_Computing/The_NeSI_High_Performance_Computers/Mahuika/","title":"Mahuika","text":"<p>Mahuika is a Cray CS400 cluster featuring Intel Xeon Broadwell nodes, FDR InfiniBand interconnect, and NVIDIA GPGPUs.</p> <p>Mahuika is designed to provide a capacity, or high throughput, HPC resource that allows researchers to run many small (from one to a few hundred CPU cores) compute jobs simultaneously, and to conduct interactive data analysis. To support jobs that require large (up to 500GB) or huge (up to 4 TB) memory, or GPGPUs, and to provide virtual lab services, Mahuika has additional nodes optimised for this purpose.</p> <p>The Mahuika login (or build) nodes, mahuika01 and mahuika02, provide access to GNU, Intel and Cray programming environments, including editors, compilers, linkers, and debugging tools. Typically, users will ssh to these nodes after logging onto the NeSI lander node.</p>","tags":["hpc","mahuika","cs400"]},{"location":"Scientific_Computing/The_NeSI_High_Performance_Computers/Mahuika/#notes","title":"Notes","text":"<ol> <li>The Cray Programming Environment on Mahuika, differs from that on     M\u0101ui.</li> <li>The <code>/home, /nesi/project</code>, and <code>/nesi/nobackup</code> filesystems     are mounted on Mahuika.</li> <li>Read about how to compile and link code on Mahuika in section     entitled: Compiling software on     Mahuika.</li> <li>An extension to Mahuika with additional, upgraded resources is also     available. see Milan Compute     Nodes     for details on access</li> </ol>","tags":["hpc","mahuika","cs400"]},{"location":"Scientific_Computing/The_NeSI_High_Performance_Computers/Mahuika/#mahuika-hpc-cluster-cray-cs400","title":"Mahuika HPC Cluster (Cray CS400)","text":"<p>Login nodes</p> <p>72 cores in 2\u00d7 Broadwell (E5-2695v4, 2.1 GHz, dual socket 18 cores per socket) nodes</p> <p>Compute nodes</p> <p>8,136 cores in 226 \u00d7 Broadwell (E5-2695v4, 2.1 GHz, dual socket 18 cores per socket) nodes; 7,552 cores in 64 HPE Apollo 2000 XL225n nodes (AMD EPYC Milan 7713) the Milan partition</p> <p>Compute nodes (reserved for NeSI Cloud) </p> <p>288 cores in 8 \u00d7 Broadwell (E5-2695v4, 2.1 GHz, dual socket 18 cores per socket) nodes</p> <p>GPUs </p> <p>9 NVIDIA Tesla P100 PCIe 12GB cards (1 node with 1 GPU, 4 nodes with 2 GPUs)</p> <p>8 NVIDIA A100 PCIe 40GB cards (4 nodes with 1 GPU, 2 nodes with 2 GPUs) </p> <p>16 NVIDIA A100 HGX 80GB cards (4 nodes with 4 GPU each)</p> <p>Hyperthreading</p> <p>Enabled (accordingly, SLURM will see ~31,500 cores)</p> <p>Theoretical Peak Performance</p> <p>308.6 TFLOPs</p> <p>Memory capacity per compute node</p> <p>128 GB</p> <p>Memory capacity per login (build) node</p> <p>512 GB</p> <p>Total System memory</p> <p>84.0 TB</p> <p>Interconnect</p> <p>FDR (54.5Gb/s) InfiniBand to EDR (100Gb/s) Core fabric. 3.97:1 Fat-tree topology</p> <p>Workload Manager</p> <p>Slurm (Multi-Cluster)</p> <p>Operating System</p> <p>CentOS 7.4 &amp; Rocky 8.5 on Milan</p>","tags":["hpc","mahuika","cs400"]},{"location":"Scientific_Computing/The_NeSI_High_Performance_Computers/Mahuika/#storage-ibm-ess","title":"Storage (IBM ESS)","text":"Scratch storage 4,412 TB (IBM Spectrum Scale, version 5.0). Total I/O bandwidth to disks is ~130 GB/s Persistent storage 1,765 TB (IBM Spectrum Scale, version 5.0). Shared between Mahuika and M\u0101ui Total I/O bandwidth to disks is ~65 GB/s (i.e. the /home and /nesi/project filesystems) Offline storage Of the order of 100 PB (compressed) <p>Scratch and persistent storage are accessible from Mahuika, as well as from M\u0101ui and the ancillary nodes. Offline storage will in due course be accessible indirectly, via a dedicated service.</p>","tags":["hpc","mahuika","cs400"]},{"location":"Scientific_Computing/The_NeSI_High_Performance_Computers/Maui/","title":"M\u0101ui","text":"<p>M\u0101ui is a Cray XC50 supercomputer featuring Skylake Xeon nodes, Aries interconnect and IBM ESS Spectrum Scale Storage. NeSI has access to 316 compute nodes on M\u0101ui.</p> <p>M\u0101ui is designed as a capability high-performance computing resource for simulations and calculations that require large numbers of CPUs working in a tightly-coupled parallel fashion, as well as interactive data analysis. To support workflows that are primarily single core jobs, for example pre- and post-processing work, and to provide virtual lab services, we offer a small number\u00a0M\u0101ui ancillary nodes.</p> <p>Tips</p> <p>The computing capacity of the M\u0101ui ancillary nodes is limited. If you  think you will need large amounts of computing power for small jobs in  addition to large jobs that can run on M\u0101ui, please Contact our Support Team about getting an  allocation on  Mahuika,  our high-throughput computing cluster.</p> <p>The login or build nodes maui01 and maui02 provide access to the full Cray Programming Environment (e.g. editors, compilers, linkers, debug tools). Typically, users will access these nodes via SSH from the NeSI lander node. Jobs can be submitted to the HPC from these nodes.</p>","tags":["hpc","info","maui","XC50","cs500"]},{"location":"Scientific_Computing/The_NeSI_High_Performance_Computers/Maui/#important-notes","title":"Important Notes","text":"<ol> <li>The Cray Programming Environment on the XC50 (supercomputer) differs     from that on Mahuika and the M\u0101ui Ancillary nodes.</li> <li>The <code>/home, /nesi/project</code>, and <code>/nesi/nobackup</code> file     systems are     mounted on M\u0101ui.</li> <li>The I/O subsystem on the XC50 can provide high bandwidth to disk     (large amounts of data), but not many separate reading or writing     operations. If your code performs a lot of disk read or write     operations, it should be run on either\u00a0the M\u0101ui ancillary     nodes\u00a0or\u00a0Mahuika.</li> </ol> <p>All M\u0101ui resources are indicated below, and the the M\u0101ui Ancillary Node resources here.</p>","tags":["hpc","info","maui","XC50","cs500"]},{"location":"Scientific_Computing/The_NeSI_High_Performance_Computers/Maui/#maui-supercomputer-cray-xc50","title":"M\u0101ui Supercomputer (Cray XC50)","text":"<p>Login nodes (also known as eLogin nodes)</p> <p>80 cores in 2 \u00d7 Skylake (Gold 6148, 2.4 GHz, dual socket 20 cores per socket) nodes</p> <p>Compute nodes</p> <p>18,560 cores in 464 \u00d7 Skylake (Gold 6148, 2.4 GHz, dual socket 20 cores per socket) nodes;</p> <p>Hyperthreading</p> <p>Enabled (accordingly, SLURM will see 37,120 cores)</p> <p>Theoretical Peak Performance</p> <p>1.425 PFLOPS</p> <p>Memory capacity per compute node</p> <p>232 nodes have 96 GB, the remaining 232 have 192 GB each</p> <p>Memory capacity per login (build) node</p> <p>768 GB</p> <p>Total System memory</p> <p>66.8 TB</p> <p>Interconnect</p> <p>Cray Aries, Dragonfly topology</p> <p>Workload Manager</p> <p>Slurm (Multi-Cluster)</p> <p>Operating System</p> <p>Cray Linux Environment CLE7.0UP04 SUSE Linux Enterprise Server 15 SP3 </p>","tags":["hpc","info","maui","XC50","cs500"]},{"location":"Scientific_Computing/The_NeSI_High_Performance_Computers/Maui/#storage-ibm-ess","title":"Storage (IBM ESS)","text":"Scratch Capacity (accessible from all M\u0101ui, Mahuika, and Ancillary nodes). 4,412 TB (IBM Spectrum Scale, version 5.0). Total I/O bandwidth to disks is 130 GB/s Persistent storage (accessible from all M\u0101ui, Mahuika, and Ancillary nodes). 1,765 TB (IBM Spectrum Scale, version 5.0) Shared Storage. Total I/O bandwidth to disks is 65 GB/s (i.e. the /home and /nesi/project filesystems) Offline storage (accessible from all M\u0101ui, Mahuika, and Ancillary nodes). Of the order of 100 PB (compressed)","tags":["hpc","info","maui","XC50","cs500"]},{"location":"Scientific_Computing/The_NeSI_High_Performance_Computers/Maui_Ancillary/","title":"M\u0101ui Ancillary","text":"<p>The M\u0101ui Ancillary Nodes provide access to a Virtualised environment that supports:</p> <ol> <li>Pre- and post-processing of data for jobs running on the     M\u0101ui     Supercomputer or     Mahuika HPC     Cluster. Typically, as serial processes on a Slurm partition running     on a set of Ancillary node VMs or baremetal servers.</li> <li>Virtual laboratories that provide interactive access to data stored     on the M\u0101ui (and Mahuika) storage together with domain analysis     toolsets (e.g. Seismic, Genomics, Climate, etc.). To access the     Virtual Laboratory nodes, users will first logon to the NeSI Lander     node, then ssh to the relevant Virtual Laboratory. Users may submit     jobs to Slurm partitions from Virtual Laboratory nodes.</li> <li>Remote visualisation of data resident on the filesystems.</li> <li>GPGPU computing.</li> </ol> <p>Scientific Workflows may access resources across the M\u0101ui Supercomputer and any (multi-cluster) Slurm partitions on the M\u0101ui or Mahuika systems.</p>","tags":["maui","XC50","cs500"]},{"location":"Scientific_Computing/The_NeSI_High_Performance_Computers/Maui_Ancillary/#notes","title":"Notes","text":"<ol> <li>The <code>/home, /nesi/project</code>, and <code>/nesi/nobackup</code> filesystems     are mounted on the M\u0101ui Ancillary Nodes.</li> <li>The M\u0101ui Ancillary nodes have Skylake processors, while the Mahuika     nodes use Broadwell processors.</li> </ol>","tags":["maui","XC50","cs500"]},{"location":"Scientific_Computing/The_NeSI_High_Performance_Computers/Maui_Ancillary/#ancillary-node-specifications","title":"Ancillary Node Specifications","text":"Multi-Purpose nodes 1,120 cores in 28 \u00d7 Skylake (Gold 6148, 2.4 GHz, dual socket 20 cores per socket) nodes, which will appear as 2,240 logical cores. Hyperthreading Enabled Local Disk 1.2TB SSD Operating System CentOS 7.4 GPGPUs 5 NVIDIA Tesla P100 PCIe 12GB (5 nodes with 1 GPU) Remote Visualisation NICE DCV Memory capacity per Multi-Purpose node 768 GB Interconnect EDR (100 Gb/s) InfiniBand Workload Manager Slurm (Multi-Cluster) OpenStack The Cray CS500 Ancillary nodes will normally be presented to users as Virtual Machines, provisioned from the physical hardware as required. <p>The M\u0101ui_Ancil nodes have different working environment than the M\u0101ui (login) nodes. Therefore a CS500 login node is provided, to create and submit your jobs on this architecture. To use you need to login from M\u0101ui login nodes to:</p> <pre><code>w-mauivlab01.maui.nesi.org.nz\n</code></pre> <p>If you are looking for accessing this node from your local machine you could add the following section to <code>~/.ssh/config</code> (extending the recommended terminal setup)</p> <pre><code>Host w-mauivlab01 \n  User &lt;username&gt; \n  Hostname w-mauivlab01.maui.nesi.org.nz \n  ProxyCommand ssh -W %h:%p maui \n  ForwardX11 yes\n  ForwardX11Trusted yes\n  ServerAliveInterval 300\n  ServerAliveCountMax 2\n</code></pre>","tags":["maui","XC50","cs500"]},{"location":"Scientific_Computing/The_NeSI_High_Performance_Computers/Overview/","title":"Overview","text":"<p>The NeSI High Performance Computers M\u0101ui and Mahuika\u00a0provide the New Zealand research community with access to a national data-centric and data intensive research computing environment built on leading edge high performance computing (HPC) systems.</p> <ul> <li>M\u0101ui, which in Maori mythology is credited with catching a giant     fish using a fishhook taken from his grandmother's jaw-bone; the     giant fish would become the North Island of New Zealand, provides a     Capability (i.e. Supercomputer) HPC resource on which researchers     can run simulations and calculations that require large numbers     (e.g. thousands) of processing cores working in a tightly-coupled,     parallel fashion.</li> <li>Mahuika, which in Maori mythology, is a fire deity, from whom M\u0101ui     obtained the secret of making fire, provides a Capacity (i.e.     Cluster) HPC resource to allow researchers to run many small (e.g.     from 1 core to a few hundred cores) compute jobs simultaneously     (aka\u00a0 High Throughput Computing).</li> </ul> <p>M\u0101ui and Mahuika share the same high performance filesystems, accordingly, data created on either system are visible on the other (i.e. without the need to copy data between systems). However, they have different processors (Skylake on M\u0101ui, and Broadwell on Mahuika), and different flavours of Linux (SLES on M\u0101ui and CentOS on Mahuika), so shared applications should be explicitly compiled and linked for each architecture. These systems and Ancillary Nodes on Mahuika and on\u00a0M\u0101ui\u00a0 provide the research community with:</p> <ul> <li>Leading edge HPCs (both Capacity and Capability) via a single point     of access;</li> <li>New user facing services that can act on the data held within the NeSI HPC infrastructure, including:</li> <li>Pre- and post-processing systems to support workflows;</li> <li>Virtual Laboratories that provide interactive access to science domain specific tools [Coming soon];</li> <li>Remote visualisation services [Coming soon];</li> <li>Advanced data analytics tools, and</li> <li>The ability to seamlessly move data between high performance disk storage and offline tape.</li> <li>Offsite replication of critical data (both online and offline).</li> </ul> <p>These systems are accessed via a \u201clander\u201d node using two-factor authentication.</p> <p>NeSI researchers have access to all compute nodes on Mahuika, and 316 compute nodes on M\u0101ui.</p>","tags":["hpc","info"]},{"location":"Scientific_Computing/Training/Introduction_to_computing_on_the_NeSI_HPC/","title":"Introduction to computing on the NeSI HPC","text":"<p>Warning</p> <p>This page has been automatically migrated and may contain formatting errors.</p> <ul> <li>Introduction to computing on the NeSI HPC (Part     1) </li> <li>Introduction to computing on the NeSI HPC platform (Part     2) </li> <li>Introduction to computing on the NeSI HPC (Part     3)</li> <li>Introduction to computing on the NeSI HPC (Part     4)</li> </ul>","tags":[]},{"location":"Scientific_Computing/Training/Webinars/","title":"Webinars","text":"<p>Warning</p> <p>This page has been automatically migrated and may contain formatting errors.</p> <p>Our webinar playlist covers the following topics:</p> <ul> <li>Troubleshooting on     NeSI</li> <li>Make the most of your HPC     allocation</li> <li></li> <li>Genomics workflows and how they can streamline your     research</li> <li>Using Rmarkdown to create clear, reproducible     analyses</li> <li>Git Basics for     researchers</li> <li>Scripting at the speed of compiled code:     Vectorisation</li> <li>Job scaling and running tests on     NeSI</li> <li>Sharing Data with Groups Using     Globus</li> <li>High performance     modelling</li> <li>Jupyter Tips &amp; Tricks From interactive experiments to batch jobs     and     more</li> <li>Tips &amp; tricks for hosting a successful online     event</li> <li>Who needs GPUs? Tips for determining if your code is a     fit</li> <li>NeSI and REANNZ infrastructure: tools and tips for research     success</li> <li>Should I use GPUs for my     research?</li> <li>Globus for IT Professionals     webinar</li> <li>Webinar: Jupyter on NeSI - April     2021</li> <li>4 Tips for Getting Started on     NeSI</li> <li>RMarkdown for Researchers - Weave together narrative text and     code</li> <li>From Cells to Clouds - Fluid Mechanics at     Scale</li> <li>Modelling gene regulatory networks via high performance     computing</li> <li>Introducing Docker container technologies to     researchers</li> <li>Researcher Reflections Panel - tips for navigating your     supercomputing     journey</li> <li>Reproducible research workflows with     containers</li> <li>Git Basics for     researchers</li> <li>Using Rmarkdown to create clear, reproducible     analyses</li> <li>Genomics workflows and how they can streamline your     research</li> <li>Make the most of your HPC     allocation</li> <li>Scripting at the speed of compiled code -     Vectorisation</li> <li>Getting Started on     NeSI</li> <li>Building a Convolution Neural Network to Classify Underwater Sounds     (NeSI/     NIWA)</li> <li>NeSI's National Data Transfer Platform \u2014 Sharing Data with Groups     Using     Globus</li> <li>Job scaling and running tests on     NeSI</li> <li>Python Profiling on     NeSI</li> </ul>","tags":[]},{"location":"Scientific_Computing/Training/Workshops/","title":"Workshops","text":"<p>Warning</p> <p>This page has been automatically migrated and may contain formatting errors.</p> <p>Please visit https://www.genomics-aotearoa.org.nz/training to know more about upcoming training events.</p> <p>Below are a select list of workshop materials covered by the Genomics Aotearoa-NeSI Training Team:</p> <ul> <li>RNA-seq     workshop\u00a0[Link] [Workshop     Link]</li> <li>Genomics Data Carpentry (GDC)     Workshops [Link]</li> <li>Genomics workflows: How CWL can streamline your research: The     link to the presentation     is\u00a0here</li> <li>Annual Metagenomics Summer School: [Link]</li> <li>Genomics Data Carpentry Workshops</li> <li>Genotyping by Sequencing: [Workshop     Link]</li> </ul>","tags":[]},{"location":"Storage/Data_Recovery/File_Recovery/","title":"File Recovery","text":"","tags":[]},{"location":"Storage/Data_Recovery/File_Recovery/#snapshots","title":"Snapshots","text":"<p>Snapshots are read only copies of the file system taken every day at 12:15, and retained for seven days.  </p> <p>Files from you project directory can be found in\u00a0<code>/nesi/project/.snapshots/</code> followed by the weekday (capitalised) and project code, e.g;</p> <pre><code>/nesi/project/.snapshots/Sunday/nesi99999/\n</code></pre> <p>And for home directory;</p> <pre><code>/home/username/.snapshots/Sunday/\n</code></pre> <p>Warning</p> <p>Files in <code>/nesi/nobackup/</code> are not snapshotted.</p> <p>Recovering a file or a directory from the snapshot is as simple as copying it over, e.g.</p> <pre><code>cp /nesi/project/.snapshots/Sunday/nesi99999/file.txt /nesi/project/nesi99999/file.txt\n</code></pre> <p>Tip</p> <p>For copying directories use the flag -ir or just -r if you don't want to be prompted before overwriting</p>","tags":[]},{"location":"Storage/Data_Transfer_Services/Data_Transfer_using_Globus_V5/","title":"Data Transfer using Globus V5","text":"","tags":[]},{"location":"Storage/Data_Transfer_Services/Data_Transfer_using_Globus_V5/#globus","title":"Globus","text":"<p>Globus is a third-party service for transferring large amounts of data between Globus Data Transfer Nodes (DTNs). For example you can transfer data between the NeSI Wellington DTN V5 and your personal workstation endpoint, or an endpoint from your institution. With Globus, very high data transfer rates are achievable. This service allows data to be accessible to any person who has a Globus account. The newest implementation (v5) provides extra features and some key differences from the previous setup that you can find here.</p> <p>To use Globus on NeSI platforms, you need:</p> <ol> <li>A Globus account\u00a0(see Initial Globus Sign-Up and Globus    ID)</li> <li>An active NeSI account (see\u00a0Creating a NeSI    Account)</li> <li> <p>Access privileges on the non-NeSI Globus endpoint/collection you    plan on transferring data from or to. This other endpoint/collection    could be a personal one on your workstation, or it could be managed    by your institution or a third party.</p> <ul> <li>Note that a NeSI user account does not create a Globus account, and     similarly a Globus account does not create a NeSI user account. Nor     can you, as the end user, link the two through any website.</li> </ul> </li> </ol> <p>Both your accounts (NeSI and Globus) must exist before you try to use our DTN.</p> <p>The NeSI Wellington DTN endpoint is protected by a second factor authentication (2FA).\u00a0 Also note, your NeSI username and password are case-sensitive.</p>","tags":[]},{"location":"Storage/Data_Transfer_Services/Data_Transfer_using_Globus_V5/#the-nesi-data-transfer-node","title":"The NeSI Data Transfer Node","text":"<p>The NeSI Data Transfer Node (DTN) acts as an interface between our HPC facility storage and a worldwide network of Globus endpoints. This is achieved using Globus.org, a web-based service that solves many of the challenges encountered moving large volumes of data between systems. While NeSI supports use of other data transfer tools and protocols such as <code>scp</code>, Globus\u00a0provides the most comprehensive, efficient, and easy to use service for NeSI users who need to move large data sets (more than a few gigabytes at a time).</p>","tags":[]},{"location":"Storage/Data_Transfer_Services/Data_Transfer_using_Globus_V5/#types-of-globus-endpoints-or-data-transfer-nodes","title":"Types of Globus endpoints or Data Transfer Nodes","text":"<p>Globus data transfers take place between endpoints. An endpoint is nothing more than an operating system (Windows, Linux, etc) that has the Globus endpoint software installed on it. Endpoints come in two kinds: personal and server. Within a endpoint users can access data via collections, with specific permissions and the ability to shared with others.</p> <p>The NeSI DTN is an example of a server endpoint. These type of endpoints are usually configured to access large capacity and high-performance parallel filesystems. Endpoints can be unmanaged or managed by a subscription. NeSI DTN is a server type, managed endpoint (by NeSI subscription) which allows authorised users to provide data transfer and data sharing services on behalf of their Globus accounts.</p> <p>Your institution may have its own managed server endpoint, and if so we encourage you to use that endpoint for your data transfers between your institution and NeSI. You may need to apply to the person or group administering the managed server endpoint, most likely your IT team, to get access to the endpoint. Your institution may even have several endpoints, in which case we recommend that you consider which one would be best suited for your data transfer requirements. If you need any help in regards to this, get in touch with us via support@nesi.org.nz, or consult your institution's IT team.</p> <p>If your institution doesn't have a managed server endpoint, you can set up a personal endpoint using software provided by Globus (see below). Please be aware that even if you set up a personal endpoint, you may still need to consult your IT team in order to make it usable, especially if your institution has an aggressive firewall.</p>","tags":[]},{"location":"Storage/Data_Transfer_Services/Data_Transfer_using_Globus_V5/#transferring-data-using-a-managed-endpoint","title":"Transferring data using a managed endpoint","text":"<p>As an example, to move files between the NeSI HPC Storage (accessible from M\u0101ui and Mahuika) and the Otago University high-capacity central file storage (another managed server endpoint):</p> <p>Info</p> <p>Log in to the NeSI File Manager where you are able to search for DTNs in the Collection field. Here is a listing of available endpoints on the New Zealand Data Transfer Platform.</p> <p>Find the NeSI endpoint by typing in \"NeSI Wellington DTN V5\". Select the endpoint \"NeSI Wellington DTN V5\" from the list, and you will be asked to authenticate your access to the endpoint. Click Continue to the next step.</p> <p></p> <p>You can choose either of &lt;username&gt;@wlg-dtn-oidc.nesi.org.nz or NeSI Wellington OIDC Server (wlg-dtn-oidc.nesi.org.nz), they are all linked to the same website. If this is your first time login, you may ask to bind your primary identity to the OIDC login, you need to allow that.</p> <p></p> <p>The NeSI Wellington DTN V5 endpoint is protected by a second factor authentication (2FA-same as accessing NeSI clusters).\u00a0 In the 'Username' field, enter your M\u0101ui/Mahuika username. In the 'Password' field, your <code>Password</code> will be equal to <code>Login Password (First Factor)</code> + <code>Authenticator Code (Second Factor)</code>\u00a0e.g.\u00a0<code>password123456</code>. (Do not use any additional characters or spaces between your password and the token number.)</p> <p></p> <p>After the login, you will navigate to the default root(display as \"/\") path, then you could change the path to</p> <p>(1) your /home/&lt;username&gt;\u00a0directory,</p> <p>(2) project directory (read-only) /nesi/project/&lt;project_code&gt;</p> <p>(3) project sub-directories of /nesi/nobackup/&lt;project_code&gt;\u00a0 - see Globus Paths, Permissions,\u00a0 Storage Allocation.  </p> <p>Navigate to your selected directory. e.g. the nobackup filesystem /nesi/nobackup/&lt;project_code&gt; and select the two-endpoint panel for transfer.</p> <p></p> <p>Select the target endpoint and authenticate.</p> <p>When you have activated endpoints in both transfer windows, you can start transferring files between them.</p> <p></p> <p>Select files you wish to transfer and select the corresponding \"Start\" button:  </p> <p></p> <p>To find other NeSI endpoints, type in \"nesi#\":</p> <p></p>","tags":[]},{"location":"Storage/Data_Transfer_Services/Data_Transfer_using_Globus_V5/#in-brief","title":"In brief:","text":"<ul> <li>Sign in to the NeSI Globus Web App https://transfer.nesi.org.nz/.   You will be taken to the\u00a0File Manager page   https://transfer.nesi.org.nz/file-manager</li> <li>If this is your first time, you will need to create a\u00a0Globus   account.</li> <li>Open the two-endpoint panel    located   on the top-right of the File Manager page.</li> <li>Select the Endpoints you wish to move files between (start typing   \"nesi#\" to see the list of NeSI DTNs to select from).   Authenticate   at both endpoints.</li> <li>At Globus.org the endpoint defaults to   \"/home/&lt;username&gt;\" path (represented by \"/~/\") on Mahuika or   M\u0101ui. We do not recommend uploading data to your home directory, as   home directories are very small. Instead, navigate to an appropriate   project directory under /nobackup (see Globus Paths, Permissions,   Storage   Allocation).</li> <li>Transfer the files by clicking the appropriate    button   depending on the direction of the transfer.</li> <li>Check your email for confirmation about the job completion report.</li> </ul>","tags":[]},{"location":"Storage/Data_Transfer_Services/Data_Transfer_using_Globus_V5/#transferring-data-using-a-personal-endpoint","title":"Transferring data using a personal endpoint","text":"<p>To transfer files into/out of your laptop, desktop computer or any other system you control, configure it as a Globus Personal Endpoint (see Personal Globus Endpoint Configuration\u00a0for transfers between personal endpoints).</p>","tags":[]},{"location":"Storage/Data_Transfer_Services/Data_Transfer_using_Globus_V5/#file-sharing","title":"File sharing","text":"<p>To share files with others outside your filesystem, see\u00a0https://docs.globus.org/how-to/share-files/</p>","tags":[]},{"location":"Storage/Data_Transfer_Services/Data_Transfer_using_Globus_V5/#using-globus-to-transfer-data-to-or-from-the-cloud","title":"Using Globus to transfer data to or from the cloud","text":"<p>Globus connectors enable a uniform interface for accessing, moving, and sharing across a variety of cloud providers. We do not currently have a connector subscription (note a subscription is required per cloud provider) so we can\u2019t use globus to transfer to/from cloud storage. If you see this as key for you, please let us know (support@nesi.org.nz).</p> <p>Our current advice for moving data to or from the cloud is to use tools such as Rclone\u00a0 (https://rclone.org/) or the cloud CLI's such as aswcli for S3 (https://aws.amazon.com/cli/) or gcloud CLI (https://cloud.google.com/sdk/gcloud). If you have any trouble or would like further advice, please Contact our Support Team.</p>","tags":[]},{"location":"Storage/Data_Transfer_Services/Data_transfer_between_NeSI_and_a_PC_without_NeSI_two_factor_authentication/","title":"Data transfer between NeSI and a PC without NeSI two-factor authentication","text":"<p>Warning</p> <p>This page has been automatically migrated and may contain formatting errors.</p> <p>This article shows how to transfer potentially large amounts of data between NeSI and your personal computer, without requiring 2FA (two-factor authentication) each time you initiate the transfer.\u00a0 This is particularly useful in the context of automated, or scripted data transfers.</p> <p>The approach is based on using Globus and a guest collection on the source side. Globus allows you to copy and synchronise files between NeSI's platforms and other computers, including your personal computer.</p> <p>A collection is a directory whose content can be shared. A guest collection allows you to share data without having to type in your credentials each time your transfer files.</p> <p>See this support page on how to set up Globus. Here, we assume you have an account on NeSI and have registered and created an account on Globus.</p>","tags":[]},{"location":"Storage/Data_Transfer_Services/Data_transfer_between_NeSI_and_a_PC_without_NeSI_two_factor_authentication/#step-1-create-a-guest-collection-on-nesi","title":"Step 1: Create a guest collection on NeSI","text":"<ul> <li>Go to https://app.globus.org/file-manager</li> <li>In the \"Collection\" search box type NeSI Wellington DTN V5 and     select this collection</li> <li>You may then need to log onto NeSI DTN to see the files</li> <li>Find the root folder of your guest collection, the directory you     would like to share, and<ul> <li>click on the \u201cShare\u201d button,</li> <li>click on \u201cAdd Guest Collection\u201d</li> <li>provide a \"Display Name\"</li> <li>press on \"Create Collection\"</li> </ul> </li> <li>You should now see your new guest collection at     https://app.globus.org/collections?scope=administered-by-me</li> </ul>","tags":[]},{"location":"Storage/Data_Transfer_Services/Data_transfer_between_NeSI_and_a_PC_without_NeSI_two_factor_authentication/#step-2-download-and-install-globus-connect-personal","title":"Step 2: Download and install Globus Connect Personal","text":"<p>On your personal computer, download \"Globus Connect Personal\" from https://app.globus.org/file-manager/gcp. Versions exist for Mac, Windows and Linux. Follow the instructions to install and set up the software. Also see our support page about Personal Globus Endpoint Configuration.</p>","tags":[]},{"location":"Storage/Data_Transfer_Services/Data_transfer_between_NeSI_and_a_PC_without_NeSI_two_factor_authentication/#step-3-share-a-directory-on-your-personal-computer","title":"Step 3: Share a directory on your personal computer","text":"<ul> <li>Launch \"Globus Connect Personal\" and go to \"Preferences\".\u00a0</li> <li>Select \"Access\"<ul> <li>click on the \"+\" sign to share a new directory</li> <li>navigate your directory and press \"Open\"</li> <li>make the directory writable</li> </ul> </li> </ul> <p>Note: By default your entire home directory will be exposed. It is good practice to only share specific directories. You can remove your home directory by highlighting it and clicking on the \"-\" sign.</p> <p></p>","tags":[]},{"location":"Storage/Data_Transfer_Services/Data_transfer_between_NeSI_and_a_PC_without_NeSI_two_factor_authentication/#step-4-test-a-file-transfer","title":"Step 4: Test a file transfer","text":"<ul> <li>Go to https://app.globus.org</li> <li>Log in</li> <li>In the \"FILE MANAGER\" tab, type the source and destination     collections. The source path should be relative to the guest     collection root. However, the destination path is absolute, as can     be seen in the picture below.</li> <li>Click on the files you want to transfer and press \"Start\"</li> </ul>","tags":[]},{"location":"Storage/Data_Transfer_Services/Download_and_share_CMIP6_data_for_NIWA_researchers/","title":"Download and share CMIP6 data (for NIWA researchers)","text":"<p>Warning</p> <p>This page has been automatically migrated and may contain formatting errors.</p> <p>The\u00a0Coupled Model Intercomparison Project, which began in 1995 under the auspices of the\u00a0World Climate Research Programme (WCRP), is now in its sixth phase (CMIP6). CMIP6 orchestrates somewhat independent model intercomparison activities and their experiments,\u00a0which have adopted a common infrastructure for collecting, organising, and distributing output from models performing common sets of experiments.</p> <p>This document shows how to explore which CMIP6 data are available and how to download the data once you have figured out what you need. The data will be downloaded asynchronously\u00a0- no need to stare at a screen for hours.\u00a0 The downloaded data will reside in a shared directory and hence will also be accessible to your collaborators.</p> <p>The instructions are geared towards members of the niwa02916 group - send a message to support@nesi.org.nz\u00a0if you are a NIWA employee and want to become part of this group. Other NeSI users may want to read\u00a0this, which explains how to install the synda tool. Once installed, you can then type similar commands to the ones below to test your configuration.</p>","tags":[]},{"location":"Storage/Data_Transfer_Services/Download_and_share_CMIP6_data_for_NIWA_researchers/#setup","title":"Setup","text":"<p>On mahuika or\u00a0w-mauivlab01.maui.nesi.org.nz:</p> <pre><code>source /nesi/project/niwa02916/synda_env.sh\n</code></pre> <p>This will load the Anaconda3 environment and set the ST_HOME variable. You should also now be able to invoke synda commands, a tool that can be used to synchronise CMIP data with Earth System Grid Federation archives. A full list of options can be obtained with</p> <pre><code>synda -h\n</code></pre> <p>Below we demonstrate how synda might be used.</p>","tags":[]},{"location":"Storage/Data_Transfer_Services/Download_and_share_CMIP6_data_for_NIWA_researchers/#find-some-datasets","title":"Find some datasets","text":"<p>CMIP6 datasets are organised by institution_id, experiment_id, variable etc. A full list can be glanced from\u00a0https://esgf-node.llnl.gov/search/cmip6/. A possible search might involve</p> <pre><code>synda search institution_id=NCAR experiment_id=1pctCO2 variable=ta\n</code></pre> <p>which returns</p> <p>new\u00a0 CMIP6.CMIP.NCAR.CESM2-WACCM.1pctCO2.r1i1p1f1.day.ta.gn.v20190425</p> <p>new\u00a0 CMIP6.CMIP.NCAR.CESM2-WACCM.1pctCO2.r1i1p1f1.Amon.ta.gn.v20190425</p> <p>...</p> <p>as well as some other datasets.\u00a0</p>","tags":[]},{"location":"Storage/Data_Transfer_Services/Download_and_share_CMIP6_data_for_NIWA_researchers/#find-out-how-big-the-datasets-are","title":"Find out how big the datasets are","text":"<p>Once you know what you want to download, it's a good idea to check the size of the dataset:</p> <pre><code>synda stat\u00a0CMIP6.CMIP.NCAR.CESM2-WACCM.1pctCO2.r1i1p1f1.day.ta.gn.v20190425\n</code></pre> <p>This prints \"Total files count: 16,\u00a0New files count: 16,\u00a0Total size: 48.7 GB,\u00a0New files size: 48.7 GB\". The \"New\" indicates that the files have not yet been downloaded. You can see that there are 16 files to download, taking nearly 50GB of disk space.</p>","tags":[]},{"location":"Storage/Data_Transfer_Services/Download_and_share_CMIP6_data_for_NIWA_researchers/#downloadinstall-the-dataset","title":"Download/install the dataset","text":"<pre><code>synda install\u00a0CMIP6.CMIP.NCAR.CESM2-WACCM.1pctCO2.r1i1p1f1.day.ta.gn.v20190425\n</code></pre> <p>The first time you may get a message requesting you to start a daemon, if so do</p> <pre><code>synda daemon start\n</code></pre> <p>This will put your request in a queue. The transfer will take place as a background process so you can close your terminal if you want and come back later to check progress.</p> <p>The data will end up under\u00a0$ST_HOME/data/CMIP6/CMIP/NCAR/CESM2-WACCM/1pctCO2/r1i1p1f1/day/ta/gn/v20190425 in this case.\u00a0</p> <p>You can type</p> <pre><code>synda queue\n</code></pre> <p>to see progress.</p> <p>Note that if another researcher (or you) decide execute the same synda install command then synda will recognise the files to be already installed and will print the message</p> <p>INFO: Nothing to install (matching files are already installed or waiting in the download queue)</p>","tags":[]},{"location":"Storage/Data_Transfer_Services/Globus_Quick_Start_Guide/","title":"Globus Quick Start Guide","text":"<p>Warning</p> <p>This page has been automatically migrated and may contain formatting errors.</p> <p>This is intended to be a quick-start guide, for more detailed information, please see our other Globus articles here: Globus documentation</p> <p>Globus is a third-party service for transferring large amounts of data between two Globus Data Transfer Nodes (DTNs). To use Globus to transfer data to or from NeSI, you need:</p> <ol> <li>A NeSI account</li> <li>A Globus account</li> <li>Access to Globus DTNs or endpoint  <ul> <li>Access to a DTN (e.g., at your home institution)</li> <li>Personal endpoint if no DTN is available</li> </ul> </li> </ol>","tags":[]},{"location":"Storage/Data_Transfer_Services/Globus_Quick_Start_Guide/#globus-account","title":"Globus Account","text":"<p>Please note that a Globus account is not the same as a NeSI account. You will need both Globus and NeSI accounts in order to transfer data to or from NeSI HPC facilities.</p> <p>To get a Globus account, go to https://transfer.nesi.org.nz/ and sign up using one of the available options on the page. Please note that the \"existing organizational login\" is somewhat limited, if your organisation is not listed, please sign in (sign up) using any of the other methods.</p> <p></p> <p>For more detailed instructions please see Initial Globus Sign-Up, and your Globus Identities.</p>","tags":[]},{"location":"Storage/Data_Transfer_Services/Globus_Quick_Start_Guide/#globus-endpoint-activation","title":"Globus Endpoint Activation","text":"<p>A NeSI account is required in addition to a Globus account to transfer data to or from NeSI facilities.  </p> <p>To transfer data, between two sites, you need to have access to a DTN or endpoint at each location. For example, one on NeSI (NeSI Wellington DTN V5), the other to University of Otago's central file storage. You will also need the appropriate read and write permissions from where you're copying to and from. Please note that the NeSI <code>project</code> directory is read only, and <code>nobackup</code> is read and write.</p> <p>A list of some Institutional endpoints can be found here: National-Data-Transfer-Platform. You can also set up your own personal endpoint to transfer data to or from your personal computer, however, administrative access to your computer is required</p> <p>To activate the NeSI endpoint click go to https://transfer.nesi.org.nz/\u00a0 and click \"file manager\" on the menu bar on the left.</p> <ol> <li>Next to \"Collection\", search for \"NeSI Wellington DTN V5\", select     it, then click \"Continue\".</li> <li>In the 'Username' field, enter your NeSI HPC username. In the     'Password' field, the password is     <code>Login Password (First Factor)</code> +     <code>Authenticator Code (Second Factor)</code>\u00a0e.g.\u00a0<code>password123456</code>. Please     do not save your password on \"Browser settings\" as it will     change every time due to the 2nd factor requirement.</li> </ol> <p></p>","tags":[]},{"location":"Storage/Data_Transfer_Services/Globus_Quick_Start_Guide/#transferring-data","title":"Transferring Data","text":"<p>To transfer data, activate your two endpoints and navigate to the appropriate folders, then select the files or folders of interest. To initiate the transfer, select one of the two directional arrows. In the image below, the 'config' folder is being transferred from the location on the right, to the location on the left.</p> <p></p> <p>To see the progress of the transfer, please click 'Activity' on the left hand menu bar.</p> <p>If you have any questions or issues using Globus to transfer data to or from NeSI, email support@nesi.org.nz.</p>","tags":[]},{"location":"Storage/Data_Transfer_Services/Globus_V5_Paths-Permissions-Storage_Allocation/","title":"Globus V5 Paths, Permissions, Storage Allocation","text":"<p>Warning</p> <p>This page has been automatically migrated and may contain formatting errors.</p>","tags":[]},{"location":"Storage/Data_Transfer_Services/Globus_V5_Paths-Permissions-Storage_Allocation/#globus-default-directory","title":"Globus default directory","text":"<p>If you point Globus File Manager to an endpoint collection where you have an account/access, it will open a single panel pointing to the root path directory, displayed as '<code>/home/&lt;username&gt;</code>'.</p> <p></p>","tags":[]},{"location":"Storage/Data_Transfer_Services/Globus_V5_Paths-Permissions-Storage_Allocation/#on-nesis-mauimahuika-clusters-this-means","title":"On NeSI's M\u0101ui/Mahuika clusters this means:","text":"Globus path Visible to Globus HPC Filesystem Globus usage Permissions <code>/home/&lt;username&gt;</code> yes (default) <code>/home/&lt;username&gt;</code> possible, not recommended read and write access <code>/nesi/nobackup/&lt;project_code&gt;</code> yes <code>/nesi/nobackup/&lt;project_code&gt;</code> yes read and write access <code>/nesi/project/&lt;project_code&gt;</code> yes <code>/nesi/project/&lt;project_code&gt;</code> yes read only access <p>For more information about NeSI filesystem, check here.</p>","tags":[]},{"location":"Storage/Data_Transfer_Services/Globus_V5_Paths-Permissions-Storage_Allocation/#performing-globus-transfers-tofrom-mauimahuika","title":"Performing Globus transfers to/from M\u0101ui/Mahuika","text":"<ul> <li>If transferring files off the cluster, move/copy files onto     <code>/nesi/project</code> or <code>/nesi/nobackup</code> first, via your HPC access</li> <li>Sign in to Globus and navigate the file manager to the path     associated with your project (viz. <code>/nesi/project/&lt;project_code&gt;</code> or     <code>/nesi/nobackup/&lt;project_code&gt;</code>)</li> <li>Click the \"two-panels\" area in the file manager and select the other     endpoint</li> <li>Select source of transfer</li> <li>Transfer data (from), using the appropriate \"start\" button</li> <li>If transferring files onto the cluster, the fastest location will be     <code>/nesi/nobackup/&lt;project_code&gt;</code></li> </ul>","tags":[]},{"location":"Storage/Data_Transfer_Services/Globus_V5_Paths-Permissions-Storage_Allocation/#tips","title":"Tips","text":"<p>1.\u00a0 Globus bookmarks can be created for <code>/nesi/project</code> or <code>/nesi/nobackup</code> paths and these bookmarks pinned.</p> <p>2.\u00a0 Symbolic links can be created in your project directories and nobackup directories to enable easy moving of files to and from. To create a symbolic link from a first to a second directory and vice-versa (using full paths for &lt;first&gt; and &lt;second&gt;):</p> <pre><code>$ cd &lt;first&gt;\n$ ln -s &lt;full_path_to_second&gt; &lt;alias_to_second&gt;\n\n$ cd &lt;second&gt;\n$ ln -s &lt;full_path_to_first&gt;  &lt;alias_to_first&gt;\n</code></pre> <p>Alias can be any value which is convenient to you. .i.e. easy to identify After you do this, there will be an alias listed in each directory that points to the other directory. You can see this with the ls command, and cd from each to the other using its alias.</p>","tags":[]},{"location":"Storage/Data_Transfer_Services/Globus_V5_endpoint_activation/","title":"Globus V5 endpoint activation","text":"<p>Warning</p> <p>This page has been automatically migrated and may contain formatting errors.</p>","tags":[]},{"location":"Storage/Data_Transfer_Services/Globus_V5_endpoint_activation/#activating-an-endpoint","title":"Activating an Endpoint","text":"<p>When you select an endpoint to transfer data to/from, you may be asked to authenticate with that endpoint:</p> <p> Transfers are only possible once you have supplied credentials that authenticate your access to the endpoint. This process is known as \"activating the endpoint\".\u00a0 The endpoint remains active for 24 hours.\u00a0\u00a0</p> <p>The NeSI Wellington DTN V5 endpoint is protected by a second factor authentication (2FA-same as accessing NeSI clusters).\u00a0 In the 'Username' field, enter your M\u0101ui/Mahuika username. In the 'Password' field, your <code>Password</code> will be equal to <code>Login Password (First Factor)</code> + <code>Authenticator Code (Second Factor)</code>\u00a0e.g.\u00a0<code>password123456</code>. (Do not use any additional characters or spaces between your password and the token number.)</p> <p></p> <p>Check the status of your endpoints at https://www.globus.org/app/console/endpoints </p>","tags":[]},{"location":"Storage/Data_Transfer_Services/Globus_V5_endpoint_activation/#managing-endpoint-activation","title":"Managing Endpoint Activation","text":"<p>If a transfer is in progress and will not finish in time before your credentials expire, that transfer will pause and you will need to reauthenticate for it to continue.</p>","tags":[]},{"location":"Storage/Data_Transfer_Services/Initial_Globus_Sign_Up-and_your_Globus_Identities/","title":"Initial Globus Sign-Up, and your Globus Identities","text":"<p>Warning</p> <p>This page has been automatically migrated and may contain formatting errors.</p> <p>Globus provides logins for NeSI users via their organisation, GitHub, Google or GlobusID.</p> <p>To sign up to Globus, please go to https://transfer.nesi.org.nz/</p>","tags":[]},{"location":"Storage/Data_Transfer_Services/Initial_Globus_Sign_Up-and_your_Globus_Identities/#1-sign-up-to-globus","title":"1) Sign-Up to Globus","text":"<p>To sign up to globus, you can look for your organisation in the drop-down box. If your organisation is not present in the drop-down box, you can also use any of the available methods - this then becomes your primary identity in Globus.</p> <p></p>","tags":[]},{"location":"Storage/Data_Transfer_Services/Initial_Globus_Sign_Up-and_your_Globus_Identities/#2-link-other-globus-identities-to-your-primary-identity","title":"2) Link other Globus identities to your primary identity","text":"<p>From the Globus docs:</p> <p>Login to a Globus account, via its primary identity or one of its linked identities, implies login to the account\u2019s primary identity and all identities linked to that account\u2019s primary identity. In other words, login to a Globus account potentially grants access to all resources accessible via all identities linked to that Globus account\u2019s primary identity.</p> <p>If you have other identities in Globus (for example, a globusID), link them\u00a0 to your Google ID account following the instructions at https://docs.globus.org/how-to/link-to-existing/:</p> <p></p> <p>Note:\u00a0</p> <p>If you had a Globus account before February 2016, that account ID is now your \"GlobusID\".</p> <p>Your groups and data-shares are associated with your login so you should ensure that your primary identity is the login you will generally use.</p>","tags":[]},{"location":"Storage/Data_Transfer_Services/National_Data_Transfer_Platform/","title":"National Data Transfer Platform","text":"<p>Warning</p> <p>This page has been automatically migrated and may contain formatting errors.</p> <p>The National Data Transfer Platform uses Globus, a third-party service for transferring large amounts of data between Globus Data Transfer Nodes (DTNs). New Zealand DTNs facilitate the transfer of data to and from institutional storage. This service allows data to be accessible to any person who has a Globus account.</p> Endpoint Name\u00a0 \u00a0\u00a0 Description Recommended Use Apply for Use Contact\u00a0 <code>NeSI Wellington DTN V5</code> NeSI Globus Endpoint Version 5, located at NIWA Wellington (Greta Point) <p>Transferring\u00a0 files to/from M\u0101ui/mahuika, and file-sharing.</p> see conditions support@nesi.org.nz <code>University of Otago \u2013 HCS</code> <code>University of Otago \u2013 CHC HCS</code> Endpoint for the High Capacity Research Storage Cluster, Dunedin Campus, University of Otago and Endpoint for the High Capacity Research Storage Cluster, Christchurch Campus, University of Otago Primary endpoint for Otago Dunedin;\u00a0 uses local service accounts or globus sharing. Complete this form\u00a0 university@otago.ac.nz <code>University of Auckland Research Data Collection</code> Endpoint provides access to UoA research data.\u00a0 Transferring files between UoA research drives and M\u0101ui/mahuika <p>Apply by email</p> researchdata@auckland.ac.nz <code>AgResearch DTN01</code> A Globus endpoint attached to AgResearch\u2019s institutional Linux storage platform Sharing large datasets with external collaborators and moving large datasets between NeSI\u2019s facility and AgResearch\u2019s internal storage platform Apply by email servicedesk@agresearch.co.nz <code>PFR Globus Connect Server</code> Endpoint provides access to Plant &amp; Food Research data\u00a0 Generally for internal users, but also for sharing large datasets with collaborators Contact the Plant and Food person you are wanting to share data with. <p><code>MWLR PN-DTN-username</code></p> Customised endpoints for users to transfer data between MWLR and NeSI, or to share data with third-party collaborators  Generally for internal users, but also for sharing large datasets with collaborators Contact the MWLR person you are wanting to share data with. IToperations@landcareresearch.co.nz <p><code>Scion Data</code></p> <p> </p> Endpoint provides access to Scion research data Sharing large datasets with external collaborators and moving large datasets between NeSI\u2019s facility and Scion\u2019s internal storage platform Contact the Scion person you are wanting to share data with. <code>ESR Endpoint</code> <p> </p>  Endpoint provides access to ESR data\u00a0 Generally for internal users, but also for sharing large datasets with collaborators Contact the ESR person you are wanting to share data with.\u00a0 <code>NeSI Wellington DTN</code> Old NeSI Globus Endpoint, being decommissioned 2021-12-08 <p>Transferring files to/from M\u0101ui/mahuika, and file-sharing.</p> see conditions support@nesi.org.nz","tags":[]},{"location":"Storage/Data_Transfer_Services/National_Data_Transfer_Platform/#how-to-establish-a-new-zealand-node","title":"How to establish a New Zealand node","text":"<p>Contact our Support Team can provide details on how to join the national data transfer platform.</p>","tags":[]},{"location":"Storage/Data_Transfer_Services/Personal_Globus_Endpoint_Configuration/","title":"Personal Globus Endpoint Configuration","text":"<p>Warning</p> <p>This page has been automatically migrated and may contain formatting errors.</p> <p>Using Globus to transfer and share files from/to a personal computer</p>","tags":[]},{"location":"Storage/Data_Transfer_Services/Personal_Globus_Endpoint_Configuration/#install-globus-connect-personal-on-your-workstationlaptop","title":"Install Globus Connect Personal on your workstation/laptop","text":"<p>See the Globus howto page on how to install a personal Globus endpoint on your computer (OS-specific instructions).</p> <p>Once your personal endpoint is created and you have activated it, check Globus's Endpoints administered by you to see whether your endpoint shows up as active.</p> <p></p>","tags":[]},{"location":"Storage/Data_Transfer_Services/Personal_Globus_Endpoint_Configuration/#personal-endpoint-file-transfer-and-sharing","title":"Personal Endpoint file-transfer and sharing","text":"<p>If you need\u00a0to share files from your personal computer or move files between two personal computers,\u00a0you need to be part of a subscription with\u00a0Globus Plus\u00a0enabled.</p> <p>Globus Plus is a part of\u00a0NeSI's Globus subscription. To join Globus Plus, you must become a member of\u00a0the Globus sponsor group New Zealand eScience Infrastructure.</p> <p>Check if your account already has this membership by viewing the Globus Plus tab under your Account:</p> <p></p> <p>If you do not see an entry for\u00a0New Zealand eScience Infrastructure on this page, then:</p> <ol> <li>Click the \"Add Globus Plus Sponsor\" link.</li> <li>Select \"New Zealand eScience Infrastructure\" from the list of     potential sponsors.</li> <li>Follow the on-screen instructions.</li> </ol> <p>Once you have completed the process, your request to join the group will be sent to us for approval. You'll get confirmation when your application has been accepted.\u00a0If you haven't received a confirmation within five business days after you asked to add NeSI as a sponsor, please Contact our Support Team.</p>","tags":[]},{"location":"Storage/Data_Transfer_Services/Re_creating_Shared_Collections_and_Bookmarks_in_Globus_V5/","title":"Re-creating Shared Collections and Bookmarks in Globus V5","text":"<p>Warning</p> <p>This page has been automatically migrated and may contain formatting errors.</p> <p>Shared Collections created in the previous NeSI endpoint NeSI Wellington DTN\u00a0** need to be re-created in the new endpoint NeSI Wellington DTN V5. (The Shared Collections have been renamed Guest Collections).\u00a0\u00a0  </p>","tags":[]},{"location":"Storage/Data_Transfer_Services/Re_creating_Shared_Collections_and_Bookmarks_in_Globus_V5/#guest-collections","title":"Guest Collections","text":"<p>Instructions on creating and sharing Guest Collections are available here</p> <p>In summary:</p> <ol> <li> <p>To re-create existing Collections, select Share and *Create Guest     Collection  </p> <p> </p> <p>* 2.  Enter the file path of the directory to be shared.  </p> <p> </p> <p>This can also be copied from your existing Shared Collection on *NeSI Wellington DTN  </p> <p> </p> <p>* 3.  Add Permissions for an individual or a Group (existing, or create a new group)  </p> <p> </p> </li> <li> <p>Users you share with will receive an email notification containing a     link to the new Guest Collection.</p> </li> </ol>","tags":[]},{"location":"Storage/Data_Transfer_Services/Re_creating_Shared_Collections_and_Bookmarks_in_Globus_V5/#bookmarks","title":"Bookmarks","text":"<ol> <li> <p>Create bookmarks to NeSI Wellington DTN V5 and new Guest     Collections  </p> <p> </p> </li> <li> <p>Bookmarks to NeSI Wellington DTN and Shared Collections on NeSI     Wellington DTN should be deleted.</p> </li> </ol> <p> </p>","tags":[]},{"location":"Storage/Data_Transfer_Services/Syncing_files_between_NeSI_and_another_computer_with_globus_automate/","title":"Sync'ing files between NeSI and another computer with globus-automate","text":"<p>Warning</p> <p>This page has been automatically migrated and may contain formatting errors.</p> <p>It is common to generate large amounts of simulation data on NeSI and then having to migrate the files to another computer for storage or post-processing.</p> <p>Here we show how to transfer data from NeSI to another computer programmatically, that is without using a web graphical user interface and without typing your credentials each time you initiate the transfer.</p> <p>You can also use this approach to synchronise your files, that is to copy only the files that don't yet exist at the destination point, or refresh the files that have changed since you last triggered a transfer.</p> <p>We'll assume that you have a NeSI account, you have registered at https://globus.org, and have created a guest collections on NeSI and and a private mapped collection on the destination computer (follow the instructions our corresponding support page). A guest collection is directory whose content is shared via Globus.</p>","tags":[]},{"location":"Storage/Data_Transfer_Services/Syncing_files_between_NeSI_and_another_computer_with_globus_automate/#step-1-write-a-json-file-describing-the-transfer","title":"Step 1: Write a JSON file describing the transfer","text":"<p>On NeSI, create a file named <code>transfer_input.json</code> with the following content:</p> <pre><code>{\n  \"source_endpoint_id\": \"ENDPOINT1\",\n  \"destination_endpoint_id\": \"ENDPOINT2\",\n  \"transfer_items\": [\n    {\n      \"source_path\": \"SOURCE_FOLDER\",\n      \"destination_path\": \"DESTINATION_FOLDER\",\n      \"recursive\": true\n    }\n  ],\n  \"sync_level\": SYNC_LEVEL, \n  \"notify_on_succeeded\": true,\n  \"notify_on_failed\": true,\n  \"notify_on_inactive\": true,\n  \"verify_checksum\": true\n}\n</code></pre> <p>where</p> <ul> <li><code>ENDPOINT1</code> is the source endpoint UUID, which you can get     https://app.globus.org/collections by clicking on the collection     of your choice. Using a guest collection will allow you to transfer     the data without two-factor authentication</li> <li><code>ENDPOINT2</code> is the destination UUID, e.g. your personal endpoint     UUID, which may be for your private mapped collection if you're     transferring to your personal computer</li> <li><code>SOURCE_FOLDER</code> is the relative path of the source folder in the     source endpoint. This is a directory, it cannot be a file. Use \"/\"     if you do not intend to transfer the data from sub-directories</li> <li><code>DESTINATION_FOLDER</code> is the absolute path of the destination     folder in the destination endpoint when the destination is a private     mapped collection</li> <li><code>SYNC_LEVEL</code> specifies the synchronisation level in the range 0-3.     <code>SYNC_LEVEL=0</code> will transfer new files that do not exist on     destination. Leaving this setting out will overwrite all the files     on destination. Click     here     to see how other sync_level settings can be used to update data in     the destination directory based on modification time and checksums.</li> </ul>","tags":[]},{"location":"Storage/Data_Transfer_Services/Syncing_files_between_NeSI_and_another_computer_with_globus_automate/#step-2-initiate-the-transfer","title":"Step 2: Initiate the transfer","text":"<p>Load the <code>globus-automate-client</code> environment module</p> <pre><code>module purge &amp;&amp; module load globus-automate-client/0.16.1.post1-gimkl-2022\n</code></pre> <p>then start the transfer using</p> <pre><code>globus-automate action run --action-url https://actions.globus.org/transfer/transfer \\\n    --body transfer_input.json\n</code></pre> <p>The first printed line will display the <code>ACTION_ID</code>. You can monitor progress with</p> <pre><code>globus-automate action status --action-url \\\n    https://actions.globus.org/transfer/transfer ACTION_ID\n</code></pre> <p>or on the web at https://app.globus.org/activity.</p>","tags":[]},{"location":"Storage/File_Systems_and_Quotas/Automatic_cleaning_of_nobackup_file_system/","title":"Automatic cleaning of nobackup file system","text":"<p>Warning</p> <p>This page has been automatically migrated and may contain formatting errors.</p> <p>The automatic cleaning feature is a programme of regular deletion of selected files from project directories in our nobackup file system. We do this to optimise the availability of this file system for active research computing workloads and to ensure NeSI can reliably support large-scale compute and analytics workflows.</p> <p>Files are deleted if they meet all of the following criteria:</p> <ul> <li>The file was first created more than 120 days ago, and has not been     accessed, and neither its data nor its metadata has been modified,     for at least 120 days.</li> <li>The file was identified as a candidate for deletion two weeks     previously, and as such is listed in a the project's     nobackup\u00a0<code>.policy</code> directory.</li> </ul> <p>Prerequisite</p> <p>You can get a list of files marked for deletion with the command  <code>nn_doomed_list</code>.  Usage: nn_doomed_list [-h] [--project [PROJECTS]]  [--unlimited] [--limit LENGTHLIMIT]  optional arguments:  -h, --help show this help message and exit  --project [PROJECTS], -p [PROJECTS]  Comma-separated list of projects to process. If not given, process all  projects of which the user is a member  --unlimited, -u Do not limit the length of the output file  --limit LENGTHLIMIT, -l LENGTHLIMIT  Maximum length of the output file (lines)  If no arguments are given, nn_doomed_list checks and displays all  project directories the user is a member of.\u00a0  Default limit of the output file is 40 lines.\u00a0</p> <p>The general process will follow a schedule as follows:</p> <ul> <li> <p>Notify (at 106 days), then two weeks later Delete (at 120     days).</p> </li> <li> <p>Every fortnight on Tuesday morning, we will be reviewing files     stored in the nobackup filesystem and identifying candidates for     expiry.</p> </li> <li> <p>Project teams will be notified by email if they have file candidates     for deletion. Emails will be sent two weeks in advance of any     deletion taking place.</p> </li> </ul> <p>Prerequisite</p> <p>Due to the nature of email, we cannot guarantee that any  particular email message will be successfully delivered and  received, for instance our emails could be blocked by your mail  server or your inbox could be too full. We suggest that you check  <code>/nesi/nobackup/&lt;project_code&gt;/.policy</code>\u00a0(see below) for a list of  deletion candidates, for each of your projects, whether you  received an email from us or not.</p> <ul> <li>Immediately after deletion is complete, a new set of candidate files     will be identified for expiry during the next automated cleanup.     These candidate files are all files within the project's nobackup     that have not been created, accessed or modified within the last 106     days.</li> </ul> <p>A file containing the list of candidates for deletion during the next cleanup, along with the date of the next cleanup, will be created in a directory called <code>.policy/to_delete</code> inside the project's nobackup directory. For example, the candidates for future deletion from the directory <code>/nesi/nobackup/nesi12345</code> are recorded in <code>/nesi/nobackup/nesi12345/.policy/to_delete/&lt;date&gt;.filelist.gz</code>. Project team members are able to view the contents of\u00a0<code>.policy</code>\u00a0(but not delete or modify those contents). The gzip compressed filelist can be viewed and searched with the\u00a0<code>zless</code>\u00a0and <code>zgrep</code>\u00a0commands respectively, e.g., <code>zless /nesi/nobackup/nesi12345/.policy/to_delete/&lt;date&gt;.filelist.gz</code>.</p> <p>Prerequisite</p> <p>Objects other than files, such as directories and symbolic links, are  not deleted under this policy, even if at deletion time they are  empty, broken, or otherwise redundant. These entities typically take  up no disk space apart from a small amount of metadata, but still  count towards the project's inode (file count) quota.</p>","tags":[]},{"location":"Storage/File_Systems_and_Quotas/Automatic_cleaning_of_nobackup_file_system/#what-should-i-do-with-expiring-data-on-the-nobackup-filesystem","title":"What should I do with expiring data on the nobackup filesystem?","text":"<p>If the data is transient and no longer required for continued processing on NeSI then we would appreciate if you deleted it yourself, but\u00a0you can also let the automated process do this.</p> <p>If you have files identified as candidates for deletion that you need to keep beyond the scheduled expiry date, you have four options:</p> <ul> <li> <p>Move the file to your persistent project directory,     e.g.,\u00a0<code>/nesi/project/nesi12345</code>. You may need to request more disk     space, more inodes, or both, in your persistent project directory     before you can do this. Contact our Support Team. We     assess such requests on a case-by-case basis.\u00a0 Note:\u00a0 You can save     space by compressing data.\u00a0 Standard tools such as `gzip`     `bzip2` etc are available.</p> </li> <li> <p>Move or copy the file to a storage system outside NeSI, for example     a research storage device at your institution. We expect most     projects to do this for finalised output data and appreciate prompt     egress of data once it is no longer used for processing.</p> </li> <li> <p>Modify the file before the deletion date, in which case the file     will not be deleted even though it is listed in <code>.policy</code>. This must     only be done in cases where you expect to begin active use of the     data again within the next month.</p> </li> <li> <p>Note: Accessing (Open/Close and Open/Save) or Moving (`mv`) does     not update the timestamp of the file. Copying (`cp`) does create a     new timestamped file.  </p> </li> </ul> <p>Prerequisite</p> <p>Doing this for large numbers of files, or for files that together  take up a large amount of disk space, in your project's nobackup  directory, without regard for your project's computational  activity, constitutes a breach of NeSI's acceptable use  policy.</p>","tags":[]},{"location":"Storage/File_Systems_and_Quotas/Automatic_cleaning_of_nobackup_file_system/#where-should-i-put-my-data","title":"Where should I put my data?","text":"How often will my team's HPC jobs be accessing the data? **How often will my team's HPC jobs be modifying the data?\u00a0** **Recommended option\u00a0** Often Often (at least once every two months) Leave in the nobackup directory (but ensure key result data is copied to the persistent project directory) Often Seldom Put in the persistent project directory Seldom Seldom Store the data elsewhere (e.g. at your institution) <p>In general, the persistent project directory should be used for reference data, tools, and job submission and management scripts. The nobackup directory should be used for holding large reference working datasets (e.g., an extraction of compressed input data) and as a destination for writing and modifying temporary data. It can also be used to build and edit code, provided that the code is under version control and changes are regularly checked into upstream revision control systems.</p>","tags":[]},{"location":"Storage/File_Systems_and_Quotas/Automatic_cleaning_of_nobackup_file_system/#if-i-need-a-file-that-was-deleted-from-nobackup-what-should-i-do","title":"If I need a file that was deleted from nobackup, what should I do?","text":"<p>Please [contact our support team Contact our Support Team\u00a0as soon as possible after you find that the file is missing. To reduce the risk of this outcome again in future, please [contact us in advance Contact our Support Team\u00a0so that we can discuss your data storage options with you.</p>","tags":[]},{"location":"Storage/File_Systems_and_Quotas/Automatic_cleaning_of_nobackup_file_system/#i-have-research-data-on-nobackup-that-i-cant-store-in-my-project-directory-or-at-my-institution-right-now-what-should-i-do","title":"I have research data on nobackup that I can't store in my project directory or at my institution right now. What should I do?","text":"<p>Please [contact our support team Contact our Support Team without delay so we can discuss your short- and medium-term data storage needs. Our intention is to work with you to move your valuable data to an appropriate combination of:</p> <ul> <li>persistent project storage on NeSI,</li> <li>high performance /nobackup storage (temporary scratch space) on     NeSI,</li> <li>slow nearline storage (not released yet, on our roadmap), and\u00a0</li> <li>institutional storage infrastructure.</li> </ul>","tags":[]},{"location":"Storage/File_Systems_and_Quotas/Automatic_cleaning_of_nobackup_file_system/#user-webinars","title":"User Webinars","text":"<p>On 14 and 26 November 2019, we hosted webinars to explain these upcoming changes and answer user questions. If you missed these sessions, the archived materials are available at the links below:</p> <ul> <li>Video recordings:\u00a0     14 November 2019 - https://youtu.be/KPNNSwDJU7A      26 November 2019\u00a0(repeat of 14 Nov session)     -\u00a0https://youtu.be/iVTdlsiBTB4</li> <li>Slides:\u00a0 (same slides were used for both presentations) https://drive.google.com/file/d/1kLwghsj9es8oMqdWj-VhUvaklW6JkrwO/view?usp=sharing </li> <li>Q&amp;A transcriptions:\u00a0     14 November 2019     -\u00a0https://drive.google.com/file/d/1tImzibZ3DcN7QOttZEZoYsR43mEiS5KJ/view?usp=sharing      26 November 2019     -\u00a0https://drive.google.com/file/d/1OSb71hhZnjnU9xsRALcpYM485va7aUxK/view?usp=sharing</li> </ul>","tags":[]},{"location":"Storage/File_Systems_and_Quotas/Data_Compression/","title":"Data Compression","text":"<p>Warning</p> <p>This page has been automatically migrated and may contain formatting errors.</p>","tags":["compression","data_compression","lz4","data compression","zlib","z library","z-library"]},{"location":"Storage/File_Systems_and_Quotas/Data_Compression/#background","title":"Background","text":"<p>Spectrum Scale filesystems (previously GPFS) allow users to compress data (but not metadata) transparently on demand without the need to change metadata (file creation and modification dates, etc). This means that that data can be compressed and then used without first needing to decompress it, as the decompression happens automatically in the background without the need for commands. It allows you to treat the data as if it were not compressed.</p> <p>The data will need to be re-compressed after it is used if you are to maintain it in the compressed state. This can be done by the user (front end) or at the back end via a policy for example. In the future, it is intended that there will be some automatic process regularly compressing flagged files, but at this time, it is only done manually by the NeSI team for specific filesets identified as suitable for the compression; or by the user manually on their own filesets.</p> <p>For purposes of this Guide, we are going to focus on the user side and what the users can do. As a default, the Zlib compression algorithm will be used, although depending on the version of the filesystem, additional ones might be added. It is possible to change algorithms at any time for any file (we will cover that further ahead) when the compression is requested.</p>","tags":["compression","data_compression","lz4","data compression","zlib","z library","z-library"]},{"location":"Storage/File_Systems_and_Quotas/Data_Compression/#compression-methods","title":"Compression Methods","text":"<p>There are two methods for compressing and decompressing data: on-demand and deferred:</p>","tags":["compression","data_compression","lz4","data compression","zlib","z library","z-library"]},{"location":"Storage/File_Systems_and_Quotas/Data_Compression/#on-demand-synchronous","title":"On-Demand (synchronous)","text":"<p>note: as at 2 May 2023, the `mm` commands are not available by default, contact support@nesi.org.nz for assistance</p> <p>This method (using the <code>mmchattr</code> command) acts similar to <code>gzip</code>/<code>gunzip</code> commands where the file being targeted is compressed or decompressed on command invocation. If the command fails halfway through the file or is cancelled, the file will be marked as \"illcompressed\" . This state means that the file is only partially compressed.</p> <p><code>ls</code> command will show files with their original sizes. However,\u00a0 <code>du</code> commands will calculate the approximate usage of the file system as opposed to the uncompressed usage. This will be the total counting against quotas as well. Therefore, if files are compressed, quota usage will decrease. And vice versa, if files are decompressed, fully or partially, quota usage will increase. Be aware that if, in the process of decompression, the quota will be exceeded, an error message will be displayed</p>","tags":["compression","data_compression","lz4","data compression","zlib","z library","z-library"]},{"location":"Storage/File_Systems_and_Quotas/Data_Compression/#deferred","title":"Deferred","text":"<p>This method (also using the <code>mmchattr</code> command) does not decompress or compress data immediately but, instead marks them for compression/decompression to be invoked later. The user can later schedule a secondary task to compress or decompress tagged data. In the future, the deferred tag will flag the data for automatic compression/decompression. This tagging process is quick and can be done by using the same command as above with one extra flag (<code>-I defer</code>). During this process, there is no change in space occupancy for any of the files involved.</p>","tags":["compression","data_compression","lz4","data compression","zlib","z library","z-library"]},{"location":"Storage/File_Systems_and_Quotas/Data_Compression/#how-to-process-deferred-tagged-files","title":"How to process deferred tagged files","text":"<p>Users can process compression/decompression on the tagged files via the <code>mmrestripefile</code> command (using <code>-z</code> flag).</p> <pre><code>$ mmrestripefile -z FileA.txt\nScanning FileA.txt\nScan completed successfully.\n</code></pre>","tags":["compression","data_compression","lz4","data compression","zlib","z library","z-library"]},{"location":"Storage/File_Systems_and_Quotas/Data_Compression/#states-of-a-compressed-file","title":"States of a compressed file","text":"<p>Compressed files on Scale filesystems can be in 4 different states depending on the extended attributes of the file when manipulated for compression. We can check those attributes with the <code>mmlsattr</code> command:</p> <pre><code>$ mmlsattr -L FileA.txt\nfile name: FileA.txt\nmetadata replication: 1 max 2\ndata replication: 1 max 2\nimmutable: no\nappendOnly: no\nflags:\nstorage pool name: data\nfileset name: home_user001\nsnapshot name:\ncreation time: Wed Jul 6 00:54:27 2022\nMisc attributes: ARCHIVE\nEncrypted: no\n</code></pre> <p>The misc attributes will have or not have a <code>COMPRESSION</code> value, depending on if the file is or not tagged for compression. In addition, a file will exhibit the flag <code>illcompressed</code> when the desired final state is not the achieved yet (fully compressed or uncompressed).</p> <p>A file that is fully compressed (not showing the flag <code>illcompressed</code> and having the misc attribute <code>COMPRESSION</code> ), if updated or appended data to, becomes automatically <code>illcompressed</code> and either needs to be re-compressed using the <code>mmchattr --compression yes</code> command or the <code>mmrestripefile -z</code> one (because it's already tagged for compression).</p>","tags":["compression","data_compression","lz4","data compression","zlib","z library","z-library"]},{"location":"Storage/File_Systems_and_Quotas/Data_Compression/#the-different-states","title":"The different states","text":"<ul> <li> <p>Uncompressed and untagged for compression (default) - as     shown for the file <code>FileA.txt</code> above.</p> </li> <li> <p>Partially compressed and tagged for compression - When file     is partially compressed (either because it was decompressed for     access or the full compression didn\u2019t finish). It is still marked     for compression as the <code>COMPRESSION</code> misc attribute suggests, but     because it's not fully compressed the <code>illcompressed</code> flag will be     shown.</p> </li> <li> <p>Fully compressed and tagged for compression - The file is     fully compressed to its maximum possible state and because the file     is tagged for compression, only the misc attribute <code>COMPRESSION</code>     will be shown.</p> </li> <li> <p>Full or partially compressed and untagged for compression -     The file might be fully or partially compressed and in this case     because the misc attribute <code>COMPRESSION</code> is not shown, it means the     file is untagged for being compressed (meaning it's tagged to be in     the uncompressed state). When a fully compressed file is untagged,     the flag <code>illcompressed</code> will be shown. After full decompression is     complete the file will become uncompressed and untagged for     compression.</p> </li> </ul>","tags":["compression","data_compression","lz4","data compression","zlib","z library","z-library"]},{"location":"Storage/File_Systems_and_Quotas/Data_Compression/#using-different-compression-algorithms","title":"Using different compression algorithms","text":"<p>The default algorithm is the Zlib and will be shown on the misc attributes of a tagged file as \u201clibrary z\u201d. Depending on the Scale version installed, files can be tagged with different algorithms.</p> <p>Currently supported compression libraries are:</p> <ul> <li> <p>z Cold data. Favours compression efficiency over access speed.</p> </li> <li> <p>lz4 Active, non-specific data. Favours access speed over compression     efficiency.</p> </li> </ul>","tags":["compression","data_compression","lz4","data compression","zlib","z library","z-library"]},{"location":"Storage/File_Systems_and_Quotas/Data_Compression/#performance-impacts","title":"Performance impacts","text":"<p>Experiments showed that I/O performance was definitely affected if a file was in a compressed state. The extent of the effect, however, depends on the magnitude of I/O operations on the affected files.\u00a0 I/O intensive workloads may experience a significant performance drop.  </p> <p>If compression has a significant impact on your software performance, please confirm it first by running a test job with and without compression and then contact us at support@nesi.org.nz. We will help you minimise the impact of compression on your workflow or find other ways to help you manage your project storage.</p> <p>If you are interested in learning more about this type of data compression you can find further details on the IBM website.</p>","tags":["compression","data_compression","lz4","data compression","zlib","z library","z-library"]},{"location":"Storage/File_Systems_and_Quotas/File_permissions_and_groups/","title":"File permissions and groups","text":"<p>Warning</p> <p>This page has been automatically migrated and may contain formatting errors.</p> <p>Prerequisite</p> <ul> <li>How can I let my fellow project team members read or write my      files?</li> <li>How can I give read-only team members access to my      files?</li> <li>NeSI file systems and      quotas</li> </ul> <p>Access to data (i.e. files and directories) on NeSI is controlled by POSIX permissions, supplemented with Access Control Lists (ACLs). Default permissions differ from file system to file system.</p>","tags":[]},{"location":"Storage/File_Systems_and_Quotas/File_permissions_and_groups/#group-membership","title":"Group membership","text":"<p>Each user has a private user group, of which that user is by default the only member. Each user is also a member of various other groups, such as:</p> <ul> <li>A group for each active NeSI project of which that user is a member</li> <li>Groups for all active users, all active Mahuika users, all active     M\u0101ui users, etc. as appropriate</li> <li>A group representing all active NeSI users who are affiliated with     the user's institution</li> <li>Groups for specific licensed software to which that user has been     granted access</li> </ul> <p>You can see which groups you are a member of at any time by running the following command on a Mahuika, M\u0101ui or M\u0101ui ancillary login node:</p> <pre><code>groups\n</code></pre>","tags":[]},{"location":"Storage/File_Systems_and_Quotas/File_permissions_and_groups/#files-in-home-directories","title":"Files in home directories","text":"<p>Your home directory is owned by you, and its group is usually your personal group. (For historical reasons, some NIWA users' home directories have <code>niwa_nesi_users</code> or <code>niwa_unix_users</code> as the group.) By default, files and directories that are created in your home directory, or copied to your home directory from another network or file system, inherit this ownership scheme. You can override these defaults depending on how you use the <code>cp</code>, <code>scp</code>, <code>rsync</code>, etc. commands. Please consult the documentation for your copying program.</p> <p>Prerequisite</p> <p>If you choose to preserve the original owner and group, but that owner  and group (name or numeric ID) don't both exist at the destination,  your files may end up with odd permissions that you can't fix, for  example if you're copying from your workstation to NeSI.</p> <p>The default permissions mode for new home directories is as follows:</p> <ul> <li>The owner has full privileges: read, write, and (where appropriate)     execute.</li> <li>The group and world have no privileges.</li> </ul> <p>Some home directories have the \"setgid\" bit set. This has the effect that files and subdirectories created within the home directory will inherit the owner and group of their parent directory, rather than of the person doing the creating.</p> <p>Home directories and their contents do not have any ACLs by default.</p>","tags":[]},{"location":"Storage/File_Systems_and_Quotas/File_permissions_and_groups/#files-in-project-directories","title":"Files in project directories","text":"<p>Every new project almost always gets two directories, namely a persistent directory in <code>/nesi/project</code>\u00a0and a scratch directory in <code>/nesi/nobackup</code>. Both these directories are group directories. Both top level project directories are owned by root (i.e. the super-user) and their group is the project group. Files and directories that are created in either place are also in the project group, but the owner is the creating user; or, if the entity doing the creating was an automatic process, the user in whose name the process ran.</p> <p>Your project directory and nobackup directory should both have the \"setgid\" bit set, so that files created in either directory inherit the project group.</p> <p>Prerequisite</p> <p>The setgid bit only applies the directory's group to files that are  newly created in that directory, or copied to the directory over the  internet. If a file or directory is moved or copied from elsewhere on  the cluster, using for example the <code>mv</code> or <code>cp</code> command, that file or  directory will keep its original owner and group. Moreover, a  directory moved from elsewhere will probably not have its setgid bit  set, meaning that files and subdirectories later created within that  directory will inherit neither the group nor the setgid bit.  You probably don't want this to happen. For instructions on how to  prevent it, please see our article:\u00a0How can I let my fellow project  team members read or write my  files?</p> <p>By default, the world, i.e. people not in the project team, have no privileges in respect of a project directory, with certain exceptions.</p> <p>Unlike home directories, project directories are set up with ACLs. The default ACL for a project directory is as follows:</p> <ul> <li>The owner of a file or directory is allowed to read, write, execute     and modify the ACL of that file or directory</li> <li>Every member of the file or directory's group is allowed to read,     write and execute the file or directory, but not modify its ACL</li> <li>Members of NeSI's support team are allowed to read and execute the     file or directory, but not change it or modify its ACL</li> </ul> <p>Some projects also have read and execute privileges granted to a group \"apache-web02-access\".</p> <p>Each directory has two ACLs: One is for the directory itself, and the other is for files and directories that are created in future within that directory. We have set up both of these ACLs to be the same as each other for the two top level project directories.</p> <p>Prerequisite</p> <p>Some project teams, especially those with broader memberships, benefit  from read-only groups. A read-only group gets added to a project's ACL  once, and then individual members can be added to or removed from that  group as required. This approach involves much less editing of file  metadata than adding and removing individuals from the ACLs directly.  If you would like a read-only group created for your project, please   Contact our Support Team.</p> <p>The owner of a file or directory may create, edit or revoke that file or directory's ACL and, in the case of a directory, also the directory's default (heritable) ACL.</p> <p>Prerequisite</p> <p>Every time you edit an ACL of a file in the home or persistent project  directory, the file's metadata changes and triggers a backup of that  file. Doing so recursively on a large number of files and directories,  especially if they together amount to a lot of disk space, can strain  our backup system. Please consider carefully before doing a recursive  ACL change, and if possible make the change early on in the life of  the project on NeSI, so that only a few files are affected.</p>","tags":[]},{"location":"Storage/File_Systems_and_Quotas/File_permissions_and_groups/#other-directories","title":"Other directories","text":"<p>We may from time to time create and maintain other directories, for example for users of a particular piece of software or database. If you believe you have data storage requirements that don't neatly fit within the home and project scheme described above, please [contact our support team Contact our Support Team. If we agree to set up a special-purpose directory for you, we will discuss and agree upon a suitable permissions model.</p>","tags":[]},{"location":"Storage/File_Systems_and_Quotas/I-O_Performance_Considerations/","title":"I/O Performance Considerations","text":"<p>Warning</p> <p>This page has been automatically migrated and may contain formatting errors.</p> <p>It is important to understand the different I/O performance characteristics of nodes that connect to storage using native Spectrum Scale clients, and those that employ Cray\u2019s DVS solution.</p> <p>Applications that make heavy demands on metadata services and or have high levels of small I/O activity should generally not be run on M\u0101ui (the Cray XC50).</p>","tags":["storage"]},{"location":"Storage/File_Systems_and_Quotas/I-O_Performance_Considerations/#nodes-which-access-storage-via-native-spectrum-scale-clients","title":"Nodes which access storage via native Spectrum Scale Clients","text":"<p>All Mauhika HPC Cluster, Mahuika Ancillary, M\u0101ui Ancillary and M\u0101ui login (aka build) nodes have native Spectrum Scale clients installed and provide high performance access to storage:</p> <ul> <li>Metadata operations of the order of 190,000 file creates /second to     a unique directory can be expected;</li> <li>For 8MB transfer size, single stream I/O is ~3.3GB/s Write and     ~5GB/s Read;</li> <li>For 4KB transfer size, single stream I/O is ~1.3GB/s Write and     ~2GB/s Read.</li> </ul>","tags":["storage"]},{"location":"Storage/File_Systems_and_Quotas/I-O_Performance_Considerations/#nodes-which-access-storage-via-dvs","title":"Nodes which access storage via DVS","text":"<p>M\u0101ui (XC50) utilizes a file system projection method via software, known as DVS (Data Virtualisation Service), to expose the Spectrum Scale file systems to XC compute nodes. DVS adds an additional layer of hardware and software between the XC compute nodes and storage (see Figure).</p> <p></p> <p>Figure 1: Cray XC50 DVS architecture.</p> <p>This reduces the I/O performance of M\u0101ui for metadata and small (e.g. 4KB) I/O operations. It does not impact the total bandwidth available from the M\u0101ui (i.e. ~130 GB/s when writing to the filesystem). Accordingly, the equivalent performance numbers for DVS connected compute nodes are:</p> <ul> <li>Metadata operations of the order of 36,000 file creates /second to a     unique directory can be expected, i.e. approximately 23% of that     achievable on a node that has a Spectrum Scale client.</li> <li>For 8MB transfer size, single stream I/O, is ~3.2GB/s for Write and     ~3.2 GB/s for Read;</li> <li>For 4KB transfer size, single stream I/O, is ~2.3GB/s for Write and     ~2.5GB/s for Read (when using IOBUF      \u2013 see Caution below). When     IOBUF is not used Read and     Write performance is &lt;1GB/s.</li> </ul> <p>Unless Cray\u2019s  IOBUF  capability is suitable for an application, users should avoid the use of single-stream I/O with small buffers on M\u0101ui.</p>","tags":["storage"]},{"location":"Storage/File_Systems_and_Quotas/I-O_Performance_Considerations/#iobuf-caution","title":"IOBUF - Caution","text":"<p>Cray\u2019s IOBUF ( man iobuf ) is an I/O buffering library that can reduce the I/O wait time for programs that read or write large (or small) files sequentially. IOBUF intercepts standard I/O calls such as read and open and replaces the stdio ( glibc, libio ) layer of buffering with an additional layer of buffering, thus improving program performance by enabling asynchronous prefetching and caching of file data.</p> <p>Caution: IOBUF is not suitable for all I/O styles. IOBUF does not maintain coherent buffering between processes which open the same file. For this reason, do not use IOBUF with shared file I/O, such as MPI-IO routines like MPI_File_write_all . IOBUF is not thread-safe, so do not use it with multithreaded programs in which the threads perform buffered I/O. IOBUF can be linked into programs that use these I/O styles, but buffering should not be enabled on those files.</p>","tags":["storage"]},{"location":"Storage/File_Systems_and_Quotas/I-O_Performance_Considerations/#data-compression","title":"Data compression","text":"<p>The file system the NeSI platforms use allow for transparent compression of data, meaning you can reduce the storage footprint of your data without needing to add any extra steps in your workflow unless you want to decompress the data after use. However, testing has shown that there can be an impact on job performance due to I/O. You can find out more about tests and results with regards to jobs performance of transparent data compression on the NeSI platforms on our Data Compression support page.</p>","tags":["storage"]},{"location":"Storage/File_Systems_and_Quotas/NeSI_File_Systems_and_Quotas/","title":"NeSI File Systems and Quotas","text":"<p>Warning</p> <p>This page has been automatically migrated and may contain formatting errors.</p> <p>Prerequisite</p> <p>Transparent File Compression - we  have recently started rolling out compression of inactive data on the  NeSI Project filesystem. Please see the documentation  below to learn more about how this  works and what data will be compressed.</p> <p>M\u0101ui and Mahuika, along with all the ancillary nodes, share access to the same IBM Storage Scale file systems. Storage Scale was previously known as Spectrum Scale, and before that as GPFS, or General Parallel File System - we'll generally refer to it as \"Scale\" where the context is clear.</p> <p>You may query your actual usage and disk allocations using the following command:\u00a0</p> <p><code>$ nn_storage_quota</code></p> <p>The values for 'nn_storage_quota' are updated approximately every hour and cached between updates.</p>","tags":["info","mahuika","storage","maui","quota"]},{"location":"Storage/File_Systems_and_Quotas/NeSI_File_Systems_and_Quotas/#_1","title":"NeSI File Systems and Quotas","text":"","tags":["info","mahuika","storage","maui","quota"]},{"location":"Storage/File_Systems_and_Quotas/NeSI_File_Systems_and_Quotas/#file-system-specifications","title":"File System Specifications","text":"Filesystem /home /nesi/project /nesi/nobackup /nesi/nearline Default disk space No default; allocations are based on eligibility and technical requirements \u00a0 \u00a0 - soft quota 20 GB 100 GB 10 TB \u00a0 \u00a0 - hard quota 110 GB 12 TB Default file count (inode) No default; allocations are based on eligibility and technical requirements \u00a0 \u00a0 - soft quota 1,000,000 files 100,000 files 1,000,000 files \u00a0 \u00a0 - hard quota 1,100,000 files 110,000 files 1,100,000 files Intended use User-specific files such as configuration files, environment setup, source code, etc. Persistent project-related data, project-related software, etc. Data created or used by compute jobs that is intended to be temporary Medium- to long-term storage of research data associated with past, present or planned compute projects Total capacity 175 TB 1,590 TB 4,400 TB Will grow as tapes are purchased Data retention time 180 days after the user ceases to be a member of any active project 90 days after the end of the project's last HPC Compute &amp; Analytics allocation. See also Transparent File Data Compression. With certain exceptions, individual files will be deleted after being untouched for 120 days. See Automatic cleaning of nobackup file system for more information. 90 days after the end of the project's last HPC Compute &amp; Analytics allocation, all remaining data is subject to deletion. 180 days after the end of the project's last nearline storage allocation Data backup schedule (Excluding snapsots) Daily, last 10 versions of any given file retained for up to 90 days. Daily, last 10 versions of any given file retained for up to 90 days. None Replication\u00a0 between Wellington and Auckland tape libraries (under development) Snapshots Daily (retention period 7 days) Daily (retention period 7 days) None None Access speed Moderate Moderate Fast Slow Access interfaces <ul> <li>Native Scale mounts</li> <li>SCP</li> <li>Globus data transfer</li> </ul> <ul> <li>Native Scale mounts</li> <li>SCP</li> </ul> <ul> <li>Native Scale mounts</li> <li>SCP</li> <li>Globus data transfer</li> </ul> Nearline commands","tags":["info","mahuika","storage","maui","quota"]},{"location":"Storage/File_Systems_and_Quotas/NeSI_File_Systems_and_Quotas/#soft-versus-hard-quotas","title":"Soft versus hard quotas","text":"<p>We use Scale soft and hard quotas for both disk space and inodes.</p> <ul> <li>Once you exceed a fileset's soft quota, a one-week countdown timer     starts. When that timer runs out, you will no longer be able to     create new files or write more data in that fileset. You can reset     the countdown timer by dropping down to under the soft quota limit.</li> <li>You will not be permitted to exceed a fileset's hard quota at all.     Any attempt to try will produce an error; the precise error will     depend on how your software responds to running out of disk space.</li> </ul> <p>When quotas are first applied to a fileset, or are reduced, it is possible to end up with more data or files in the fileset than the quota allows for. This outcome does not trigger deletion of any existing data, but will prevent creation of new data or files.</p>","tags":["info","mahuika","storage","maui","quota"]},{"location":"Storage/File_Systems_and_Quotas/NeSI_File_Systems_and_Quotas/#notes","title":"Notes:","text":"<ul> <li>You may request an increase in storage and inode quota if needed by     a project. This may in turn be reduced as part of managing overall     risk, where large amounts of quota aren't used for a long period (~6     Months).</li> <li>If you need to compile or install a software package that is large     or is intended for use by a project team, please build it     in\u00a0<code>/nesi/project/&lt;project_code&gt;</code>\u00a0rather than <code>/home/&lt;username&gt;</code>.</li> <li>As the <code>/nesi/nobackup</code> file system provides the highest     performance, input files should be moved or copied to this file     system before starting any job that makes use of them. Likewise, job     scripts should be written so as to write output files to the     <code>/nesi/nobackup</code> file system. If you wish to keep your data for the     long term, you can include as a final part of your job script an     operation to copy or move the output data to the <code>/nesi/project</code>     file system.</li> <li>Keep in mind that data on <code>/nesi/nobackup</code> is not backed up,     therefore users are advised to move valuable data     to\u00a0<code>/nesi/project/&lt;project_code&gt;</code>, or, if the data is seldom used,     to other storage such as an institutional storage facility, as soon     as batch jobs are completed. Please do not use the <code>touch</code>     command to prevent the cleaning policy from removing files, because     this behaviour would deprive the community of a shared resource.</li> </ul>","tags":["info","mahuika","storage","maui","quota"]},{"location":"Storage/File_Systems_and_Quotas/NeSI_File_Systems_and_Quotas/#home","title":"/home","text":"<p>This file system is accessible from login, compute and ancillary nodes. Users should not run jobs from this filesystem. All home directories are backed up daily, both via the Spectrum Protect backup system, which retains the last 10 versions of all files for up to 90 days, and via Scale snapshots. No cleaning policy will be applied to your home directory as long as your My NeSI account is active and you are a member of at least one active project.</p>","tags":["info","mahuika","storage","maui","quota"]},{"location":"Storage/File_Systems_and_Quotas/NeSI_File_Systems_and_Quotas/#nesiproject","title":"/nesi/project","text":"<p>This filesystem is accessible from all login, compute and ancillary nodes. Contents are backed up daily, via the Spectrum Protect backup system, which retains the last 10 versions of all files for 90 days. No cleaning policy is applied.</p> <p>It provides storage space for datasets, shared code or configuration scripts that need to be accessed by users within a project, and potentially by other projects. Read and write performance increases using larger files, therefore you should consider archiving small files with the <code>nn_archive_files</code> utility, or a similar archiving package such as\u00a0<code>tar</code> .</p> <p>Each NeSI project receives quota allocations for <code>/nesi/project/&lt;project_code&gt;</code>, based on the requirements you tell us about in your\u00a0application for a new NeSI project, and separately covering disk space and number of files.</p>","tags":["info","mahuika","storage","maui","quota"]},{"location":"Storage/File_Systems_and_Quotas/NeSI_File_Systems_and_Quotas/#nesinobackup","title":"/nesi/nobackup","text":"<p>The <code>/nesi/nobackup</code> file system has the highest performance of all NeSI file systems, with greater than 140 GB/s bandwidth from compute nodes to disk. It provides access to a very large (4.4 PB) resource for short-term project usage.</p> <p>To prevent project teams from inadvertently bringing the file system down for everyone by writing unexpectedly large amounts of data, we apply per-project quotas to both disk space and number of files on this file system. The default per-project quotas are as described in the above table; if you require more temporary (scratch) space for your project than the default quota allows for, you can discuss your requirements with us during the project application process, or  Contact our Support Team at any time.</p> <p>To ensure this file system remains fit-for-purpose, we have a regular cleaning policy as described in Automatic cleaning of nobackup filesystem.</p> <p>Do not use the <code>touch</code> command or an equivalent to prevent the cleaning policy from removing unused files, because this behaviour would deprive the community of a shared resource.</p> <p>The purpose of this policy is to ensure that any user will be able to analyse datasets up to 1 PB in size.</p>","tags":["info","mahuika","storage","maui","quota"]},{"location":"Storage/File_Systems_and_Quotas/NeSI_File_Systems_and_Quotas/#nesinearline","title":"/nesi/nearline","text":"<p>Prerequisite</p> <p>The nearline service, including its associated file systems, is in an  Early Access phase, and allocations are by invitation. We appreciate  your patience as we develop, test and deploy this service. If you  would like to participate in the Early Access Programme, please  [contact our support  team Contact our Support Team.</p> <p>The <code>/nesi/nearline</code> filesystem is a data cache for the Hierarchical Storage Management System, which automatically manages the movement of files between high performance disk storage and magnetic tape storage in an Automatic Tape Library (ATL). Files will remain on <code>/nesi/nearline</code> temporarily, typically for hours to days, before being moved to tape. A catalogue of files on tape will remain on the disk for quick access.</p> <p>See this page for more information about the nearline service.</p>","tags":["info","mahuika","storage","maui","quota"]},{"location":"Storage/File_Systems_and_Quotas/NeSI_File_Systems_and_Quotas/#snapshots","title":"Snapshots","text":"<p>If you have accidentally deleted data you can recover it from a\u00a0snapshot. Snapshots are taken daily of <code>home/</code> and <code>project</code> directories If you cannot find it in a snapshot, please ask us to recover it for you by emailing NeSI Support.</p>","tags":["info","mahuika","storage","maui","quota"]},{"location":"Storage/File_Systems_and_Quotas/NeSI_File_Systems_and_Quotas/#contributions-of-small-files-towards-quotas","title":"Contributions of Small Files Towards Quotas","text":"<p>The Scale file system makes use of a feature called data-in-inode. This feature will ensure that, once all of a (non-encrypted) file's required metadata has been written to our metadata storage, if all the file's data is able to fit within the file's remaining inode space (4 KiB minus metadata), it will be written there instead of to the data storage.</p> <p>For files larger than 4 KiB (minus the space needed to store the file's metadata), the data written to disk will be stored in one or more sub-blocks of 256 KiB each (which are 1/32 of the filesystem Block Size), and the \"size\" allocated on disk will be rounded up to the nearest 256 KiB. Users or projects requiring many small files may find themselves using large amounts of disk space. Use of data-in-inode mitigates the effect of a large block size on such people and project teams.</p> <p>However, very small files, as well as zero-size entities such as directories and symbolic links, still count towards the relevant fileset's inode quota. If therefore you expect you will need to store large numbers of very small files in your home directory or in a project's persistent storage, please [contact our support team Contact our Support Team to discuss your storage needs.</p>","tags":["info","mahuika","storage","maui","quota"]},{"location":"Storage/File_Systems_and_Quotas/NeSI_File_Systems_and_Quotas/#transparent-file-data-compression","title":"Transparent File Data Compression","text":"<p>The Scale file system has the ability to transparently compress file data. That is, file contents/data can be compressed behind the scenes, taking up less space on disk, while appearing uncompressed to applications reading or altering the file. Scale automatically handles decompression before passing data to user-space applications. This in-line decompression may have a small IO performance/latency impact, though this is mitigated by space and bandwidth savings.</p> <p>Transparent file data compression can be controlled and applied by users via file attributes, you can find out more about using this method on our Data Compression support page. File data compression can also be automatically applied by administrators through the Scale policy engine. We leverage this latter feature to regularly identify and compress inactive data on the <code>/nesi/project</code> file system.</p>","tags":["info","mahuika","storage","maui","quota"]},{"location":"Storage/File_Systems_and_Quotas/NeSI_File_Systems_and_Quotas/#what-project-data-is-automatically-compressed","title":"What Project data is automatically compressed?","text":"<p>Our current policy compresses files that have not been accessed (either read from or written to) within the last 365 days, i.e., very inactive cold data. We may decrease this in future.</p> <p>Additionally, we only automatically compress files in the range of 4kB - 10GB in size. Files larger than this can be compressed by user interaction - see the instructions for the\u00a0<code>mmchattr</code> command on the\u00a0Data Compression support page. Also note that the Scale filesystem will only store compressed blocks when the compression space saving is &gt;=10%.</p>","tags":["info","mahuika","storage","maui","quota"]},{"location":"Storage/Nearline_long_term_storage/Nearline_Long_Term_Storage_Service/","title":"Nearline Long-Term Storage Service","text":"<p>NeSI's Nearline service allows you to store your data on our hierarchical system, which consists of a staging area (disk) connected to a tape library. Users of this service gain access to more persistent storage space for their research data, in return for slower access to those files that are stored on tape. We recommend that you use this service for larger datasets that you will only need to access occasionally and will not need to change in situ. The retrieval of data may be delayed, due to tape handling, queuing of the nearline backend service and size of the data to be ingested or retrieved.</p> <p>Due to the tape storage backend Nearline is intended for use with relatively large files and should not be used for a large number of small files. Files smaller than 64 MB will not be accepted for upload and should be combined into archive files using <code>nn_archive_files</code>, <code>tar</code> or a similar tool. Likewise, Nearline write semantics are different from a normal filesystem - overwriting existing files (e.g. when the source data has been updated) is not supported, these must first be removed (purged from Nearline) before being written (put to Nearline) again.</p> <p>Warning</p> <p>A Nearline project gets locked when writing to or deleting from it. Until this process is finished no other write or delete operation can be performed on the same project and the user will see a status message \"project locked by none\".</p>","tags":["storage","nearline","tape"]},{"location":"Storage/Nearline_long_term_storage/Nearline_Long_Term_Storage_Service/#what-you-can-do","title":"What you can do","text":"<p>The client allows you to carry out the following operations:</p> <ul> <li>View files: View a list of files stored in a Nearline directory.</li> <li>Traverse a directory: View a list of files stored in a Nearline directory, including files stored in all its subdirectories.</li> <li>Put: Copy files from your project or nobackup folder into Nearline.</li> <li>Get: Retrieve files from Nearline into your project or nobackup folder, without deleting them from Nearline.</li> <li>Compare the contents of a local directory with the contents of a Nearline directory.</li> <li>Purge: Delete files stored in Nearline.</li> <li>View job status: View a list of jobs (put/get/purge) you have run, along with their status.</li> <li>View quota: View your Nearline quota and usage.</li> </ul>","tags":["storage","nearline","tape"]},{"location":"Storage/Nearline_long_term_storage/Nearline_Long_Term_Storage_Service/#getting-started","title":"Getting started","text":"<p>Before getting started, your account will must be 'activated' to use the Nearline service. This involves issuing a SSL certificate to your user profile and provisioning your Nearline directories. To have this done for you, please Contact our Support Team.</p> <p>Nearline has a common tool for access, with a set of <code>nl*</code> commands, which are accessible by loading the following module:</p> <pre><code>module load nearline\n</code></pre>","tags":["storage","nearline","tape"]},{"location":"Storage/Nearline_long_term_storage/Nearline_Long_Term_Storage_Service/#viewing-files-in-nearline","title":"Viewing files in nearline","text":"<p>With the following command, you can print the list of files and directories within the specified Nearline directory:</p> <pre><code>nlls /nesi/nearline/&lt;projectID&gt;\n</code></pre> <p>Similar to the shell command <code>ls</code> you can list subdirectories as well:</p> <pre><code>nlls /nesi/nearline/&lt;projectID&gt;/results/\n</code></pre> <p>Furthermore, you can use the additional option <code>-l</code> to get the detailed list including <code>mode</code>, <code>owner</code>, <code>group</code>, <code>filesize</code>, and <code>timestamp</code>. The option <code>-s</code>, an alternative to <code>-l</code>, will additionally show each file's migration status. Note that, due to technical limitations, <code>-s</code> does not work on single files and so <code>nlls -s</code> requires a directory as its argument.</p> <pre><code>$ nlls -s /nesi/nearline/nesi12345/results/\nmode        s  owner               group      filesize    timestamp    filename\n___________________________________________________________________________________________________________________________\n-rw-rw----+ r  userName        nesi12345      33.93 MB       Jun 17    file1.tar.gz\n-rw-rw----+ r  userName        nesi12345      33.93 MB       Jun 17    file2.tar.gz\n-rw-rw----+ r  userName        nesi12345      34.03 MB       Jun 17    file3.tar.gz\n</code></pre> <p>Status (\"s\" column of the <code>-s</code> output) legend:</p> <ul> <li>migrated (m) - data of a specific Nearline file is on tape (does   not necessarily mean that the file is replicated across sites)</li> <li>pre-migrated (p) - data of a specific Nearline file is on both   the staging filesystem and the tape.</li> <li>resident (r) - data of a specific Nearline file is only on the   staging filesystem.</li> </ul> <p>Bug</p> <p>The <code>-l</code> and\u00a0<code>-s</code> flags may fail if the nearline directory has a large amount of files. You will receive a long Python stack trace if this occurs.</p>","tags":["storage","nearline","tape"]},{"location":"Storage/Nearline_long_term_storage/Nearline_Long_Term_Storage_Service/#traversing-files-within-nearline","title":"Traversing files within nearline","text":"<p>If you want to see all the files within a Nearline directory and its subdirectories, you can run <code>nltraverse</code>.</p> <pre><code>nltraverse /nesi/nearline/&lt;projectID&gt;\n</code></pre> <p>Optionally, you can run <code>nltraverse</code> with the <code>-s</code> command-line switch, which, as with <code>nlls</code>, will display the migration status of each file found.</p> <p>Bug</p> <p>The <code>-s</code>flag may fail if a nearline directory has a large amount of files.\u00a0  You will receive a long Python stack trace if this occurs.</p>","tags":["storage","nearline","tape"]},{"location":"Storage/Nearline_long_term_storage/Nearline_Long_Term_Storage_Service/#comparing-files-in-nearline-to-those-on-disk","title":"Comparing files in nearline to those on disk","text":"<p>If you want to compare a local (online storage) directory to a directory on Nearline, you can use the <code>nlcompare</code> command. The syntax of this command is:</p> <pre><code>nlcompare &lt;local_directory&gt; &lt;nearline_directory&gt;\n</code></pre> <p>This command will print out the lists of files giving their last modified times, sizes and file paths.</p> <p><code>nlcompare</code> is particularly useful if you want to compare a directory on Nearline to a corresponding directory in <code>/nesi/project</code> or <code>/nesi/nobackup</code>. See Verifying uploads to Nearline storage for more information on how to do a comparison and verification.</p> <p>If the contents of the Nearline directory and the corresponding local directory differ, the lists will be kept, and can be compared using any text file comparison program, such as <code>diff</code> or <code>vimdiff</code>.</p>","tags":["storage","nearline","tape"]},{"location":"Storage/Nearline_long_term_storage/Nearline_Long_Term_Storage_Service/#puttingingesting-files-into-nearline","title":"Putting/Ingesting files into nearline","text":"<p>Data can be copied to Nearline using the <code>nlput</code> command. The syntax is:</p> <pre><code>nlput [ --nowait ] &lt;projectID&gt; { &lt;src_dir&gt; | &lt;file_list&gt; }\n</code></pre> <p>The source directory or file list needs to be located under <code>/nesi/</code><code>project</code><code>/</code> or <code>/nesi/</code><code>nobackup</code><code>/</code>and specified as such.</p> <p>Note</p> <p>The following will not work:  <pre><code>cd /nesi/project/nesi12345\nnlput nesi12345 some_directory\n</code></pre>  It is necessary to do this instead:  <pre><code>nlput nesi12345 /nesi/project/nesi12345/some_directory\n</code></pre></p> <p>The data will be mapped into the same directory structure under <code>/nesi/</code><code>nearline</code><code>/</code>\u00a0(see below).</p> <p>Warning</p> <p>Please ensure your file or directory names do not contain spaces,  non-standard characters or symbols. This may cause issues when  uploading or downloading files.</p> <p>The recommended file size to archive is between 1 GB and 1 TB. The client will not accept any directory or file list containing any file smaller than 64 MB or larger than 1 TB.</p> <p>The Nearline client also checks file and directory permissions. Specifically, before uploading a directory or the contents of a file list, <code>nlput</code> will check the following, and will reject any directory or file list that does not satisfy all these criteria:</p> <ul> <li>Every file must be readable by you, the operator.</li> <li>Every file must be readable and writable by its owner.</li> <li>Every file must be readable and writable by its group.</li> <li>The POSIX group of every file must be the project selected for   upload.</li> </ul> <p>If you are uploading a directory rather than the contents of a file list, the following additional permission restrictions apply:</p> <ul> <li>Every subdirectory must be readable, writable and executable by its   owner.</li> <li>Every subdirectory must be readable, writable and executable by its   group.</li> <li>The POSIX group of every subdirectory must be the project selected   for upload.</li> </ul> <p>The existing directory structure starting after <code>/nesi/project/&lt;projectID&gt;/</code> or <code>/nesi/nobackup/&lt;projectID&gt;/</code> will be mapped onto <code>/nesi/nearline/&lt;projectID&gt;/</code></p> <p>Warning</p> <p>Files and directories are checked for existence and only new files are transferred to Nearline. Files already on Nearline will not be updated to reflect newer source files. Thus, files that already exist on Nearline (either tape or staging disk) will be skipped in the migration process, though you should receive a notification of this If you wish to replace an existing file at a specific file path (instead of creating a copy at a different file path) then the original copy on Nearline must be purged.</p> <p><code>nlput</code> takes only a directory or a file list. A single file is treated as a file list and read line by line, searching for valid file names. Single files can only be migrated using a file list containing the full path of the file to be transferred.</p>","tags":["storage","nearline","tape"]},{"location":"Storage/Nearline_long_term_storage/Nearline_Long_Term_Storage_Service/#put-directory","title":"Put - directory","text":"<p>Warning</p> <p>If you try to upload to Nearline a path containing spaces, especially multiple consecutive spaces, you will get some very unexpected results, such as the job being dropped. We are aware of the issue and may introduce a fix in a future release. In the meantime, we suggest avoiding supplying such arguments to <code>nlput</code>. You can work around it by renaming the directory and all its ancestors to avoid spaces, or by putting the directory (or its ancestor whose name contains a space) into an archive file. This problem does not affect when your directory to upload happens to have contents (files or directories) with spaces in their names, i.e. to cause a problem the space must be in the name of the directory to be uploaded or one of its ancestor directories.</p> <p>All files and subdirectories within a specified directory will be transferred into Nearline. The target location maps with the source location. As an example:</p> <pre><code>nlput nesi12345 /nesi/nobackup/nesi12345/To/Archive/Results/\n</code></pre> <p>will copy all data within the <code>Results</code> directory into</p> <p><code>/nesi/nearline/nesi12345/To/Archive/Results/</code>.</p> <p>Warning</p> <p>If you put <code>/nesi/</code><code>project</code><code>/nesi12345/To/Archive/Results/</code> on Nearline as well as <code>/nesi/</code><code>nobackup</code><code>/nesi12345/To/Archive/Results/</code>, the contents of both source locations (<code>project</code> and <code>nobackup</code>) will be merged into <code>/nesi/nearline/nesi12345/To/Archive/Results/</code>. Within <code>/nesi/nearline/nesi12345/</code>,\u00a0files with the same name and path will be skipped.</p>","tags":["storage","nearline","tape"]},{"location":"Storage/Nearline_long_term_storage/Nearline_Long_Term_Storage_Service/#put-file-list","title":"Put - file list","text":"<p>Warning</p> <p>The file list must be located within <code>/nesi/project</code> or <code>/nesi/nobackup</code>. Any other location will cause obscure errors and failures.</p> <p>The <code>file_list</code> is a file containing a list of files to be transferred. It can specify only one file per line\u00a0and directories are ignored.</p> <p>The target location will again map with the source location, see above.</p>","tags":["storage","nearline","tape"]},{"location":"Storage/Nearline_long_term_storage/Nearline_Long_Term_Storage_Service/#update","title":"Update","text":"<p>As a good practice:</p> <ul> <li>Migrate only large files (SquashFS archives, tarballs, or files that   are individually large), or directories containing exclusively large   files.</li> <li>Do not try to modify a file in the source (nobackup or project)   directory once there is a copy of it on Nearline.</li> <li>Before deleting any data from your project or nobackup directory   that has been uploaded to Nearline, please consider whether you   require verification of the   transfer.   We recommend that you do at least a basic verification of all   transfers.</li> </ul> <p>If you need to update data on the Nearline file system with a newer version of data from nobackup or project:</p> <ol> <li>Compare the contents of the source directory    (on\u00a0<code>/nesi/project</code>\u00a0or\u00a0<code>/nesi/nobackup</code>) and the target directory    (on\u00a0<code>/nesi/nearline</code>). To look at one directory    on\u00a0<code>/nesi/nearline</code>\u00a0at a time, use\u00a0<code>nlls</code>; if you need to compare a    large number of files across a range of directories, or for more    thorough verification (e.g. checksums), read this    article    or Contact our Support Team.</li> <li>Once you know which files you need to update (i.e. only files whose    Nearline version is out of date), remove the old files on Nearline    using <code>nlpurge</code>.</li> <li>Copy the updated files to the Nearline file system using <code>nlput</code>.</li> </ol> <p>Warning</p> <p>For technical reasons, files (data and metadata) and directory structures on Nearline cannot be safely changed once present, even by the system administrators, except by deletion and recreation. If you wish to rename your files or restructure your directories, you must follow the process below.</p> <p>If you need to edit data, rename files, or restructure directories that exist on Nearline but are no longer on project or nobackup:</p> <ol> <li>Retrieve the files and directories you wish to change using the    <code>nlget</code> command (see below).</li> <li>Make the changes you wish to make.</li> <li>Follow the instructions above for updating data on Nearline with a    new version of the data from project or nobackup.</li> </ol>","tags":["storage","nearline","tape"]},{"location":"Storage/Nearline_long_term_storage/Nearline_Long_Term_Storage_Service/#gettingretrieving-files-from-nearline","title":"Getting/Retrieving files from nearline","text":"<p>Data can be retrieved from Nearline using then <code>nlget</code> command. The syntax is:</p> <pre><code>nlget [ --nowait ] &lt;projectID&gt; { &lt;src_dir&gt; | &lt;file_list&gt; } &lt;dest_dir&gt;\n</code></pre> <p>Similar to <code>nlput</code> (see above), nlget accepts a Nearline directory <code>src_dir</code> (no single files on Nearline accepted) or a local file list <code>file_list</code>, defining the source of the data to be retrieved from Nearline.</p> <p>Warning</p> <ul> <li>The local file list must be located within <code>/nesi/project</code> or   <code>/nesi/nobackup</code>. Any other location will be rejected.</li> <li>Paths to files or directories to be retrieved must be absolute and   start with <code>/nesi/nearline</code>, whether supplied on the command line   (as a directory) or as entries in a file list.</li> <li>Directories whose names contain spaces, especially multiple   consecutive spaces, cannot be retrieved from Nearline directly   using <code>nlget</code>. You must retrieve the contents of such a directory   using a filelist, or retrieve one of its ancestors that doesn't   have a space in the name or path. That is, instead of retrieving   <code>/nesi/project/nesi12345/ab/c\u00a0 d</code> directly, retrieve   <code>/nesi/project/nesi12345/ab</code>. We are aware of the problem and may   address it in a later Nearline release.</li> </ul> <p>The destination <code>dest_dir</code> needs to be defined. The whole directory structure after <code>/nesi/nearline/</code>\u00a0will be created at the destination and the specified data written into it. For example,</p> <pre><code>nlget nesi00000 /nesi/nearline/nesi00000/dir/to/results/ /nesi/nobackup/nesi00000\n</code></pre> <p>will create the directory structure <code>/nesi/nobackup/nesi00000/nesi00000/dir/to/results/</code> if that directory structure does not already exist, and copy the data within the <code>Results</code> directory into it.\u00a0 Note that the output path will include the project root in the path.</p> <p>Warning</p> <p>Any given file will not be retrieved if a file of the same name already exists in the destination directory. If you wish to retrieve a new copy of a file that already exists at the destination directory then you must either change the destination directory, or delete the existing copy of the file in the that directory.</p> <p><code>nlget</code> takes only one directory or one file list. Single files, if local, are treated as a file list and read line by line, searching for valid file names. A single Nearline file can only be retrieved using a local file list specifying the full path of the file to be retrieved.</p>","tags":["storage","nearline","tape"]},{"location":"Storage/Nearline_long_term_storage/Nearline_Long_Term_Storage_Service/#purgingremoving-files-from-nearline","title":"Purging/Removing files from nearline","text":"<p>The <code>nlpurge</code> command deletes specified data on the Nearline file system permanently. The syntax is</p> <pre><code>nlpurge [--nowait] &lt;src_dir&gt;\nnlpurge [ --nowait ] &lt;projectID&gt; { &lt;src_dir&gt; | &lt;file_list&gt; }\n</code></pre> <p>A directory <code>src_dir</code> already on Nearline (no single files accepted) or a file list <code>file_list</code> needs to be specified (see <code>nlput</code> above).</p> <p>If the thing to be deleted is a directory, the project code is optional. If you are instead deleting the entries of a file list, the project code is compulsory, and moreover all entries in the file list must denote files within (or supposed to be within) the chosen project's Nearline directory.</p> <p>Warning</p> <ul> <li>If a file list is used, it must be located within <code>/nesi/project</code>   or <code>/nesi/nobackup</code> and referred to by its full path starting with   one of those places (symlinks in the path are OK).</li> <li>Paths to files or directories to be purged must be absolute and   start with <code>/nesi/nearline</code>, whether supplied on the command line   (as a directory) or as entries in a file list.</li> <li>Purging the entire Nearline directory for a project, e.g.   <code>nlpurge /nesi/nearline/nesi12345</code>, is not permitted. To empty a   project's Nearline directory, you must purge its contents one by   one (if directories), or by means of a filelist (if files).</li> </ul>","tags":["storage","nearline","tape"]},{"location":"Storage/Nearline_long_term_storage/Nearline_Long_Term_Storage_Service/#view-nearline-job-status","title":"View nearline\u00a0 job status","text":"<p>The tool\u00a0<code>nljobstatus</code> provides current status of submitted (queued, running and completed) tasks. The syntax is:</p> <pre><code>nljobstatus [ &lt;jobid&gt; ]\n</code></pre> <p>If no job ID is specified the full list of your successfully submitted and accepted jobs is returned. In this list, each job looks like the following:</p> <pre><code>nljobstatus\n+----------+------------+----------------------------+-----------+-------------+\n|  Jobid   | Project ID |         Job Status         | Job Host  |  Job User   |\n+----------+------------+----------------------------+-----------+-------------+\n| 4e23f517 |     13     |   job done successfully    | librarian | userName    |\n| -dfef-40 |            |                            |           |             |\n| e9-a83c- |            |                            |           |             |\n| 3da78b06 |            |                            |           |             |\n|   0310   |            |                            |           |             |\n+----------+------------+----------------------------+-----------+-------------+\n</code></pre> <p>With\u00a0a job identifier <code>jobid</code>, information for a specific job can be listed:</p> <pre><code>nljobstatus 4e23f517-dfef-40e9-a83c-3da78b060310\n+--------------------------------------+\n|                Jobid                 |\n+--------------------------------------+\n| 4e23f517-dfef-40e9-a83c-3da78b060310 |\n+--------------------------------------+\n+------------+-----------------------+-----------+-------------+\n| Project ID |      Job Status       | Job Host  |  Job User   |\n+------------+-----------------------+-----------+-------------+\n|     13     | job done successfully | librarian | userName    |\n+------------+-----------------------+-----------+-------------+\n+---------------------+---------------------+---------------------+\n|   Job Start Time    |   Job Update Time   |    Job End Time     |\n+---------------------+---------------------+---------------------+\n| 2019-09-13T03:11:22 | 2019-09-13T03:11:44 | 2019-09-13T03:11:45 |\n+---------------------+---------------------+---------------------+\n</code></pre> <p>If an <code>nlput</code> or <code>nlpurge</code> is running in that project, the project is locked until the task is finished.</p> <p>If a job stays in one state for an unexpectedly long time, please contact NeSI Support.</p>","tags":["storage","nearline","tape"]},{"location":"Storage/Nearline_long_term_storage/Nearline_Long_Term_Storage_Service/#view-nearline-quota","title":"View nearline quota","text":"<p>With the command <code>nlquotalist</code>, the usage and limits of a Nearline project quota can be listed:</p> <p>The output looks like:</p> <pre><code>nlquotalist nesi12345\nProjectname                                       Available           Used                Inodes         IUsed\n___________________________________________________________________________________________________________________________\nnesi12345                                         30.00 TB            27.16 TB            1000000        412\n</code></pre> <p>This quota is different from the project quota on GPFS (<code>/nesi/project/&lt;projectID&gt;</code>).</p>","tags":["storage","nearline","tape"]},{"location":"Storage/Nearline_long_term_storage/Nearline_Long_Term_Storage_Service/#data-management","title":"Data management","text":"<p>In case you have the same directory structure on your project and nobackup directories, be careful when archiving data from both. They will be merged in the Nearline file system. Further, when retrieving data from Nearline, keep in mind that the directory structure up to your projectID will be retrieved:</p> <p></p>","tags":["storage","nearline","tape"]},{"location":"Storage/Nearline_long_term_storage/Nearline_Long_Term_Storage_Service/#underlying-mechanism","title":"Underlying mechanism","text":"<p>The Nearline file system consists of two parts: Disk, mainly for buffering data, and the tape library. It consists of a client running on the login/compute node and the backend on the Nearline file system. It is important to know that even if you cancel a client process, the corresponding backend process remains scheduled or running until finished.</p> <p>The process of what data goes into tape and when is automated, and is not something you will have control over. The service is designed to optimise interaction with the Nearline filesystem and avoid problem workloads for the benefit of all users.</p> <p>If your files are on tape, it will take time to retrieve them. Access to tape readers is on a first come first served basis, and the amount of wait time will vary dramatically depending on overall usage. We cannot guarantee access to your files within any particular timeframe, and indeed wait times could be hours or even in some cases more than a day.</p>","tags":["storage","nearline","tape"]},{"location":"Storage/Nearline_long_term_storage/Nearline_Long_Term_Storage_Service/#known-issues","title":"Known issues","text":"<p>Prerequisite Retrievals</p> <p>Some users of Nearline have reported that attempts to retrieve files from tape using <code>nlget</code> (see below) will not retrieve all files. Instead, only some files will come back, and the job will finish with the following output: <pre><code>recall failed some syncs might still run (042)\n</code></pre> We are aware of this problem, which is caused by the Nearline job timing out while waiting for a tape drive to become available. This problem may also occur if you attempt to retrieve multiple files, together adding to a large amount of data, from Nearline. Unfortunately, a proper fix requires a fundamental redesign and rebuild of the Nearline server architecture, work that is on hold ending decisions regarding the direction in which we take NeSI's data services. We appreciate your patience as we work through these decisions. In the meantime, if you encounter this problem, the recommended workaround is to wait a couple of hours (or overnight, if at the end of a day) and try again once a tape drive is more likely to be free. You may have to try several times, waiting between each attempt. We apologise for any inconvenience caused to you by tape drive contention.</p>","tags":["storage","nearline","tape"]},{"location":"Storage/Nearline_long_term_storage/Nearline_Long_Term_Storage_Service/#support-contact","title":"Support contact","text":"<p>Please Contact our Support Team about your user experience which may include functionality issues, intuitive or counter-intuitive behaviours, behaviours or features that you like, suggestions for improvements, transfers taking too long, etc.</p> <p>We welcome feedback from our users.</p>","tags":["storage","nearline","tape"]},{"location":"Storage/Nearline_long_term_storage/Preparing_small_files_for_migration_to_Nearline_storage/","title":"Preparing small files for migration to Nearline storage","text":"<p>Migration of files from your project or nobackup directory to your nearline directory is a two-step process. In the first step, the data is copied from project or nobackup to a staging file system with a maximum capacity of 500 TB. In the second step, the data on the staging file system is moved to tape.</p> <p>To reduce the burden on our tape drives and file catalogue, project teams are strongly encouraged to store only large files on nearline, and in fact attemps to upload files smaller than 64 MB will be rejected. Because your project or nobackup directory, or any subdirectory of the same, will almost certainly contain some small files and may have a large number of them, this article offers instructions for how to straightforwardly find all these small files and combine them into a few large archive files, perhaps as few as one.</p>","tags":[]},{"location":"Storage/Nearline_long_term_storage/Preparing_small_files_for_migration_to_Nearline_storage/#cant-i-just-compress-the-whole-project-or-nobackup-directory-or-at-least-all-its-contents","title":"Can't I just compress the whole project (or nobackup) directory, or at least all its contents?","text":"<p>Yes, you certainly can do that. This is unlikely to suit you, however:</p> <ul> <li>Without special options, creating a SquashFS, tarball or other   archive file is effectively taking a copy of the contents of every   file in the directory. Unless your project or nobackup directory   starts out at less than half full, you may well not have the disk   space to create the full file.</li> <li>There are options to some archiving programs, including   the\u00a0<code>nn_archive_files</code>,\u00a0<code>mksquashfs</code> and\u00a0<code>tar</code> programs, that will   cause the software to delete files during or just after the   compression process. It is likely, however, that you will want at   least some files to remain in your online storage.</li> <li>There are a few projects that have more than 500 TB of data, and   such an archive file would be too big to be copied to the staging   file system. Even if it were not, however, copying one very large   archive file takes a long time, retrieval takes a long time as well,   and since any interruption to either process will necessitate   starting from scratch, the risk of wasted time increases   (interruptions become more likely, and the likely consequences of   interruptions become more severe).</li> </ul>","tags":[]},{"location":"Storage/Nearline_long_term_storage/Preparing_small_files_for_migration_to_Nearline_storage/#what-is-the-recommended-option-then","title":"What is the recommended option, then?","text":"<p>If the directory is too big to be copied as a whole, we recommend that you find all the small files within a directory, then group those small files into an archive file, leaving large files to be copied to nearline individually.</p> <p>You do not have to create one single archive file for all small files in <code>/nesi/project/&lt;project_code&gt;</code> or <code>/nesi/nobackup/&lt;project_code&gt;</code>, and in fact you may prefer to create archive files pertaining to particular subdirectories. There is no harm in either approach.</p> <p>Tip</p> <p>The archive creation process can take quite a long time. So that you can freely log out of the cluster, and to protect the process in case you're accidentally disconnected, you should create the archive by means of a Slurm job, or else in a <code>tmux</code> or <code>screen</code> session.</p> <p>Archive creation is very simple, and can be achieved through the following:</p> <pre><code>startdir=$(pwd -P) &amp;&amp; \\\narchive_file=\"archive.squash\" &amp;&amp; \\\ncd /nesi/project/nesi12345/my_directory &amp;&amp; \\\nfind . -type f -and -size -100M -print0 | xargs -0 -I {} nn_archive_files -p nesi12345 -t &lt;time-limit&gt; -n &lt;num-processors&gt; --verify --append --delete-files -- {} \"${archive_file}\"\n# Return to where you started\ncd \"${startdir}\"\n</code></pre> <p>Some notes on the above script:</p> <ul> <li>The name of the archive is saved as a variable, <code>$archive_file</code>, so   that it is kept consistent whenever it is used.</li> <li>While we have suggested creating the archive in situ   (<code>archive_file=\"archive.squash\"</code>) as an example, there is no reason   not to use a relative or even absolute path   (e.g.\u00a0<code>archive_file=\"/path/to/archive.squash\"</code>). You can also put it   where you started running the sequence of commands from:   <code>archive_file=\"${startdir}/archive.squash\"</code>.</li> <li>We recommend going to the directory (<code>cd &lt;dir&gt;</code>) before running the   <code>find</code> command, so that the archive stores files as relative paths,   not absolute paths. This choice will make a big difference when you   come to extract the archive. In the example above, we go one step   further: The &amp;&amp; means, \"Only run the next command if this command is   successful, i.e. it completes with an exit code of 0.\"</li> <li>The <code>-type f</code> option restricts the search to look for files only.   Directories, symbolic links and other items will not be found.   However, files within subdirectories will be found.</li> <li>The <code>-size -100M</code> option restricts the search to items that are less   than 100 MB. This size criterion is not the only valid option, but   it likely represents a good balance between creating an overly large   archive on the one hand, and leaving many small files to be   individually copied on the other.</li> <li>The conjunction <code>-and</code> does exactly what you expect: it limits   search results to items satisfying both criteria. (<code>find</code> also   recognises the option <code>-or</code>, not relevant here.)</li> <li>The option <code>-print0</code> separates results with the null character, so   that spaces and other special characters in file names don't get   misinterpreted as record separators.</li> <li>Piping to <code>xargs -0</code> gracefully handles a long list of arguments   separated by null characters. <code>xargs</code> breaks up long lists of   arguments, sending the arguments in small batches to the simple   command given as an argument to <code>xargs</code>. In this case, that simple   command is <code>nn_archive_files</code> with flags and arguments.</li> <li>The option <code>-I {}</code> to <code>xargs</code> instructs <code>xargs</code> to replace every   later instance of <code>{}</code> with the name of the actual result, in this   case a found file, or more precisely a relative path to a found   file.</li> <li>The\u00a0<code>--append</code> option causes the list of checksums to be appended   to, rather than overwritten.</li> <li><code>--delete-files</code> will delete each found file once that file has been   added to the ever-growing archive.</li> <li>As given above, the command will submit one, or a series of, Slurm   jobs. You can wait until they're done.</li> </ul>","tags":[]},{"location":"Storage/Nearline_long_term_storage/Verifying_uploads_to_Nearline_storage/","title":"Verifying uploads to Nearline storage","text":"<p>Our Long-Term Storage Service is currently in an Early Access phase, and we encourage researchers using the service to verify their data before deleting it from the project directory (persistent storage) or nobackup directory (temporary storage).</p> <p>Service Status</p> <p>The verification options outlined below are intended to support the Early Access phase of Nearline development. Verification options may change as the Early Access Programme continues and as the Nearline service moves into production. We will update our documentation to reflect all such changes. Your feedback on which verification options you think are necessary will help us decide on future directions for the Nearline service. Please Contact our Support Team to request verification or to offer suggestions regarding this or any other aspect of our Nearline service.</p> <p>There are several options for verification, depending on the level of assurance you require.</p>","tags":[]},{"location":"Storage/Nearline_long_term_storage/Verifying_uploads_to_Nearline_storage/#level-1-transfer-status-report","title":"Level 1: Transfer status report","text":"<p>The most basic form of verification is to look at the results of\u00a0<code>nljobstatus</code>. If all the Nearline job IDs associated with movement of data to Nearline (i.e.\u00a0<code>nlput</code> commands) report\u00a0<code>job done successfully</code>, that gives you a basic level of confidence that the files were in fact copied over to nearline.</p> <p>Warning</p> <p>The above check is reliable only if all <code>nlput</code> commands were concerned solely with uploading new files to nearline. Because of the way <code>nlput</code> is designed, a command trying to update files that already existed on nearline will silently skip those files and still report success.</p>","tags":[]},{"location":"Storage/Nearline_long_term_storage/Verifying_uploads_to_Nearline_storage/#level-2-file-counts-and-sizes","title":"Level 2: File counts and sizes","text":"<p>You can get a higher level of assurance by checking the number of files, and their sizes and last modified times, in a particular directory on nearline, and optionally to compare that number and size to the corresponding directory on <code>/nesi/project</code> or <code>/nesi/nobackup</code>. We can also enable comparisons of file permissions if requested, though differences in permissions or even modification times do not necessarily suggest a problem as long as the names and sizes are the same. If you are interested in verifying file permissions, please [contact our support team Contact our Support Team.</p> <p>To get a list of file names, sizes and dates in a particular directory on nearline, run the following command with the necessary modifications. Note that the <code>nlcompare</code> command traverses all subdirectories within your chosen directory, and may therefore take some time if you verify a directory at the top of a complex directory tree.</p> <pre><code>nlcompare &lt;local_directory&gt; &lt;nearline_directory&gt;\n</code></pre> <p>This command will generate lists of files giving their last modified times, sizes and file paths. If there are any differences, the lists will be kept and you will be invited to compare the lists against each other, which you can do using a comparison program such as\u00a0<code>diff</code> or <code>vimdiff</code>.</p> <p>Warning</p> <p>The above check is useful only if the corresponding files in  <code>/nesi/project</code> and/or <code>/nesi/nobackup</code> have not been modified or  deleted, nor any new files added, since they were copied to nearline.  For this reason, if you want to carry out this level of checking, you  should do so as soon as possible after you have established that the  <code>nlput</code> operation completed successfully.</p>","tags":[]},{"location":"Storage/Nearline_long_term_storage/Verifying_uploads_to_Nearline_storage/#level-3-checksums","title":"Level 3: Checksums","text":"<p>For especially important files, you can get a still higher level of assurance by retrieving those files individually or in small numbers from nearline and running checksums (e.g. SHA256 sums) on them, comparing the checksums to the corresponding original files in <code>/nesi/project</code> or <code>/nesi/nobackup</code>. If the checksums come out identical, it is virtually certain that the files contain the same data, even if their modification dates and times are reported differently.</p> <p>Warning</p> <p>The above check is reliable only if the corresponding file in <code>/nesi/project</code> and/or <code>/nesi/nobackup</code> has not been modified since it was copied to nearline. For this reason, if you want to carry out this level of checking, you should do so as soon as possible after you have established that the <code>nlput</code> operation completed successfully and the file has been migrated to tape. Also, this check is very expensive, so you should not perform it on large numbers of files or on files that collectively take up a lot of disk space. Instead, please reserve this level of verification for your most valuable research data.</p>","tags":[]},{"location":"Storage/Release_Notes_Nearline/Long_Term_Storage_Nearline_release_notes_v1-1-0-14/","title":"Long-Term Storage - Nearline release notes  v1.1.0.14","text":"","tags":["releasenote"]},{"location":"Storage/Release_Notes_Nearline/Long_Term_Storage_Nearline_release_notes_v1-1-0-14/#version-11014","title":"Version 1.1.0.14","text":"<p>Released 5 November 2020.</p> <p>This release includes the following changes:</p> <ul> <li><code>nlls</code>, <code>nlget</code>, <code>nlpurge</code>, <code>nlput</code> and <code>nljobstatus</code> now come with     a debug mode, accessible via the <code>--debug</code> command line switch.</li> <li>Help documentation, as well as the usage message when a nearline     command is run with incorrect arguments, has been improved.</li> <li><code>nljobstatus</code> now includes more comprehensive job status     information. In particular, the job status message now includes a     brief description of the stage the job is up to, and whether the job     is at that moment pending (waiting in a queue to start the next     operation), running, or complete.</li> <li>The <code>nlls</code> command's <code>-ls</code> switch has been replaced with <code>-s</code>,     though <code>-ls</code> still works, being interpreted as equivalent to     <code>-l -s</code>. <code>nlls</code> also now comes with a <code>-b</code> switch, for reporting     individual sizes in bytes instead of in human-readable sizes.</li> <li><code>nltraverse</code> has been improved, and now reports file sizes, and sums     of file sizes, in bytes, for greater accuracy and ease of comparison     with the output of <code>ls</code>.</li> <li>There have been numerous other bug fixes to improve performance and     reduce the risk of unexpected failures and errors.</li> </ul>","tags":["releasenote"]},{"location":"Storage/Release_Notes_Nearline/Long_Term_Storage_Nearline_release_notes_v1-1-0-21/","title":"Long Term Storage - Nearline release notes v1.1.0.21","text":"<p>Released Wednesday 4 August 2021.</p> <p>This is a minor release incorporating bug fixes and improvements.</p> <ul> <li>Certain server errors when a bad job is submitted now generate a     more informative error message in the client program than, \"Internal     Server Error.\"</li> <li>Nearline client programs now log to the <code>~/.librarian</code> directory, so     you no longer need to explicitly decorate the Nearline command with     complex strings in order to capture basic troubleshooting     information.</li> <li>A bug causing <code>nlput</code> with a file list to fail if any entries in the     file list were missing from Nearline has been fixed. Now, <code>nlput</code>     will work even though the file is not already present on Nearline.</li> <li><code>nlput</code> no longer throws an exception if, when you are prompted for     a y/n response, you hit Enter thereby submitting an empty string.     Instead, it asks the same question again.</li> <li>If a local directory into which files are to be retrieved does not     exist, <code>nlget</code> will now carry out the retrieval. Previously, <code>nlget</code>     would create the directory but then abandon the retrieval.</li> <li>We have clarified in help messages that <code>nlpurge</code> does not accept a     single file (on Nearline) as the file to be purged. The argument     that is not the project code must be either a directory on Nearline,     or a local file list.</li> <li>A bug has been fixed in the Nearline server whereby the server would     incorrectly calculate the changes to the project's disk space and     file count usage if an <code>nlpurge</code> command were to fail (or skip some     files) for any reason after it was accepted by the server.</li> <li><code>nlpurge</code> can now be used to delete empty directories from Nearline,     provided the directory is given directly as an argument and not     included in a file list.</li> <li><code>nlpurge</code> deals gracefully with the situation in which a directory     to be purged is not a subdirectory somewhere within the specified     project's Nearline directory, by printing an informative error     message.</li> <li><code>nlpurge</code> will no longer accept a file list argument if any of the     entries in the file list point to files (on Nearline) that are     outside the specified project's Nearline directory. Instead, an     error message will be displayed, listing all affected lines in the     file list.</li> <li>A bug that required users to start <code>nlpurge</code> file list entries with     <code>/scale_wlg_nearline/filesets/nearline/</code> has been fixed. Now,     entries must start with the more intuitive <code>/nesi/nearline/</code>.</li> <li>A bug causing <code>nlls</code> (and commands depending on it, like     <code>nltraverse</code>) to fail if an empty directory is listed or included in     the traverse operation has been fixed.</li> </ul>","tags":["releasenote","nearline"]},{"location":"Storage/Release_Notes_Nearline/Long_Term_Storage_Nearline_release_notes_v1-1-0-22/","title":"Long Term Storage - Nearline release notes v1.1.0.22","text":"<p>Released Friday 27 August 2021.</p> <p>This is a minor release incorporating bug fixes and improvements.</p> <ul> <li>A bug causing the programs <code>nlls</code>, <code>nltraverse</code> and <code>nlcompare</code> to     misbehave when dealing with invisible files and directories (whose     names start with <code>.</code>), and other files and directories whose names     contain unorthodox characters such as spaces or other characters     having special meaning to the shell, has been fixed.</li> <li>A bug causing <code>nlls</code> to return <code>Internal Server Error</code> when the     operator specifies a subdirectory of a project directory that     doesn't exist on Nearline has been fixed. The error     <code>no such file or directory</code> is now returned instead.</li> <li>Some small improvements have been made to server configuration     parsing and detection of inappropriate or missing configuration     values.</li> </ul> <p>During testing of this release, we found that attempts to run <code>nlput</code> or <code>nlget</code> using arguments containing spaces, especially multiple consecutive spaces, fail at the Nearline datamover stage while running <code>rsync</code>. This issue has been recorded and documented. For now, the recommended workaround is to rename such files or directories before uploading them to Nearline, or, alternatively, to store them in an archive that does not contain spaces in its name.\u00a0</p>","tags":["releasenote","nearline"]},{"location":"Storage/Release_Notes_Nearline/Long_term_Storage_Nearline_release_notes_v1-1-0-18/","title":"Long-term Storage - Nearline release notes v1.1.0.18","text":"<p>This release incorporates several minor but significant bug fixes and new features.</p> <p>In particular:</p> <ul> <li>To run <code>nljobstatus</code> with a particular job ID, you no longer need     the <code>-j</code> switch before the job ID. <code>nljobstatus &lt;jobID&gt;</code> will     suffice.</li> <li>The <code>nlput</code> program will now check to see whether any of the files     requested for upload already exist on nearline. If it finds any of     them, it will ask you if you want to continue anyway, warning you     that the already existing files will not be altered or updated by     the nlput process.</li> <li>The <code>nlput</code> program will also offer to create a filelist of already     existing files, in order to help you more conveniently delete them     from nearline if you wish to replace them with an updated version.     Users taking advantage of this feature are encouraged to review the     filelist after it has been generated, in case there are any files     included that you do not wish to delete.</li> <li><code>nlput</code>, <code>nlget</code> and <code>nlpurge</code> now verify that files and filelists     are in allowed locations, and (in the case of filelists) that the     individual filelist entries are in allowed locations:<ul> <li>For <code>nlput</code>, all files to be uploaded must be within either     <code>/nesi/project</code> or <code>/nesi/nobackup</code>, whether they come from a     directory or are specified in a filelist</li> <li>For <code>nlget</code>, all files to be retrieved must be within     <code>/nesi/nearline</code>, and the destination must be within     <code>/nesi/project</code> or <code>/nesi/nobackup</code></li> <li>For <code>nlpurge</code>, all files to be deleted must be within     <code>/nesi/nearline</code></li> <li>For <code>nlput</code>, <code>nlget</code> and <code>nlpurge</code> with filelists, the filelist     must be within <code>/nesi/project</code> or <code>/nesi/nobackup</code></li> </ul> </li> <li>A bug causing projects to be locked indefinitely when <code>nlput</code> is     given a filelist as an argument has been fixed.</li> <li>An attempt to remove a nonexistent directory from nearline using     <code>nlpurge</code> will no longer lock the project.</li> <li>Various bugs causing locks to persist on nearline projects even once     the locking process has ended have been fixed. Previously, many     error conditions causing nearline server tasks to end prematurely     would have left orphaned locks on involved projects.</li> </ul>","tags":["releasenote"]},{"location":"Storage/Release_Notes_Nearline/Long_term_Storage_Nearline_release_notes_v1-1-0-19/","title":"Long-term Storage - Nearline release notes v1.1.0.19","text":"<p>Released Thursday 4 March 2021.</p> <p>This release includes a number of significant changes and new features:</p> <ul> <li>The <code>nltraverse</code> command is now supported by an <code>nlcompare</code> command.     With <code>nlcompare</code>, you can compare a directory within <code>/nesi/project</code>     or <code>/nesi/nobackup</code> with a corresponding directory on     <code>/nesi/nearline</code>, and it will show any differences in file names,     sizes, ownerships, permissions and last modified timestamps. Please     note that <code>nlcompare</code> does not compare file contents.</li> <li>File size limits are now in place when running <code>nlput</code> (not     applicable to <code>nlget</code> or <code>nlpurge</code>):<ul> <li>a minimum per-file size limit of 64 MB;</li> <li>a maximum per-file size limit of 1 TB.</li> </ul> </li> <li>Permission restrictions are now in place when running <code>nlput</code> (not     applicable to <code>nlget</code> or <code>nlpurge</code>):<ul> <li>You, as the operator, must be able to read every file selected     for upload.</li> <li>The group of every file must match the project code you choose.     If there is a mismatch, it may be that the project code has been     mistyped.</li> <li>The permissions of every file must be set so that both the     file's owner and the file's group are allowed to read and write     the file.</li> <li>Where a directory (as opposed to a filelist) is specified for     upload, that directory and every subdirectory therein must also     be readable and executable by the operator, belong to the     specified group, and be readable, writable and executable by the     file owner and group.</li> </ul> </li> <li>Attempts to run <code>nlget</code> and <code>nlpurge</code> on files or directories not     present on nearline will now fail before the job is submitted to the     server, with a clear error message, instead of failing on the server     side, after a delay and with an obscure error message.</li> <li>Certain server errors that previously caused <code>KeyError</code> in the     client will now be reported as     <code>RuntimeError: Internal Server Error</code>.</li> <li>Server-side logging and tracking with state files have been     improved.</li> </ul>","tags":["releasenote"]},{"location":"Storage/Release_Notes_Nearline/Long_term_Storage_Nearline_release_notes_v1-1-0-20/","title":"Long-term Storage - Nearline release notes v1.1.0.20","text":"<p>Released Thursday 15 April 2021.</p> <p>This is a minor release incorporating bug fixes and improvements.</p> <ul> <li>The <code>nlcompare</code> command will no longer call attention to differences     between files and directories that are solely due to the expected     difference at the start of the absolute path, i.e. the textual     difference between <code>/nesi/nearline/&lt;project_code&gt;</code> and     <code>/nesi/project/&lt;project_code&gt;</code> (or <code>/nesi/nobackup/&lt;project_code&gt;</code>)     at the start of the path is ignored as irrelevant. <code>nlcompare</code>     continues to highlight the differences that might actually matter:     files present on nearline but missing from the project or nobackup     directory (or vice versa), files that have been renamed, and files     with different sizes or last modified times.</li> <li>The <code>nlget</code> command now gives a prompt and informative error message     if you attempt to retrieve a single file from Nearline, instead of,     as previously, submitting the job to the server, which would, after     a wait that might well be lengthy depending on demand for the     service, respond with <code>pol_failed</code>\u00a0or some other uninformative     error.</li> <li>The <code>nlls</code> command now gives a prompt and meaningful error message     if run on a single file with the <code>-s</code> command-line switch, instead     of, as previously, returning no results.</li> <li>The in-program usage message for the <code>nlpurge</code> command, which is     printed when the wrong number or type of arguments is supplied, has     been improved.</li> <li>For ease of scripting, client or server errors that occur while the     client program is running and you are requesting a nearline     operation will, in almost all cases, cause the nearline client     program to exit with a non-zero exit code. Note that this is not,     and can not be, the case where the error first occurs after the job     has been accepted by the server for processing.</li> </ul>","tags":["releasenote"]},{"location":"assets/glossary/","title":"Index","text":"<p>Now fetching from https://github.com/nesi/nesi-wordlist</p>"},{"location":"assets/glossary/Glossary/","title":"Glossary","text":"<p>ABAQUS:</p> <p>Finite Element Analysis software for modeling, visualization and best-in-class implicit and explicit dynamics FEA.</p> <p>ABRicate:</p> <p>Mass screening of contigs for antimicrobial and virulence genes</p> <p>ABySS:</p> <p>Assembly By Short Sequences - a de novo, parallel, paired-end sequence assembler</p> <p>ACTC:</p> <p>ACTC converts independent triangles into triangle strips or fans.</p> <p>AGAT:</p> <p>Suite of tools to handle gene annotations in any GTF/GFF format.</p> <p>AGE:</p> <p>Alignment of sequences with structural variants.</p> <p>AMOS:</p> <p>Collection of tools for genome assembly</p> <p>AMRFinderPlus:</p> <p>NCBI Antimicrobial Resistance Gene Finder Plus</p> <p>ANIcalculator:</p> <p>Calculate the bidirectional average nucleotide identity (gANI) and</p> <p>ANNOVAR:</p> <p>Efficient software tool to utilize update-to-date information to functionally</p> <p>ANTLR:</p> <p>ANTLR, ANother Tool for Language Recognition, (formerly PCCTS)  is a language tool that provides a framework for constructing recognizers,  compilers, and translators from grammatical descriptions containing  Java, C#, C++, or Python actions.</p> <p>ANTS:</p> <p>Ants is a versatile, easy to use Python library for developing ancillary applications.                   This package is restricted to the UM user group.</p> <p>ANTs:</p> <p>ANTs extracts information from complex datasets that include imaging. ANTs is useful for managing,</p> <p>AOCC:</p> <p>AMD Optimized C/C++ &amp; Fortran compilers (AOCC) based on LLVM 13.0</p> <p>AOCL-BLIS:</p> <p>Optimized version of BLIS for AMD EPYC family of processors..</p> <p>AOCL-FFTW:</p> <p>Optimized version of FFTW for AMD EPYC family of processors.</p> <p>AOCL-ScaLAPACK:</p> <p>Optimized version of ScaLAPACK for AMD EPYC family of processors.</p> <p>APR:</p> <p>Apache Portable Runtime (APR) libraries.</p> <p>APR-util:</p> <p>Apache Portable Runtime (APR) util libraries.</p> <p>ARCSI:</p> <p>The Atmospheric and Radiometric Correction of Satellite Imagery (ARCSI) software provides a command line tool for the atmospheric correction of Earth Observation imagery. The aim of ARCSI is to provide as automatic as possible method of retrieving the atmospheric correction parameters and using them to parameterise 6S.Universal Command Line Environment for AWS.</p> <p>ARIBA:</p> <p>Antimicrobial Resistance Identification By Assembly</p> <p>ATK:</p> <p>ATK provides the set of accessibility interfaces that are implemented by other toolkits and applications.</p> <p>AUGUSTUS:</p> <p>AUGUSTUS is a program that predicts genes in eukaryotic genomic sequences</p> <p>AdapterRemoval:</p> <p>Ssearches for and removes remnant adapter sequences</p> <p>Advisor:</p> <p>Vectorization Optimization and Thread Prototyping</p> <p>AlphaFold:</p> <p>AlphaFold can predict protein structures with atomic accuracy even where no similar structure is known</p> <p>AlphaFold2DB:</p> <p>AlphaFold2 databases</p> <p>AlwaysIntelMKL:</p> <p>Overrides the MKL internal utility function mkl_serv_intel_cpu_true</p> <p>Amber:</p> <p>Amber (originally Assisted Model Building with Energy</p> <p>Anaconda2:</p> <p>Built to complement the rich, open source Python community, the Anaconda platform provides an enterprise-ready data analytics platform  that empowers companies to adopt a modern open data science analytics architecture.  IMPORTANT: This version of Anaconda Python comes with Intel MKL support to speed up            certain types of mathematical computations, such as linear algebra or FFT.            The module sets             MKL_NUM_THREADS=1             to run MKL on a single thread by default, avoiding accidental oversubscription            of cores. The number of threads can be increased for large problems, please            refer to the Intel MKL documentation for guidance.</p> <p>Anaconda3:</p> <p>Built to complement the rich, open source Python community, the Anaconda platform provides an enterprise-ready data analytics platform  that empowers companies to adopt a modern open data science analytics architecture.</p> <p>Apptainer:</p> <p>Apptainer is a portable application stack packaging and runtime utility.</p> <p>Armadillo:</p> <p>C++ linear algebra library (matrix maths) aiming towards a good balance between speed and ease of use.</p> <p>Arrow:</p> <p>Apache Arrow, a cross-language development platform for in-memory data.</p> <p>Aspera-CLI:</p> <p>IBM Aspera Command-Line Interface (the Aspera CLI) is</p> <p>Augustus:</p> <p>AUGUSTUS is a program that predicts genes in eukaryotic genomic sequences.</p> <p>AutoDock-GPU:</p> <p>OpenCL and Cuda accelerated version of AutoDock. It leverages its embarrasingly</p> <p>AutoDock_Vina:</p> <p>AutoDock Vina is an open-source program for doing molecular docking.</p> <p>BBMap:</p> <p>BBMap short read aligner, and other bioinformatic tools.</p> <p>BCFtools:</p> <p>Manipulate variant calls in the Variant Call Format (VCF) and its binary counterpart BCF.</p> <p>BCL-Convert:</p> <p>Converts per cycle binary data output by Illumina sequencers containing basecall</p> <p>BEAST:</p> <p>Bayesian MCMC phylogenetic analysis of molecular sequences for reconstructing</p> <p>BEDOPS:</p> <p>BEDOPS is an open-source command-line toolkit that performs highly</p> <p>BEDTools:</p> <p>The BEDTools utilities allow one to address common genomics tasks such as finding feature overlaps</p> <p>BEEF:</p> <p>BEEF is a library implementing the Bayesian Error Estimation Functional, a description of which can be found here:  http://dx.doi.org/10.1103/PhysRevB.85.235149</p> <p>BLASR:</p> <p>BLASR (Basic Local Alignment with Successive Refinement) rapidly maps</p> <p>BLAST:</p> <p>Basic Local Alignment Search Tool, or BLAST, is an algorithm</p> <p>BLASTDB:</p> <p>BLAST databases downloaded from NCBI.</p> <p>BLAT:</p> <p>BLAT on DNA is designed to quickly find sequences of 95% and greater similarity of length 25 bases or more.</p> <p>BLIS:</p> <p>BLIS is a portable software framework for instantiating high-performance</p> <p>BOLT-LMM:</p> <p>The BOLT-LMM algorithm for mixed model association testing,</p> <p>BRAKER:</p> <p>Pipeline for fully automated prediction of protein coding genes with GeneMark-ES/ET</p> <p>BUSCO:</p> <p>Assessing genome assembly and annotation completeness with Benchmarking Universal Single-Copy Orthologs</p> <p>BWA:</p> <p>Burrows-Wheeler Aligner (BWA) is an efficient program that aligns</p> <p>BamTools:</p> <p>BamTools provides both a programmer's API and an end-user's toolkit for handling BAM files.</p> <p>Bandage:</p> <p>Bandage is a program for visualising de novo assembly graphs</p> <p>Basilisk:</p> <p>Basilisk is a Free Software program for the solution of partial differential equations on adaptive Cartesian meshes.</p> <p>BayPass:</p> <p>Genome-Wide Scan for Adaptive Differentiation and Association Analysis with population-specific covariables</p> <p>BayeScan:</p> <p>Identify candidate loci under natural selection from genetic data,</p> <p>BayesAss:</p> <p>Program for inference of recent immigration rates between populations using unlinked multilocus genotypes</p> <p>Bazel:</p> <p>Bazel is a build tool that builds code quickly and reliably.</p> <p>Beagle:</p> <p>Package for phasing genotypes and for imputing ungenotyped markers.</p> <p>BerkeleyGW:</p> <p>The BerkeleyGW Package is a set of computer codes that calculates the quasiparticle</p> <p>Bifrost:</p> <p>Highly parallel construction, indexing and querying of colored and compacted de Bruijn graphs.</p> <p>Bio-DB-BigFile:</p> <p>Read BigWig and BigBed genome feature databases</p> <p>Bio-DB-HTS:</p> <p>Read files using HTSlib including BAM/CRAM, Tabix and BCF database files</p> <p>BioConductor:</p> <p>Tools for the analysis of high-throughput genomic data in R.</p> <p>Bismark:</p> <p>A tool to map bisulfite converted sequence reads and</p> <p>Bison:</p> <p>Bison is a general-purpose parser generator that converts an annotated context-free grammar into a deterministic LR or generalized LR (GLR) parser employing LALR(1) parser tables.</p> <p>BlenderPy:</p> <p>Blender provides a pipeline for 3D modeling, rigging, animation, simulation, rendering,</p> <p>Boost:</p> <p>Boost provides free peer-reviewed portable C++ source libraries.</p> <p>Bowtie:</p> <p>Ultrafast, memory-efficient short read aligner.</p> <p>Bowtie2:</p> <p>Ultrafast and memory-efficient tool for aligning</p> <p>Bpipe:</p> <p>A platform for running big bioinformatics jobs that consist of a series of processing stages</p> <p>Bracken:</p> <p>Hghly accurate statistical method that computes the abundance of</p> <p>BreakDancer:</p> <p>Genome-wide detection of structural variants from next generation paired-end sequencing reads.</p> <p>BreakSeq2:</p> <p>Nucleotide-resolution analysis of structural variants</p> <p>CCL:</p> <p>Clozure CL (often called CCL for short) is a free Common Lisp implementation</p> <p>CD-HIT:</p> <p>CD-HIT is a very widely used program for clustering and</p> <p>CDO:</p> <p>CDO is a collection of command line Operators to manipulate and analyse Climate and NWP model Data.</p> <p>CFITSIO:</p> <p>CFITSIO is a library of C and Fortran subroutines for reading and writing data files in</p> <p>CGAL:</p> <p>The goal of the CGAL Open Source Project is to provide easy access to efficient</p> <p>CMake:</p> <p>CMake, the cross-platform, open-source build system.  CMake is a family of tools designed to build, test and package software.</p> <p>CNVnator:</p> <p>Copy Number Variation discovery and genotyping from depth of read mapping.</p> <p>CNVpytor:</p> <p>Python package and command line tool for CNV/CNA analysis from depth-of-coverage by mapped read</p> <p>COMSOL:</p> <p>COMSOL is a multiphysics solver that provides a unified workflow for electrical, mechanical, fluid, and chemical applications.</p> <p>CONCOCT:</p> <p>Program for unsupervised binning of metagenomic contigs by using nucleotide composition,</p> <p>CP2K:</p> <p>CP2K is a freely available (GPL) program, written in Fortran 95, to perform atomistic and molecular</p> <p>CPU:</p> <p>Slectronic circuitry executes instructions of a computer program.</p> <p>CRAMINO:</p> <p>A tool for quick quality assessment of cram and bam files, intended for long read sequencing</p> <p>CTPL:</p> <p>C++ Thread Pool Library</p> <p>CUnit:</p> <p>Automated testing framework for C.</p> <p>Canu:</p> <p>Sequence assembler designed for high-noise single-molecule sequencing.</p> <p>CapnProto:</p> <p>Fast data interchange format and capability-based RPC system.</p> <p>Catch2:</p> <p>A modern, C++-native, header-only,  test framework for unit-tests, TDD and BDD</p> <p>CellRanger:</p> <p>Cell Ranger is a set of analysis pipelines that process Chromium</p> <p>Centrifuge:</p> <p>Classifier for metagenomic sequences</p> <p>Cereal:</p> <p>C++11 serialization library</p> <p>Charm++:</p> <p>An asynchronous message-driven C++ communication library targeted towards tightly coupled, high-performance parallel machines.</p> <p>CheckM:</p> <p>CheckM provides a set of tools for assessing the quality of genomes recovered from isolates,</p> <p>CheckM2:</p> <p>Rapid assessment of genome bin quality using machine learning</p> <p>CheckV:</p> <p>Assess the quality of metagenome-assembled viral genomes.</p> <p>Circlator:</p> <p>A tool to circularize genome assemblies</p> <p>Circos:</p> <p>Package for visualizing data in a circular layout - this makes Circos ideal for exploring</p> <p>Clair3:</p> <p>Syumphonizing pileup and full-alignment for high-performance long-read variant calling.</p> <p>Clustal-Omega:</p> <p>Clustal Omega is a multiple sequence alignment</p> <p>ClustalW2:</p> <p>ClustalW2 is a general purpose multiple sequence alignment program for DNA or proteins.</p> <p>Corset:</p> <p>Clusters contigs and counts reads from de novo assembled transcriptomes.</p> <p>CoverM:</p> <p>DNA read coverage and relative abundance calculator focused on metagenomics applications</p> <p>CppUnit:</p> <p>C++ port of the JUnit framework for unit testing.</p> <p>CrayCCE:</p> <p>Toolchain using Cray compiler wrapper, using PrgEnv-cray (PE release: February 2023).</p> <p>CrayGNU:</p> <p>Toolchain using Cray compiler wrapper, using PrgEnv-gnu module (PE release: 23.02).</p> <p>CrayIntel:</p> <p>Toolchain using Cray compiler wrapper, using PrgEnv-intel (PE release: February 2023 with Intel 19 compiler).</p> <p>CubeGUI:</p> <p>Graphical report explorer report explorer for Scalasca and Score-P</p> <p>CubeLib:</p> <p>Cube general purpose C++ library component and command-line tools.</p> <p>CubeWriter:</p> <p>Cube high-performance C writer library component.</p> <p>Cufflinks:</p> <p>Transcript assembly, differential expression, and differential regulation for RNA-Seq</p> <p>Cytoscape:</p> <p>Cytoscape is an open source software platform for visualizing molecular interaction networks and</p> <p>D-Genies:</p> <p>D-Genies also allows to display dot plots from other aligners by uploading their PAF or MAF alignment file.</p> <p>DAS_Tool:</p> <p>DAS Tool is an automated method that integrates the results of a flexible number of binning</p> <p>DB:</p> <p>Berkeley DB enables the development of custom data management solutions,</p> <p>DBG2OLC:</p> <p>DBG2OLC:Efficient Assembly of Large Genomes Using Long Erroneous Reads of the Third Generation</p> <p>DIAMOND:</p> <p>Sequence aligner for protein and translated DNA searches</p> <p>DISCOVARdenovo:</p> <p>Assembler suitable for large genomes based on Illumina reads of length 250 or longer.</p> <p>DRAM:</p> <p>Tool for annotating metagenomic assembled genomes and VirSorter identified viral contigs..</p> <p>DaliLite:</p> <p>Tool set for simulating/evaluating SVs, merging and comparing SVs within and among samples,</p> <p>DeconSeq:</p> <p>A tool that can be used to automatically detect and efficiently remove sequence contaminations</p> <p>DeepLabCut:</p> <p>Efficient method for 3D markerless pose estimation based on transfer learning with deep neural networks.</p> <p>DefaultModules:</p> <p>Defines the set of modules loaded by default</p> <p>Delft3D:</p> <p>Integrated simulation of sediment transport and morphology, waves, water quality and ecology.</p> <p>Delft3D_FM:</p> <p>3D modeling suite to investigate hydrodynamics, sediment transport and morphology and water quality for fluvial, estuarine and coastal environments</p> <p>Delly:</p> <p>Structural variant discovery by integrated paired-end and split-read analysis</p> <p>Dorado:</p> <p>High-performance, easy-to-use, open source basecaller for Oxford Nanopore reads.</p> <p>Dsuite:</p> <p>Fast calculation of the ABBA-BABA statistics across many populations/species</p> <p>EDTA:</p> <p>Automated whole-genome de-novo TE annotation and benchmarking the annotation performance of TE libraries.</p> <p>EIGENSOFT:</p> <p>The EIGENSOFT package combines functionality from our population genetics methods (Patterson et al.</p> <p>ELPA:</p> <p>Eigenvalue SoLvers for Petaflop-Applications .</p> <p>EMAN:</p> <p>EMAN is a powerful image processing library as well as a complete software suite</p> <p>EMAN2:</p> <p>Greyscale scientific image processing suite with a primary focus on processing data from transmission electron microscopes</p> <p>EMBOSS:</p> <p>EMBOSS is 'The European Molecular Biology Open Software Suite'.</p> <p>EMIRGE:</p> <p>Reconstructs full length ribosomal genes from short read</p> <p>ENMTML:</p> <p>R package for integrated construction of Ecological Niche Models.</p> <p>ESMF:</p> <p>The Earth System Modeling Framework (ESMF) is software for building and coupling weather,  climate, and related models.</p> <p>ETE:</p> <p>A Python framework for the analysis and visualization of phylogenetic trees</p> <p>EasyBuild:</p> <p>EasyBuild is a software build and installation framework</p> <p>Eigen:</p> <p>Eigen is a C++ template library for linear algebra:</p> <p>Elmer:</p> <p>Elmer is an open source multiphysical simulation software mainly developed by</p> <p>Embree:</p> <p>Embree is a collection of high-performance ray tracing kernels, developed at Intel. The target users of Embree are graphics application engineers who want to improve the performance of their photo-realistic rendering application by leveraging Embree's performance-optimized ray tracing kernels.</p> <p>EnergyPlus:</p> <p>Energy simulation program used to model energy consumption and water use in buildings.</p> <p>ErlangOTP:</p> <p>Erlang is a programming language used to build massively scalable</p> <p>EukRep-EukCC:</p> <p>Completeness and contamination estimator for metagenomic assembled microbial eukaryotic genomes.</p> <p>ExaBayes:</p> <p>Bayesian tree inference, particularly suitable for large-scale analyses.</p> <p>ExaML:</p> <p>Exascale Maximum Likelihood for phylogenetic inference using MPI.</p> <p>Extrae:</p> <p>Extrae is capable of instrumenting applications based on MPI, OpenMP, pthreads, CUDA1, OpenCL1, and StarSs1 using different instrumentation approaches</p> <p>FALCON:</p> <p>Falcon: a set of tools for fast aligning long reads for consensus and assembly</p> <p>FASTX-Toolkit:</p> <p>Tools for Short-Reads FASTA/FASTQ files preprocessing.</p> <p>FCM:</p> <p>FCM Build - A powerful build system for modern Fortran software applications. FCM Version Control - Wrappers to the Subversion version control system, usage conventions and processes for scientific software development.</p> <p>FDS:</p> <p>Fire Dynamics Simulator (FDS) is a large-eddy simulation (LES) code for low-speed flows,  with an emphasis on smoke and heat transport from fires.</p> <p>FFTW:</p> <p>FFTW is a C subroutine library for computing the discrete Fourier transform (DFT)  in one or more dimensions, of arbitrary input size, and of both real and complex data.</p> <p>FFmpeg:</p> <p>A complete, cross-platform solution to record, convert and stream audio and video.</p> <p>FLTK:</p> <p>FLTK is a cross-platform C++ GUI toolkit for UNIX/Linux (X11), Microsoft Windows,</p> <p>FTGL:</p> <p>FTGL is a free cross-platform Open Source C++ library that uses Freetype2 to simplify rendering fonts in OpenGL applications. FTGL supports bitmaps, pixmaps, texture maps, outlines, polygon mesh, and extruded polygon rendering modes.</p> <p>FastANI:</p> <p>Tool for fast alignment-free computation of</p> <p>FastME:</p> <p>FastME: a comprehensive, accurate and fast distance-based phylogeny inference program.</p> <p>FastQC:</p> <p>A set of tools (in Java) for working with next generation sequencing data in the BAM format.</p> <p>FastQ_Screen:</p> <p>FastQ Screen allows you to screen a library of sequences in FastQ</p> <p>FastTree:</p> <p>FastTree infers approximately-maximum-likelihood phylogenetic trees from alignments of nucleotide</p> <p>File-Rename:</p> <p>A Perl version of the rename utility, with support for regular expressions.</p> <p>Filtlong:</p> <p>Tool for filtering long reads by quality.</p> <p>FimTyper:</p> <p>Identifies the FimH type in total or partial sequenced isolates of E. coli..</p> <p>FlexiBLAS:</p> <p>FlexiBLAS is a wrapper library that enables the exchange of the BLAS and LAPACK implementation</p> <p>Flye:</p> <p>Flye is a de novo assembler for long and noisy reads, such as those produced by PacBio</p> <p>FoX:</p> <p>FoX is an XML library written in Fortran 95.</p> <p>FragGeneScan:</p> <p>FragGeneScan is an application for finding (fragmented) genes in short reads.</p> <p>FreeBayes:</p> <p>Genetic variant detector designed to find polymorphisms smaller than the length of a short-read sequencing alignment.</p> <p>FreeSurfer:</p> <p>FreeSurfer is a set of tools for analysis and visualization of structural and functional brain imaging data.</p> <p>FriBidi:</p> <p>Free Implementation of the Unicode Bidirectional Algorithm.</p> <p>GATK:</p> <p>The Genome Analysis Toolkit or GATK is a software package developed at the Broad Institute</p> <p>GCC:</p> <p>The GNU Compiler Collection includes front ends for C, C++, Objective-C, Fortran, Java, and Ada,</p> <p>GCCcore:</p> <p>The GNU Compiler Collection includes front ends for C, C++, Objective-C, Fortran, Java, and Ada,</p> <p>GD:</p> <p>Interface to Gd Graphics Library</p> <p>GDAL:</p> <p>GDAL is a translator library for raster geospatial data formats that is released under an X/MIT style  Open Source license by the Open Source Geospatial Foundation. As a library, it presents a single abstract data model  to the calling application for all supported formats. It also comes with a variety of useful command-line utilities for  data translation and processing.  NOTE: The GDAL IO cache by default uses 5% of total memory. This seems not necessary. This module sets GDAL_CACHEMAX=256 (256MB),   which should have no performance impact. Feel free to change if necessary, using 'export GDAL_CACHEMAX=xxx' (in your job script)   after loading the GDAL module.</p> <p>GEMMA:</p> <p>Genome-wide Efficient Mixed Model Association</p> <p>GEOS:</p> <p>GEOS (Geometry Engine - Open Source) is a C++ port of the  Java Topology Suite (JTS)</p> <p>GLM:</p> <p>OpenGL Mathematics (GLM) is a header only C++ mathematics library for graphics software based on</p> <p>GLPK:</p> <p>GNU Linear Programming Kit is intended for solving large-scale linear programming (LP), mixed integer programming (MIP), and other related problems.</p> <p>GLib:</p> <p>GLib is one of the base libraries of the GTK+ project</p> <p>GMAP-GSNAP:</p> <p>GMAP: A Genomic Mapping and Alignment Program for mRNA and EST Sequences</p> <p>GMP:</p> <p>GMP is a free library for arbitrary precision arithmetic,</p> <p>GMT:</p> <p>GMT is an open source collection of about 80 command-line tools for manipulating  geographic and Cartesian data sets (including filtering, trend fitting, gridding, projecting,  etc.) and producing PostScript illustrations ranging from simple x-y plots via contour maps  to artificially illuminated surfaces and 3D perspective views; the GMT supplements add another  40 more specialized and discipline-specific tools.</p> <p>GOLD:</p> <p>A genetic algorithm for docking flexible ligands into protein binding sites</p> <p>GObject-Introspection:</p> <p>GObject introspection is a middleware layer between C libraries</p> <p>GRADS:</p> <p>The Grid Analysis and Display System (GrADS) is an interactive desktop tool that is used for easy access, manipulation, and visualization of earth science data.</p> <p>GRASS:</p> <p>The Geographic Resources Analysis Support System - used for geospatial data management and analysis, image processing, graphics and maps production, spatial modeling, and visualization</p> <p>GRIDSS:</p> <p>GRIDSS is a module software suite containing tools useful for the detection of genomic rearrangements.</p> <p>GROMACS:</p> <p>GROMACS is a versatile package to perform molecular dynamics,  i.e. simulate the Newtonian equations of motion for systems with hundreds to millions of particles.</p> <p>GSL:</p> <p>The GNU Scientific Library (GSL) is a numerical library for C and C++  programmers.  The library provides a wide range of mathematical routines  such as random number generators, special functions and least-squares fitting.</p> <p>GST-plugins-base:</p> <p>GStreamer plug-ins and elements.</p> <p>GStreamer:</p> <p>GStreamer is a library for constructing graphs of media-handling  components. The applications it supports range from simple  Ogg/Vorbis playback, audio/video streaming to complex audio  (mixing) and video (non-linear editing) processing.</p> <p>GTDB-Tk:</p> <p>A toolkit for assigning objective taxonomic classifications to bacterial and archaeal genomes.</p> <p>GTK+:</p> <p>GTK+ is the primary library used to construct user interfaces in GNOME.</p> <p>GTS:</p> <p>GTS stands for the GNU Triangulated Surface Library.  It is an Open Source Free Software Library intended to provide a set of useful  functions to deal with 3D surfaces meshed with interconnected triangles.</p> <p>GeneMark-ES:</p> <p>Eukaryotic gene prediction suite with automatic training</p> <p>GenomeThreader:</p> <p>GenomeThreader is a software tool to compute gene structure predictions.</p> <p>Gerris:</p> <p>Gerris is a Free Software program for the solution of the partial differential equations describing fluid flow. This module also includes GfsView, a visualisation application for Gerris output.</p> <p>GetOrganelle:</p> <p>Toolkit to assemble organelle genome from genomic skimming data.</p> <p>GlimmerHMM:</p> <p>Gene finder based on a Generalized Hidden Markov Model.</p> <p>Go:</p> <p>An open source programming language</p> <p>Graphviz:</p> <p>Graphviz is open source graph visualization software. Graph visualization</p> <p>Gubbins:</p> <p>Genealogies Unbiased By recomBinations In Nucleotide Sequences</p> <p>Guile:</p> <p>Guile is the GNU Ubiquitous Intelligent Language for Extensions,</p> <p>HDF:</p> <p>A set of file formats designed to store and organize large amounts of data.</p> <p>HDF-EOS:</p> <p>HDF-EOS (Hierarchical Data Format - Earth Observing System) is a self-describing file format for transfer of various types of data between different machines based upon HDF. HDF-EOS is a standard format to store data collected from EOS satellites: Terra, Aqua and Aura.</p> <p>HDF-EOS5:</p> <p>HDF-EOS (Hierarchical Data Format - Earth Observing System) is a self-describing file format for transfer of various types of data between different machines based upon HDF. HDF-EOS is a standard format to store data collected from EOS satellites: Terra, Aqua and Aura.</p> <p>HDF5:</p> <p>HDF5 is a unique technology suite that makes possible the management of</p> <p>HISAT2:</p> <p>HISAT2 is a fast and sensitive alignment program for mapping next-generation sequencing reads</p> <p>HMMER:</p> <p>HMMER is used for searching sequence databases for homologs of protein sequences,</p> <p>HMMER2:</p> <p>HMMER is used for searching sequence databases for homologs of protein sequences,</p> <p>HOPS:</p> <p>Pipeline which focuses on screening MALT data for the presence of a user-specified list of target species.</p> <p>HPC:</p> <p>Like a regular computer, but larger. Primarily used for heating data centers.</p> <p>HTSeq:</p> <p>HTSeq is a Python library to facilitate processing and analysis</p> <p>HTSlib:</p> <p>A C library for reading/writing high-throughput sequencing data.</p> <p>HarfBuzz:</p> <p>HarfBuzz is an OpenType text shaping engine.</p> <p>HpcGridRunner:</p> <p>HPC GridRunner is a simple command-line interface to high throughput computing using a variety of different grid computing platforms, including LSF, SGE, SLURM, and PBS.</p> <p>Humann:</p> <p>Pipeline for efficiently and accurately determining the coverage and abundance of microbial pathways in a community from metagenomic data.</p> <p>HybPiper:</p> <p>Extracting Coding Sequence and Introns for Phylogenetics from High-Throughput Sequencing Reads Using Target Enrichment.</p> <p>Hypre:</p> <p>Hypre is a library for solving large, sparse linear systems of equations on massively</p> <p>ICU:</p> <p>ICU is a mature, widely used set of C/C++ and Java libraries providing Unicode and Globalization  support for software applications.</p> <p>IDBA:</p> <p>Iterative de Bruijn graph assembler for second-generation sequencing reads.</p> <p>IDBA-UD:</p> <p>IDBA-UD is a iterative De Bruijn Graph De Novo Assembler for Short Reads</p> <p>IDL:</p> <p>IDL is the trusted scientific programming language used across disciplines to extract meaningful visualizations from complex numerical data.</p> <p>IGV:</p> <p>The Integrative Genomics Viewer (IGV) is a high-performance visualization</p> <p>IMPUTE:</p> <p>Genotype imputation and haplotype phasing.</p> <p>IQ-TREE:</p> <p>Efficient phylogenomic software by maximum likelihood</p> <p>IRkernel:</p> <p>R packages for providing R kernel for Jupyter.</p> <p>ISA-L:</p> <p>Intelligent Storage Acceleration Library</p> <p>ImageMagick:</p> <p>ImageMagick is a software suite to create, edit, compose, or convert bitmap images</p> <p>Infernal:</p> <p>Infernal ('INFERence of RNA ALignment') is for searching DNA sequence databases</p> <p>Inspector:</p> <p>Intel Inspector XE is an easy to use memory error checker and thread checker for serial</p> <p>InterProScan:</p> <p>Sequence analysis application (nucleotide and protein sequences) that combines</p> <p>JAGS:</p> <p>Just Another Gibbs Sampler - a program for the statistical analysis of Bayesian hierarchical models by Markov Chain Monte Carlo.</p> <p>JUnit:</p> <p>A programmer-oriented testing framework for Java.</p> <p>JasPer:</p> <p>The JasPer Project is an open-source initiative to provide a free  software-based reference implementation of the codec specified in the JPEG-2000 Part-1 standard.</p> <p>Java:</p> <p>Java Platform, Standard Edition (Java SE) lets you develop and deploy  Java applications on desktops and servers.</p> <p>Jellyfish:</p> <p>Jellyfish is a tool for fast, memory-efficient counting of k-mers in DNA.</p> <p>JsonCpp:</p> <p>JsonCpp is a C++ library that allows manipulating JSON values,</p> <p>Julia:</p> <p>A high-level, high-performance dynamic language for technical computing.  This version was compiled from source with USE_INTEL_JITEVENTS=1 to enable profiling with VTune.</p> <p>JupyterLab:</p> <p>An extensible environment for interactive and reproducible computing, based on the Jupyter Notebook and Architecture.</p> <p>KAT:</p> <p>The K-mer Analysis Toolkit (KAT) contains a number of tools that analyse and compare K-mer spectra.</p> <p>KEALib:</p> <p>KEALib provides an implementation of the GDAL data model. The format supports raster attribute tables, image pyramids, meta-data and in-built statistics while also handling very large files and compression throughout. Based on the HDF5 standard, it also provides a base from which other formats can be derived and is a good choice for long term data archiving. An independent software library (libkea) provides complete access to the KEA image format and a GDAL driver allowing KEA images to be used from any GDAL supported software.</p> <p>KMC:</p> <p>Disk-based programm for counting k-mers from (possibly gzipped) FASTQ/FASTA files.</p> <p>Kaiju:</p> <p>Kaiju is a program for sensitive taxonomic classification of high-throughput</p> <p>Kent_tools:</p> <p>Collection of tools used by the UCSC genome browser.</p> <p>KorfSNAP:</p> <p>Semi-HMM-based Nucleic Acid Parser</p> <p>Kraken2:</p> <p>Taxonomic sequence classifier.</p> <p>KronaTools:</p> <p>Krona Tools is a set of scripts to create Krona charts from</p> <p>KyotoCabinet:</p> <p>Library of routines for managing a database.</p> <p>LAME:</p> <p>LAME is a high quality MPEG Audio Layer III (MP3) encoder licensed under the LGPL.</p> <p>LAMMPS:</p> <p>LAMMPS is a classical molecular dynamics code, and an acronym for Large-scale Atomic/Molecular Massively Parallel Simulator. LAMMPS has potentials for solid-state materials (metals, semiconductors) and soft matter (biomolecules, polymers) and coarse-grained or mesoscopic systems. It can be used to model atoms or, more generically, as a parallel particle simulator at the atomic, meso, or continuum scale. LAMMPS runs on single processors or in parallel using message-passing techniques and a spatial-decomposition of the simulation domain. The code is designed to be easy to modify or extend with new functionality.</p> <p>LAST:</p> <p>LAST finds similar regions between sequences.</p> <p>LASTZ:</p> <p>LASTZ is a program for aligning DNA sequences, a pairwise aligner. Originally designed to</p> <p>LDC:</p> <p>D programming language compiler</p> <p>LEfSe:</p> <p>Determines the features most likely to explain differences between classes by coupling standard tests for statistical significance</p> <p>LINKS:</p> <p>Alignment-free scaffolding of genome assembly drafts with long reads</p> <p>LLVM:</p> <p>The LLVM Core libraries provide a modern source- and target-independent  optimizer, along with code generation support for many popular CPUs  (as well as some less common ones!) These libraries are built around a well  specified code representation known as the LLVM intermediate representation  (\"LLVM IR\"). The LLVM Core libraries are well documented, and it is  particularly easy to invent your own language (or port an existing compiler)  to use LLVM as an optimizer and code generator.   This build includes the clang C/C++ compiler frontend.</p> <p>LMDB:</p> <p>LMDB is a fast, memory-efficient database. With memory-mapped files, it has the read performance</p> <p>LSD2:</p> <p>Least-squares methods to estimate rates and dates from phylogenies</p> <p>LTR_retriever:</p> <p>Highly accurate and sensitive program for identification of LTR retrotransposons; The LTR Assembly Index (LAI) is also included in this package.</p> <p>LUMPY:</p> <p>A probabilistic framework for structural variant discovery.</p> <p>LZO:</p> <p>Portable lossless data compression library</p> <p>LibTIFF:</p> <p>tiff: Library and tools for reading and writing TIFF data files</p> <p>Libav:</p> <p>Libraries for dealing with multimedia formats of all sorts.  Forked from FFmpeg</p> <p>Libint:</p> <p>Libint library is used to evaluate the traditional (electron repulsion) and certain novel two-body  matrix elements (integrals) over Cartesian Gaussian functions used in modern atomic and molecular theory.</p> <p>LittleCMS:</p> <p>Color management engine.</p> <p>Loki:</p> <p>Loki is a C++ library of designs, containing flexible implementations of common design patterns and</p> <p>LongStitch:</p> <p>A genome assembly correction and scaffolding pipeline using long reads</p> <p>M4:</p> <p>GNU M4 is an implementation of the traditional Unix macro processor. It is mostly SVR4 compatible</p> <p>MAFFT:</p> <p>Multiple sequence alignment program offering a range of methods.</p> <p>MAGMA:</p> <p>Tool for gene analysis and generalized gene-set analysis of GWAS data.</p> <p>MAKER:</p> <p>Genome annotation pipeline</p> <p>MATIO:</p> <p>matio is an C library for reading and writing Matlab MAT files.</p> <p>MATLAB:</p> <p>A high-level language and interactive environment for numerical computing.</p> <p>MCL:</p> <p>The MCL algorithm is short for the Markov Cluster Algorithm, a fast</p> <p>MCR:</p> <p>The Matlab Compiler Runtime is required for running compiled MATLAB executables without MATLAB itself.</p> <p>MEGAHIT:</p> <p>An ultra-fast single-node solution for large and complex</p> <p>METABOLIC:</p> <p>Metabolic And Biogeochemistry anaLyses In microbes</p> <p>METIS:</p> <p>METIS is a set of serial programs for partitioning graphs, partitioning finite element meshes,</p> <p>MMseqs2:</p> <p>MMseqs2: ultra fast and sensitive search and clustering suite</p> <p>MOB-suite:</p> <p>Software tools for clustering, reconstruction and typing of plasmids from draft assemblies.</p> <p>MODFLOW:</p> <p>MODFLOW is the U.S. Geological Survey modular finite-difference flow model, which is a computer code that solves the groundwater flow equation. The program is used by hydrogeologists to simulate the flow of groundwater through aquifers.</p> <p>MPFR:</p> <p>The MPFR library is a C library for multiple-precision</p> <p>MPI:</p> <p>A standardised message-passing standard designed to function on parallel computing architectures.</p> <p>MSMC:</p> <p>Multiple Sequentially Markovian Coalescent, infers population size and gene flow from multiple genome sequences</p> <p>MUMPS:</p> <p>A parallel sparse direct solver</p> <p>MUMmer:</p> <p>MUMmer is a system for rapidly aligning entire genomes,</p> <p>MUSCLE:</p> <p>MUSCLE is a program for creating multiple alignments of amino acid or nucleotide</p> <p>MUST:</p> <p>MUST detects usage errors of the Message Passing Interface (MPI) and reports them to the user.</p> <p>MaSuRCA:</p> <p>MaSuRCA is whole genome assembly software. It combines the efficiency of the de Bruijn graph</p> <p>Magma:</p> <p>Magma is a large, well-supported software package designed for computations in algebra, number theory, algebraic geometry and algebraic combinatorics. It provides a mathematically rigorous environment     for defining and working with structures such as groups, rings, fields, modules, algebras, schemes, curves, graphs, designs, codes and many others. Magma also supports a number of databases designed     to aid computational research in those areas of mathematics which are algebraic in nature.</p> <p>MarkerMiner:</p> <p>Workflow for effective discovery of SCN loci in flowering plants angiosperms</p> <p>Mash:</p> <p>Fast genome and metagenome distance estimation using MinHash</p> <p>Maven:</p> <p>Binary maven install, Apache Maven is a software project management and comprehension tool. Based on</p> <p>MaxBin:</p> <p>MaxBin is software for binning assembled metagenomic sequences based on</p> <p>Meraculous:</p> <p>Eukaryotic genome assembler for Illumina sequence data.</p> <p>Mesa:</p> <p>Mesa is an open-source implementation of the OpenGL specification -  a system for rendering interactive 3D graphics.   Note that this build enables CPU-based rendering with OpenSWR and LLVM.  The module is intended to be used with visualisation software, such as  ParaView, on nodes where no GPU hardware is available.   Both on-screen and off-screen rendering are supported.  IMPORTANT: The OpenSWR software rasteriser can use multiple threads for            best performance. The number of threads is controlled by the            environment variable KNOB_MAX_WORKER_THREADS. The module sets             KNOB_MAX_WORKER_THREADS=1             by default to avoid accidental oversubscription of nodes.</p> <p>Meson:</p> <p>Meson is a cross-platform build system designed to be both as fast and as user friendly as possible.</p> <p>MetaBAT:</p> <p>An efficient tool for accurately reconstructing single genomes from complex microbial communities</p> <p>MetaEuk:</p> <p>MetaEuk is a modular toolkit designed for large-scale gene discovery and annotation in eukaryotic</p> <p>MetaGeneAnnotator:</p> <p>MetaGeneAnnotator is a gene-finding program for prokaryote and phage.</p> <p>MetaPhlAn:</p> <p>MetaPhlAn is a computational tool for profiling the composition of microbial</p> <p>MetaPhlAn2:</p> <p>MetaPhlAn is a computational tool for profiling the composition of microbial</p> <p>MetaSV:</p> <p>Structural-variant caller</p> <p>MetaVelvet:</p> <p>A short read assember for metagenomics</p> <p>Metaxa2:</p> <p>Taxonomic classification of rRNA.</p> <p>Miniconda3:</p> <p>A platform for Python-based data analytics</p> <p>Minimac3:</p> <p>Low memory and more computationally efficient implementation of the genotype imputation algorithms.</p> <p>MitoZ:</p> <p>Toolkit which aims to automatically filter pair-end raw data,</p> <p>Molcas:</p> <p>Molcas is an ab initio quantum chemistry software package</p> <p>Molpro:</p> <p>Molpro is a complete system of ab initio programs for molecular electronic structure calculations.</p> <p>Mono:</p> <p>An open source, cross-platform, implementation of C# and the CLR that is</p> <p>Monocle3:</p> <p>An analysis toolkit for single-cell RNA-seq.</p> <p>Mothur:</p> <p>Mothur is a single piece of open-source, expandable software</p> <p>MotionCorr:</p> <p>Motion Correction for Dose-Fractionation Stack, by Dr. Xueming Li of the Cheng Laboratory at UCSF. The actual executable is named dosefgpu_driftcorr</p> <p>MrBayes:</p> <p>MrBayes is a program for the Bayesian estimation of phylogeny.</p> <p>Mule:</p> <p>Mule is an API written in Python which allows you to access and manipulate files produced by the UM (Unified Model, of the Met Office (UK)).</p> <p>MultiQC:</p> <p>Aggregate results from bioinformatics analyses across many samples into a single</p> <p>NAMD:</p> <p>NAMD is a parallel molecular dynamics code designed for high-performance simulation of</p> <p>NASM:</p> <p>NASM: General-purpose x86 assembler</p> <p>NCARG:</p> <p>NCAR Graphics is a Fortran and C based software package for</p> <p>NCCL:</p> <p>The NVIDIA Collective Communications Library (NCCL) implements multi-GPU and multi-node collective</p> <p>NCL:</p> <p>NCL is an interpreted language designed specifically for scientific data analysis and   visualization.</p> <p>NCO:</p> <p>manipulates and analyzes data stored in netCDF-accessible formats, including DAP, HDF4, and HDF5</p> <p>NCVIEW:</p> <p>Ncview is a visual browser for netCDF format files. Typically you would use ncview to get a quick and easy, push-button look at your netCDF files. You can view simple movies of the data, view along various dimensions, take a look at the actual data values, change color maps, invert the data, etc.</p> <p>NGS:</p> <p>NGS is a new, domain-specific API for accessing reads, alignments and pileups produced from</p> <p>NIWA:</p> <p>Crown Research Institute, conducts research across a broad range of disciplines in the environmental sciences.</p> <p>NLopt:</p> <p>NLopt is a free/open-source library for nonlinear optimization, providing a common interface for a number of different free optimization routines available online as well as original implementations of various other algorithms</p> <p>NSPR:</p> <p>Netscape Portable Runtime (NSPR) provides a platform-neutral API for system level</p> <p>NSS:</p> <p>Network Security Services (NSS) is a set of libraries designed to support cross-platform development</p> <p>NVHPC:</p> <p>C, C++ and Fortran compilers included with the NVIDIA HPC SDK (previously: PGI)</p> <p>NWChem:</p> <p>NWChem aims to provide its users with computational chemistry tools that are scalable both in</p> <p>NanoComp:</p> <p>Comparing runs of Oxford Nanopore sequencing data and alignments</p> <p>NanoLyse:</p> <p>Removing reads mapping to the lambda genome.</p> <p>NanoPlot:</p> <p>Plotting suite for Oxford Nanopore sequencing data and alignments.</p> <p>NanoStat:</p> <p>Tool for phasing genomic variants using DNA sequencing reads, also called read-based phasing or haplotype assembly.</p> <p>NeSI:</p> <p>New Zealand national high performance computing platform.</p> <p>Newton-X:</p> <p>NX is a general-purpose program package for simulating the dynamics of electronically excited molecules and molecular assemblies.</p> <p>NextGenMap:</p> <p>NextGenMap is a flexible highly sensitive short read mapping tool that</p> <p>Nextflow:</p> <p>Nextflow is a reactive workflow framework and a programming DSL</p> <p>Ninja:</p> <p>Ninja is a small build system with a focus on speed.</p> <p>Nsight-Compute:</p> <p>NVIDIA\u00ae Nsight\u2122 Compute is an interactive kernel profiler for CUDA applications. It provides detailed</p> <p>Nsight-Systems:</p> <p>NVIDIA\u00ae Nsight\u2122 Systems is a system-wide performance analysis tool designed to visualize an</p> <p>OASIS3-MCT:</p> <p>The OASIS coupler is a software allowing synchronized exchanges of coupling information between numerical codes representing different components of the climate system.</p> <p>OBITools:</p> <p>Manipulate various data and sequence files.</p> <p>OCI:</p> <p>Oracle Call Interface (OCI) is the comprehensive, high performance, native C language interface to Oracle Database for custom or packaged applications.  NOTE: This package is only available on Maui Ancil nodes that provide database access.</p> <p>OMA:</p> <p>Orthologous MAtrix project is a method and database for the inference</p> <p>OPARI2:</p> <p>source-to-source instrumentation tool for OpenMP and hybrid codes.</p> <p>ORCA:</p> <p>ORCA is a flexible, efficient and easy-to-use general purpose tool for quantum chemistry</p> <p>OSPRay:</p> <p>OSPRay features interactive CPU rendering capabilities geared towards Scientific Visualization applications. Advanced shading effects such as Ambient Occlusion, shadows, and transparency can be rendered interactively, enabling new insights into data exploration.</p> <p>OSU-Micro-Benchmarks:</p> <p>OSU Micro-Benchmarks for MPI</p> <p>Octave:</p> <p>GNU Octave is a high-level interpreted language, primarily intended for numerical computations.</p> <p>Octopus:</p> <p>Octopus is a scientific program aimed at the ab initio virtual experimentation</p> <p>OpenBLAS:</p> <p>OpenBLAS is an optimized BLAS library based on GotoBLAS2 1.13 BSD version.</p> <p>OpenBabel:</p> <p>Open Babel is a chemical toolbox designed to speak the many</p> <p>OpenCMISS:</p> <p>OpenCMISS is a set of libraries and applications which provide modelling and</p> <p>OpenCV:</p> <p>OpenCV (Open Source Computer Vision Library) is an open source computer vision</p> <p>OpenFAST:</p> <p>Wind turbine multiphysics simulation tool</p> <p>OpenFOAM:</p> <p>OpenFOAM is a free, open source CFD software package.</p> <p>OpenJPEG:</p> <p>An open-source JPEG 2000 codec written in C</p> <p>OpenMPI:</p> <p>The Open MPI Project is an open source MPI-3 implementation. This version is built with CUDA support enabled.</p> <p>OpenSSL:</p> <p>The OpenSSL Project is a collaborative effort to develop a robust, commercial-grade, full-featured,</p> <p>OpenSees:</p> <p>OpenSees is a software framework for developing applications to simulate the performance of structural and geotechnical systems subjected to earthquakes.</p> <p>OpenSeesPy:</p> <p>OpenSees is a software framework for developing applications to simulate the performance of structural and geotechnical systems subjected to earthquakes.</p> <p>OpenSlide:</p> <p>OpenSlide is a C library that provides a simple interface to</p> <p>OrfM:</p> <p>A simple and not slow open reading frame (ORF) caller.</p> <p>OrthoFiller:</p> <p>Identifies missing annotations for evolutionarily conserved genes.</p> <p>OrthoFinder:</p> <p>OrthoFinder is a fast, accurate and comprehensive platform for comparative genomics</p> <p>OrthoMCL:</p> <p>Genome-scale algorithm for grouping orthologous protein sequences.</p> <p>PALEOMIX:</p> <p>pipelines and tools designed to aid the rapid processing of High-Throughput Sequencing (HTS) data.</p> <p>PAML:</p> <p>PAML is a package of programs for phylogenetic</p> <p>PAPI:</p> <p>PAPI provides the tool designer and application engineer with a consistent interface and</p> <p>PBJelly:</p> <p>PBJelly is a highly automated pipeline that aligns long sequencing reads (such as PacBio RS reads or</p> <p>PCRE:</p> <p>The PCRE library is a set of functions that implement regular expression pattern matching using</p> <p>PCRE2:</p> <p>The PCRE library is a set of functions that implement regular expression pattern matching using the same syntax  and semantics as Perl 5.</p> <p>PDT:</p> <p>Program Database Toolkit (PDT) is a framework for analyzing source code written in several programming languages and for making rich program</p> <p>PEAR:</p> <p>Memory-efficient,fully parallelized and highly accurate pair-end read merger.</p> <p>PEST++:</p> <p>PEST++ is a software suite aimed at supporting</p> <p>PETSc:</p> <p>PETSc, pronounced PET-see (the S is silent), is a suite of data structures and routines for the</p> <p>PFFT:</p> <p>PFFT is a software library for computing massively parallel, fast Fourier</p> <p>PGI:</p> <p>C, C++ and Fortran compilers from The Portland Group - PGI</p> <p>PHASIUS:</p> <p>A tool to visualize phase block structure from (many) BAM or CRAM files together with BED annotation</p> <p>PLINK:</p> <p>PLINK is a free, open-source whole genome association analysis toolset,</p> <p>PLUMED:</p> <p>PLUMED is an open source library for free energy calculations in molecular systems which</p> <p>POSIX:</p> <p>A set of standard operating system interfaces based on the Unix operating system</p> <p>PRANK:</p> <p>Probabilistic multiple alignment program for DNA, codon and amino-acid sequences. .</p> <p>PROJ:</p> <p>Program proj is a standard Unix filter function which converts  geographic longitude and latitude coordinates into cartesian coordinates</p> <p>Pango:</p> <p>Pango is a library for laying out and rendering of text, with an emphasis on internationalization.</p> <p>ParMETIS:</p> <p>ParMETIS is an MPI-based parallel library that implements a variety of algorithms for partitioning unstructured graphs,</p> <p>ParaView:</p> <p>ParaView is a scientific parallel visualizer.  This version supports CPU-only rendering without X context using the OSMesa library, it does not support GPU rendering, and it does not provide a GUI.  Use the GALLIUM_DRIVER environment variable to choose a software renderer, it is recommended to use  GALLIUM_DRIVER=swr  for best performance.  Ray tracing using the OSPRay library is also supported.</p> <p>Parallel:</p> <p>parallel: Build and execute shell commands in parallel</p> <p>ParallelIO:</p> <p>A high-level Parallel I/O Library for structured grid applications</p> <p>Paraver:</p> <p>Performance visualization and analysis tool based on traces.</p> <p>Peregrine:</p> <p>Genome assembler for long reads (length &gt; 10kb, accuracy &gt; 99%).</p> <p>Perl:</p> <p>Larry Wall's Practical Extraction and Report Language</p> <p>PhyML:</p> <p>Phylogenetic estimation using Maximum Likelihood</p> <p>PhyloPhlAn:</p> <p>Integrated pipeline for large-scale phylogenetic profiling of genomes and metagenomes.</p> <p>Pilon:</p> <p>Pilon is an automated genome assembly improvement and variant detection tool</p> <p>PnetCDF:</p> <p>Parallel netCDF: A Parallel I/O Library for NetCDF File Access</p> <p>Porechop:</p> <p>Porechop is a tool for finding and removing adapters from Oxford Nanopore reads.</p> <p>PostgreSQL:</p> <p>Client-side programs and libraries for accessing PostgreSQL databases.</p> <p>Prodigal:</p> <p>Prodigal (Prokaryotic Dynamic Programming Genefinding Algorithm)</p> <p>ProtHint:</p> <p>Pipeline for predicting and scoring hints (in the form of introns, start and</p> <p>Proteinortho:</p> <p>Proteinortho is a tool to detect orthologous genes within different species.</p> <p>PyOpenGL:</p> <p>PyOpenGL is the most common cross platform Python binding to OpenGL and related APIs.</p> <p>PyQt:</p> <p>PyQt5 is a set of Python bindings for v5 of the Qt application framework from The Qt Company.</p> <p>PyTorch:</p> <p>Tensors and Dynamic neural networks in Python with strong GPU acceleration.</p> <p>Python:</p> <p>Python is a programming language that lets you work more quickly and integrate your systems more effectively.</p> <p>Python-GPU:</p> <p>The python packages which depend on CUDA: pycuda, pygpu, and scikit-cuda.</p> <p>Python-Geo:</p> <p>Python packages for geospatial data I/O, mostly based on the OSGEO libraries GDAL and OGR</p> <p>QIIME2:</p> <p>An open-source bioinformatics pipeline for microbiome analysis</p> <p>QUAST:</p> <p>Evaluates genome assemblies</p> <p>Qt5:</p> <p>Qt is a comprehensive cross-platform C++ application framework.</p> <p>QuantumESPRESSO:</p> <p>Quantum ESPRESSO  is an integrated suite of computer codes</p> <p>QuickTree:</p> <p>Efficient implementation of the Neighbor-Joining algorithm, capable of reconstructing phylogenies from huge alignments .</p> <p>R:</p> <p>R is a free software environment for statistical computing and graphics.</p> <p>R-Geo:</p> <p>R packages for Geometric and Geospatial data which depend</p> <p>R-bundle-Bioconductor:</p> <p>Bioconductor provides tools for the analysis and comprehension</p> <p>RANGS-GSHHS:</p> <p>A binary file set RANGS (Regionally Accessible Nested Global Shorelines) based on GSHHS (Global Self-consistent Hierarchical High-resolution Shorelines) data.  Note: RANGS-GSHHS is used with NCL.</p> <p>RAxML:</p> <p>RAxML search algorithm for maximum likelihood based inference of phylogenetic trees.</p> <p>RAxML-NG:</p> <p>RAxML-NG is a phylogenetic tree inference tool which uses maximum-likelihood (ML)</p> <p>RDP-Classifier:</p> <p>The RDP Classifier is a naive Bayesian classifier that can rapidly and accurately provides taxonomic</p> <p>RECON:</p> <p>De novo identification and classification of repeat sequence families from genomic sequences</p> <p>RMBlast:</p> <p>RMBlast supports RepeatMasker searches by adding a few necessary features to the stock NCBI blastn program. These include:</p> <p>RNAmmer:</p> <p>consistent and rapid annotation of ribosomal RNA genes.</p> <p>ROCm:</p> <p>Platform for GPU Enabled HPC and UltraScale Computing</p> <p>ROOT:</p> <p>The ROOT system provides a set of OO frameworks with all the functionality</p> <p>RSEM:</p> <p>Estimates gene and isoform expression levels from RNA-Seq data</p> <p>RSGISLib:</p> <p>The Remote Sensing and GIS software library (RSGISLib) is a</p> <p>Racon:</p> <p>Ultrafast consensus module for raw de novo genome assembly of long uncorrected reads.</p> <p>Ragout:</p> <p>Tool for chromosome assembly using multiple references.</p> <p>RapidNJ:</p> <p>An algorithmic engineered implementation of canonical neighbour-joining.</p> <p>Raven:</p> <p>De novo genome assembler for long uncorrected reads.</p> <p>Rcorrector:</p> <p>kmer-based error correction method for RNA-seq data.</p> <p>Relion:</p> <p>RELION (for REgularised LIkelihood OptimisatioN, pronounce rely-on)</p> <p>RepeatMasker:</p> <p>Screens DNA sequences for interspersed repeats and low complexity DNA sequences.</p> <p>RepeatModeler:</p> <p>De novo transposable element (TE) family identification and modeling package.</p> <p>RepeatScout:</p> <p>De novo identification of repeat families in large genomes</p> <p>Riskscape:</p> <p>RiskScape is an open-source spatial data processing application used for multi-hazard risk analysis. RiskScape is highly customisable, letting modellers tailor the risk analysis to suit the problem domain and input data being modelled.</p> <p>Roary:</p> <p>Rapid large-scale prokaryote pan genome analysis</p> <p>Rosetta:</p> <p>Rosetta is the premier software suite for modeling macromolecular structures. As a flexible,</p> <p>Rstudio:</p> <p>RStudio is a set of integrated tools designed to help you be more productive with R.</p> <p>Ruby:</p> <p>Ruby is a dynamic, open source programming language with</p> <p>Rust:</p> <p>Systems programming language that runs blazingly fast, prevents segfaults,</p> <p>SAGE:</p> <p>Ppackage containing programs for use in the genetic analysis of</p> <p>SAMtools:</p> <p>Samtools is a suite of programs for interacting with high-throughput sequencing data.</p> <p>SAS:</p> <p>SAS is a statistical software suite developed by SAS Institute for data management, advanced analytics, multivariate analysis, business intelligence, criminal investigation, and predictive analytics. - Homepage: https://www.sas.com/en_nz/home.html/</p> <p>SCOTCH:</p> <p>Software package and libraries for sequential and parallel graph partitioning, static mapping, and sparse matrix block ordering, and sequential mesh and hypergraph partitioning.</p> <p>SCP:</p> <p>Means of securely transferring files between over an SSH connection.</p> <p>SCons:</p> <p>SCons is a software construction tool.</p> <p>SDL2:</p> <p>Simple DirectMedia Layer, a cross-platform multimedia library</p> <p>SEPP:</p> <p>SATe-enabled Phylogenetic Placement. Phylogenetic placement of short reads into reference alignments and trees.</p> <p>SHAPEIT4:</p> <p>Estimation of haplotypes (aka phasing)</p> <p>SIONlib:</p> <p>Scalable I/O library for parallel access to task-local files.</p> <p>SIP:</p> <p>SIP is a tool that makes it very easy to create Python bindings for C and C++ libraries.</p> <p>SKESA:</p> <p>SKESA is a de-novo sequence read assembler for cultured single isolate genomes based on DeBruijn graphs.</p> <p>SMRT-Link:</p> <p>PacBio\u2019s open-source software suite is designed for use with Single Molecule,</p> <p>SNVoter-NanoMethPhase:</p> <p>SNVoter - A top up tool to enhance SNV calling from Nanopore sequencing data &amp;</p> <p>SOAPdenovo2:</p> <p>Short Oligonucleotide Analysis Package - novel short-read assembly</p> <p>SOCI:</p> <p>Database access library for C++ that makes the illusion of embedding SQL queries in the</p> <p>SPAdes:</p> <p>Genome assembler for single-cell and isolates data sets</p> <p>SPECFEM3D:</p> <p>SPECFEM3D Cartesian simulates acoustic (fluid), elastic (solid), coupled acoustic/elastic, poroelastic or seismic wave propagation in any type of conforming mesh of hexahedra (structured or not.) It can, for instance, model seismic waves propagating in sedimentary basins or any other regional geological model following earthquakes. It can also be used for non-destructive testing or for ocean acoustics.</p> <p>SPIDER:</p> <p>System for Processing Image Data from Electron microscopy and Related fields</p> <p>SQLite:</p> <p>SQLite: SQL Database Engine in a C Library</p> <p>SQLplus:</p> <p>SQL*Plus is an interactive and batch query tool that is installed with every Oracle Database installation. It has a command-line user interface and a Windows Graphical User Interface (GUI).</p> <p>SSAHA2:</p> <p>Pairwise sequence alignment program designed for the efficient mapping of sequencing</p> <p>SSH:</p> <p>A network communication protocol that enables two computers to communicate</p> <p>STAR:</p> <p>Fast universal RNA-seq aligner</p> <p>STAR-Fusion:</p> <p>Processes the output generated by the STAR aligner to map junction reads and spanning reads to a reference annotation set</p> <p>SUNDIALS:</p> <p>SUNDIALS: SUite of Nonlinear and DIfferential/ALgebraic Equation Solvers</p> <p>SURVIVOR:</p> <p>Tool set for simulating/evaluating SVs, merging and comparing SVs within and among samples,</p> <p>SWIG:</p> <p>SWIG is a software development tool that connects programs written in C and C++ with  a variety of high-level programming languages.</p> <p>Salmon:</p> <p>Salmon is a wicked-fast program to produce a highly-accurate,</p> <p>Sambamba:</p> <p>Tools for working with SAM/BAM data</p> <p>ScaLAPACK:</p> <p>The ScaLAPACK (or Scalable LAPACK) library includes a subset of LAPACK routines</p> <p>Scalasca:</p> <p>Tool that supports the performance optimization of</p> <p>Score-P:</p> <p>Measurement infrastructure is a highly scalable and easy-to-use</p> <p>SeqAn3:</p> <p>C++ library of efficient algorithms and data structures for the</p> <p>SeqKit:</p> <p>Ultrafast toolkit for FASTA/Q file manipulation</p> <p>SeqMonk:</p> <p>A tool to visualise and analyse high throughput mapped sequence data.</p> <p>SiBELia:</p> <p>A comparative genomics tool for analysing genomic variations that correlate with pathogens, or</p> <p>Siesta:</p> <p>SIESTA is both a method and its computer program implementation, to perform efficient electronic</p> <p>SignalP:</p> <p>SignalP predicts the presence  and  location of signal peptide  cleavage sites</p> <p>Sniffles:</p> <p>A fast structural variant caller for long-read sequencing.</p> <p>SortMeRNA:</p> <p>SortMeRNA is a biological sequence analysis tool for filtering, mapping and OTU-picking NGS reads.</p> <p>SourceTracker:</p> <p>SourceTracker is a Bayesian approach to estimating the proportion of a novel community that comes</p> <p>Spack:</p> <p>Spack is a package manager for supercomputers, Linux, and macOS. It makes installing scientific</p> <p>Spark:</p> <p>Spark is Hadoop MapReduce done in memory</p> <p>SqueezeMeta:</p> <p>fully automated metagenomics pipeline, from reads to bins.</p> <p>Stacks:</p> <p>Stacks is a software pipeline for building loci from short-read sequences, such as those generated on</p> <p>StringTie:</p> <p>StringTie is a fast and highly efficient assembler of RNA-Seq alignments into potential transcripts.</p> <p>Structure:</p> <p>The program structure is a free software package for using multi-locus genotype data to investigate</p> <p>Subread:</p> <p>High performance read alignment, quantification and mutation discovery</p> <p>Subversion:</p> <p>Subversion is an open source version control system.</p> <p>SuiteSparse:</p> <p>SuiteSparse is a collection of libraries manipulate sparse matrices.</p> <p>SuperLU:</p> <p>Solution of large, sparse, nonsymmetric systems of linear equations.</p> <p>Supernova:</p> <p>Supernova is a software package for de novo assembly from Chromium Linked-Reads</p> <p>TMHMM:</p> <p>Prediction of transmembrane helices in proteins</p> <p>TSEBRA:</p> <p>Transcript Selector for BRAKER</p> <p>TURBOMOLE:</p> <p>Program Package For Electronic Structure Calculations.</p> <p>TWL-NINJA:</p> <p>Nearly Infinite Neighbor Joining Application.</p> <p>Tcl:</p> <p>Tcl (Tool Command Language) is a very powerful but easy to learn dynamic programming language,</p> <p>TensorFlow:</p> <p>An open-source software library for Machine Intelligence</p> <p>TensorRT:</p> <p>NVIDIA TensorRT is a platform for high-performance deep learning inference</p> <p>Theano:</p> <p>Theano is a Python library that allows you to define, optimize,</p> <p>Tk:</p> <p>Tk is an open source, cross-platform widget toolchain that provides a library of basic elements for</p> <p>TopHat:</p> <p>TopHat is a fast splice junction mapper for RNA-Seq reads.</p> <p>TransDecoder:</p> <p>TransDecoder identifies candidate coding regions within transcript sequences.</p> <p>TreeMix:</p> <p>TreeMix is a method for inferring the patterns of population splits and mixtures in the history of a</p> <p>Trilinos:</p> <p>The Trilinos Project is an effort to develop algorithms and enabling technologies</p> <p>TrimGalore:</p> <p>A wrapper of FastQC and cutadapt to automate quality and adapter trimming</p> <p>Trimmomatic:</p> <p>Trimmomatic performs a variety of useful trimming tasks for illumina</p> <p>Trinity:</p> <p>Trinity represents a novel method for the efficient and robust de novo reconstruction</p> <p>Trinotate:</p> <p>C++ library of efficient algorithms and data structures for the</p> <p>Trycycler:</p> <p>Tool for generating consensus long-read assemblies for bacterial genomes.</p> <p>TuiView:</p> <p>TuiView is a lightweight raster GIS with powerful raster attribute table manipulation</p> <p>TurboVNC:</p> <p>TurboVNC is a derivative of VNC (Virtual Network Computing) that is tuned to provide</p> <p>UCX:</p> <p>Unified Communication X</p> <p>UDUNITS:</p> <p>UDUNITS supports conversion of unit specifications between formatted and binary forms,  arithmetic manipulation of units, and conversion of values between compatible scales of measurement.</p> <p>USEARCH:</p> <p>USEARCH is a unique sequence analysis tool which offers search and clustering algorithms that are</p> <p>Unicycler:</p> <p>Assembly pipeline for bacterial genomes. It can assemble Illumina-only read sets</p> <p>VASP:</p> <p>The Vienna Ab initio Simulation Package (VASP) is a computer program for atomic scale materials modelling, e.g. electronic structure calculations and quantum-mechanical molecular dynamics, from first principles.</p> <p>VCF-kit:</p> <p>VCF-kit is a command-line based collection of utilities for performing analysis on</p> <p>VCFtools:</p> <p>The aim of VCFtools is to provide</p> <p>VEP:</p> <p>Variant Effect Predictor (VEP) determines the effect of your</p> <p>VESTA:</p> <p>VESTA is a 3D visualization program for structural models, volumetric data such as electron/nuclear densities, and crystal morphologies. Run 'VESTA-gui' to launch.</p> <p>VIBRANT:</p> <p>Virus Identification By iteRative ANnoTation</p> <p>VMD:</p> <p>VMD is a molecular visualization program for displaying, animating, and analyzing large biomolecular</p> <p>VPN:</p> <p>Method of extending access to a private network.</p> <p>VSEARCH:</p> <p>An open source alternative to the metagenomics tool USEARCH.</p> <p>VTK:</p> <p>The Visualization Toolkit (VTK) is an open-source, freely available software system for  3D computer graphics, image processing and visualization. VTK consists of a C++ class library and several  interpreted interface layers including Tcl/Tk, Java, and Python. VTK supports a wide variety of visualization  algorithms including: scalar, vector, tensor, texture, and volumetric methods; and advanced modeling techniques  such as: implicit modeling, polygon reduction, mesh smoothing, cutting, contouring, and Delaunay triangulation.</p> <p>VTune:</p> <p>Intel VTune Amplifier XE is the premier performance profiler for C, C++, C#, Fortran,  Assembly and Java.</p> <p>Valgrind:</p> <p>Valgrind: Debugging and profiling tools</p> <p>VarScan:</p> <p>Variant calling and somatic mutation/CNV detection for next-generation sequencing data</p> <p>Velvet:</p> <p>Sequence assembler for very short reads</p> <p>VelvetOptimiser:</p> <p>Perl script for optimising the three primary parameter options of the Velvet de novo sequence assembler.</p> <p>ViennaRNA:</p> <p>The Vienna RNA Package consists of a C code library and several</p> <p>Vim:</p> <p>Vim is an advanced text editor that seeks to provide the power   of the de-facto Unix editor 'Vi', with a more complete feature set.</p> <p>VirHostMatcher:</p> <p>Tools for computing various oligonucleotide frequency (ONF) based distance/dissimialrity measures.</p> <p>VirSorter:</p> <p>VirSorter: mining viral signal from microbial genomic data.</p> <p>VirtualGL:</p> <p>VirtualGL is an open source toolkit that gives any Linux or</p> <p>VisIt:</p> <p>VisIt is an Open Source, interactive, scalable, visualization, animation and analysis tool.  This version supports interactive CPU-only rendering with the VisIt GUI using the Mesa library. It does not support GPU rendering.  Use the GALLIUM_DRIVER environment variable to choose a software renderer, it is recommended to use  GALLIUM_DRIVER=swr  for best performance.</p> <p>WAAFLE:</p> <p>Workflow to Annotate Assemblies and Find LGT Events.</p> <p>WhatsHap:</p> <p>Tool for phasing genomic variants using DNA sequencing reads, also called read-based phasing or haplotype assembly.</p> <p>Wise2:</p> <p>Aligning proteins or protein HMMs to DNA</p> <p>XCONV:</p> <p>Xconv is a program designed to convert model output into a format suitable for use in various plotting packages. Xconv is designed to be simple to use with a point and click, windows based interface.</p> <p>XGKS:</p> <p>XGKS is a level 2C implementation of the ANSI Graphical Kernel System</p> <p>XHMM:</p> <p>Calls copy number variation (CNV) from normalized read-depth data from exome capture or other targeted sequencing experiments.</p> <p>XIOS:</p> <p>XIOS stands for XML-IO-Server and is a library dedicated to I/O management in climate codes. This version uses netCDF4 with the parallel HDF5 library, and it contains a patch for various problems in the mesh connectivity algorithm</p> <p>XMDS2:</p> <p>Fast integrator of stochastic partial differential equations.</p> <p>XSD:</p> <p>CodeSynthesis XSD is an open-source, cross-platform W3C XML Schema to C++ data binding compiler.</p> <p>XZ:</p> <p>xz: XZ utilities</p> <p>Xerces-C++:</p> <p>Xerces-C++ is a validating XML parser written in a portable</p> <p>YAXT:</p> <p>Yet Another eXchange Tool - Library that performs halo exchange with MPI for domain decomposed simulations.</p> <p>Yade:</p> <p>Yade is an extensible open-source framework for discrete numerical models,</p> <p>Yasm:</p> <p>Yasm: Complete rewrite of the NASM assembler with BSD license</p> <p>ZeroMQ:</p> <p>ZeroMQ looks like an embeddable networking library but acts like a concurrency framework.</p> <p>Zip:</p> <p>Zip is a compression and file packaging/archive utility.</p> <p>Zonation:</p> <p>Spatial conservation prioritisation framework for large-scale conservation planning.</p> <p>angsd:</p> <p>Program for analysing NGS data.</p> <p>ant:</p> <p>Apache Ant is a Java library and command-line tool whose mission is to drive processes described in</p> <p>antiSMASH:</p> <p>antiSMASH allows the rapid genome-wide identification, annotation and analysis of secondary</p> <p>any2fasta:</p> <p>Convert various sequence formats to FASTA</p> <p>argtable:</p> <p>Argtable is an ANSI C library for parsing GNU style</p> <p>aria2:</p> <p>aria2 is a lightweight multi-protocol &amp; multi-source command-line download utility.</p> <p>arpack-ng:</p> <p>ARPACK is a collection of Fortran77 subroutines designed to solve large scale eigenvalue problems.</p> <p>at-spi2-atk:</p> <p>AT-SPI 2 toolkit bridge</p> <p>attr:</p> <p>Commands for Manipulating Filesystem Extended Attributes</p> <p>azul-zulu:</p> <p>Java Development Kit (JDK), and a compliant implementation of the Java Standard Edition (SE) specification.</p> <p>bamUtil:</p> <p>Repository that contains several programs</p> <p>barrnap:</p> <p>Barrnap predicts the location of ribosomal RNA genes in genomes.</p> <p>bcl2fastq2:</p> <p>bcl2fastq Conversion Software both demultiplexes data and converts BCL files generated by</p> <p>binutils:</p> <p>binutils: GNU binary utilities</p> <p>bioawk:</p> <p>An extension to awk, adding the support of several common biological data formats</p> <p>blasr_libcpp:</p> <p>Blasr_libcpp is a library used by blasr and other executables such as samtoh5, loadPulses for</p> <p>breseq:</p> <p>breseq is a computational pipeline for the analysis of short-read re-sequencing data</p> <p>bsddb3:</p> <p>bsddb3 is a nearly complete Python binding of the</p> <p>bzip2:</p> <p>bzip2 is a freely available, patent free, high-quality data compressor. It typically</p> <p>c-ares:</p> <p>c-ares is a C library for asynchronous DNS requests (including name resolves)</p> <p>cairo:</p> <p>Cairo is a 2D graphics library with support for multiple output devices.</p> <p>cdbfasta:</p> <p>Fasta file indexing and retrival tool</p> <p>chewBBACA:</p> <p>A complete suite for gene-by-gene schema creation and strain identification..</p> <p>chopper:</p> <p>Rust implementation of NanoFilt+NanoLyse</p> <p>cromwell:</p> <p>Workflow Management System geared towards scientific workflows.</p> <p>ctags:</p> <p>Ctags generates an index (or tag) file of language objects found in source files that allows these</p> <p>ctffind:</p> <p>ctffind is a program for finding CTFs of electron micrographs</p> <p>cuDNN:</p> <p>The NVIDIA CUDA Deep Neural Network library (cuDNN) is a GPU-accelerated library of primitives for     deep neural networks.</p> <p>cutadapt:</p> <p>cutadapt removes adapter sequences</p> <p>cuteSV:</p> <p>Fast and scalable long-read-based SV detection</p> <p>cwltool:</p> <p>Common Workflow Language tool description reference implementation</p> <p>cyvcf2:</p> <p>cython + htslib == fast VCF and BCF processing</p> <p>dammit:</p> <p>de novo transcriptome annotator..</p> <p>deepTools:</p> <p>deepTools is a suite of python tools particularly developed for the efficient analysis of</p> <p>devtools:</p> <p>R functions that simplify and expedite common tasks in package development.</p> <p>double-conversion:</p> <p>Efficient binary-decimal and decimal-binary conversion routines for IEEE doubles.</p> <p>drep:</p> <p>Rapid and accurate comparison and de-replication of microbial genomes</p> <p>dtcmp:</p> <p>DTCMP Library provides pre-defined and user-defined</p> <p>duphold:</p> <p>uphold your DUP and DEL calls</p> <p>duplex-tools:</p> <p>Range of tools to support operations on Duplex Sequencing read pairs.</p> <p>eDNA:</p> <p>A suite of tools to conduct metabarcoding analyses targeting any group of organisms. Includes utilities</p> <p>ecCodes:</p> <p>ecCodes is a package developed by ECMWF which provides an application programming interface and</p> <p>eccodes:</p> <p>ecCodes is a package developed by ECMWF which provides an application programming interface and a set of tools for decoding and encoding messages in the following formats:  - WMO FM-92 GRIB edition 1 and edition 2 - WMO FM-94 BUFR edition 3 and edition 4  - WMO GTS abbreviated header (only decoding)  A useful set of command line tools provide quick access to the messages.</p> <p>ectyper:</p> <p>Standalone versatile serotyping module for Escherichia coli..</p> <p>edlib:</p> <p>Lightweight, super fast library for sequence alignment using edit (Levenshtein) distance.</p> <p>eggnog-mapper:</p> <p>Tool for fast functional annotation of novel sequences (genes or proteins)</p> <p>ensmallen:</p> <p>C++ header-only library for numerical optimization</p> <p>entrez-direct:</p> <p>an advanced method for accessing the NCBI's set of interconnected databases</p> <p>exonerate:</p> <p>Generic tool for pairwise sequence comparison</p> <p>expat:</p> <p>Expat is an XML parser library written in C. It is a stream-oriented parser  in which an application registers handlers for things the parser might find  in the XML document (like start tags)</p> <p>fastStructure:</p> <p>fastStructure is an algorithm for inferring population structure from large SNP genotype data. It is based on a variational Bayesian framework for posterior inference and is written in Python2.x.</p> <p>fastp:</p> <p>A tool designed to provide fast all-in-one preprocessing for FastQ files.</p> <p>fastq-tools:</p> <p>A collection of small and efficient programs for performing some common and</p> <p>fcGENE:</p> <p>Format converting tool for genotype Data.</p> <p>fgbio:</p> <p>A set of tools to analyze genomic data with a focus on Next Generation Sequencing.</p> <p>fineRADstructure:</p> <p>A package for population structure inference from RAD-seq data</p> <p>flatbuffers:</p> <p>FlatBuffers: Memory Efficient Serialization Library</p> <p>flex:</p> <p>Flex (Fast Lexical Analyzer) is a tool for generating scanners. A scanner,  sometimes called a tokenizer, is a program which recognizes lexical patterns  in text.</p> <p>fmlrc:</p> <p>Tool for performing  hybrid correction of long read sequencing</p> <p>fmt:</p> <p>Formatting library providing a fast and safe alternative to C stdio and C++ iostreams.</p> <p>forge:</p> <p>Arm Forge combines Arm DDT, the leading debugger for time-saving high performance application debugging, and Arm MAP, the trusted performance profiler for invaluable optimization advice.</p> <p>foss:</p> <p>GNU Compiler Collection (GCC) based compiler toolchain, including</p> <p>funcx-endpoint:</p> <p>funcX is a distributed Function as a Service (FaaS) platform that enables flexible,</p> <p>fxtract:</p> <p>Extract sequences from a fastx (fasta or fastq) file given a subsequence.</p> <p>g2clib:</p> <p>Library contains GRIB2 encoder/decoder ('C' version).</p> <p>g2lib:</p> <p>Library contains GRIB2 encoder/decoder and search/indexing routines.</p> <p>ga4gh:</p> <p>A reference implementation of the GA4GH API</p> <p>geany:</p> <p>A GTK+ based text editor with with basic features of an integrated development environment.</p> <p>genometools:</p> <p>GenomeTools: A Comprehensive Software Library for Efficient Processing of</p> <p>gettext:</p> <p>GNU 'gettext' is an important step for the GNU Translation Project, as it is an asset on which we may build many other steps. This package offers to programmers, translators, and even users, a well integrated set of tools and documentation</p> <p>gffread:</p> <p>GFF/GTF parsing utility providing format conversions,</p> <p>giflib:</p> <p>giflib is a library for reading and writing gif images.</p> <p>gimkl:</p> <p>GNU Compiler Collection (GCC) based compiler toolchain with Intel MPI and MKL</p> <p>gimpi:</p> <p>GNU Compiler Collection (GCC) based compiler toolchain with Intel MPI.</p> <p>git:</p> <p>Git is a free and open source distributed version control system designed</p> <p>globus-automate-client:</p> <p>Client for the Globus Flows service.</p> <p>globus-compute-endpoint:</p> <p>Globus Compute is a distributed Function as a Service (FaaS) platform that enables flexible,</p> <p>gmsh:</p> <p>Gmsh is a 3D finite element grid generator with a build-in CAD engine and post-processor..</p> <p>gnuplot:</p> <p>Portable interactive, function plotting utility</p> <p>gompi:</p> <p>GNU Compiler Collection (GCC) based compiler toolchain,</p> <p>google-sparsehash:</p> <p>An extremely memory-efficient hash_map implementation. 2 bits/entry overhead! The SparseHash library</p> <p>googletest:</p> <p>Google's C++ test framework</p> <p>gperf:</p> <p>Pperfect hash function generator.</p> <p>grib_api:</p> <p>The ECMWF GRIB API is an application program interface accessible from C, FORTRAN and Python programs developed for encoding and decoding WMO FM-92 GRIB edition 1 and edition 2 messages. A useful set of command line tools is also provided to give quick access to GRIB messages.  Note that JPEG and Python support have been disabled for this build.</p> <p>grive2:</p> <p>Command line tool for Google Drive.</p> <p>gsort:</p> <p>Tool to sort genomic files according to a genomefile.</p> <p>h5pp:</p> <p>A simple C++17 wrapper for HDF5.</p> <p>help2man:</p> <p>help2man produces simple manual pages from the '--help' and '--version' output of other commands.</p> <p>hifiasm:</p> <p>Hifiasm: a haplotype-resolved assembler for accurate Hifi reads.</p> <p>hunspell:</p> <p>Spell checker and morphological analyzer library and program designed for languages</p> <p>hypothesis:</p> <p>Hypothesis is an advanced testing library for Python. It lets you write tests which are parametrized</p> <p>icc:</p> <p>Intel C and C++ compilers</p> <p>iccifort:</p> <p>Intel C, C++ &amp; Fortran compilers</p> <p>ifort:</p> <p>Intel Fortran compiler</p> <p>iimpi:</p> <p>Intel C/C++ and Fortran compilers, alongside Intel MPI.</p> <p>imkl:</p> <p>Intel Math Kernel Library is a library of highly optimized,</p> <p>imkl-FFTW:</p> <p>FFTW interfaces using Intel oneAPI Math Kernel Library</p> <p>impi:</p> <p>Intel MPI Library, compatible with MPICH ABI</p> <p>intel:</p> <p>Compiler toolchain including Intel compilers, Intel MPI and Intel Math Kernel Library (MKL).</p> <p>intel-compilers:</p> <p>Intel C, C++ &amp; Fortran compilers (classic and oneAPI)</p> <p>ipyrad:</p> <p>ipyrad is an interactive toolkit for assembly and analysis of restriction-site associated genomic</p> <p>ispc:</p> <p>Intel SPMD Program Compilers; An open-source compiler for high-performance  SIMD programming on the CPU. ispc is a compiler for a variant of the C programming language,  with extensions for 'single program, multiple data' (SPMD) programming.  Under the SPMD model, the programmer writes a program that generally appears  to be a regular serial program, though the execution model is actually that  a number of program instances execute in parallel on the hardware.</p> <p>jbigkit:</p> <p>JBIG-KIT is a software implementation of the JBIG1 data compression standard</p> <p>jcvi:</p> <p>Collection of Python libraries to parse bioinformatics files, or perform computation related to assembly, annotation, and comparative genomics.</p> <p>jemalloc:</p> <p>A general purpose malloc(3) implementation that emphasizes fragmentation avoidance and</p> <p>jq:</p> <p>Lightweight and flexible command-line JSON processor.</p> <p>json-c:</p> <p>JSON-C implements a reference counting object model that allows you to easily construct JSON objects</p> <p>jvarkit:</p> <p>Java utilities for Bioinformatics</p> <p>kalign2:</p> <p>Kalign is a fast multiple sequence alignment program for biological sequences.</p> <p>kallisto:</p> <p>kallisto is a program for quantifying abundances of transcripts from RNA-Seq data, or more generally</p> <p>kineto:</p> <p>A CPU+GPU Profiling library that provides access to timeline traces and hardware performance counters</p> <p>kma:</p> <p>KMA is a mapping method designed to map raw reads directly against redundant databases,</p> <p>libFLAME:</p> <p>libFLAME is a portable library for dense matrix computations,</p> <p>libGLU:</p> <p>The OpenGL Utility Library (GLU) is a computer graphics library for OpenGL.</p> <p>libKML:</p> <p>Reference implementation of OGC KML 2.2</p> <p>libcircle:</p> <p>API for distributing embarrassingly parallel workloads using self-stabilization.</p> <p>libconfig:</p> <p>A Library for processing structured configuration files</p> <p>libdeflate:</p> <p>Heavily optimized library for DEFLATE/zlib/gzip compression and decompression.</p> <p>libdrm:</p> <p>Direct Rendering Manager runtime library.</p> <p>libdwarf:</p> <p>The DWARF Debugging Information Format is of interest to programmers working on compilers</p> <p>libepoxy:</p> <p>Library for handling OpenGL function pointer management</p> <p>libevent:</p> <p>The libevent API provides a mechanism to execute a callback function when a specific  event occurs on a file descriptor or after a timeout has been reached.  Furthermore, libevent also support callbacks due to signals or regular timeouts.</p> <p>libffi:</p> <p>The libffi library provides a portable, high level programming interface to various calling</p> <p>libgcrypt:</p> <p>Libgpg-error is a small library that defines common error values for all GnuPG components.</p> <p>libgd:</p> <p>GD is an open source code library for the dynamic creation of images by programmers.</p> <p>libgeotiff:</p> <p>Library for reading and writing coordinate system information from/to GeoTIFF files</p> <p>libglvnd:</p> <p>libglvnd is a vendor-neutral dispatch layer for arbitrating OpenGL API calls between multiple vendors.</p> <p>libgpg-error:</p> <p>Libgpg-error is a small library that defines common error values for all GnuPG components.</p> <p>libgpuarray:</p> <p>Arrays on GPU device memory, for Theano</p> <p>libgtextutils:</p> <p>ligtextutils is a dependency of fastx-toolkit and is provided via the same upstream</p> <p>libiconv:</p> <p>Libiconv converts from one character encoding to another through Unicode conversion</p> <p>libjpeg-turbo:</p> <p>libjpeg-turbo is a fork of the original IJG libjpeg which uses SIMD to accelerate baseline JPEG</p> <p>libmatheval:</p> <p>GNU libmatheval is a library (callable from C and Fortran) to parse</p> <p>libpciaccess:</p> <p>Generic PCI access library.</p> <p>libpng:</p> <p>libpng is the official PNG reference library</p> <p>libreadline:</p> <p>The GNU Readline library provides a set of functions for use by applications that</p> <p>libspatialite:</p> <p>SpatiaLite is an open source library intended to extend the SQLite core to support</p> <p>libtool:</p> <p>GNU libtool is a generic library support script. Libtool hides the complexity of using shared libraries</p> <p>libunistring:</p> <p>This library provides functions for manipulating Unicode strings and for manipulating C strings</p> <p>libunwind:</p> <p>Define a portable and efficient C programming API to determine the call-chain of a program.</p> <p>libvpx:</p> <p>The WebM Project is dedicated to developing a high-quality, open video format for the web that's freely available to everyone.</p> <p>libxc:</p> <p>Libxc is a library of exchange-correlation functionals for density-functional theory.  The aim is to provide a portable, well tested and reliable set of exchange and correlation functionals.</p> <p>libxslt:</p> <p>Libxslt is the XSLT C library developed for the GNOME project</p> <p>libxsmm:</p> <p>LIBXSMM is a library for small dense and small sparse matrix-matrix multiplications</p> <p>libzstd:</p> <p>Fast lossless compression algorithm.</p> <p>lighttpd:</p> <p>A web server.</p> <p>likwid:</p> <p>Command line tools for Linux to support programmers in developing high</p> <p>lp_solve:</p> <p>Mixed Integer Linear Programming (MILP) solver</p> <p>lwgrp:</p> <p>The light-weight group library defines data structures and collective operations to</p> <p>lz4:</p> <p>LZ4 is lossless compression algorithm, providing compression speed at 400 MB/s per core.  It features an extremely fast decoder, with speed in multiple GB/s per core.</p> <p>magma:</p> <p>The MAGMA project aims to develop a dense linear algebra library similar to</p> <p>makedepend:</p> <p>The makedepend package contains a C-preprocessor like utility to determine build-time dependencies.</p> <p>manta:</p> <p>Manta calls structural variants (SVs) and indels from mapped paired-end sequencing reads.</p> <p>mapDamage:</p> <p>tracks and quantifies DNA damage patterns among ancient</p> <p>meRanTK:</p> <p>High performance toolkit for complete analysis of methylated RNA data.</p> <p>medaka:</p> <p>Medaka is a tool to create a consensus sequence from nanopore sequencing data.</p> <p>megalodon:</p> <p>Tool to extract high accuracy modified base and sequence variant calls from raw nanopore reads</p> <p>metaWRAP:</p> <p>Flexible pipeline for genome-resolved metagenomic data analysis.</p> <p>miRDeep2:</p> <p>Completely overhauled tool which discovers microRNA genes by analyzing sequenced RNAs</p> <p>midl:</p> <p>The Met Office library is a collection of routines written in IDL at the Met Office, originally written in PV-WAVE in the Hadley Centre in the early 1990s, with development continuing to the present day. Its main purpose is to support analysis and visualisation of PP data produced by the Unified Model (UM).</p> <p>mimalloc:</p> <p>mimalloc is a general purpose allocator with excellent performance characteristics.</p> <p>miniasm:</p> <p>Fast OLC-based de novo assembler for noisy long reads.</p> <p>minieigen:</p> <p>A small wrapper for core parts of Eigen, c++ library for linear algebra.</p> <p>minimap2:</p> <p>Minimap2 is a fast sequence mapping and alignment</p> <p>mjpegtools:</p> <p>The mjpeg programs are a set of tools that can do recording of videos and playback, simple cut-and-paste editing and the MPEG compression of audio and video under Linux.</p> <p>mlpack:</p> <p>Fast, and flexible C++ machine learning library with bindings to other languages</p> <p>mo_tidl:</p> <p>The Met Office library is a collection of routines written in IDL at the Met Office, originally written in PV-WAVE in the Hadley Centre in the early 1990s, with development continuing to the present day. Its main purpose is to support analysis and visualisation of PP data produced by the Unified Model (UM).</p> <p>modbam2bed:</p> <p>A program to aggregate modified base counts stored in a modified-base BAM file to a bedMethyl file.</p> <p>mpi4py:</p> <p>This package provides Python bindings for the Message Passing Interface (MPI) standard. It is implemented on top of the MPI-1/2/3 specification and exposes an API which grounds on the standard MPI-2 C++ bindings.</p> <p>mpifileutils:</p> <p>MPI-Based File Utilities For Distributed Systems</p> <p>muParser:</p> <p>muParser is an extensible high performance math expression</p> <p>nano:</p> <p>nano is a text editor.</p> <p>nanoQC:</p> <p>Create fastQC-like plots for Oxford Nanopore sequencing data.</p> <p>nanofilt:</p> <p>Filtering and trimming of long read sequencing data.</p> <p>nanoget:</p> <p>Functions to extract information from Oxford Nanopore sequencing data and alignments</p> <p>nanomath:</p> <p>A few simple math function for other Oxford Nanopore processing scripts</p> <p>nanopolish:</p> <p>Software package for signal-level analysis of Oxford Nanopore sequencing data.</p> <p>ncbi-vdb:</p> <p>The SRA Toolkit and SDK from NCBI is a collection of tools and libraries for</p> <p>nccmp:</p> <p>nccmp compares two NetCDF files bitwise, semantically or with a user defined tolerance (absolute or relative percentage). Parallel comparisons are done in local memory without requiring temporary files. Highly recommended for regression testing scientific models or datasets in a test-driven development environment.</p> <p>ncview:</p> <p>Visual browser for netCDF format files.</p> <p>ne:</p> <p>ne is a free (GPL'd) text editor based on the POSIX standard</p> <p>nearline:</p> <p>NeSI nearline client provides 'End User' access to NeSI's archive filesystem Nearline which is connected to a Tape library.</p> <p>netCDF:</p> <p>NetCDF (network Common Data Form) is a set of software libraries</p> <p>netCDF-C++:</p> <p>NetCDF (network Common Data Form) is a set of software libraries</p> <p>netCDF-C++4:</p> <p>NetCDF (network Common Data Form) is a set of software libraries   and machine-independent data formats that support the creation, access, and sharing of array-oriented   scientific data.</p> <p>netCDF-Fortran:</p> <p>NetCDF (network Common Data Form) is a set of software libraries   and machine-independent data formats that support the creation, access, and sharing of array-oriented   scientific data.</p> <p>nettle:</p> <p>Nettle is a cryptographic library that is designed to fit easily</p> <p>networkx:</p> <p>NetworkX is a Python package for the creation, manipulation, and study of the structure, dynamics,</p> <p>nodejs:</p> <p>Node.js is a platform built on Chrome's JavaScript runtime  for easily building fast, scalable network applications. Node.js uses an  event-driven, non-blocking I/O model that makes it lightweight and efficient,  perfect for data-intensive real-time applications that run across distributed devices.</p> <p>nseg:</p> <p>Used to mask nucleic acid sequences</p> <p>nsync:</p> <p>nsync is a C library that exports various synchronization primitives, such as mutexes</p> <p>nullarbor:</p> <p>Reads to report pipeline for bacterial isolate NGS data.</p> <p>ont-guppy-gpu:</p> <p>Data processing toolkit that contains the Oxford Nanopore Technologies' basecalling algorithms,</p> <p>padloc:</p> <p>Prokaryotic Antiviral Defence LOCator</p> <p>pairtools:</p> <p>CLI tools to process mapped Hi-C data</p> <p>panaroo:</p> <p>A pangenome analysis pipeline.</p> <p>pandoc:</p> <p>Almost universal document converter</p> <p>parallel-fastq-dump:</p> <p>parallel fastq-dump wrapper</p> <p>parasail:</p> <p>parasail is a SIMD C (C99) library containing implementations</p> <p>patchelf:</p> <p>PatchELF is a small utility to modify the dynamic linker and RPATH of ELF executables.</p> <p>pauvre:</p> <p>Tools for plotting Oxford Nanopore and other long-read data.</p> <p>pfunit:</p> <p>pFUnit is a unit testing framework enabling JUnit-like testing of serial  and MPI-parallel software written in Fortran.</p> <p>phyx:</p> <p>phyx performs phylogenetics analyses on trees and sequences.</p> <p>picard:</p> <p>A set of tools (in Java) for working with next generation sequencing data in the BAM format.</p> <p>pixman:</p> <p>Pixman is a low-level software library for pixel manipulation, providing features such as image</p> <p>pplacer:</p> <p>Places query sequences on a fixed reference phylogenetic tree</p> <p>preseq:</p> <p>Software for predicting library complexity and genome coverage in high-throughput sequencing.</p> <p>prodigal:</p> <p>Prodigal (Prokaryotic Dynamic Programming Genefinding Algorithm)</p> <p>prodigal-gv:</p> <p>A fork of Prodigal meant to improve gene calling</p> <p>prokka:</p> <p>Prokka is a software tool for the rapid annotation of prokaryotic genomes.</p> <p>proovread:</p> <p>PacBio hybrid error correction through iterative short read consensus</p> <p>protobuf:</p> <p>Google Protocol Buffers</p> <p>protobuf-python:</p> <p>Python Protocol Buffers runtime library.</p> <p>psmc:</p> <p>Infers population size history from a diploid sequence using the PSMC model.</p> <p>pstoedit:</p> <p>pstoedit translates PostScript and PDF graphics into other vector formats.</p> <p>pullseq:</p> <p>Utility program for extracting sequences from a fasta/fastq file</p> <p>purge_dups:</p> <p>purge haplotigs and overlaps in an assembly based on read depth</p> <p>purge_haplotigs:</p> <p>Pipeline to help with curating heterozygous diploid genome assemblies</p> <p>pv:</p> <p>Monitors the progress of data through a unix pipeline.</p> <p>pyani:</p> <p>Whole-genome classification using Average Nucleotide Identity</p> <p>pycoQC:</p> <p>Computes metrics and generates interactive QC plots for Oxford Nanopore technologies sequencing data.</p> <p>pymol-open-source:</p> <p>PyMOL (open source version) molecular visualization system.</p> <p>pyspoa:</p> <p>Python bindings to spoa.</p> <p>qcat:</p> <p>Command-line tool for demultiplexing Oxford Nanopore reads from FASTQ files</p> <p>rDock:</p> <p>rDock is a fast and versatile Open Source docking program that</p> <p>randfold:</p> <p>Minimum free energy of folding randomization test software</p> <p>rasusa:</p> <p>Randomly subsample sequencing reads to a specified coverage.</p> <p>razers3:</p> <p>Tool for mapping millions of short genomic reads onto a reference genome.</p> <p>re2c:</p> <p>re2c is a free and open-source lexer generator for C and C++. Its main goal is generating</p> <p>rnaQUAST:</p> <p>Tool for evaluating RNA-Seq assemblies using reference genome and gene database</p> <p>rsync:</p> <p>rsync is an open source utility that provides fast incremental file transfer.  rsync is freely available under the GNU General Public License</p> <p>rust-fmlrc:</p> <p>FM-index Long Read Corrector (Rust implementation)</p> <p>samblaster:</p> <p>samblaster is a fast and flexible program for marking duplicates in read-id grouped paired-end SAM files.</p> <p>samclip:</p> <p>Filter SAM file for soft and hard clipped alignments.</p> <p>sbt:</p> <p>sbt is a build tool for Scala, Java, and more.</p> <p>seqmagick:</p> <p>Seqmagick is a utility built in the spirit of imagemagick to expose the</p> <p>seqtk:</p> <p>Seqtk is a fast and lightweight tool for processing sequences in the FASTA or FASTQ format.</p> <p>shumlib:</p> <p>Shumlib is the collective name for a set of libraries which are used by the UM; the UK Met Office's Unified Model, that may be of use to external tools or applications where identical functionality is desired. The hope of the project is to enable developers to quickly and easily access parts of the UM code that are commonly duplicated elsewhere, at the same time benefiting from any improvements or optimisations that might be made in support of the UM itself.</p> <p>sismonr:</p> <p>Simulation of In Silico Multi-Omic Networks R package.</p> <p>slow5tools:</p> <p>Toolkit for converting (FAST5 &lt;-&gt; SLOW5), compressing, viewing, indexing</p> <p>smafa:</p> <p>Smafa attempts to align or cluster pre-aligned biological sequences, handling sequences</p> <p>smoove:</p> <p>simplifies and speeds calling and genotyping SVs for short reads.</p> <p>snakemake:</p> <p>The Snakemake workflow management system is a tool to create reproducible and scalable data analyses.</p> <p>snaphu:</p> <p>SNAPHU is an implementation of the Statistical-cost, Network-flow Algorithm for Phase Unwrapping</p> <p>snappy:</p> <p>Snappy is a compression/decompression library. It does not aim for maximum compression, or compatibility with any other compression library; instead, it aims for very high speeds and reasonable compression.</p> <p>snp-sites:</p> <p>Finds SNP sites from a multi-FASTA alignment file.</p> <p>snpEff:</p> <p>SnpEff is a variant annotation and effect prediction tool.</p> <p>somalier:</p> <p>extract informative sites, evaluate relatedness, and</p> <p>spaln:</p> <p>Stand-alone program that maps and aligns a set of cDNA or protein sequences onto a whole genomic sequence in a single job.</p> <p>spdlog:</p> <p>Fast C++ logging library.</p> <p>spglib:</p> <p>Spglib is a C library for finding and handling crystal symmetries.</p> <p>splat:</p> <p>Splat deploys (builds and installs) the components of heterogenous systems such as cylc suites, described by splat config files, into target directory trees.</p> <p>spoa:</p> <p>c++ implementation of the partial order alignment (POA) algorithm</p> <p>sratoolkit:</p> <p>The SRA Toolkit, and the source-code SRA System Development</p> <p>srun-wrapper:</p> <p>Provides an mpirun script which merely wraps the srun command.</p> <p>sublime:</p> <p>Text editor with GUI</p> <p>supercomputer:</p> <p>Like a regular computer, but larger. Primarily used for heating data centers.</p> <p>supercomputing:</p> <p>Like a regular computer, but larger. Primarily used for heating data centers.</p> <p>swarm:</p> <p>A robust and fast clustering method for amplicon-based studies.</p> <p>swissknife:</p> <p>Perl module for reading and writing UniProtKB data in plain text format.</p> <p>tRNAscan-SE:</p> <p>Transfer RNA detection</p> <p>tabix:</p> <p>Generic indexer for TAB-delimited genome position files</p> <p>tabixpp:</p> <p>C++ wrapper to tabix indexer</p> <p>tbb:</p> <p>Intel(R) Threading Building Blocks (Intel(R) TBB) lets you easily write parallel C++ programs that</p> <p>tbl2asn:</p> <p>Command-line program that automates the creation of</p> <p>tmux:</p> <p>tmux is a terminal multiplexer. It lets you switch easily</p> <p>trf:</p> <p>Locates tandem repeats in DNA sequences.</p> <p>trimAl:</p> <p>Tool for automated alignment trimming in large-scale phylogenetic analyses</p> <p>um2netcdf:</p> <p>um2netcdf converts Unified Model output files in fieldsfile format into netCDF format.</p> <p>unimap:</p> <p>Fork of minimap2 optimized for assembly-to-reference</p> <p>unrar:</p> <p>RAR is a powerful archive manager.</p> <p>util-linux:</p> <p>Set of Linux utilities</p> <p>vcflib:</p> <p>Genetic variant detector designed to find polymorphisms smaller than the length of a short-read sequencing alignment.</p> <p>vg:</p> <p>variation graph data structures, interchange formats, alignment, genotyping, and variant calling methods</p> <p>wgsim:</p> <p>Wgsim is a small tool for simulating sequence reads from a reference genome.</p> <p>wheel:</p> <p>A built-package format for Python.</p> <p>wtdbg:</p> <p>de novo sequence assembler for long noisy reads produced by PacBio or Oxford Nanopore Technologies.</p> <p>wxWidgets:</p> <p>widget toolkit and tools library for creating graphical user interfaces for cross-platform applications.</p> <p>xbitmaps:</p> <p>provides bitmaps for x</p> <p>xkbcommon:</p> <p>keyboard keymap compiler and support library</p> <p>yacrd:</p> <p>Chimeric Read Detector for long reads</p> <p>yajl:</p> <p>Yet Another JSON Library. Why does the world need another C library for parsing JSON? Good question.</p> <p>yak:</p> <p>Yet another k-mer analyzer</p> <p>yaml-cpp:</p> <p>YAML parser and emitter in C++</p> <p>zlib:</p> <p>zlib is designed to be a free, general-purpose, legally unencumbered -- that is,</p> <p>zstd:</p> <p>Zstandard is a real-time compression algorithm, providing high compression ratios.  It offers a very wide range of compression/speed trade-off, while being backed by a very fast decoder.  It also offers a special mode for small data, called dictionary compression, and can create dictionaries  from any sample set.</p>"},{"location":"assets/glossary/dictionary/","title":"Dictionary","text":"<p>lockdown onboarding rollout deprioritised NeSI's NeSI NZ HTTPS CUDA HPCs HPC supercomputers supercomputer GPUs GPU MPI srun Nano M\u0101ui Mahuika ng\u0101 nga mihi kia ora eScience nearline nobackup precompiled namespaces namespace runtime sudo walltimes walltime libio inodes inode parallelisation Globus's Globus Jupyter's Jupyter Slurm's Slurm Glib Skylake Lmod's Lmod JupyterHub's JupyterHub GitHub's GitHub OpenMP's OpenMP OMP's OMP</p>"}]}